<!doctype html><html lang=en dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.134.3"><meta name=generator content="Relearn 7.2.1"><meta name=description content="When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.
In this section, weâ€™ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated."><meta name=author content><meta name=twitter:card content="summary"><meta name=twitter:title content="Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops"><meta name=twitter:description content="When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.
In this section, weâ€™ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated."><meta property="og:url" content="https://splunk.github.io/observability-workshop/v5.77/en/scenarios/optimize_monitoring/5-improve-alert-timeliness/index.html"><meta property="og:site_name" content="Splunk Observability Cloud Workshops"><meta property="og:title" content="Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops"><meta property="og:description" content="When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.
In this section, weâ€™ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops"><meta itemprop=description content="When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.
In this section, weâ€™ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated."><meta itemprop=dateModified content="2024-04-02T16:20:29-04:00"><meta itemprop=wordCount content="128"><title>Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops</title>
<link href=https://splunk.github.io/observability-workshop/v5.77/en/scenarios/optimize_monitoring/5-improve-alert-timeliness/index.html rel=canonical type=text/html title="Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops"><link href=/observability-workshop/v5.77/images/favicon.ico?1738252789 rel=icon type=image/x-icon sizes=any><link href=/observability-workshop/v5.77/css/fontawesome-all.min.css?1738252789 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/observability-workshop/v5.77/css/fontawesome-all.min.css?1738252789 rel=stylesheet></noscript><link href=/observability-workshop/v5.77/css/auto-complete.css?1738252789 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/observability-workshop/v5.77/css/auto-complete.css?1738252789 rel=stylesheet></noscript><link href=/observability-workshop/v5.77/css/perfect-scrollbar.min.css?1738252789 rel=stylesheet><link href=/observability-workshop/v5.77/css/theme.min.css?1738252789 rel=stylesheet><link href=/observability-workshop/v5.77/css/format-print.min.css?1738252789 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="../../../..",window.relearn.relBaseUri="../../../../../..",window.relearn.absBaseUri="https://splunk.github.io/observability-workshop/v5.77",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["auto","splunk-light","splunk-dark"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script><script src=https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js crossorigin=anonymous></script><script src=https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web-session-recorder.js crossorigin=anonymous></script><script>SplunkRum.init({realm:"us1",rumAccessToken:"dp3FKraOS_wVhe-l7eCOsA",applicationName:"observability-workshop",deploymentEnvironment:"splunk.github.io",version:"1.0"}),SplunkSessionRecorder.init({app:"observability-workshop",realm:"us1",rumAccessToken:"dp3FKraOS_wVhe-l7eCOsA"})</script></script><style>:root{--MAIN-WIDTH-MAX:130rem;--MENU-WIDTH-L:23rem}</style></head><body class="mobile-support print disableInlineCopyToClipboard" data-url=/observability-workshop/v5.77/en/scenarios/optimize_monitoring/5-improve-alert-timeliness/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div><div class="topbar-button topbar-button-toc" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title="Table of Contents (CTRL+ALT+t)"><i class="fa-fw fas fa-list-alt"></i></button><div class=topbar-content><div class=topbar-content-wrapper></div></div></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/v5.77/en/index.html><span itemprop=name>Splunk Observability Workshops</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/v5.77/en/scenarios/index.html><span itemprop=name>Scenarios</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/v5.77/en/scenarios/optimize_monitoring/index.html><span itemprop=name>Optimize Cloud Monitoring</span></a><meta itemprop=position content="3">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>5. Improve Timeliness of Alerts</span><meta itemprop=position content="4"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=/observability-workshop/v5.77/en/scenarios/optimize_monitoring/5-improve-alert-timeliness/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></div><div class="topbar-button topbar-button-prev" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/observability-workshop/v5.77/en/scenarios/optimize_monitoring/4-correlate-metrics-logs/2-create-log-based-chart/index.html title="Create Log-based Chart (ðŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a></div><div class="topbar-button topbar-button-next" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/observability-workshop/v5.77/en/scenarios/optimize_monitoring/5-improve-alert-timeliness/1-create-custom-detector/index.html title="Create Custom Detector (ðŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable scenarios" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=improve-timeliness-of-alerts>Improve Timeliness of Alerts</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>1 minutes</span>
</span>&nbsp;
<span class="badge cstyle blue badge-with-title"><span class=badge-title class=text-muted>Author
</span><span class=badge-content>Tim Hard</span></span><p>When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.</p><p>In this section, we&rsquo;ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated.</p><p><a href=#R-image-60f19a68b59ec8e2219a7991b83bab07 class=lightbox-link><img alt="Detector Dashboard" class="noborder lazy lightbox noshadow figure-image" loading=lazy src=../images/detector-dashboard.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-60f19a68b59ec8e2219a7991b83bab07><img alt="Detector Dashboard" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src=../images/detector-dashboard.png></a></p><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Apr 2, 2024</span></span></footer></article><section><h1 class=a11y-only>Subsections of 5. Improve Timeliness of Alerts</h1><article class=default><header class=headline></header><h1 id=create-custom-detector>Create Custom Detector</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>10 minutes</span>
</span>&nbsp;
<span class="badge cstyle blue badge-with-title"><span class=badge-title class=text-muted>Author
</span><span class=badge-content>Tim Hard</span></span><p>Splunk Observability Cloud provides detectors, events, alerts, and notifications to keep you informed when certain criteria are met. There are a number of pre-built <strong>AutoDetect Detectors</strong> that automatically surface when common problem patterns occur, such as when an EC2 instanceâ€™s CPU utilization is expected to reach its limit. Additionally, you can also create custom detectors if you want something more optimized or specific. For example, you want a message sent to a Slack channel or to an email address for the Ops team that manages this Kubernetes cluster when Memory Utilization on their pods has reached 85%.</p><details open class="box cstyle notices green"><summary class=box-label><i class="fa-fw fas fa-running"></i>
Exercise: Create Custom Detector</summary><div class=box-content><p>In this section you&rsquo;ll create a detector on <strong>Pod Memory Utilization</strong> which will trigger if utilization surpasses 85%</p><ol><li><p>On the <strong>Kubernetes Pods Dashboard</strong> you cloned in section <a href=../../3-reuse-content-across-teams/2-clone-dashboards>3.2 Dashboard Cloning</a>, click the <strong>Get Alerts</strong> button (bell icon) for the <strong>Memory usage (%)</strong> chart -> Click <strong>New detector from chart</strong>.</p><p><a href=#R-image-8ce95e73bded0077eef9bb3c70dfdad7 class=lightbox-link><img alt="New Detector from Chart" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/new-detector.png?width=40vw" style=height:auto;width:40vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-8ce95e73bded0077eef9bb3c70dfdad7><img alt="New Detector from Chart" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/new-detector.png?width=40vw"></a></p></li><li><p>In the <strong>Create detector</strong> add your initials to the detector name.</p><p><a href=#R-image-5c32d15cd15022ef3c5004fe9ca4e256 class=lightbox-link><img alt="Create Detector: Update Detector Name" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/create-detector-name.png?width=40vw" style=height:auto;width:40vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-5c32d15cd15022ef3c5004fe9ca4e256><img alt="Create Detector: Update Detector Name" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/create-detector-name.png?width=40vw"></a></p></li><li><p>Click <strong>Create alert rule</strong>.</p><p>These conditions are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Importantly, multiple rules can be included in the same detector configuration which minimizes the total number of alerts that need to be created and maintained. You can see which signal this detector will alert on by the bell icon in the <strong>Alert On</strong> column. In this case, this detector will alert on the Memory Utilization for the pods running in this Kubernetes cluster.</p><p><a href=#R-image-984328530b68d90b6d55de914c0f3374 class=lightbox-link><img alt="Alert Signal" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-signals.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-984328530b68d90b6d55de914c0f3374><img alt="Alert Signal" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-signals.png?width=60vw"></a></p></li><li><p>Click <strong>Proceed To Alert Conditions</strong>.</p><p>Many pre-built alert conditions can be applied to the metric you want to alert on. This could be as simple as a static threshold or something more complex, for example, is memory usage deviating from the historical baseline across any of your 50,000 containers?</p><p><a href=#R-image-a03c08f20ec65ac088e6659aa7be72a4 class=lightbox-link><img alt="Alert Conditions" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-conditions.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-a03c08f20ec65ac088e6659aa7be72a4><img alt="Alert Conditions" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-conditions.png?width=60vw"></a></p></li><li><p>Select <strong>Static Threshold</strong>.</p></li><li><p>Click <strong>Proceed To Alert Settings</strong>.</p><p>In this case, you want the alert to trigger if any pods exceed 85% memory utilization. Once youâ€™ve set the alert condition, the configuration is back-tested against the historical data so you can confirm that the alert configuration is accurate, meaning will the alert trigger on the criteria youâ€™ve defined? This is also a great way to confirm if the alert generates too much noise.</p><p><a href=#R-image-b0114d153afba375070872d3d5737382 class=lightbox-link><img alt="Alert Settings" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-settings.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-b0114d153afba375070872d3d5737382><img alt="Alert Settings" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-settings.png?width=60vw"></a></p></li><li><p>Enter 85 in the <strong>Threshold</strong> field.</p></li><li><p>Click <strong>Proceed To Alert Message</strong>.</p><p>Next, you can set the severity for this alert, you can include links to runbooks and short tips on how to respond, and you can customize the message that is included in the alert details. The message can include parameterized fields from the actual data, for example, in this case, you may want to include which Kubernetes node the pod is running on, or the <code>store.location</code> configured when you deployed the application, to provide additional context.</p><p><a href=#R-image-2777349f7d254a8e955e143176a5a46b class=lightbox-link><img alt="Alert Message" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-message.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-2777349f7d254a8e955e143176a5a46b><img alt="Alert Message" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-message.png?width=60vw"></a></p></li><li><p>Click <strong>Proceed To Alert Recipients</strong>.</p><p>You can choose where you want this alert to be sent when it triggers. This could be to a team, specific email addresses, or to other systems such as ServiceNow, Slack, Splunk On-Call or Splunk ITSI. You can also have the alert execute a webhook which enables me to leverage automation or to integrate with many other systems such as homegrown ticketing tools. <strong>For the purpose of this workshop do not include a recipient</strong></p><p><a href=#R-image-e1acdf8443253007d2866e903a2a4415 class=lightbox-link><img alt="Alert Recipients" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-recipients.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-e1acdf8443253007d2866e903a2a4415><img alt="Alert Recipients" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-recipients.png?width=60vw"></a></p></li><li><p>Click <strong>Proceed To Alert Activation</strong>.</p><p><a href=#R-image-4b59cc2c135c8936b8492365d798943f class=lightbox-link><img alt="Activate Alert" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-activate-alert.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-4b59cc2c135c8936b8492365d798943f><img alt="Activate Alert" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-activate-alert.png?width=60vw"></a></p></li><li><p>Click <strong>Activate Alert</strong>.</p><p><a href=#R-image-c44a2c80ed1628e29d46b6d2973df4b6 class=lightbox-link><img alt="Activate Alert Message" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-activation.png?width=40vw" style=height:auto;width:40vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-c44a2c80ed1628e29d46b6d2973df4b6><img alt="Activate Alert Message" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-activation.png?width=40vw"></a></p><p>You will receive a warning because no recipients were included in the Notification Policy for this detector. This can be warning can be dismissed.</p></li><li><p>Click <strong>Save</strong>.</p><p><a href=#R-image-6111934eca0137eeee94636e41614973 class=lightbox-link><img alt="Activate Alert Message" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-new-detector.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-6111934eca0137eeee94636e41614973><img alt="Activate Alert Message" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-new-detector.png?width=60vw"></a></p><p>You will be taken to your newly created detector where you can see any triggered alerts.</p></li><li><p>In the upper right corner, Click <strong>Close</strong> to close the Detector.</p></li></ol><p>The detector status and any triggered alerts will automatically be included in the chart because this detector was configured for this chart.</p><p><a href=#R-image-063498146f3fa4cec1cbf2f4115533c5 class=lightbox-link><img alt="Alert Chart" class="noborder lazy lightbox noshadow figure-image" loading=lazy src="../../images/alert-chart.png?width=40vw" style=height:auto;width:40vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-063498146f3fa4cec1cbf2f4115533c5><img alt="Alert Chart" class="noborder lazy lightbox noshadow lightbox-image" loading=lazy src="../../images/alert-chart.png?width=40vw"></a></p><p><strong>Congratulations! You&rsquo;ve successfully created a detector that will trigger if pod memory utilization exceeds 85%. After a few minutes, the detector should trigger some alerts. You can click the detector name in the chart to view the triggered alerts.</strong></p></div></details><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Apr 4, 2024</span></span></footer></article></section></div></main></div><script src=/observability-workshop/v5.77/js/clipboard.min.js?1738252789 defer></script><script src=/observability-workshop/v5.77/js/perfect-scrollbar.min.js?1738252789 defer></script><script src=/observability-workshop/v5.77/js/theme.js?1738252789 defer></script></body></html>