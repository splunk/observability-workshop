var relearn_searchindex = [
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Self-Service Observability",
    "content": "Background Let’s review a few background concepts on Open Telemetry before jumping into the details.\nFirst we have the Open Telemetry Collector, which lives on hosts or kubernetes nodes. These collectors can collect local information (like cpu, disk, memory, etc.). They can also collect metrics from other sources like prometheus (push or pull) or databases and other middleware.\nSource: OTel Documentation\nThe way the OTel Collector collects and sends data is using pipelines. Pipelines are made up of:\nReceivers: Collect telemetry from one or more sources; they are pull- or push-based. Processors: Take data from receivers and modify or transform them. Unlike receivers and exporters, processors process data in a specific order. Exporters: Send data to one or more observability backends or other destinations. Source: OTel Documentation\nThe final piece is applications which are instrumented; they will send traces (spans), metrics, and logs.\nBy default the instrumentation is designed to send data to the local collector (on the host or kubernetes node). This is desirable because we can then add metadata on it – like which pod or which node/host the application is running on.",
    "description": "Background Let’s review a few background concepts on Open Telemetry before jumping into the details.\nFirst we have the Open Telemetry Collector, which lives on hosts or kubernetes nodes. These collectors can collect local information (like cpu, disk, memory, etc.). They can also collect metrics from other sources like prometheus (push or pull) or databases and other middleware.\nSource: OTel Documentation",
    "tags": [],
    "title": "Background",
    "uri": "/observability-workshop/v6.5/en/scenarios/self-service-observability/1-background/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Profiling Workshop",
    "content": "Introduction For this workshop, we’ll be using a Java-based application called The Door Game. It will be hosted in Kubernetes.\nPre-requisites You will start with an EC2 instance and perform some initial steps in order to get to the following state:\nDeploy the Splunk distribution of the OpenTelemetry Collector Deploy the MySQL database container and populate data Build and deploy the doorgame application container Initial Steps The initial setup can be completed by executing the following steps on the command line of your EC2 instance.\nYou’ll be asked to enter a name for your environment. Please use profiling-workshop-yourname (where yourname is replaced by your actual name).\ncd workshop/profiling ./1-deploy-otel-collector.sh ./2-deploy-mysql.sh ./3-deploy-doorgame.sh Let’s Play The Door Game Now that the application is deployed, let’s play with it and generate some observability data.\nGet the external IP address for your application instance using the following command:\nkubectl describe svc doorgame | grep \"LoadBalancer Ingress\" The output should look like the following:\nLoadBalancer Ingress: 52.23.184.60 You should be able to access The Door Game application by pointing your browser to port 81 of the provided IP address. For example:\nhttp://52.23.184.60:81 You should be met with The Door Game intro screen:\nClick Let's Play to start the game:\nDid you notice that it took a long time after clicking Let's Play before we could actually start playing the game?\nLet’s use Splunk Observability Cloud to determine why the application startup is so slow.",
    "description": "Introduction For this workshop, we’ll be using a Java-based application called The Door Game. It will be hosted in Kubernetes.\nPre-requisites You will start with an EC2 instance and perform some initial steps in order to get to the following state:\nDeploy the Splunk distribution of the OpenTelemetry Collector Deploy the MySQL database container and populate data Build and deploy the doorgame application container Initial Steps The initial setup can be completed by executing the following steps on the command line of your EC2 instance.",
    "tags": [],
    "title": "Build the Sample Application",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/profiling/1-build-application/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop \u003e 7. Use Tags for Monitoring",
    "content": "Dashboards Navigate to Metric Finder, then type in the name of the tag, which is credit_score_category (remember that the dots in the tag name were replaced by underscores when the Monitoring MetricSet was created). You’ll see that multiple metrics include this tag as a dimension:\nBy default, Splunk Observability Cloud calculates several metrics using the trace data it receives. See Learn about MetricSets in APM for more details.\nBy creating an MMS, credit_score_category was added as a dimension to these metrics, which means that this dimension can now be used for alerting and dashboards.\nTo see how, let’s click on the metric named service.request.duration.ns.p99, which brings up the following chart:\nAdd filters for sf_environment, sf_service, and sf_dimensionalized. Then set the Extrapolation policy to Last value and the Display units to Nanosecond:\nWith these settings, the chart allows us to visualize the service request duration by credit score category:\nNow we can see the duration by credit score category. In my example, the red line represents the exceptional category, and we can see that the duration for these requests sometimes goes all the way up to 5 seconds.\nThe orange represents the very good category, and has very fast response times.\nThe green line represents the poor category, and has response times between 2-3 seconds.\nIt may be useful to save this chart on a dashboard for future reference. To do this, click on the Save as… button and provide a name for the chart:\nWhen asked which dashboard to save the chart to, let’s create a new one named Credit Check Service - Your Name (substituting your actual name):\nNow we can see the chart on our dashboard, and can add more charts as needed to monitor our credit check service:",
    "description": "Dashboards Navigate to Metric Finder, then type in the name of the tag, which is credit_score_category (remember that the dots in the tag name were replaced by underscores when the Monitoring MetricSet was created). You’ll see that multiple metrics include this tag as a dimension:\nBy default, Splunk Observability Cloud calculates several metrics using the trace data it receives. See Learn about MetricSets in APM for more details.",
    "tags": [],
    "title": "Use Tags with Dashboards",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/7-alerting-dashboards-slos/1-dashboards/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM",
    "content": "How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty1 or your web browser. Verify your connection to your AWS/EC2 cloud instance. Using Putty (Optional) Using Multipass (Optional) 1. AWS/EC2 IP Address In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2.\nTo get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader.\nSearch for your AWS/EC2 instance by looking for your first and last name, as provided during registration for this workshop.\nFind your allocated IP address, SSH command (for Mac OS, Linux and the latest Windows versions) and password to enable you to connect to your workshop instance.\nIt also has the Browser Access URL that you can use in case you cannot connect via ssh or Putty - see EC2 access via Web browser\nImportant Please use SSH or Putty to gain access to your EC2 instance if possible and make a note of the IP address as you will need this during the workshop.\n2. SSH (Mac OS/Linux) Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device, or on Windows 10 and above.\nTo use SSH, open a terminal on your system and type ssh splunk@x.x.x.x (replacing x.x.x.x with the IP address found in Step #1).\nWhen prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes.\nEnter the password provided in the Google Sheet from Step #1.\nUpon successful login, you will be presented with the Splunk logo and the Linux prompt.\n3. SSH (Windows 10 and above) The procedure described above is the same on Windows 10, and the commands can be executed either in the Windows Command Prompt or PowerShell. However, Windows regards its SSH Client as an “optional feature”, which might need to be enabled.\nYou can verify if SSH is enabled by simply executing ssh\nIf you are shown a help text on how to use the ssh-command (like shown on the screenshot below), you are all set.\nIf the result of executing the command looks something like the screenshot below, you want to enable the “OpenSSH Client” feature manually.\nTo do that, open the “Settings” menu, and click on “Apps”. While in the “Apps \u0026 features” section, click on “Optional features”.\nHere, you are presented with a list of installed features. On the top, you see a button with a plus icon to “Add a feature”. Click it. In the search input field, type “OpenSSH”, and find a feature called “OpenSSH Client”, or respectively, “OpenSSH Client (Beta)”, click on it, and click the “Install”-button.\nNow you are set! In case you are not able to access the provided instance despite enabling the OpenSSH feature, please do not shy away from reaching out to the course instructor, either via chat or directly.\nAt this point you are ready to continue and start the workshop\n4. Putty (For Windows Versions prior to Windows 10) If you do not have SSH pre-installed or if you are on a Windows system, the best option is to install putty, you can find here.\nImportant If you cannot install Putty, please go to Web Browser (All).\nOpen Putty and enter in the Host Name (or IP address) field the IP address provided in the Google Sheet.\nYou can optionally save your settings by providing a name and pressing Save.\nTo then login to your instance click on the Open button as shown above.\nIf this is the first time connecting to your AWS/EC2 workshop instance, you will be presented with a security dialogue, please click Yes.\nOnce connected, login in as splunk and the password is the one provided in the Google Sheet.\nOnce you are connected successfully you should see a screen similar to the one below:\nAt this point, you are ready to continue and start the workshop\n5. Web Browser (All) If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser.\nNote This assumes that access to port 6501 is not restricted by your company’s firewall.\nOpen your web browser and type http://x.x.x.x:6501 (where X.X.X.X is the IP address from the Google Sheet).\nOnce connected, login in as splunk and the password is the one provided in the Google Sheet.\nOnce you are connected successfully you should see a screen similar to the one below:\nUnlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions.\nWhen the workshop asks you to copy instructions into your terminal, please do the following:\nCopy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below:\nThis will open a dialogue box asking for the text to be pasted into the web terminal:\nPaste the text in the text box as shown, then press OK to complete the copy and paste process.\nNote Unlike regular SSH connection, the web browser has a 60-second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal.\nSimply click the Connect button and you will be reconnected and will be able to continue.\nAt this point you are ready to continue and start the workshop.\n6. Multipass (All) If you are unable to access AWS, but want to install software locally, follow the instructions for using Multipass.\nDownload Putty ↩︎",
    "description": "How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty1 or your web browser. Verify your connection to your AWS/EC2 cloud instance. Using Putty (Optional) Using Multipass (Optional) 1. AWS/EC2 IP Address In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2.\nTo get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader.",
    "tags": [],
    "title": "How to connect to your workshop environment",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/initial-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 6. Advanced Features",
    "content": "When we installed the Splunk Distribution of the OpenTelemetry Collector using the Helm chart earlier, we configured it to enable AlwaysOn Profiling and Metrics. This means that OpenTelemetry Java will automatically generate CPU and Memory profiling for the application, sending them to Splunk Observability Cloud.\nWhen you deploy the PetClinic application and set the annotation, the collector automatically detects the application and instruments it for traces and profiling. We can verify this by examining the startup logs of one of the Java containers we are instrumenting by running the following script:\nThe logs will show the flags that were picked up by the Java automatic discovery and configuration:\n​ Run the script Example output . ~/workshop/petclinic/scripts/get_logs.sh 2024/02/15 09:42:00 Problem with dial: dial tcp 10.43.104.25:8761: connect: connection refused. Sleeping 1s 2024/02/15 09:42:01 Problem with dial: dial tcp 10.43.104.25:8761: connect: connection refused. Sleeping 1s 2024/02/15 09:42:02 Connected to tcp://discovery-server:8761 Picked up JAVA_TOOL_OPTIONS: -javaagent:/otel-auto-instrumentation-java/javaagent.jar Picked up _JAVA_OPTIONS: -Dspring.profiles.active=docker,mysql -Dsplunk.profiler.call.stack.interval=150 OpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended [otel.javaagent 2024-02-15 09:42:03:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: splunk-1.30.1-otel-1.32.1 [otel.javaagent 2024-02-15 09:42:03:768 +0000] [main] INFO com.splunk.javaagent.shaded.io.micrometer.core.instrument.push.PushMeterRegistry - publishing metrics for SignalFxMeterRegistry every 30s [otel.javaagent 2024-02-15 09:42:07:478 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - ----------------------- [otel.javaagent 2024-02-15 09:42:07:478 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - Profiler configuration: [otel.javaagent 2024-02-15 09:42:07:480 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.enabled : true [otel.javaagent 2024-02-15 09:42:07:505 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.directory : /tmp [otel.javaagent 2024-02-15 09:42:07:505 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.recording.duration : 20s [otel.javaagent 2024-02-15 09:42:07:506 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.keep-files : false [otel.javaagent 2024-02-15 09:42:07:510 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.logs-endpoint : http://10.13.2.38:4317 [otel.javaagent 2024-02-15 09:42:07:513 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - otel.exporter.otlp.endpoint : http://10.13.2.38:4317 [otel.javaagent 2024-02-15 09:42:07:513 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.memory.enabled : true [otel.javaagent 2024-02-15 09:42:07:515 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.tlab.enabled : true [otel.javaagent 2024-02-15 09:42:07:516 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.memory.event.rate : 150/s [otel.javaagent 2024-02-15 09:42:07:516 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.call.stack.interval : PT0.15S [otel.javaagent 2024-02-15 09:42:07:517 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.include.internal.stacks : false [otel.javaagent 2024-02-15 09:42:07:517 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.tracing.stacks.only : false [otel.javaagent 2024-02-15 09:42:07:517 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - ----------------------- [otel.javaagent 2024-02-15 09:42:07:518 +0000] [main] INFO com.splunk.opentelemetry.profiler.JfrActivator - Profiler is active. We are interested in the section written by the com.splunk.opentelemetry.profiler.ConfigurationLogger or the Profiling Configuration.\nWe can see the various settings you can control, such as the splunk.profiler.directory, which is the location where the agent writes the call stacks before sending them to Splunk. (This may be different depending on how you configure your containers.)\nAnother parameter you may want to change is splunk.profiler.call.stack.interval. This is how often the system captures a CPU Stack trace. You may want to reduce this interval setting if you have short spans like the ones we have in our Pet Clinic application. For the demo application, we did not change the default interval value, so Spans may not always have a CPU Call Stack related to them.\nYou can find how to set these parameters here. In the example below, we see how to set a higher collection rate for call stacks in the deployment.yaml, by setting this value in the JAVA_OPTIONS config section.\nenv: - name: JAVA_OPTIONS value: \"-Xdebug -Dsplunk.profiler.call.stack.interval=150\"",
    "description": "When we installed the Splunk Distribution of the OpenTelemetry Collector using the Helm chart earlier, we configured it to enable AlwaysOn Profiling and Metrics. This means that OpenTelemetry Java will automatically generate CPU and Memory profiling for the application, sending them to Splunk Observability Cloud.\nWhen you deploy the PetClinic application and set the annotation, the collector automatically detects the application and instruments it for traces and profiling. We can verify this by examining the startup logs of one of the Java containers we are instrumenting by running the following script:",
    "tags": [],
    "title": "Always-On Profiling \u0026 Metrics",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/6-profiling-db-query/1-profiling/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 4. Splunk APM",
    "content": "When you click into the APM section of Splunk Observability Cloud you are greated with an overview of your APM data including top services by error rates, and R.E.D. metrics for services and workflows.\nThe APM Service Map displays the dependencies and connections among your instrumented and inferred services in APM. The map is dynamically generated based on your selections in the time range, environment, workflow, service, and tag filters.\nYou can see the services involved in any of your APM user workflows by clicking into the Service Map. When you select a service in the Service Map, the charts in the Business Workflow sidepane are updated to show metrics for the selected service. The Service Map and any indicators are syncronized with the time picker and chart data displayed.\nExercise Click on the wire-transfer-service in the Service Map. Splunk APM also provides built-in Service Centric Views to help you see problems occurring in real time and quickly determine whether the problem is associated with a service, a specific endpoint, or the underlying infrastructure. Let’s have a closer look.\nExercise In the right hand pane, click on wire-transfer-service in blue.",
    "description": "When you click into the APM section of Splunk Observability Cloud you are greated with an overview of your APM data including top services by error rates, and R.E.D. metrics for services and workflows.\nThe APM Service Map displays the dependencies and connections among your instrumented and inferred services in APM. The map is dynamically generated based on your selections in the time range, environment, workflow, service, and tag filters.",
    "tags": [],
    "title": "1. APM Explore",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/4-apm/1-apm-explore/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6. Splunk APM",
    "content": "The APM Service Map displays the dependencies and connections among your instrumented and inferred services in APM. The map is dynamically generated based on your selections in the time range, environment, workflow, service, and tag filters.\nWhen we clicked on the APM link in the RUM waterfall, filters were automatically added to the service map view to show the services that were involved in that WorkFlow Name (frontend:/cart/checkout).\nYou can see the services involved in the workflow in the Service Map. In the side pane, under Business Workflow, charts for the selected workflow are displayed. The Service Map and Business Workflow charts are synchronized. When you select a service in the Service Map, the charts in the Business Workflow pane are updated to show metrics for the selected service.\nExercise Click on the paymentservice in the Service Map. Splunk APM also provides built-in Service Centric Views to help you see problems occurring in real time and quickly determine whether the problem is associated with a service, a specific endpoint, or the underlying infrastructure. Let’s have a closer look.\nExercise In the right hand pane, click on paymentservice in blue.",
    "description": "The APM Service Map displays the dependencies and connections among your instrumented and inferred services in APM. The map is dynamically generated based on your selections in the time range, environment, workflow, service, and tag filters.\nWhen we clicked on the APM link in the RUM waterfall, filters were automatically added to the service map view to show the services that were involved in that WorkFlow Name (frontend:/cart/checkout).\nYou can see the services involved in the workflow in the Service Map. In the side pane, under Business Workflow, charts for the selected workflow are displayed. The Service Map and Business Workflow charts are synchronized. When you select a service in the Service Map, the charts in the Business Workflow pane are updated to show metrics for the selected service.",
    "tags": [],
    "title": "1. APM Explore",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/6-apm/1-apm-explore/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 5. APM Features",
    "content": "The above map shows all the interactions between all the services. The map may still be in an interim state as it will take the PetClinic Microservice application a few minutes to start up and fully synchronize. Reducing the time filter to a custom time of 2 minutes by entering -2m will help. You can click on the Refresh button (1) in the top right-hand corner of the screen. The initial startup-related errors indicated by red circles will eventually disappear.\nNext, let’s examine the metrics that are available for each service that is instrumented by visiting the request, error, and duration (RED) metrics Dashboard.\nFor this exercise, we are going to use a common scenario you would use if your service operation was showing high latency or errors.\nClick on customers-service in the Dependency map, then make sure the customers-service is selected in the Services dropdown box (1). Next, select GET /owners from the Operations dropdown (2) adjacent to the Service name.\nThis should give you the workflow with a filter on GET /owners as shown below:",
    "description": "The above map shows all the interactions between all the services. The map may still be in an interim state as it will take the PetClinic Microservice application a few minutes to start up and fully synchronize. Reducing the time filter to a custom time of 2 minutes by entering -2m will help. You can click on the Refresh button (1) in the top right-hand corner of the screen. The initial startup-related errors indicated by red circles will eventually disappear.",
    "tags": [],
    "title": "APM Service Map",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/5-traces/1-service-map/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop",
    "content": "Introduction For this workshop, we’ll be using a microservices-based application. This application is for an online retailer and normally includes more than a dozen services. However, to keep the workshop simple, we’ll be focusing on two services used by the retailer as part of their payment processing workflow: the credit check service and the credit processor service.\nPre-requisites You will start with an EC2 instance and perform some initial steps in order to get to the following state:\nDeploy the Splunk distribution of the OpenTelemetry Collector Build and deploy creditcheckservice and creditprocessorservice Deploy a load generator to send traffic to the services Initial Steps The initial setup can be completed by executing the following steps on the command line of your EC2 instance:\ncd workshop/tagging ./0-deploy-collector-with-services.sh Java There are implementations in multiple languages available for creditcheckservice. Run\n./0-deploy-collector-with-services.sh java to pick Java over Python.\nView your application in Splunk Observability Cloud Now that the setup is complete, let’s confirm that it’s sending data to Splunk Observability Cloud. Note that when the application is deployed for the first time, it may take a few minutes for the data to appear.\nNavigate to APM, then use the Environment dropdown to select your environment (i.e. tagging-workshop-instancename).\nIf everything was deployed correctly, you should see creditprocessorservice and creditcheckservice displayed in the list of services:\nClick on Explore on the right-hand side to view the service map. We can see that the creditcheckservice makes calls to the creditprocessorservice, with an average response time of at least 3 seconds:\nNext, click on Traces on the right-hand side to see the traces captured for this application. You’ll see that some traces run relatively fast (i.e. just a few milliseconds), whereas others take a few seconds.\nIf you toggle Errors only to on, you’ll also notice that some traces have errors:\nToggle Errors only back to off and sort the traces by duration, then click on one of the longer running traces. In this example, the trace took five seconds, and we can see that most of the time was spent calling the /runCreditCheck operation, which is part of the creditprocessorservice.\nCurrently, we don’t have enough details in our traces to understand why some requests finish in a few milliseconds, and others take several seconds. To provide the best possible customer experience, this will be critical for us to understand.\nWe also don’t have enough information to understand why some requests result in errors, and others don’t. For example, if we look at one of the error traces, we can see that the error occurs when the creditprocessorservice attempts to call another service named otherservice. But why do some requests results in a call to otherservice, and others don’t?\nWe’ll explore these questions and more in the workshop.",
    "description": "Introduction For this workshop, we’ll be using a microservices-based application. This application is for an online retailer and normally includes more than a dozen services. However, to keep the workshop simple, we’ll be focusing on two services used by the retailer as part of their payment processing workflow: the credit check service and the credit processor service.\nPre-requisites You will start with an EC2 instance and perform some initial steps in order to get to the following state:",
    "tags": [],
    "title": "Build the Sample Application",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/1-build-application/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop \u003e 3. Capture Tags with OpenTelemetry",
    "content": "Let’s add some tags to our traces, so we can find out why some customers receive a poor experience from our application.\nIdentify Useful Tags We’ll start by reviewing the code for the creditCheck function of creditcheckservice (which can be found in the file /home/splunk/workshop/tagging/creditcheckservice-java/src/main/java/com/example/creditcheckservice/CreditCheckController.java):\n@GetMapping(\"/check\") public ResponseEntity\u003cString\u003e creditCheck(@RequestParam(\"customernum\") String customerNum) { // Get Credit Score int creditScore; try { String creditScoreUrl = \"http://creditprocessorservice:8899/getScore?customernum=\" + customerNum; creditScore = Integer.parseInt(restTemplate.getForObject(creditScoreUrl, String.class)); } catch (HttpClientErrorException e) { return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(\"Error getting credit score\"); } String creditScoreCategory = getCreditCategoryFromScore(creditScore); // Run Credit Check String creditCheckUrl = \"http://creditprocessorservice:8899/runCreditCheck?customernum=\" + customerNum + \"\u0026score=\" + creditScore; String checkResult; try { checkResult = restTemplate.getForObject(creditCheckUrl, String.class); } catch (HttpClientErrorException e) { return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(\"Error running credit check\"); } return ResponseEntity.ok(checkResult); } We can see that this function accepts a customer number as an input. This would be helpful to capture as part of a trace. What else would be helpful?\nWell, the credit score returned for this customer by the creditprocessorservice may be interesting (we want to ensure we don’t capture any PII data though). It would also be helpful to capture the credit score category, and the credit check result.\nGreat, we’ve identified four tags to capture from this service that could help with our investigation. But how do we capture these?\nCapture Tags We start by adding OpenTelemetry imports to the top of the CreditCheckController.java file:\n... import io.opentelemetry.api.trace.Span; import io.opentelemetry.instrumentation.annotations.WithSpan; import io.opentelemetry.instrumentation.annotations.SpanAttribute; Next, we use the @WithSpan annotation to produce a span for creditCheck:\n@GetMapping(\"/check\") @WithSpan // ADDED public ResponseEntity\u003cString\u003e creditCheck(@RequestParam(\"customernum\") String customerNum) { ... We can now get a reference to the current span and add an attribute (aka tag) to it:\n... try { String creditScoreUrl = \"http://creditprocessorservice:8899/getScore?customernum=\" + customerNum; creditScore = Integer.parseInt(restTemplate.getForObject(creditScoreUrl, String.class)); } catch (HttpClientErrorException e) { return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(\"Error getting credit score\"); } Span currentSpan = Span.current(); // ADDED currentSpan.setAttribute(\"credit.score\", creditScore); // ADDED ... That was pretty easy, right? Let’s capture some more, with the final result looking like this:\n@GetMapping(\"/check\") @WithSpan(kind=SpanKind.SERVER) public ResponseEntity\u003cString\u003e creditCheck(@RequestParam(\"customernum\") @SpanAttribute(\"customer.num\") String customerNum) { // Get Credit Score int creditScore; try { String creditScoreUrl = \"http://creditprocessorservice:8899/getScore?customernum=\" + customerNum; creditScore = Integer.parseInt(restTemplate.getForObject(creditScoreUrl, String.class)); } catch (HttpClientErrorException e) { return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(\"Error getting credit score\"); } Span currentSpan = Span.current(); currentSpan.setAttribute(\"credit.score\", creditScore); String creditScoreCategory = getCreditCategoryFromScore(creditScore); currentSpan.setAttribute(\"credit.score.category\", creditScoreCategory); // Run Credit Check String creditCheckUrl = \"http://creditprocessorservice:8899/runCreditCheck?customernum=\" + customerNum + \"\u0026score=\" + creditScore; String checkResult; try { checkResult = restTemplate.getForObject(creditCheckUrl, String.class); } catch (HttpClientErrorException e) { return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(\"Error running credit check\"); } currentSpan.setAttribute(\"credit.check.result\", checkResult); return ResponseEntity.ok(checkResult); } Redeploy Service Once these changes are made, let’s run the following script to rebuild the Docker image used for creditcheckservice and redeploy it to our Kubernetes cluster:\n./5-redeploy-creditcheckservice.sh java Confirm Tag is Captured Successfully After a few minutes, return to Splunk Observability Cloud and load one of the latest traces to confirm that the tags were captured successfully (hint: sort by the timestamp to find the latest traces):\nWell done, you’ve leveled up your OpenTelemetry game and have added context to traces using tags.\nNext, we’re ready to see how you can use these tags with Splunk Observability Cloud!",
    "description": "Let’s add some tags to our traces, so we can find out why some customers receive a poor experience from our application.\nIdentify Useful Tags We’ll start by reviewing the code for the creditCheck function of creditcheckservice (which can be found in the file /home/splunk/workshop/tagging/creditcheckservice-java/src/main/java/com/example/creditcheckservice/CreditCheckController.java):\n@GetMapping(\"/check\") public ResponseEntity\u003cString\u003e creditCheck(@RequestParam(\"customernum\") String customerNum) { // Get Credit Score int creditScore; try { String creditScoreUrl = \"http://creditprocessorservice:8899/getScore?customernum=\" + customerNum; creditScore = Integer.parseInt(restTemplate.getForObject(creditScoreUrl, String.class)); } catch (HttpClientErrorException e) { return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(\"Error getting credit score\"); } String creditScoreCategory = getCreditCategoryFromScore(creditScore); // Run Credit Check String creditCheckUrl = \"http://creditprocessorservice:8899/runCreditCheck?customernum=\" + customerNum + \"\u0026score=\" + creditScore; String checkResult; try { checkResult = restTemplate.getForObject(creditCheckUrl, String.class); } catch (HttpClientErrorException e) { return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(\"Error running credit check\"); } return ResponseEntity.ok(checkResult); } We can see that this function accepts a customer number as an input. This would be helpful to capture as part of a trace. What else would be helpful?",
    "tags": [],
    "title": "1. Capture Tags - Java",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/3-capture-tags/1-capture-tags-java/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "Connect to your EC2 Instance We’ve prepared an Ubuntu Linux instance in AWS/EC2 for each attendee.\nUsing the IP address and password provided by your instructor, connect to your EC2 instance using one of the methods below:\nMac OS / Linux ssh splunk@IP address Windows 10+ Use the OpenSSH client Earlier versions of Windows Use Putty",
    "description": "Connect to your EC2 Instance We’ve prepared an Ubuntu Linux instance in AWS/EC2 for each attendee.\nUsing the IP address and password provided by your instructor, connect to your EC2 instance using one of the methods below:\nMac OS / Linux ssh splunk@IP address Windows 10+ Use the OpenSSH client Earlier versions of Windows Use Putty",
    "tags": [],
    "title": "Connect to EC2 Instance",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/1-connect-to-instance/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Solving Problems with O11y Cloud",
    "content": "Connect to your EC2 Instance We’ve prepared an Ubuntu Linux instance in AWS/EC2 for each attendee. Using the IP address and password provided by your instructor, connect to your EC2 instance using one of the methods below:\nmacOS / Linux ssh splunk@IP address Windows 10+ Use the OpenSSH client Earlier versions of Windows Use Putty Editing Files We’ll use vi to edit files during the workshop. Here’s a quick primer.\nTo open a file for editing:\nvi \u003cfilename\u003e To edit the file, click i to switch to Insert mode and begin entering text as normal. Use Esc to return to Command mode. To save your changes without exiting the editor, enter Esc to return to command mode then enter :w. To exit the editor without saving changes, enter Esc to return to command mode then enter :q!. To save your changes and exist the editor, enter Esc to return to command mode then enter :wq. See An introduction to the vi editor for a comprehensive introduction to vi.\nIf you’d prefer using another editor, you can use nano instead:\nnano \u003cfilename\u003e",
    "description": "Connect to your EC2 Instance We’ve prepared an Ubuntu Linux instance in AWS/EC2 for each attendee. Using the IP address and password provided by your instructor, connect to your EC2 instance using one of the methods below:\nmacOS / Linux ssh splunk@IP address Windows 10+ Use the OpenSSH client Earlier versions of Windows Use Putty Editing Files We’ll use vi to edit files during the workshop. Here’s a quick primer.",
    "tags": [],
    "title": "Connect to EC2 Instance",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/9-solving-problems-with-o11y-cloud/1-connect-to-instance/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Frontend Dashboards",
    "content": "We have some good charts in our dashboard, but let’s add a few more.\nGo to Dashboards by clicking the dasboard icon on the left side of the screen. Find the Browser app health dashboard and scroll to the Largest Contentful Paint (LCP) chart. Click the chart actions icon to open the flyout menu, and click “Copy” to add this chart to your clipboard. Now you can continue to add any other charts to your clipboard by clicking the “add to clipboard” icon. When you have collected the charts you want on your dashboard, click the “create” icon on the top right. You might need to reload the page if you were looking at charts in another browser tab. Click the “Paste charts” menu option. Now you are able to resize and edit the charts as you’d like!\nBonus: edit chart data Click the chart actions icon and select Open to edit the chart. Remove the existing Test signal. Click Add filter and type test: *yourInitials*. This will use a wildcard match so that all of the tests you have created that contain your initials (or any string you decide) will be pulled into the chart. Click into the functions to see how adding and removing dimensions changes how the data is displayed. For example, if you want all of your test location data rolled up, remove that dimension from the function. Change the chart name and description as appropriate, and click “Save and close” to commit your changes or just “Close” to cancel your changes.",
    "description": "We have some good charts in our dashboard, but let’s add a few more.\nGo to Dashboards by clicking the dasboard icon on the left side of the screen. Find the Browser app health dashboard and scroll to the Largest Contentful Paint (LCP) chart. Click the chart actions icon to open the flyout menu, and click “Copy” to add this chart to your clipboard.",
    "tags": [],
    "title": "Copying and editing charts",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/4-dashboards/1-copying-charts/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop",
    "content": "1. Dashboards Dashboards are collections of charts and visualizations that display key metrics in one place. A well-designed dashboard gives you quick, actionable insights into the health and performance of your system. They can be as simple or as detailed as needed—ranging from a few focused charts to complex views across multiple services.\nIn this module, you’ll build several charts and bring them together into the following custom dashboard.\n2. Accessing Dashboards To begin, let’s locate the dashboards in Splunk Observability suite.\nClick the Dashboards (1) button in the left-hand navigation menu. If the menu is collapsed, you can expand it by clicking the hamburger icon in the top-left corner of the screen.\nThis will take you to the main Dashboard view, where you’ll see all available dashboards—including the pre-built ones provided by Splunk Observability.\nIf your organization is already ingesting data from a Cloud API integration or through the Splunk OpenTelemetry Agent, you may also see additional dashboards relevant to those services.",
    "description": "1. Dashboards Dashboards are collections of charts and visualizations that display key metrics in one place. A well-designed dashboard gives you quick, actionable insights into the health and performance of your system. They can be as simple or as detailed as needed—ranging from a few focused charts to complex views across multiple services.\nIn this module, you’ll build several charts and bring them together into the following custom dashboard.",
    "tags": [],
    "title": "Intro to Dashboards",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 2. Preparation",
    "content": "To get Observability signals (metrics, traces and logs) into Splunk Observability Cloud we need to deploy the Splunk OpenTelemetry Collector into the Kubernetes cluster.\nFor this workshop, we will be using the Splunk OpenTelemetry Collector Helm Chart. First, we need to add the Helm chart repository to Helm and run helm repo update to ensure the latest version:\n​ Install Helm Chart Output helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 \"splunk-otel-collector-chart\" has been added to your repositories Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"splunk-otel-collector-chart\" chart repository Update Complete. ⎈Happy Helming!⎈ Splunk Observability Cloud offers wizards in the UI to walk you through the setup of the OpenTelemetry Collector on Kubernetes, but in the interest of time, we will use the Helm install command below. Additional parameters are set to enable the operator for automatic discovery and configuration and code profiling.\n--set=\"operator.enabled=true\" - this will install the OpenTelemetry operator that will be used to handle automatic discovery and configuration. --set=\"splunkObservability.profilingEnabled=true\" - this enables Code Profiling via the operator. To install the collector run the following command. Do NOT edit this:\n​ Helm Install Output helm install splunk-otel-collector --version 0.132.0 \\ --set=\"operatorcrds.install=true\", \\ --set=\"operator.enabled=true\", \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-k3s-cluster\" \\ --set=\"splunkObservability.profilingEnabled=true\" \\ --set=\"agent.service.enabled=true\" \\ --set=\"environment=$INSTANCE-workshop\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml LAST DEPLOYED: Fri Apr 19 09:39:54 2024 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Platform endpoint \"https://http-inputs-o11y-workshop-eu0.splunkcloud.com:443/services/collector/event\". Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Observability realm eu0. [INFO] You've enabled the operator's auto-instrumentation feature (operator.enabled=true)! The operator can automatically instrument Kubernetes hosted applications. - Status: Instrumentation language maturity varies. See `operator.instrumentation.spec` and documentation for utilized instrumentation details. - Splunk Support: We offer full support for Splunk distributions and best-effort support for native OpenTelemetry distributions of auto-instrumentation libraries. Ensure the Pods are reported as Running before continuing (this typically takes around 30 seconds).\n​ kubectl get pods Output kubectl get pods | grep splunk-otel splunk-otel-collector-k8s-cluster-receiver-6bd5567d95-5f8cj 1/1 Running 0 10m splunk-otel-collector-agent-tspd2 1/1 Running 0 10m splunk-otel-collector-operator-69d476cb7-j7zwd 2/2 Running 0 10m Ensure there are no errors reported by the Splunk OpenTelemetry Collector (press ctrl + c to exit) or use the installed awesome k9s terminal UI for bonus points!\n​ kubectl logs Output kubectl logs -l app=splunk-otel-collector -f --container otel-collector 2021-03-21T16:11:10.900Z INFO service/service.go:364 Starting receivers... 2021-03-21T16:11:10.900Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/watcher.go:195 Configured Kubernetes MetadataExporter {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\", \"exporter_name\": \"signalfx\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO healthcheck/handler.go:128 Health Check state change {\"component_kind\": \"extension\", \"component_type\": \"health_check\", \"component_name\": \"health_check\", \"status\": \"ready\"} 2021-03-21T16:11:11.009Z INFO service/service.go:267 Everything is ready. Begin running and processing data. 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/receiver.go:59 Starting shared informers and wait for initial cache sync. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.281Z INFO k8sclusterreceiver@v0.21.0/receiver.go:75 Completed syncing shared informer caches. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} Deleting a failed installation If you make an error installing the OpenTelemetry Collector you can start over by deleting the installation with the following command:\nhelm delete splunk-otel-collector",
    "description": "To get Observability signals (metrics, traces and logs) into Splunk Observability Cloud we need to deploy the Splunk OpenTelemetry Collector into the Kubernetes cluster.\nFor this workshop, we will be using the Splunk OpenTelemetry Collector Helm Chart. First, we need to add the Helm chart repository to Helm and run helm repo update to ensure the latest version:\n​ Install Helm Chart Output helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 \"splunk-otel-collector-chart\" has been added to your repositories Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"splunk-otel-collector-chart\" chart repository Update Complete. ⎈Happy Helming!⎈ Splunk Observability Cloud offers wizards in the UI to walk you through the setup of the OpenTelemetry Collector on Kubernetes, but in the interest of time, we will use the Helm install command below. Additional parameters are set to enable the operator for automatic discovery and configuration and code profiling.",
    "tags": [],
    "title": "Deploy the Splunk OpenTelemetry Collector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/2-preparation/1-otel/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e NodeJS Zero-Config Workshop",
    "content": "1. Create a namespace To not conflict with other workshops, we will deploy the OpenTelemetry Demo in a separate namespace called otel-demo. To create the namespace, run the following command:\nkubectl create namespace otel-demo 2. Deploy the OpenTelemetry Demo Next, change to the directory containing the OpenTelemetry Demo application:\ncd ~/workshop/apm Deploy the OpenTelemetry Demo application:\n​ Command Output kubectl apply -n otel-demo -f otel-demo.yaml serviceaccount/opentelemetry-demo created service/opentelemetry-demo-adservice created service/opentelemetry-demo-cartservice created service/opentelemetry-demo-checkoutservice created service/opentelemetry-demo-currencyservice created service/opentelemetry-demo-emailservice created service/opentelemetry-demo-featureflagservice created service/opentelemetry-demo-ffspostgres created service/opentelemetry-demo-frontend created service/opentelemetry-demo-kafka created service/opentelemetry-demo-loadgenerator created service/opentelemetry-demo-paymentservice created service/opentelemetry-demo-productcatalogservice created service/opentelemetry-demo-quoteservice created service/opentelemetry-demo-recommendationservice created service/opentelemetry-demo-redis created service/opentelemetry-demo-shippingservice created deployment.apps/opentelemetry-demo-accountingservice created deployment.apps/opentelemetry-demo-adservice created deployment.apps/opentelemetry-demo-cartservice created deployment.apps/opentelemetry-demo-checkoutservice created deployment.apps/opentelemetry-demo-currencyservice created deployment.apps/opentelemetry-demo-emailservice created deployment.apps/opentelemetry-demo-featureflagservice created deployment.apps/opentelemetry-demo-ffspostgres created deployment.apps/opentelemetry-demo-frauddetectionservice created deployment.apps/opentelemetry-demo-frontend created deployment.apps/opentelemetry-demo-kafka created deployment.apps/opentelemetry-demo-loadgenerator created deployment.apps/opentelemetry-demo-paymentservice created deployment.apps/opentelemetry-demo-productcatalogservice created deployment.apps/opentelemetry-demo-quoteservice created deployment.apps/opentelemetry-demo-recommendationservice created deployment.apps/opentelemetry-demo-redis created deployment.apps/opentelemetry-demo-shippingservice created Once the application is deployed, we need to wait for the pods to be in a Running state. To check the status of the pods, run the following command:\n​ Command Output kubectl get pods -n otel-demo NAME READY STATUS RESTARTS AGE opentelemetry-demo-emailservice-847d6fb577-bxll6 1/1 Running 0 40s opentelemetry-demo-ffspostgres-55f65465dd-2gsj4 1/1 Running 0 40s opentelemetry-demo-adservice-5b7c68859d-5hx5f 1/1 Running 0 40s opentelemetry-demo-currencyservice-c4cb78446-qsd68 1/1 Running 0 40s opentelemetry-demo-frontend-5d7cdb8786-5dl76 1/1 Running 0 39s opentelemetry-demo-kafka-79868d56d8-62wsd 1/1 Running 0 39s opentelemetry-demo-paymentservice-5cb4ccc47c-65hxl 1/1 Running 0 39s opentelemetry-demo-productcatalogservice-59d955f9d6-xtnjr 1/1 Running 0 38s opentelemetry-demo-loadgenerator-755d6cd5b-r5lqs 1/1 Running 0 39s opentelemetry-demo-quoteservice-5fbfb97778-vm62m 1/1 Running 0 38s opentelemetry-demo-redis-57c49b7b5b-b2klr 1/1 Running 0 37s opentelemetry-demo-shippingservice-6667f69f78-cwj8q 1/1 Running 0 37s opentelemetry-demo-recommendationservice-749f55f9b6-5k4lc 1/1 Running 0 37s opentelemetry-demo-featureflagservice-67677647c-85xtm 1/1 Running 0 40s opentelemetry-demo-checkoutservice-5474bf74b8-2nmns 1/1 Running 0 40s opentelemetry-demo-frauddetectionservice-77fd69d967-lnjcg 1/1 Running 0 39s opentelemetry-demo-accountingservice-96d44cfbc-vmtzb 1/1 Running 0 40s opentelemetry-demo-cartservice-7c4f59bdd5-rfkf4 1/1 Running 0 40s 3. Validate the application is running To validate the application is running, we will port-forward the frontend service. To do this, run the following command:\nkubectl port-forward svc/opentelemetry-demo-frontend 8083:8080 -n otel-demo --address='0.0.0.0' Obtain the public IP address of the instance you are running on. You can do this by running the following command:\ncurl ifconfig.me Once the port-forward is running, you can access the application by opening a browser and navigating to http://\u003cpublic IP address\u003e:8083. You should see the following:\nOnce you have confirmed the application is running, you can close the port-forward by pressing ctrl + c.\nNext, we will deploy the OpenTelemetry Collector.",
    "description": "1. Create a namespace To not conflict with other workshops, we will deploy the OpenTelemetry Demo in a separate namespace called otel-demo. To create the namespace, run the following command:\nkubectl create namespace otel-demo 2. Deploy the OpenTelemetry Demo Next, change to the directory containing the OpenTelemetry Demo application:\ncd ~/workshop/apm Deploy the OpenTelemetry Demo application:\n​ Command Output kubectl apply -n otel-demo -f otel-demo.yaml serviceaccount/opentelemetry-demo created service/opentelemetry-demo-adservice created service/opentelemetry-demo-cartservice created service/opentelemetry-demo-checkoutservice created service/opentelemetry-demo-currencyservice created service/opentelemetry-demo-emailservice created service/opentelemetry-demo-featureflagservice created service/opentelemetry-demo-ffspostgres created service/opentelemetry-demo-frontend created service/opentelemetry-demo-kafka created service/opentelemetry-demo-loadgenerator created service/opentelemetry-demo-paymentservice created service/opentelemetry-demo-productcatalogservice created service/opentelemetry-demo-quoteservice created service/opentelemetry-demo-recommendationservice created service/opentelemetry-demo-redis created service/opentelemetry-demo-shippingservice created deployment.apps/opentelemetry-demo-accountingservice created deployment.apps/opentelemetry-demo-adservice created deployment.apps/opentelemetry-demo-cartservice created deployment.apps/opentelemetry-demo-checkoutservice created deployment.apps/opentelemetry-demo-currencyservice created deployment.apps/opentelemetry-demo-emailservice created deployment.apps/opentelemetry-demo-featureflagservice created deployment.apps/opentelemetry-demo-ffspostgres created deployment.apps/opentelemetry-demo-frauddetectionservice created deployment.apps/opentelemetry-demo-frontend created deployment.apps/opentelemetry-demo-kafka created deployment.apps/opentelemetry-demo-loadgenerator created deployment.apps/opentelemetry-demo-paymentservice created deployment.apps/opentelemetry-demo-productcatalogservice created deployment.apps/opentelemetry-demo-quoteservice created deployment.apps/opentelemetry-demo-recommendationservice created deployment.apps/opentelemetry-demo-redis created deployment.apps/opentelemetry-demo-shippingservice created Once the application is deployed, we need to wait for the pods to be in a Running state. To check the status of the pods, run the following command:",
    "tags": [],
    "title": "Deploying the OpenTelemetry Demo",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/3-nodejs-kubernetes/1-otel-demo/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Horizontal Pod Autoscaling",
    "content": "1. Connect to EC2 instance You will be able to connect to the workshop instance by using SSH from your Mac, Linux or Windows device. Open the link to the sheet provided by your instructor. This sheet contains the IP addresses and the password for the workshop instances.\nInfo Your workshop instance has been pre-configured with the correct Access Token and Realm for this workshop. There is no need for you to configure these.\n2. Install Splunk OTel using Helm Install the OpenTelemetry Collector using the Splunk Helm chart. First, add the Splunk Helm chart repository and update:\n​ helm repo add helm repo add output helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update Using ACCESS_TOKEN=\u003cREDACTED\u003e Using REALM=eu0 \"splunk-otel-collector-chart\" has been added to your repositories Using ACCESS_TOKEN=\u003cREDACTED\u003e Using REALM=eu0 Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"splunk-otel-collector-chart\" chart repository Update Complete. ⎈Happy Helming!⎈ Install the OpenTelemetry Collector Helm with the following commands, do NOT edit this:\n​ helm install helm install splunk-otel-collector --version 0.132.0 \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-k3s-cluster\" \\ --set=\"logsEngine=otel\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml 3. Verify Deployment You can monitor the progress of the deployment by running kubectl get pods which should typically report that the new pods are up and running after about 30 seconds.\nEnsure the status is reported as Running before continuing.\n​ kubectl get pods kubectl get pods Output kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-agent-pvstb 2/2 Running 0 19s splunk-otel-collector-k8s-cluster-receiver-6c454894f8-mqs8n 1/1 Running 0 19s Use the label set by the helm install to tail logs (You will need to press ctrl + c to exit).\n​ kubectl logs kubectl logs -l app=splunk-otel-collector -f --container otel-collector Or use the installed k9s terminal UI.\nDeleting a failed installation If you make an error installing the Splunk OpenTelemetry Collector you can start over by deleting the installation using:\nhelm delete splunk-otel-collector",
    "description": "1. Connect to EC2 instance You will be able to connect to the workshop instance by using SSH from your Mac, Linux or Windows device. Open the link to the sheet provided by your instructor. This sheet contains the IP addresses and the password for the workshop instances.\nInfo Your workshop instance has been pre-configured with the correct Access Token and Realm for this workshop. There is no need for you to configure these.",
    "tags": [],
    "title": "Deploying the OpenTelemetry Collector in Kubernetes",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/2-hpa/1-deploy-otel/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 6. Service Health Dashboard",
    "content": "As we already saved some useful log charts in a dashboard in the Log Observer exercise, we are going to extend that dashboard.\nExercise To get back to your dashboard with the two log charts, click on Dashboards from the main menu and you will be taken to your Team Dashboard view. Under Dashboards click in Search dashboards to search for your Service Health Dashboard group. Click on the name and this will bring up your previously saved dashboard. Even if the log information is useful, it will need more information to have it make sense for our team so let’s add a bit more information The first step is adding a description chart to the dashboard. Click on the New text note and replace the text in the note with the following text and then click the Save and close button and name the chart Instructions Information to use with text note This is a Custom Health Dashboard for the **wire-transfer-service**, Please pay attention to any errors in the logs. For more detail visit [link](https://https://www.splunk.com/en_us/products/observability.html) The charts are not in a nice order, let’s correct that and rearrange the charts so that they are useful. Move your mouse over the top edge of the Instructions chart, your mouse pointer will change to a ☩. This will allow you to drag the chart in the dashboard. Drag the Instructions chart to the top left location and resize it to a 1/3rd of the page by dragging the right-hand edge. Drag and add the Log Timeline view chart next to the Instruction chart, resize it so it fills the other 2/3rd of the page to be the error rate chart next to the two the chart and resize it so it fills the page Next, resize the Log lines chart to be the width of the page and resize it the make it at least twice as long. You should have something similar to the dashboard below: This looks great, let’s continue and add more meaningful charts.",
    "description": "As we already saved some useful log charts in a dashboard in the Log Observer exercise, we are going to extend that dashboard.\nExercise To get back to your dashboard with the two log charts, click on Dashboards from the main menu and you will be taken to your Team Dashboard view. Under Dashboards click in Search dashboards to search for your Service Health Dashboard group. Click on the name and this will bring up your previously saved dashboard. Even if the log information is useful, it will need more information to have it make sense for our team so let’s add a bit more information The first step is adding a description chart to the dashboard. Click on the New text note and replace the text in the note with the following text and then click the Save and close button and name the chart Instructions Information to use with text note This is a Custom Health Dashboard for the **wire-transfer-service**, Please pay attention to any errors in the logs. For more detail visit [link](https://https://www.splunk.com/en_us/products/observability.html) The charts are not in a nice order, let’s correct that and rearrange the charts so that they are useful. Move your mouse over the top edge of the Instructions chart, your mouse pointer will change to a ☩. This will allow you to drag the chart in the dashboard. Drag the Instructions chart to the top left location and resize it to a 1/3rd of the page by dragging the right-hand edge. Drag and add the Log Timeline view chart next to the Instruction chart, resize it so it fills the other 2/3rd of the page to be the error rate chart next to the two the chart and resize it so it fills the page Next, resize the Log lines chart to be the width of the page and resize it the make it at least twice as long. You should have something similar to the dashboard below: This looks great, let’s continue and add more meaningful charts.",
    "tags": [],
    "title": "Enhancing the Dashboard",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/6-custom-dashboard/1-custom-dashboard/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 9. Service Health Dashboard",
    "content": "As we already saved some useful log charts in a dashboard in the Log Observer exercise, we are going to extend that dashboard.\nExercise To get back to your dashboard with the two log charts, click on Dashboards from the main menu and you will be taken to your Team Dashboard view. Under Dashboards click in Search dashboards to search for your Service Health Dashboard group. Click on the name and this will bring up your previously saved dashboard. Even if the log information is useful, it will need more information to have it make sense for our team so let’s add a bit more information The first step is adding a description chart to the dashboard. Click on the New text note and replace the text in the note with the following text and then click the Save and close button and name the chart Instructions Information to use with text note This is a Custom Health Dashboard for the **Payment service**, Please pay attention to any errors in the logs. For more detail visit [link](https://https://www.splunk.com/en_us/products/observability.html) The charts are not in a nice order, let’s correct that and rearrange the charts so that they are useful. Move your mouse over the top edge of the Instructions chart, your mouse pointer will change to a ☩. This will allow you to drag the chart in the dashboard. Drag the Instructions chart to the top left location and resize it to a 1/3rd of the page by dragging the right-hand edge. Drag and add the Log Timeline view chart next to the Instruction chart, resize it so it fills the other 2/3rd of the page to be the error rate chart next to the two the chart and resize it so it fills the page Next, resize the Log lines chart to be the width of the page and resize it the make it at least twice as long. You should have something similar to the dashboard below: This looks great, let’s continue and add more meaningful charts.",
    "description": "As we already saved some useful log charts in a dashboard in the Log Observer exercise, we are going to extend that dashboard.\nExercise To get back to your dashboard with the two log charts, click on Dashboards from the main menu and you will be taken to your Team Dashboard view. Under Dashboards click in Search dashboards to search for your Service Health Dashboard group. Click on the name and this will bring up your previously saved dashboard. Even if the log information is useful, it will need more information to have it make sense for our team so let’s add a bit more information The first step is adding a description chart to the dashboard. Click on the New text note and replace the text in the note with the following text and then click the Save and close button and name the chart Instructions Information to use with text note This is a Custom Health Dashboard for the **Payment service**, Please pay attention to any errors in the logs. For more detail visit [link](https://https://www.splunk.com/en_us/products/observability.html) The charts are not in a nice order, let’s correct that and rearrange the charts so that they are useful. Move your mouse over the top edge of the Instructions chart, your mouse pointer will change to a ☩. This will allow you to drag the chart in the dashboard. Drag the Instructions chart to the top left location and resize it to a 1/3rd of the page by dragging the right-hand edge. Drag and add the Log Timeline view chart next to the Instruction chart, resize it so it fills the other 2/3rd of the page to be the error rate chart next to the two the chart and resize it so it fills the page Next, resize the Log lines chart to be the width of the page and resize it the make it at least twice as long. You should have something similar to the dashboard below: This looks great, let’s continue and add more meaningful charts.",
    "tags": [],
    "title": "Enhancing the Dashboard",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/9-custom-dashboard/1-custom-dashboard/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud",
    "content": "During this technical Ingest Processor1 for Splunk Observability Cloud workshop you will have the opportunity to get hands-on with Ingest Processor in Splunk Enterprise Cloud.\nTo simplify the workshop modules, a pre-configured Splunk Enterprise Cloud instance is provided.\nThe instance is pre-configured with all the requirements for creating an Ingest Processor pipeline.\nThis workshop will introduce you to the benefits of using Ingest Processor to convert robust logs to metrics and send those metrics to Splunk Observability Cloud. By the end of these technical workshops, you will have a good understanding of some key features and capabilities of Ingest Processor in Splunk Enterprise Cloud and the value of using Splunk Observability Cloud as a destination within an Ingest Processor pipeline.\nHere are the instructions on how to access your pre-configured Splunk Enterprise Cloud instance.\nIngest Processor is a data processing capability that works within your Splunk Cloud Platform deployment. Use the Ingest Processor to configure data flows, control data format, apply transformation rules prior to indexing, and route to destinations. ↩︎",
    "description": "During this technical Ingest Processor1 for Splunk Observability Cloud workshop you will have the opportunity to get hands-on with Ingest Processor in Splunk Enterprise Cloud.\nTo simplify the workshop modules, a pre-configured Splunk Enterprise Cloud instance is provided.\nThe instance is pre-configured with all the requirements for creating an Ingest Processor pipeline.\nThis workshop will introduce you to the benefits of using Ingest Processor to convert robust logs to metrics and send those metrics to Splunk Observability Cloud. By the end of these technical workshops, you will have a good understanding of some key features and capabilities of Ingest Processor in Splunk Enterprise Cloud and the value of using Splunk Observability Cloud as a destination within an Ingest Processor pipeline.",
    "tags": [],
    "title": "Getting Started",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/1-getting-started/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence",
    "content": "Monitoring and Alerting with Splunk, AppDynamics, and Splunk Observability Cloud Introduction and Overview In today’s complex IT landscape, ensuring the performance and availability of applications and services is paramount. This workshop will introduce you to a powerful combination of tools – Splunk, AppDynamics, Splunk Observability Cloud, and Splunk IT Service Intelligence (ITSI) – that work together to provide comprehensive monitoring and alerting capabilities.\nThe Challenge of Modern Monitoring Modern applications often rely on distributed architectures, microservices, and cloud infrastructure. This complexity makes it challenging to pinpoint the root cause of performance issues or outages. Traditional monitoring tools often focus on individual components, leaving gaps in understanding the overall health and performance of a service.\nThe Solution: Integrated Observability A comprehensive observability strategy requires integrating data from various sources and correlating it to gain actionable insights. This workshop will demonstrate how Splunk, AppDynamics, Splunk Observability Cloud, and ITSI work together to achieve this:\nSplunk: Acts as the central platform for log analytics, security information and event management (SIEM), and broader data analysis. It ingests data from AppDynamics, Splunk Observability Cloud, and other sources, enabling powerful search, visualization, and correlation capabilities. Splunk provides a holistic view of your IT environment.\nSplunk Observability Cloud: Offers full-stack observability, encompassing infrastructure metrics, distributed traces, and logs. It provides a unified view of the health and performance of your entire infrastructure, from servers and containers to cloud services and custom applications. Splunk Observability Cloud helps correlate performance issues across the entire stack.\nAppDynamics: Provides deep Application Performance Monitoring (APM). It instruments applications to capture detailed performance metrics, including transaction traces, code-level diagnostics, and user experience data. AppDynamics excels at identifying performance bottlenecks within the application.\nSplunk IT Service Intelligence (ITSI): Provides service intelligence by correlating data from all the other platforms. ITSI allows you to define services, map dependencies, and monitor Key Performance Indicators (KPIs) that reflect the overall health and performance of those services. ITSI is essential for understanding the business impact of IT issues.\nData Flow and Integration A key concept to understand is how data flows between these platforms:\nSplunk Observability Cloud and AppDynamics collect data: They monitor applications and infrastructure, gathering performance metrics and traces.\nData is sent to Splunk: AppDynamics and Splunk Observability Cloud integrate with Splunk to forward their collected data alongside logs sent directly to Splunk.\nSplunk analyzes and indexes data: Splunk processes and stores the data, making it searchable and analyzable.\nITSI leverages Splunk data: ITSI uses the data in Splunk to create services, define KPIs, and monitor the overall health of your IT operations.\nWorkshop Objectives By the end of this workshop, you will:\nUnderstand the complementary roles of Splunk, AppDynamics, Splunk Observability Cloud, and ITSI. Create basic alerts in Splunk, Observability Cloud and AppDynamics. Configure a new Service and a simple KPI and alerting in ITSI. Understand the concept of episodes in ITSI. This workshop provides a foundation for building a robust observability practice. We will focus on the alerting configuration workflows, preparing you to explore more advanced features and configurations in your own environment. We will not be covering ITSI or Add-On installation and configuration.\nHere are the instructions on how to access your pre-configured Splunk Enterprise Cloud instance.",
    "description": "Monitoring and Alerting with Splunk, AppDynamics, and Splunk Observability Cloud Introduction and Overview In today’s complex IT landscape, ensuring the performance and availability of applications and services is paramount. This workshop will introduce you to a powerful combination of tools – Splunk, AppDynamics, Splunk Observability Cloud, and Splunk IT Service Intelligence (ITSI) – that work together to provide comprehensive monitoring and alerting capabilities.\nThe Challenge of Modern Monitoring Modern applications often rely on distributed architectures, microservices, and cloud infrastructure. This complexity makes it challenging to pinpoint the root cause of performance issues or outages. Traditional monitoring tools often focus on individual components, leaving gaps in understanding the overall health and performance of a service.",
    "tags": [],
    "title": "Getting Started",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/1-getting-started/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring",
    "content": "During this technical Optimize Cloud Monitoring Workshop, you will build out an environment based on a lightweight Kubernetes1 cluster.\nTo simplify the workshop modules, a pre-configured AWS/EC2 instance is provided.\nThe instance is pre-configured with all the software required to deploy the Splunk OpenTelemetry Connector2 and the microservices-based OpenTelemetry Demo Application3 in Kubernetes which has been instrumented using OpenTelemetry to send metrics, traces, spans and logs.\nThis workshop will introduce you to the benefits of standardized data collection, how content can be re-used across teams, correlating metrics and logs, and creating detectors to fire alerts. By the end of these technical workshops, you will have a good understanding of some of the key features and capabilities of the Splunk Observability Cloud.\nHere are the instructions on how to access your pre-configured AWS/EC2 instance\nKubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. ↩︎\nOpenTelemetry Collector offers a vendor-agnostic implementation on how to receive, process and export telemetry data. In addition, it removes the need to run, operate and maintain multiple agents/collectors to support open-source telemetry data formats (e.g. Jaeger, Prometheus, etc.) sending to multiple open-source or commercial back-ends. ↩︎\nThe OpenTelemetry Demo Application is a microservice-based distributed system intended to illustrate the implementation of OpenTelemetry in a near real-world environment. ↩︎",
    "description": "During this technical Optimize Cloud Monitoring Workshop, you will build out an environment based on a lightweight Kubernetes1 cluster.\nTo simplify the workshop modules, a pre-configured AWS/EC2 instance is provided.\nThe instance is pre-configured with all the software required to deploy the Splunk OpenTelemetry Connector2 and the microservices-based OpenTelemetry Demo Application3 in Kubernetes which has been instrumented using OpenTelemetry to send metrics, traces, spans and logs.\nThis workshop will introduce you to the benefits of standardized data collection, how content can be re-used across teams, correlating metrics and logs, and creating detectors to fire alerts. By the end of these technical workshops, you will have a good understanding of some of the key features and capabilities of the Splunk Observability Cloud.",
    "tags": [],
    "title": "Getting Started",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/1-getting-started/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "1. Sign in to Splunk Observability Cloud You should have received an e-mail from Splunk inviting you to the Workshop Org. This e-mail will look like the screenshot below, if you cannot find it, please check your Spam/Junk folders or inform your Instructor. You can also check for other solutions in our login F.A.Q..\nTo proceed click the Join Now button or click on the link provided in the e-mail.\nIf you have already completed the registration process you can skip the rest and proceed directly to Splunk Observability Cloud and log in:\nhttps://app.eu0.signalfx.com (EMEA) https://app.us1.signalfx.com (APAC/AMER) If this is your first time using Splunk Observability Cloud, you will be presented with the registration form. Enter your full name, and desired password. Please note that the password requirements are:\nMust be between 8 and 32 characters Must contain at least one capital letter Must have at least one number Must have at least one symbol (e.g. !@#$%^\u0026*()_+) Click the checkbox to agree to the terms and conditions and click the SIGN IN NOW button.",
    "description": "Learn how to get started with Splunk Observability Cloud.",
    "tags": [],
    "title": "Getting Started",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/1-homepage/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "1. Sign in to Splunk Observability Cloud You should have received an e-mail from Splunk inviting you to the Workshop Org. This e-mail will look like the screenshot below, if you cannot find it, please check your Spam/Junk folders or inform your Instructor. You can also check for other solutions in our login F.A.Q..\nTo proceed click the Join Now button or click on the link provided in the e-mail.\nIf you have already completed the registration process you can skip the rest and proceed directly to Splunk Observability Cloud and log in:\nhttps://app.eu0.signalfx.com (EMEA) https://app.us1.signalfx.com (APAC/AMER) If this is your first time using Splunk Observability Cloud, you will be presented with the registration form. Enter your full name, and desired password. Please note that the password requirements are:\nMust be between 8 and 32 characters Must contain at least one capital letter Must have at least one number Must have at least one symbol (e.g. !@#$%^\u0026*()_+) Click the checkbox to agree to the terms and conditions and click the SIGN IN NOW button.",
    "description": "Learn how to get started with Splunk Observability Cloud.",
    "tags": [],
    "title": "Getting Started",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/1-homepage/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk OnCall",
    "content": "Aim The aim of this module is for you to configure your personal profile which controls how you will be notified by Splunk On-Call whenever you get paged.\n1. Contact Methods Switch to the Splunk On-Call UI and click on your login name in the top right hand corner and chose Profile from the drop down. Confirm your contact methods are listed correctly and add any additional phone numbers and e-mail address you wish to use.\n2. Mobile Devices To install the Splunk On-Call app for your smartphone search your phones App Store for Splunk On-Call to find the appropriate version of the app. The publisher should be listed as VictorOps Inc.\nApple Store\nGoogle Play\nConfiguration help guides are available:\nApple Android Install the App and login, then refresh the Profile page and your device should now be listed under the devices section. Click the Test push notification button and confirm you receive the test message.\n3. Personal Calendar This link will enable you to sync your on-call schedule with your calendar, however as you do not have any allocated shifts yet this will currently be empty. You can add it to your calendar by copying the link into your preferred application and setting it up as a new subscription.\n4. Paging Policies Paging Polices specify how you will be contacted when on-call. The Primary Paging Policy will have defaulted to sending you an SMS assuming you added your phone number when activating your account. We will now configure this policy into a three tier multi-stage policy similar to the image below.\n4.1 Send a push notification Click the edit policy button in the top right corner for the Primary Paging Policy.\nSend a push notification to all my devices Execute the next step if I have not responded within 5 minutes Click Add a Step\n4.2 Send an e-mail Send an e-mail to [your email address] Execute the next step if I have not responded within 5 minutes Click Add a Step\n4.3 Call your number Every 5 minutes until we have reached you Make a phone call to [your phone number] Click Save to save the policy.\nWhen you are on-call or in the escalation path of an incident, you will receive notifications in this order following these time delays.\nTo cease the paging you must acknowledge the incident. Acknowledgements can occur in one of the following ways:\nExpanding the Push Notification on your device and selecting Acknowledge Responding to the SMS with the 5 digit code included Pressing 4 during the Phone Call Slack Button For more information on Notification Types, see here.\n5. Custom Paging Policies Custom paging polices enable you to override the primary policy based on the time and day of the week. A good example would be to get the system to immediately phone you whenever you get a page during the evening or weekends as this is more likely to get your attention than a push notification.\nCreate a new Custom Policy by clicking Add a Policy and configure with the following settings:\n5.1 Custom evening policy Policy Name: Evening\nEvery 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: All 7 Days Time zone Between 7pm and 9am Click Save to save the policy then add one more.\n5.2 Custom weekend policy Policy Name: Weekend\nEvery 5 minutes until we have reached you Make a phone call to [your phone number] Time Period: Sat \u0026 Sun Time zone Between 9am and 7pm Click Save to save the policy.\nThese custom paging policies will be used during the specified times in place of the Primary Policy. However, admins do have the ability to ignore these custom policies, and we will highlight how this is achieved in a later module.\nThe final option here is the setting for Recovery Notifications. These are typically low priority, will default to Push, but can also be email, sms or phone call. Your profile is now fully configured using these example configurations.\nOrganizations will have different views on how profiles should be configured and will typically issue guidelines for paging policies and times between escalations etc.\nPlease wait for the instructor before proceeding to the Teams module.",
    "description": "Aim The aim of this module is for you to configure your personal profile which controls how you will be notified by Splunk On-Call whenever you get paged.\n1. Contact Methods Switch to the Splunk On-Call UI and click on your login name in the top right hand corner and chose Profile from the drop down. Confirm your contact methods are listed correctly and add any additional phone numbers and e-mail address you wish to use.",
    "tags": [],
    "title": "User Profile",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/getting_started/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e GDI (OTel \u0026 UF)",
    "content": "Please note to begin the following lab, you must have completed the prework:\nObtain a Splunk Observability Cloud access key Understand cli commands Follow these steps if using O11y Workshop EC2 instances\n1. Verify yelp data files are present ll /var/appdata/yelp* 2. Export the following variables export ACCESS_TOKEN=\u003cyour-access-token\u003e export REALM=\u003cyour-o11y-cloud-realm\u003e export clusterName=\u003cyour-k8s-cluster\u003e 3. Clone the following repo cd /home/splunk git clone https://github.com/leungsteve/realtime_enrichment.git cd realtime_enrichment/workshop python3 -m venv rtapp-workshop source rtapp-workshop/bin/activate",
    "description": "Please note to begin the following lab, you must have completed the prework:\nObtain a Splunk Observability Cloud access key Understand cli commands Follow these steps if using O11y Workshop EC2 instances\n1. Verify yelp data files are present ll /var/appdata/yelp* 2. Export the following variables export ACCESS_TOKEN=\u003cyour-access-token\u003e export REALM=\u003cyour-o11y-cloud-realm\u003e export clusterName=\u003cyour-k8s-cluster\u003e 3. Clone the following repo cd /home/splunk git clone https://github.com/leungsteve/realtime_enrichment.git cd realtime_enrichment/workshop python3 -m venv rtapp-workshop source rtapp-workshop/bin/activate",
    "tags": [],
    "title": "Getting Started with O11y GDI - Real Time Enrichment Workshop",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/8-gdi/1-getting-started/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 3. Receivers",
    "content": "Host Metrics Receiver The Host Metrics Receiver generates metrics about the host system scraped from various sources. This is intended to be used when the collector is deployed as an agent which is what we will be doing in this workshop.\nLet’s update the /etc/otel-contrib/config.yaml file and configure the hostmetrics receiver. Insert the following YAML under the receivers section, taking care to indent by two spaces.\nsudo vi /etc/otelcol-contrib/config.yaml ​ Host Metrics Receiver Configuration receivers: hostmetrics: collection_interval: 10s scrapers: # CPU utilization metrics cpu: # Disk I/O metrics disk: # File System utilization metrics filesystem: # Memory utilization metrics memory: # Network interface I/O metrics \u0026 TCP connection metrics network: # CPU load metrics load: # Paging/Swap space utilization and I/O metrics paging: # Process count metrics processes: # Per process CPU, Memory and Disk I/O metrics. Disabled by default. # process:",
    "description": "Host Metrics Receiver The Host Metrics Receiver generates metrics about the host system scraped from various sources. This is intended to be used when the collector is deployed as an agent which is what we will be doing in this workshop.\nLet’s update the /etc/otel-contrib/config.yaml file and configure the hostmetrics receiver. Insert the following YAML under the receivers section, taking care to indent by two spaces.\nsudo vi /etc/otelcol-contrib/config.yaml ​ Host Metrics Receiver Configuration receivers: hostmetrics: collection_interval: 10s scrapers: # CPU utilization metrics cpu: # Disk I/O metrics disk: # File System utilization metrics filesystem: # Memory utilization metrics memory: # Network interface I/O metrics \u0026 TCP connection metrics network: # CPU load metrics load: # Paging/Swap space utilization and I/O metrics paging: # Process count metrics processes: # Per process CPU, Memory and Disk I/O metrics. Disabled by default. # process:",
    "tags": [],
    "title": "OpenTelemetry Collector Receivers",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/3-receivers/1-hostmetrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts",
    "content": "Download the OpenTelemetry Collector Contrib distribution The first step in installing the Open Telemetry Collector is downloading it. For our lab, we will use the wget command to download the .deb package from the OpenTelemetry Github repository.\nObtain the .deb package for your platform from the OpenTelemetry Collector Contrib releases page\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.111.0/otelcol-contrib_0.111.0_linux_amd64.deb Install the OpenTelemetry Collector Contrib distribution Install the .deb package using dpkg. Take a look at the dpkg Output tab below to see what the example output of a successful install will look like:\n​ Install dpkg Output sudo dpkg -i otelcol-contrib_0.111.0_linux_amd64.deb Selecting previously unselected package otelcol-contrib. (Reading database ... 89232 files and directories currently installed.) Preparing to unpack otelcol-contrib_0.111.0_linux_amd64.deb ... Unpacking otelcol-contrib (0.111.0) ... Setting up otelcol-contrib (0.111.0) ... Created symlink /etc/systemd/system/multi-user.target.wants/otelcol-contrib.service → /lib/systemd/system/otelcol-contrib.service.",
    "description": "Download the OpenTelemetry Collector Contrib distribution The first step in installing the Open Telemetry Collector is downloading it. For our lab, we will use the wget command to download the .deb package from the OpenTelemetry Github repository.\nObtain the .deb package for your platform from the OpenTelemetry Collector Contrib releases page\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.111.0/otelcol-contrib_0.111.0_linux_amd64.deb Install the OpenTelemetry Collector Contrib distribution Install the .deb package using dpkg. Take a look at the dpkg Output tab below to see what the example output of a successful install will look like:",
    "tags": [],
    "title": "Installing OpenTelemetry Collector Contrib",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/1-installation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 7. Workshop Wrap-up",
    "content": "During the workshop, we have seen how the Splunk Observability Cloud in combination with the OpenTelemetry signals (metrics, traces and logs) can help you to reduce mean time to detect (MTTD) and also reduce mean time to resolution (MTTR).\nWe have a better understanding of the Main User interface and its components, the Landing, Infrastructure, APM, Log Observer, Dashboard pages, and a quick peek at the Settings page. Depending on time, we did an Infrastructure exercise and looked at Metrics used in the Kubernetes Navigators and saw related services found on our Kubernetes cluster: Understood what users were experiencing and used APM to Troubleshoot a particularly long load time and error, by following its trace across the front and back end and right to the log entries. We used tools like the APM Dependency map with Breakdown to discover what is causing our issue: Used Tag Spotlight, in APM, to understand blast radius, detect trends and context for our performance issues and errors. We drilled down in Span’s in the APM Trace waterfall to see how services interacted and find errors: We used the Related content feature to follow the link between our Trace directly to the Logs related to our Trace and used filters to drill down to the exact cause of our issue. In the final exercise, we created a health dashboard to keep that running for our Developers and SREs on a TV screen:",
    "description": "During the workshop, we have seen how the Splunk Observability Cloud in combination with the OpenTelemetry signals (metrics, traces and logs) can help you to reduce mean time to detect (MTTD) and also reduce mean time to resolution (MTTR).\nWe have a better understanding of the Main User interface and its components, the Landing, Infrastructure, APM, Log Observer, Dashboard pages, and a quick peek at the Settings page. Depending on time, we did an Infrastructure exercise and looked at Metrics used in the Kubernetes Navigators and saw related services found on our Kubernetes cluster:",
    "tags": [],
    "title": "Key Takeaways",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/7-wrap-up/key-takeaways/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 10. Workshop Wrap-up",
    "content": "During the workshop, we have seen how the Splunk Observability Cloud in combination with the OpenTelemetry signals (metrics, traces and logs) can help you to reduce mean time to detect (MTTD) and also reduce mean time to resolution (MTTR).\nWe have a better understanding of the Main User interface and its components, the Landing, Infrastructure, APM, RUM, Synthetics, Dashboard pages, and a quick peek at the Settings page. Depending on time, we did an Infrastructure exercise and looked at Metrics used in the Kubernetes Navigators and saw related services found on our Kubernetes cluster: Understood what users were experiencing and used RUM \u0026 APM to Troubleshoot a particularly long page load, by following its trace across the front and back end and right to the log entries. We used tools like RUM Session replay and the APM Dependency map with Breakdown to discover what is causing our issue: Used Tag Spotlight, in both RUM and APM, to understand blast radius, detect trends and context for our performance issues and errors. We drilled down in Span’s in the APM Trace waterfall to see how services interacted and find errors: We used the Related content feature to follow the link between our Trace directly to the Logs related to our Trace and used filters to drill down to the exact cause of our issue. We then looked at Synthetics, which can simulate web and mobile traffic and we used the available Synthetic Test, first to confirm our finding from RUM/AMP and Log observer, then we created a Detector so we would be alerted if when the run time of a test exceeded our SLA.\nIn the final exercise, we created a health dashboard to keep that running for our Developers and SREs on a TV screen:",
    "description": "During the workshop, we have seen how the Splunk Observability Cloud in combination with the OpenTelemetry signals (metrics, traces and logs) can help you to reduce mean time to detect (MTTD) and also reduce mean time to resolution (MTTR).\nWe have a better understanding of the Main User interface and its components, the Landing, Infrastructure, APM, RUM, Synthetics, Dashboard pages, and a quick peek at the Settings page. Depending on time, we did an Infrastructure exercise and looked at Metrics used in the Kubernetes Navigators and saw related services found on our Kubernetes cluster:",
    "tags": [],
    "title": "Key Takeaways",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/10-wrap-up/key-takeaways/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 5. Splunk Log Observer",
    "content": "Log Observer (LO), can be used in multiple ways. In the quick tour, you used the LO no-code interface to search for specific entries in the logs. This section, however, assumes you have arrived in LO from a trace in APM using the Related Content link.\nThe advantage of this is, as it was with the link between RUM \u0026 APM, that you are looking at your logs within the context of your previous actions. In this case, the context is the time frame (1), which matches that of the trace and the filter (2) which is set to the trace_id.\nThis view will include all the log lines from all applications or services that participated in the back-end transaction started by the end-user interaction with the Online Boutique.\nEven in a small application, the sheer amount of logs found can make it hard to see the specific log lines that matter to the actual incident we are investigating.\nExercise We need to focus on just the Error messages in the logs:\nClick on the Group By drop-down box and use the filter to find Severity. Once selected click the Apply button (notice that the chart legend changes to show debug, error and info). Selecting just the error logs can be done by either clicking on the word error (1) in the legend, followed by selecting Add to filter. Then click Run Search You could also add the service name, sf_service=wire-transfer-service, to the filter if there are error lines for multiple services, but in our case, this is not necessary. Next, we will look at log entries in detail.",
    "description": "Log Observer (LO), can be used in multiple ways. In the quick tour, you used the LO no-code interface to search for specific entries in the logs. This section, however, assumes you have arrived in LO from a trace in APM using the Related Content link.\nThe advantage of this is, as it was with the link between RUM \u0026 APM, that you are looking at your logs within the context of your previous actions. In this case, the context is the time frame (1), which matches that of the trace and the filter (2) which is set to the trace_id.",
    "tags": [],
    "title": "1. Log Filtering",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/5-log-observer/1-log-filtering/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 7. Splunk Log Observer",
    "content": "Log Observer (LO), can be used in multiple ways. In the quick tour, you used the LO no-code interface to search for specific entries in the logs. This section, however, assumes you have arrived in LO from a trace in APM using the Related Content link.\nThe advantage of this is, as it was with the link between RUM \u0026 APM, that you are looking at your logs within the context of your previous actions. In this case, the context is the time frame (1), which matches that of the trace and the filter (2) which is set to the trace_id.\nThis view will include all the log lines from all applications or services that participated in the back-end transaction started by the end-user interaction with the Online Boutique.\nEven in a small application such as our Online Boutique, the sheer amount of logs found can make it hard to see the specific log lines that matter to the actual incident we are investigating.\nExercise We need to focus on just the Error messages in the logs:\nClick on the Group By drop-down box and use the filter to find Severity. Once selected click the Apply button (notice that the chart legend changes to show debug, error and info). Selecting just the error logs can be done by either clicking on the word error (1) in the legend, followed by selecting Add to filter. Then click Run Search You could also add the service name, sf_service=paymentservice, to the filter if there are error lines for multiple services, but in our case, this is not necessary. Next, we will look at log entries in detail.",
    "description": "Log Observer (LO), can be used in multiple ways. In the quick tour, you used the LO no-code interface to search for specific entries in the logs. This section, however, assumes you have arrived in LO from a trace in APM using the Related Content link.\nThe advantage of this is, as it was with the link between RUM \u0026 APM, that you are looking at your logs within the context of your previous actions. In this case, the context is the time frame (1), which matches that of the trace and the filter (2) which is set to the trace_id.",
    "tags": [],
    "title": "1. Log Filtering",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/7-log-observer/1-log-filtering/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Monolith Workshop",
    "content": "The Splunk OpenTelemetry Collector is the core component of instrumenting infrastructure and applications. Its role is to collect and send:\nInfrastructure metrics (disk, CPU, memory, etc.) Application Performance Monitoring (APM) traces Profiling data Host and application logs Remove any existing OpenTelemetry Collectors If you have completed the Splunk IM workshop, please ensure you have deleted the collector running in Kubernetes before continuing. This can be done by running the following command:\nhelm delete splunk-otel-collector The EC2 instance may already have an older version of the collector already installed. To uninstall the collector, run the following commands:\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh \u003e /tmp/splunk-otel-collector.sh sudo sh /tmp/splunk-otel-collector.sh --uninstall To ensure your instance is configured correctly, we need to confirm that the required environment variables for this workshop are set correctly. In your terminal run the following command:\n. ~/workshop/petclinic/scripts/check_env.sh In the output check that all the following environment variables are present and have values set. If any are missing, please contact your instructor:\nACCESS_TOKEN REALM RUM_TOKEN HEC_TOKEN HEC_URL INSTANCE We can then go ahead and install the Collector. Some additional parameters are passed to the installation script, they are:\n--with-instrumentation - This will install the agent from the Splunk distribution of OpenTelemetry Java, which is then loaded automatically when the PetClinic Java application starts up. No configuration is required! --deployment-environment - Sets the resource attribute deployment.environment to the value passed. This is used to filter views in the UI. --enable-profiler - Enables the profiler for the Java application. This will generate CPU profiles for the application. --enable-profiler-memory - Enables the profiler for the Java application. This will generate memory profiles for the application. --enable-metrics - Enables the exporting of Micrometer metrics --hec-token - Sets the HEC token for the collector to use --hec-url - Sets the HEC URL for the collector to use curl -sSL https://dl.signalfx.com/splunk-otel-collector.sh \u003e /tmp/splunk-otel-collector.sh \u0026\u0026 \\ sudo sh /tmp/splunk-otel-collector.sh --realm $REALM -- $ACCESS_TOKEN --mode agent --without-fluentd --with-instrumentation --deployment-environment $INSTANCE-petclinic --enable-profiler --enable-profiler-memory --enable-metrics --hec-token $HEC_TOKEN --hec-url $HEC_URL Next, we will patch the collector to expose the hostname of the instance and not the AWS instance ID. This will make it easier to filter data in the UI:\nsudo sed -i 's/gcp, ecs, ec2, azure, system/system, gcp, ecs, ec2, azure/g' /etc/otel/collector/agent_config.yaml Once the agent_config.yaml has been patched, you will need to restart the collector:\nsudo systemctl restart splunk-otel-collector Once the installation is completed, you can navigate to the Hosts with agent installed dashboard to see the data from your host, Dashboards → Hosts with agent installed.\nUse the dashboard filter and select host.name and type or select the hostname of your workshop instance (you can get this from the command prompt in your terminal session). Once you see data flowing for your host, we are then ready to get started with the APM component.",
    "description": "The Splunk OpenTelemetry Collector is the core component of instrumenting infrastructure and applications. Its role is to collect and send:\nInfrastructure metrics (disk, CPU, memory, etc.) Application Performance Monitoring (APM) traces Profiling data Host and application logs Remove any existing OpenTelemetry Collectors If you have completed the Splunk IM workshop, please ensure you have deleted the collector running in Kubernetes before continuing. This can be done by running the following command:",
    "tags": [],
    "title": "Installing the OpenTelemetry Collector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/1-petclinic-monolith/1-otel-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e RUM",
    "content": "The aim of this Splunk Real User Monitoring (RUM) workshop is to let you:\nShop for items on the Online Boutique to create traffic, and create RUM User Sessions1 that you can view in the Splunk Observability Suite. See an overview of the performance of all your application(s) in the Application Summary Dashboard Examine the performance of a specific website with RUM metrics. In order to reach this goal, we will use an online boutique to order various products. While shopping on the online boutique you will create what is called a User Session.\nYou may encounter some issues with this web site, and you will use Splunk RUM to identify the issues, so they can be resolved by the developers.\nThe workshop host will provide you with a URL for an online boutique store that has RUM enabled.\nEach of these Online Boutiques are also being visited by a few synthetic users; this will allow us to generate more live data to be analyzed later.\nA RUM User session is a “recording” of a collection of user interactions on an application, basically collecting a website or app’s performance measured straight from the browser or Mobile App of the end user. To do this a small amount of JavaScript is embedded in each page. This script then collects data from each user as he or she explores the page, and transfers that data back for analysis. ↩︎",
    "description": "The aim of this Splunk Real User Monitoring (RUM) workshop is to let you:\nShop for items on the Online Boutique to create traffic, and create RUM User Sessions1 that you can view in the Splunk Observability Suite. See an overview of the performance of all your application(s) in the Application Summary Dashboard Examine the performance of a specific website with RUM metrics. In order to reach this goal, we will use an online boutique to order various products. While shopping on the online boutique you will create what is called a User Session.",
    "tags": [],
    "title": "Overview",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/2-rum/1-overview/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "Overview of the RUM Workshop The aim of this Splunk Real User Monitoring (RUM) workshop is to let you:\nShop for some fantastic items on the Online Boutique to create traffic, and create a number of RUM User Sessions1 that you can view in the Splunk Observability Suite.\nSee an overview of the performance of all your application(s) in the Application Summary Dashboard (Both Mobile and Web based)\nExamine the performance of a specific website or Mobile App with RUM metrics.\nInvestigate issues with your website and backend services.\n(Optionally) See how to add RUM to your website.\nIn order to reach this goal, we will use an online boutique to order various products. Whilst shopping on the online boutique you will create what is called a User Session.\nYou may encounter some issues with this web site, and you will use Splunk RUM to identify the issues, so they can be resolved by the developers.\nIf this a standalone RUM workshop, the workshop host will provide you with a URL for an online boutique store that has RUM enabled.\nIf you are running this session as part of the IM/APM workshop you will be able to use your current online boutique store after we enable RUM.\nEach of these Online Boutiques are also being visited by a few synthetic users, this will allow us to generate more live data to be analyzed later.\nA RUM Users session is a “recording” of a collection of user interactions on an application, basically collecting a website or app’s performance measured straight from the browser or Mobile App of the end user. To do this a small amount of JavaScript is embedded in each page. This script then collects data from each user as he or she explores the page, and transfers that data back for analysis. ↩︎",
    "description": "Overview of the RUM Workshop The aim of this Splunk Real User Monitoring (RUM) workshop is to let you:\nShop for some fantastic items on the Online Boutique to create traffic, and create a number of RUM User Sessions1 that you can view in the Splunk Observability Suite.\nSee an overview of the performance of all your application(s) in the Application Summary Dashboard (Both Mobile and Web based)\nExamine the performance of a specific website or Mobile App with RUM metrics.",
    "tags": [],
    "title": "1. Overview",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/1-overview/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 4. Automatic discovery and configuration",
    "content": "To configure automatic discovery and configuration, the deployments need to be patched to add the instrumentation annotation. Once patched, the OpenTelemetry Collector will inject the automatic discovery and configuration library and the Pods will be restarted in order to start sending traces and profiling data. First, confirm that the api-gateway does not have the splunk-otel-java image by running the following:\n​ Describe api-gateway Describe Output kubectl describe pods api-gateway | grep Image: Image: quay.io/phagen/spring-petclinic-api-gateway:0.0.2 Next, enable the Java automatic discovery and configuration for all the services by adding the annotation to the deployments. The following command will patch the all deployments. This will trigger the OpenTelemetry Operator to inject the splunk-otel-java image into the Pods:\n​ Patch all PetClinic services Patch Output kubectl get deployments -l app.kubernetes.io/part-of=spring-petclinic -o name | xargs -I % kubectl patch % -p \"{\\\"spec\\\": {\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"instrumentation.opentelemetry.io/inject-java\\\":\\\"default/splunk-otel-collector\\\"}}}}}\" deployment.apps/config-server patched (no change) deployment.apps/admin-server patched (no change) deployment.apps/customers-service patched deployment.apps/visits-service patched deployment.apps/discovery-server patched (no change) deployment.apps/vets-service patched deployment.apps/api-gateway patched There will be no change for the config-server, discovery-server and admin-server as these have already been patched.\nTo check the container image(s) of the api-gateway pod again, run the following command:\n​ Describe api-gateway Describe Output kubectl describe pods api-gateway | grep Image: Image: ghcr.io/signalfx/splunk-otel-java/splunk-otel-java:v1.30.0 Image: quay.io/phagen/spring-petclinic-api-gateway:0.0.2 A new image has been added to the api-gateway which will pull splunk-otel-java from ghcr.io (Note: if you see two api-gateway containers, the original one is probably still terminating, so give it a few seconds).\nNavigate back to the Kubernetes Navigator in Splunk Observability Cloud. After a couple of minutes, you will see that the Pods are being restarted by the operator and the automatic discovery and configuration container will be added. This will look similar to the screenshot below:\nWait for the Pods to turn green in the Kubernetes Navigator, then go tho the next section.",
    "description": "To configure automatic discovery and configuration, the deployments need to be patched to add the instrumentation annotation. Once patched, the OpenTelemetry Collector will inject the automatic discovery and configuration library and the Pods will be restarted in order to start sending traces and profiling data. First, confirm that the api-gateway does not have the splunk-otel-java image by running the following:\n​ Describe api-gateway Describe Output kubectl describe pods api-gateway | grep Image: Image: quay.io/phagen/spring-petclinic-api-gateway:0.0.2 Next, enable the Java automatic discovery and configuration for all the services by adding the annotation to the deployments. The following command will patch the all deployments. This will trigger the OpenTelemetry Operator to inject the splunk-otel-java image into the Pods:",
    "tags": [],
    "title": "Patching the Deployment",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/4-apm/1-patching-deployment/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting",
    "content": "Introduction This workshop walks you through using the Chrome DevTools Recorder to create a synthetic transaction against a Splunk demonstration instance.\nThe exported JSON from the Chrome DevTools Recorder will then be used to create a Splunk Synthetic Monitoring Real Browser Test.\nIn addition, you will also get to learn other Splunk Synthetic Monitoring checks like API Test and Uptime Test.\nPre-requisites Google Chrome Browser installed Access to Splunk Observability Cloud",
    "description": "Introduction This workshop walks you through using the Chrome DevTools Recorder to create a synthetic transaction against a Splunk demonstration instance.\nThe exported JSON from the Chrome DevTools Recorder will then be used to create a Splunk Synthetic Monitoring Real Browser Test.\nIn addition, you will also get to learn other Splunk Synthetic Monitoring checks like API Test and Uptime Test.\nPre-requisites Google Chrome Browser installed Access to Splunk Observability Cloud",
    "tags": [],
    "title": "1. Real Browser Test",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/1-real-browser-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Advanced Synthetics",
    "content": "Write down a short user journey you want to test. Remember: smaller bites are easier to chew! In other words, get started with just a few steps. This is easier not only to create and maintain the test, but also to understand and act on the results. Test the essential features to your users, like a support contact form, login widget, or date picker.\nNote Record the test in the same type of viewport that you want to run it. For example, if you want to run a test on a mobile viewport, narrow your browser width to mobile and refresh before starting the recording. This way you are capturing the correct elements that could change depending on responsive style rules.\nOpen your starting URL in Chrome Incognito. This is important so you’re not carrying cookies into the recording, which we won’t set up in the Synthetic test by default. If you workshop instructor does not have a custom URL, feel free to use https://online-boutique-eu.splunko11y.com or https://online-boutique-us.splunko11y.com, which are in the examples below.\nOpen the Chrome DevTools Recorder Next, open the Developer Tools (in the new tab that was opened above) by pressing Ctrl + Shift + I on Windows or Cmd + Option + I on a Mac, then select Recorder from the top-level menu or the More tools flyout menu.\nNote Site elements might change depending on viewport width. Before recording, set your browser window to the correct width for the test you want to create (Desktop, Tablet, or Mobile). Change the DevTools “dock side” to pop out as a separate window if it helps.\nCreate a new recording With the Recorder panel open in the DevTools window. Click on the Create a new recording button to start.\nFor the Recording Name use your initials to prefix the name of the recording e.g. \u003cyour initials\u003e - \u003cwebsite name\u003e. Click on Start Recording to start recording your actions.\nNow that we are recording, complete a few actions on the site. An example for our demo site is:\nClick on Vintage Camera Lens Click on Add to Cart Click on Place Order Click on End recording in the Recorder panel. Export the recording Click on the Export button:\nSelect JSON as the format, then click on Save\nCongratulations! You have successfully created a recording using the Chrome DevTools Recorder. Next, we will use this recording to create a Real Browser Test in Splunk Synthetic Monitoring.\nView the example JSON file for this browser test recording { \"title\": \"RWC - Online Boutique\", \"steps\": [ { \"type\": \"setViewport\", \"width\": 1430, \"height\": 1016, \"deviceScaleFactor\": 1, \"isMobile\": false, \"hasTouch\": false, \"isLandscape\": false }, { \"type\": \"navigate\", \"url\": \"https://online-boutique-eu.splunko11y.com/\", \"assertedEvents\": [ { \"type\": \"navigation\", \"url\": \"https://online-boutique-eu.splunko11y.com/\", \"title\": \"Online Boutique\" } ] }, { \"type\": \"click\", \"target\": \"main\", \"selectors\": [ [ \"div:nth-of-type(2) \u003e div:nth-of-type(2) a \u003e div\" ], [ \"xpath//html/body/main/div/div/div[2]/div[2]/div/a/div\" ], [ \"pierce/div:nth-of-type(2) \u003e div:nth-of-type(2) a \u003e div\" ] ], \"offsetY\": 170, \"offsetX\": 180, \"assertedEvents\": [ { \"type\": \"navigation\", \"url\": \"https://online-boutique-eu.splunko11y.com/product/66VCHSJNUP\", \"title\": \"\" } ] }, { \"type\": \"click\", \"target\": \"main\", \"selectors\": [ [ \"aria/ADD TO CART\" ], [ \"button\" ], [ \"xpath//html/body/main/div[1]/div/div[2]/div/form/div/button\" ], [ \"pierce/button\" ], [ \"text/Add to Cart\" ] ], \"offsetY\": 35.0078125, \"offsetX\": 46.4140625, \"assertedEvents\": [ { \"type\": \"navigation\", \"url\": \"https://online-boutique-eu.splunko11y.com/cart\", \"title\": \"\" } ] }, { \"type\": \"click\", \"target\": \"main\", \"selectors\": [ [ \"aria/PLACE ORDER\" ], [ \"div \u003e div \u003e div.py-3 button\" ], [ \"xpath//html/body/main/div/div/div[4]/div/form/div[4]/button\" ], [ \"pierce/div \u003e div \u003e div.py-3 button\" ], [ \"text/Place order\" ] ], \"offsetY\": 29.8125, \"offsetX\": 66.8203125, \"assertedEvents\": [ { \"type\": \"navigation\", \"url\": \"https://online-boutique-eu.splunko11y.com/cart/checkout\", \"title\": \"\" } ] } ] }",
    "description": "Write down a short user journey you want to test. Remember: smaller bites are easier to chew! In other words, get started with just a few steps. This is easier not only to create and maintain the test, but also to understand and act on the results. Test the essential features to your users, like a support contact form, login widget, or date picker.\nNote Record the test in the same type of viewport that you want to run it. For example, if you want to run a test on a mobile viewport, narrow your browser width to mobile and refresh before starting the recording. This way you are capturing the correct elements that could change depending on responsive style rules.",
    "tags": [],
    "title": "Record a test",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/3-advanced-synthetics/1-recording-a-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 5. Splunk RUM",
    "content": "In Splunk Observability Cloud from the main menu, click on RUM. you arrive at the RUM Home page, this view has already been covered in the short introduction earlier.\nExercise Make sure you select your workshop by ensuring the drop-downs are set/selected as follows: The Time frame is set to -15m. The Environment selected is [NAME OF WORKSHOP]-workshop. The App selected is [NAME OF WORKSHOP]-store. The Source is set to All. Next, click on the [NAME OF WORKSHOP]-store above the Page Views / JavaScript Errors chart. This will bring up a new dashboard view breaking down the metrics by UX Metrics, Front-end Health, Back-end Health and Custom Events and comparing them to historic metrics (1 hour by default). UX Metrics: Page Views, Page Load and Web Vitals metrics. Front-end Health: Breakdown of Javascript Errors and Long Task duration and count. Back-end Health: Network Errors and Requests and Time to First Byte. Custom Events: RED metrics (Rate, Error \u0026 Duration) for custom events. Exercise Click through each of the tabs (UX Metrics, Front-end Health, Back-end Health and Custom Events) and examine the data. ​ Question Answer If you examine the charts in the Custom Events Tab, what chart shows clearly the latency Spikes?\nIt is the Custom Event Latency chart",
    "description": "In Splunk Observability Cloud from the main menu, click on RUM. you arrive at the RUM Home page, this view has already been covered in the short introduction earlier.\nExercise Make sure you select your workshop by ensuring the drop-downs are set/selected as follows: The Time frame is set to -15m. The Environment selected is [NAME OF WORKSHOP]-workshop. The App selected is [NAME OF WORKSHOP]-store. The Source is set to All. Next, click on the [NAME OF WORKSHOP]-store above the Page Views / JavaScript Errors chart. This will bring up a new dashboard view breaking down the metrics by UX Metrics, Front-end Health, Back-end Health and Custom Events and comparing them to historic metrics (1 hour by default).",
    "tags": [],
    "title": "1. RUM Dashboard",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/5-rum/1-rum-dashboard/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Lambda Tracing",
    "content": "Prerequisites Observability Workshop Instance The Observability Workshop uses the Splunk4Ninjas - Observability workshop template in Splunk Show, which provides a pre-configured EC2 instance running Ubuntu.\nYour workshop instructor will provide you with the credentials to your assigned workshop instance.\nYour instance should have the following environment variables already set:\nACCESS_TOKEN REALM These are the Splunk Observability Cloud Access Token and Realm for your workshop. They will be used by the OpenTelemetry Collector to forward your data to the correct Splunk Observability Cloud organization. Note Alternatively, you can deploy a local observability workshop instance using Multipass.\nAWS Command Line Interface (awscli) The AWS Command Line Interface, or awscli, is an API used to interact with AWS resources. In this workshop, it is used by certain scripts to interact with the resource you’ll deploy.\nYour Splunk-issued workshop instance should already have the awscli installed.\nCheck if the aws command is installed on your instance with the following command:\nwhich aws The expected output would be /usr/local/bin/aws If the aws command is not installed on your instance, run the following command:\nsudo apt install awscli Terraform Terraform is an Infrastructure as Code (IaC) platform, used to deploy, manage and destroy resource by defining them in configuration files. Terraform employs HCL to define those resources, and supports multiple providers for various platforms and technologies.\nWe will be using Terraform at the command line in this workshop to deploy the following resources:\nAWS API Gateway Lambda Functions Kinesis Stream CloudWatch Log Groups S3 Bucket and other supporting resources Your Splunk-issued workshop instance should already have terraform installed.\nCheck if the terraform command is installed on your instance:\nwhich terraform The expected output would be /usr/local/bin/terraform If the terraform command is not installed on your instance, follow Terraform’s recommended installation commands listed below:\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update \u0026\u0026 sudo apt install terraform Workshop Directory (lambda) The Workshop Directory lambda is a repository that contains all the configuration files and scripts to complete both the auto-instrumentation and manual instrumentation of the example Lambda-based application we will be using today.\nConfirm you have the workshop directory in your home directory: cd ~/workshop \u0026\u0026 ls The expected output would include lambda AWS \u0026 Terraform Variables AWS Note to the workshop instructor: create a new user in the target AWS account called lambda-workshop-user. Ensure it has full permissions to perform the required actions via Terraform. Create an access token for the lambda-workshop-user user and share the Access Key ID and Secret Access Key with the workshop participants. Delete the user when the workshop is complete.\nThe AWS CLI requires that you have credentials to be able to access and manage resources deployed by their services. Both Terraform and the Python scripts in this workshop require these variables to perform their tasks.\nConfigure the awscli with the access key ID, secret access key and region for this workshop:\naws configure This command should provide a prompt similar to the one below: AWS Access Key ID [None]: XXXXXXXXXXXXXXXX AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Default region name [None]: us-east-1 Default outoput format [None]: If the awscli is not configured on your instance, run the following command and provide the values your instructor would provide you with.\naws configure Create an IAM Role (Workshop Instructor Only) Note to the workshop instructor: This step only needs to be completed once, as the IAM role created in this step will be shared by all workshop participants:\ncd ~/workshop/lambda/iam_role terraform init terraform plan terraform apply Note to the workshop instructor: After the workshop is complete, cleanup the role as follows:\ncd ~/workshop/lambda/iam_role terraform destroy Terraform Terraform supports the passing of variables to ensure sensitive or dynamic data is not hard-coded in your .tf configuration files, as well as to make those values reusable throughout your resource definitions.\nIn our workshop, Terraform requires variables necessary for deploying the Lambda functions with the right values for the OpenTelemetry Lambda layer; For the ingest values for Splunk Observability Cloud; And to make your environment and resources unique and immediatley recognizable.\nTerraform variables are defined in the following manner:\nDefine the variables in your main.tf file or a variables.tf Set the values for those variables in either of the following ways: setting environment variables at the host level, with the same variable names as in their definition, and with TF_VAR_ as a prefix setting the values for your variables in a terraform.tfvars file passing the values as arguments when running terraform apply We will be using a combination of variables.tf and terraform.tfvars files to set our variables in this workshop.\nUsing either vi or nano, open the terraform.tfvars file in either the auto or manual directory vi ~/workshop/lambda/auto/terraform.tfvars Set the variables with their values. Replace the CHANGEME placeholders with those provided by your instructor. o11y_access_token = \"CHANGEME\" o11y_realm = \"CHANGEME\" otel_lambda_layer = [\"CHANGEME\"] prefix = \"CHANGEME\" Ensure you change only the placeholders, leaving the quotes and brackets intact, where applicable. _For the otel_lambda_layer, use the value for us-east-1 found here The prefix is a unique identifier you can choose for yourself, to make your resources distinct from other participants’ resources. We suggest using a short form of your name, for example. Also, please only lowercase letters for the prefix. Certain resources in AWS, such as S3, would through an error if you use uppercase letters. Save your file and exit the editor. Finally, copy the terraform.tfvars file you just edited to the other directory. cp ~/workshop/lambda/auto/terraform.tfvars ~/workshop/lambda/manual We do this as we will be using the same values for both the autoinstrumentation and manual instrumentation protions of the workshop File Permissions While all other files are fine as they are, the send_message.py script in both the auto and manual will have to be executed as part of our workshop. As a result, it needs to have the appropriate permissions to run as expected. Follow these instructions to set them.\nFirst, ensure you are in the lambda directory:\ncd ~/workshop/lambda Next, run the following command to set executable permissions on the send_message.py script:\nsudo chmod 755 auto/send_message.py manual/send_message.py Now that we’ve squared off the prerequisites, we can get started with the workshop!",
    "description": "Prerequisites Observability Workshop Instance The Observability Workshop uses the Splunk4Ninjas - Observability workshop template in Splunk Show, which provides a pre-configured EC2 instance running Ubuntu.\nYour workshop instructor will provide you with the credentials to your assigned workshop instance.\nYour instance should have the following environment variables already set:\nACCESS_TOKEN REALM These are the Splunk Observability Cloud Access Token and Realm for your workshop. They will be used by the OpenTelemetry Collector to forward your data to the correct Splunk Observability Cloud organization. Note Alternatively, you can deploy a local observability workshop instance using Multipass.",
    "tags": [],
    "title": "Setup",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/6-lambda-kinesis/1-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Lambda Tracing and Kinesis",
    "content": "This lab will make a tracing superhero out of you!\nIn this lab you will learn how a distributed trace is constructed for a small serverless application that runs on AWS Lambda, producing and consuming your message via AWS Kinesis.\nPre-requisites You should already have the lab content available on your EC2 lab host.\nEnsure that this lab’s required folder o11y-lambda-lab is on your home directory:\n​ Command Output cd ~ \u0026\u0026 ls o11y-lambda-lab Note If you don’t see it, fetch the lab contents by running the following command:\ngit clone https://github.com/kdroukman/o11y-lambda-lab.git Set Environment Variables In your Splunk Observability Cloud Organisation (Org) obtain your Access Token and Realm Values.\nPlease reset your environment variables from the earlier lab. Take care that for this lab we may be using different names - make sure to match the Environment Variable names below.\n​ Export Environment Variables export ACCESS_TOKEN=CHANGE_ME \\ export REALM=CHANGE_ME \\ export PREFIX=$INSTANCE Update Auto-instrumentation serverless template Update your auto-instrumentation Serverless template to include new values from the Enviornment variables.\n​ Substitute Environment Variables cat ~/o11y-lambda-lab/auto/serverless_unset.yml | envsubst \u003e ~/o11y-lambda-lab/auto/serverless.yml Examine the output of the updated serverless.yml contents (you may need to scroll up to the relevant section).\n​ Check file contents Expected Content cat ~/o11y-lambda-lab/auto/serverless.yml # USER SET VALUES ===================== custom: accessToken: \u003cupdated to your Access Token\u003e realm: \u003cupdated to your Realm\u003e prefix: \u003cupdated to your Hostname\u003e #====================================== Update Manual instrumentation template Update your manual instrumentation Serverless template to include new values from the Enviornment variables.\n​ Substitute Environment Variables cat ~/o11y-lambda-lab/manual/serverless_unset.yml | envsubst \u003e ~/o11y-lambda-lab/manual/serverless.yml Examine the output of the updated serverless.yml contents (you may need to scroll up to the relevant section).\n​ Check file contents Expected Content cat ~/o11y-lambda-lab/manual/serverless.yml # USER SET VALUES ===================== custom: accessToken: \u003cupdated to your Access Token\u003e realm: \u003cupdated to your Realm\u003e prefix: \u003cupdated to your Hostname\u003e #====================================== Set your AWS Credentials You will be provided with AWS Access Key ID and AWS Secret Access Key values - substitue these values in place of AWS_ACCESS_KEY_ID and AWS_ACCESS_KEY_SECRET in the bellow command:\n​ Set AWS Credentials sls config credentials --provider aws --key AWS_ACCCESS_KEY_ID --secret AWS_ACCESS_KEY_SECRET This command will create a file ~/.aws/credentials with your AWS Credentials populated.\nNote that we are using sls here, which is a Serverless framework for developing and deploying AWS Lambda functions. We will be using this command throughout the lab.\nNow you are set up and ready go!",
    "description": "This lab will make a tracing superhero out of you!\nIn this lab you will learn how a distributed trace is constructed for a small serverless application that runs on AWS Lambda, producing and consuming your message via AWS Kinesis.\nPre-requisites You should already have the lab content available on your EC2 lab host.\nEnsure that this lab’s required folder o11y-lambda-lab is on your home directory:",
    "tags": [],
    "title": "Setup",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/6-lambda-kinesis/1-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Environment Setup - Mac Note If you wish to run this on Mac directly, you can see the instructions here in the appendix and skip the next section on Linux.\nAll installs must continue at setting up the app.\nEnvironment Setup - Linux You can skip past the EC2 configuration to Linux Software Requirements if you already have an Ec2 that meets the specifications below !!!!\nubuntu 22.04 Security Group Security Settings HTTP inbound open on 8010 All Traffic open outbound Linux Software Requirements: docker, docker-compose, git, maven sudo apt update sudo apt upgrade sudo apt install docker docker-compose maven",
    "description": "Environment Setup - Mac Note If you wish to run this on Mac directly, you can see the instructions here in the appendix and skip the next section on Linux.\nAll installs must continue at setting up the app.\nEnvironment Setup - Linux You can skip past the EC2 configuration to Linux Software Requirements if you already have an Ec2 that meets the specifications below !!!!",
    "tags": [],
    "title": "Setting up your AWS Instance",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/1-setup-os/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 8. Splunk Synthetics",
    "content": "In Splunk Observability Cloud from the main menu, click on Synthetics. Click on All or Browser tests to see the list of active tests.\nDuring our investigation in the RUM section, we found there was an issue with the Place Order Transaction. Let’s see if we can confirm this from the Synthetics test as well. We will be using the metric First byte time for the 4th page of the test, which is the Place Order step.\nExercise In the Search box enter [WORKSHOP NAME] and select the test for your workshop (your instructor will advise as to which one to select). Under Performance KPIs set the Time Picker to Last 1 hour and hit enter. Click on Location and from the drop-down select Page. The next filter will be populated with the pages that are part of the test. Click on Duration, deselect Duration and select First byte time. Look at the legend and note the color of First byte time - Page 4. Select the highest data point for First byte time - Page 4. You will now be taken to the Run results for this particular test run.",
    "description": "In Splunk Observability Cloud from the main menu, click on Synthetics. Click on All or Browser tests to see the list of active tests.\nDuring our investigation in the RUM section, we found there was an issue with the Place Order Transaction. Let’s see if we can confirm this from the Synthetics test as well. We will be using the metric First byte time for the 4th page of the test, which is the Place Order step.",
    "tags": [],
    "title": "1. Synthetics Dashboard",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/8-synthetics/1-synthetics-dashboard/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Detectors",
    "content": "Why would we want a detector on a single Synthetic test? Some examples:\nThe endpoint, API transaction, or browser journey is highly critical We have deployed code changes and want to know if the resulting KPI is or is not as we expect We need to temporarily keep a close eye on a specific change we are testing and don’t want to create a lot of noise, and will disable the detector later We want to know about unexpected issues before a real user encounters them On the test overview page, click Create Detector on the top right. Name the detector with your team name and your initials and LCP (the signal we will eventually use), so that the instructor can better keep track of everyone’s progress.\nChange the signal to First byte time. Change the alert details, and see how the chart to the right shows the amount of alert events under those conditions. This is where you can decide how much alert noise you want to generate, based on how much your team tolerates. Play with the settings to see how they affect estimated alert noise. Now, change the signal to Largest contentful paint. This is a key web vital related to the user experience as it relates to loading time. Change the threshold to 2500ms. It’s okay if there is no sample alert event in the detector preview.\nScroll down in this window to see the notification options, including severity and recipients. Click the notifications link to customize the alert subject, message, tip, and runbook link. When you are happy with the amount of alert noise this detector would generate, click Activate.",
    "description": "Why would we want a detector on a single Synthetic test? Some examples:\nThe endpoint, API transaction, or browser journey is highly critical We have deployed code changes and want to know if the resulting KPI is or is not as we expect We need to temporarily keep a close eye on a specific change we are testing and don’t want to create a lot of noise, and will disable the detector later We want to know about unexpected issues before a real user encounters them On the test overview page, click Create Detector on the top right.",
    "tags": [],
    "title": "Test Detectors",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/5-detectors/1-test-detector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk APM",
    "content": "Ninja Deploy the Online Boutique application in Kubernetes Verify the application is running Generate some artificial traffic using Locust See APM metrics in the UI Ninja - Deploy the Online Boutique 1. Check your EC2 server This workshop module assumes you are completing this after you have run the IM workshop, and still have access to your EC2 instance.\nIf this is the case, continue with Deploy Online Boutique, otherwise, if you have received a fresh instance, please run the first two (2) sections of Deploy the OTel Collector to get the system ready for the APM workshop, then continue with the next section.\n2. Deploy Online Boutique To deploy the Online Boutique application into K3s apply the deployment:\n​ Deploy Online Boutique Deployment Output cd ~/workshop/apm kubectl apply -f deployment.yaml APM Only Deployment deployment.apps/recommendationservice created service/recommendationservice created deployment.apps/productcatalogservice created service/productcatalogservice created deployment.apps/cartservice created service/cartservice created deployment.apps/adservice created service/adservice created deployment.apps/paymentservice created service/paymentservice created deployment.apps/loadgenerator created service/loadgenerator created deployment.apps/shippingservice created service/shippingservice created deployment.apps/currencyservice created service/currencyservice created deployment.apps/redis-cart created service/redis-cart created deployment.apps/checkoutservice created service/checkoutservice created deployment.apps/frontend created service/frontend created service/frontend-external created deployment.apps/emailservice created service/emailservice created deployment.apps/rum-loadgen-deployment created In case of a message about a VARIABLE being unset Delete the deployment running kubectl delete -f deployment.yaml.\nThen, export the variable as described in the guide/message, followed by re-running the deployment script above.\nTo ensure the Online Boutique application is running:\n​ Get Pods Get Pods Output kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-849cf595bf-l7mnq 1/1 Running 0 31m splunk-otel-collector-agent-pxrgp 2/2 Running 0 31m productcatalogservice-8464cd56d-n8f89 1/1 Running 0 1m redis-cart-bcf44df97-djv6z 1/1 Running 0 1m checkoutservice-8558fd7b95-b9pn8 1/1 Running 0 1m shippingservice-7cc4bdd6f4-xsvnx 1/1 Running 0 1m recommendationservice-647d57fd44-l7tkq 1/1 Running 0 1m frontend-66c5d589d-55vzb 1/1 Running 0 1m emailservice-6ff5bbd67d-pdcm2 1/1 Running 0 1m paymentservice-6866558995-8xmf2 1/1 Running 0 1m currencyservice-8668d75d6f-mr68h 1/1 Running 0 1m rum-loadgen-deployment-58ccf7bd8f-cr4pr 1/1 Running 0 1m rum-loadgen-deployment-58ccf7bd8f-qjr4b 1/1 Running 0 1m rum-loadgen-deployment-58ccf7bd8f-fvb4x 1/1 Running 0 1m cartservice-7b58c88c45-xvxhq 1/1 Running 0 1m loadgenerator-6bdc7b4857-9kxjd 1/1 Running 2 (49s ago) 1m adservice-7b68d5b969-89ft2 1/1 Running 0 1m Info Usually it should only take around 1min 30secs for the pods to transition into a Running state.\nValidate Online Boutique is deployed In the Splunk UI click on Infrastructure this will bring you to the Infrastructure Overview dashboard, then click on Kubernetes.\nUse the Cluster dropdown to select the cluster name, you will see the new pods started and containers deployed. When you click on your Cluster in the Splunk UI you should have a view that looks like below:\nIf you select the WORKLOADS tab again you should now see that there are some Deployments and ReplicaSets:\nVisit the Online Boutique Ninja - Visit the Online Boutique you just deployed Details The Online Boutique is viewable on port 81 of the EC2 instance’s IP address. The IP address is the one you used to SSH into the instance at the beginning of the workshop.\nOpen your web browser and go to http://\u003cec2-ip-address\u003e:81/ where you will then be able to see the Online Boutique running.\nYour Workshop instructor will provide you with a URL to access the Online Boutique. Enter this URL into your browser and you will see the Online Boutique homepage.",
    "description": "Verify the Online Boutique application is deployed into Kubernetes (K3s) and generates some artificial traffic using a Load Generator (Locust).",
    "tags": [],
    "title": "1. The Online Boutique",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/2-apm/online-boutique/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics",
    "content": "Introduction The simplest way to keep an eye on endpoint availability is with an Uptime test. This lightweight test can run internally or externally around the world, as frequently as every minute. Because this is the easiest (and cheapest!) test to set up, and because this is ideal for monitoring availability of your most critical enpoints and ports, let’s start here.\nPre-requisites Publicly accessible HTTP(S) endpoint(s) to test Access to Splunk Observability Cloud",
    "description": "Introduction The simplest way to keep an eye on endpoint availability is with an Uptime test. This lightweight test can run internally or externally around the world, as frequently as every minute. Because this is the easiest (and cheapest!) test to set up, and because this is ideal for monitoring availability of your most critical enpoints and ports, let’s start here.\nPre-requisites Publicly accessible HTTP(S) endpoint(s) to test Access to Splunk Observability Cloud",
    "tags": [],
    "title": "Uptime Test",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/1-uptime/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Workshop Setup",
    "content": "Configure your Org using SWiPE Important Information SWiPE does not provision EC2 instances. These instances are provisioned separately using Splunk Show.\nSWiPE is an online tool designed to help you configure a workshop environment in Splunk Observability Cloud. You can access SWiPE here.\nSWiPE will perform the following tasks your workshop environment:\nCreate and Invite Users to the Organization\nUpload a .csv file containing the email addresses (one per line), or copy and paste the email addresses directly (one per line). NOTE: Users will immediately be sent an email invitation to join the organization. Configure Admin Access for Ninja Workshops\nIf the Ninja Workshop option is enabled, all users will be provisioned with Admin access control. Configure a Custom HEC URL and Token (Optional)\nIf Override HEC URL and Token is enabled, you can configure a custom HTTP Event Collector (HEC) URL and token. This overrides the default values used for sending logs to Splunk Cloud. Create a Team and Add Users\nSWiPE will create a team and automatically add all the users. Generate a SWiPE ID\nSWiPE will create a unique SWiPE ID for your workshop. You will need to copy this ID and use it when provisioning workshop instances in Splunk Show. Workshops with more than 40 users If your workshop has more than 40 users, we recommend informing the support team in advance. This ensures that the trial or workshop environment is properly scaled to handle the load.",
    "description": "Configure your Org using SWiPE Important Information SWiPE does not provision EC2 instances. These instances are provisioned separately using Splunk Show.\nSWiPE is an online tool designed to help you configure a workshop environment in Splunk Observability Cloud. You can access SWiPE here.\nSWiPE will perform the following tasks your workshop environment:\nCreate and Invite Users to the Organization",
    "tags": [],
    "title": "1. Using SWiPE",
    "uri": "/observability-workshop/v6.5/en/workshop-setup/1-swipe/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 8. Real User Monitoring",
    "content": "Lets start with a quick, high level tour of RUM by clicking RUM in the left-hand menu. Then change the Environment filter (1) to the name of your workshop instance from the dropdown box, selecting \u003cINSTANCE\u003e-workshop (1) (where INSTANCE is the value from the shell script you ran earlier). Make sure it is the only one selected.\nThen change the App (2) dropdown box to the name of your app, it will be \u003cINSTANCE\u003e-store\nOnce you have selected your Environment and App, you will see an overview page showing the RUM status of your Application. (If your Summary Dashboard is just a single row of numbers, you are looking at the condensed view. You can expand it by clicking on the \u003e (1) in front of the Application name). If any JavaScript errors occurred, they will show up as shown below:\nTo continue, click on the blue link (with your workshop name) to get to the details page. This will bring up a new dashboard view breaking down the interactions by UX Metrics, Front-end Health, Back-end Health and Custom Events and comparing them to historic metrics (1 hour by default).\nNormally, you have only one line inside the first chart. Click on the link that relates to your Petclinic shop, http://198.19.249.202:81 in our example:\nThis will bring us to the Tag Spotlight page.",
    "description": "Lets start with a quick, high level tour of RUM by clicking RUM in the left-hand menu. Then change the Environment filter (1) to the name of your workshop instance from the dropdown box, selecting \u003cINSTANCE\u003e-workshop (1) (where INSTANCE is the value from the shell script you ran earlier). Make sure it is the only one selected.\nThen change the App (2) dropdown box to the name of your app, it will be \u003cINSTANCE\u003e-store",
    "tags": [],
    "title": "Select the RUM view for the Petclinic App",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/8-rum/1-rum-tour/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud",
    "content": "Introduction\nThe goal of this workshop is to give you hands-on experience troubleshooting an issue using Splunk Observability Cloud to identify its root cause. We’ve provided a fully instrumented microservices-based application that actually mimic a wire transfer workflow that is running on Kubernetes, which sends metrics, traces, and logs to Splunk Observability Cloud for real-time analysis.\nWho Should Attend?\nThis workshop is ideal for anyone looking to gain practical knowledge of Splunk Observability. It’s designed for individuals with little or no prior experience with the platform.\nWhat You’ll Need\nAll you need is your laptop and a browser with access to external websites. The workshop can be attended either in-person or via Zoom. If you don’t have the Zoom client installed, you can still join using your browser.\nWorkshop Overview\nIn this 3-hour session, we’ll cover the fundamentals of Splunk Observability—the only platform offering streaming analytics and NoSample Full Fidelity distributed tracing—in an interactive, hands-on setting. Here’s what you can expect:\nOpenTelemetry\nLearn why OpenTelemetry is essential for modern observability and how it enhances visibility into your systems.\nTour of the Splunk Observability User Interface\nTake a guided tour of Splunk Observability Cloud’s interface, where we’ll show you how to navigate the five key components: APM, Log Observer, and Infrastructure.\nSplunk Application Performance Monitoring (APM)\nGain end-to-end visibility of your customers’ request path using APM traces. You’ll explore how telemetry from various services is captured and visualized in Splunk Observability Cloud, helping you detect anomalies and errors.\nSplunk Log Observer (LO)\nLearn how to leverage the “Related Content” feature to easily navigate between components. In this case, we’ll move from an APM trace to the related logs for deeper insight into issues.\nBy the end of this session, you’ll have gained practical experience with Splunk Observability Cloud and a solid understanding of how to troubleshoot and resolve issues across your application stack.",
    "description": "Workshop Overview",
    "tags": [],
    "title": "Workshop Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/1-workshop-goals/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "Introduction\nThe goal of this workshop is to give you hands-on experience troubleshooting an issue using Splunk Observability Cloud to identify its root cause. We’ve provided a fully instrumented microservices-based application running on Kubernetes, which sends metrics, traces, and logs to Splunk Observability Cloud for real-time analysis.\nWho Should Attend?\nThis workshop is ideal for anyone looking to gain practical knowledge of Splunk Observability. It’s designed for individuals with little or no prior experience with the platform.\nWhat You’ll Need\nAll you need is your laptop and a browser with access to external websites. The workshop can be attended either in-person or via Zoom. If you don’t have the Zoom client installed, you can still join using your browser.\nWorkshop Overview\nIn this 3-hour session, we’ll cover the fundamentals of Splunk Observability—the only platform offering streaming analytics and NoSample Full Fidelity distributed tracing—in an interactive, hands-on setting. Here’s what you can expect:\nOpenTelemetry\nLearn why OpenTelemetry is essential for modern observability and how it enhances visibility into your systems.\nTour of the Splunk Observability User Interface\nTake a guided tour of Splunk Observability Cloud’s interface, where we’ll show you how to navigate the five key components: APM, RUM, Log Observer, Synthetics, and Infrastructure.\nGenerate Real User Data\nDive into a simulated retail experience on the Online Boutique Website. Using your browser, mobile, or tablet, explore the site and generate real user data that includes metrics (Is there a problem?), traces (Where is the problem?), and logs (What’s causing the problem?).\nSplunk Real User Monitoring (RUM)\nAnalyze the real user data collected from participants’ browser sessions. Your task is to identify poorly performing sessions and begin the troubleshooting process.\nSplunk Application Performance Monitoring (APM)\nGain end-to-end visibility by linking a RUM trace (front-end) to an APM trace (back-end). You’ll explore how telemetry from various services is captured and visualized in Splunk Observability Cloud, helping you detect anomalies and errors.\nSplunk Log Observer (LO)\nLearn how to leverage the “Related Content” feature to easily navigate between components. In this case, we’ll move from an APM trace to the related logs for deeper insight into issues.\nSplunk Synthetics\nDiscover how Synthetics can help with 24/7 monitoring of your application. We’ll walk you through setting up a simple synthetic test that runs every minute to monitor the performance and availability of the Online Boutique website.\nBy the end of this session, you’ll have gained practical experience with Splunk Observability Cloud and a solid understanding of how to troubleshoot and resolve issues across your application stack.",
    "description": "Workshop Overview",
    "tags": [],
    "title": "Workshop Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/1-workshop-goals/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "1. Exploring the Sample Data In the list of available dashboards, look for a group called Sample Data (2).\nThis group contains dashboards that showcase visualizations built from sample metrics. These will give you a sense of how different data can be represented using charts and dashboards in Splunk Observability.\nExpand the Sample Data dashboard group by clicking on it, then select the Sample Charts (3) dashboard.\nThis dashboard showcases a variety of chart types, styles, and formatting options available in Splunk Observability. It’s a great way to get a feel for how flexible and customizable your dashboards can be.\nThe sample data runs on a 10-minute cycle, generating different patterns and behaviors over time.\nTo see these changes in action, adjust the time range in the top-right corner of the dashboard to Last 15 minutes or, for a better overview, select Last 1 hour. This will help you observe how the data updates and cycles through different conditions.\nTake a moment to explore the charts in this dashboard, each one provides a different perspective on how sample data can be visualized.",
    "description": "1. Exploring the Sample Data In the list of available dashboards, look for a group called Sample Data (2).\nThis group contains dashboards that showcase visualizations built from sample metrics. These will give you a sense of how different data can be represented using charts and dashboards in Splunk Observability.\nExpand the Sample Data dashboard group by clicking on it, then select the Sample Charts (3) dashboard.",
    "tags": [],
    "title": "Locating our Sample Data",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-01-sample_data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "In this section, we’ll start by exploring how charts are built and displayed in Splunk Observability. By examining and interacting with an existing chart, you’ll get a feel for how the chart editor works—how data sources are selected, how visual options are configured, and how different settings shape what you see.\n1. Select a chart To get started, make sure you have the SAMPLE CHARTS dashboard open and adjust the time range in the top-right corner of the dashboard back to -5M for Last 5 Minutes or select reset to default\nFind the Latency histogram chart, then click on the three dots (…) (1) in the upper-right corner of the chart. From the menu, select Open (3). You can also simply click on the chart title (Latency histogram) (2) to open it directly.\nOnce the chart editor opens, you’ll see the configuration settings for the Latency histogram chart.\nThe chart editor is where you can control how your data is visualized. You can change the chart type, apply transformation functions, adjust time settings, and customize other visual and data-related options to match your specific needs.\nIn the Plot Editor (1) tab, under the Signal (2) section, you’ll find the metric currently being used: demo.trans.latency (3). This signal represents the latency data that the chart is plotting. You can use this area to edit or add additional signals to compare or enrich the visualization.\nYou’ll notice several Line plots displayed in the chart. The label 18 ts (4) indicates that the chart is currently plotting 18 individual metric time series. To explore different visualization styles, try clicking on the various chart type icons in the editor. As you hover over each icon, its name will appear—helping you understand what each type represents.\nFor example, click on the Heat Map icon to see how the same data is represented in a different format.\nNote You can visualize your metrics using a variety of chart types—choose the one that best represents the insights you want to highlight.\nFor a detailed overview of available chart types and when to use them, check out Choosing a chart type.",
    "description": "In this section, we’ll start by exploring how charts are built and displayed in Splunk Observability. By examining and interacting with an existing chart, you’ll get a feel for how the chart editor works—how data sources are selected, how visual options are configured, and how different settings shape what you see.\n1. Select a chart To get started, make sure you have the SAMPLE CHARTS dashboard open and adjust the time range in the top-right corner of the dashboard back to -5M for Last 5 Minutes or select reset to default",
    "tags": [],
    "title": "Exploring Charts",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-02-exploring_charts/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "In this section, we’ll start exploring how charts are structured by editing an existing one. This is a great way to get hands-on experience with the chart editor and understand how chart settings, data sources, and visualization options all come together.\nWith the Latency histogram chart open in the chart editor, let’s begin configuring it for our workshop.\nClick on the Line chart type icon (1) in the visualization pane to change the chart type. The data will now be displayed as a line plot.\n2. Changing the Chart’s Time Window You can adjust the time range for the chart to view more historical data. To do this, click on the Time (1) dropdown in the top-right corner of the chart editor and select Past 15 minutes (2).\nThis will expand the time window and allow you to see more of the metric trends over a longer period.\n3. Exploring the Data Table Click on the Data Table (1) tab in the chart editor.\nThe Data Table gives you a behind-the-scenes look at the metric time series powering the chart.\nEach row in the table represents a single time series, and each column shows a dimension associated with that series. For the metric demo.trans.latency, you’ll see the following dimensions:\ndemo_datacenter (2) demo_customer (3) demo_host (4) In the demo_datacenter column, you’ll notice there are two data centers: Paris (5) and Tokyo (6)\nEach has multiple associated time series.\nAs you move your cursor across the chart (horizontally through time), the Data Table updates in real time to reflect the values at the current time point. If you click on a specific line in the chart, it will pin that time series in the table, showing a fixed value—this is indicated with a pinned value (7).\nWhen you’re ready, click back on the Plot editor (8) tab to close the data table.\nNext, let’s save this chart to a dashboard, so we can reuse it later!",
    "description": "In this section, we’ll start exploring how charts are structured by editing an existing one. This is a great way to get hands-on experience with the chart editor and understand how chart settings, data sources, and visualization options all come together.\nWith the Latency histogram chart open in the chart editor, let’s begin configuring it for our workshop.\nClick on the Line chart type icon (1) in the visualization pane to change the chart type. The data will now be displayed as a line plot.",
    "tags": [],
    "title": "Editing Charts",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-03-editing-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "Once you’ve customized a chart to fit your needs, the next step is to save it as part of a dashboard. Saving your work lets you reuse, share, and monitor key visualizations over time. In this section, you’ll learn how to name and describe your chart, and how to add it to a dashboard for easy access later.\n1. Saving a Chart To begin saving your chart, let’s give it a clear name and description.\nClick on the chart title, currently labeled Copy of Latency Histogram, and rename it to “Active Latency” (1).\nNext, update the chart description. Click on the existing text, Spread of latency values across time, and change it to: Overview of latency values in real-time. (2)\nThese updates help make the chart easier to identify and understand when it’s part of a larger dashboard or shared with others.\nClick the Save As (3) button to begin the saving process. The chart will use the name Active Latency that you set earlier, but you can update the name here if needed.\nOnce you’re ready, click the Ok (1) button to confirm and continue.\n2. Creating a dashboard Now that we’re saving the chart, we need somewhere to store it—a dashboard.\nDashboards help organize and group related charts together, making it easier to monitor key metrics in one view. For this workshop, we’ll create a new dashboard to hold the charts we are building.\nIn the Choose dashboard dialog, click the New Dashboard (1) button.\nImportant: Do not select an existing dashboard—make sure to create a new one for this exercise.\nYou’ll now see the New Dashboard dialog, where you can configure the details of your new dashboard.\nStart by giving your dashboard a name. For this workshop, use the following format: YOUR_NAME-Dashboard Replace YOUR_NAME with your actual name to make your dashboard easy to identify.\nNext, update the permissions. Set them to Restricted Read and Write access, to ensure that only you (or specific users) can view and modify the dashboard. Make sure your user account is included and has both read and write access.\nYou should now see your own user account listed in the permissions, which means you are currently the only one who can edit this dashboard.\nIf needed, you can add additional users or teams by selecting them from the dropdown below.\nFor the purposes of this workshop, let’s remove the restrictions. Change the permissions setting to Everyone can Read and Write so that access isn’t limited during the session.\nNow, click the Save button to continue. Your new dashboard will be created and automatically selected, allowing you to save your chart directly into it\nMake sure your newly created dashboard is selected (1), then click the Ok button (2) to proceed.\nYou’ll now be taken to your dashboard. In the top-left corner, you’ll see that YOUR_NAME-DASHBOARD is part of a Dashboard Group is part of a dashboard group with the same name. You can add additional dashboards to this dashboard group to organize charts around different use cases, systems, or projects.",
    "description": "Once you’ve customized a chart to fit your needs, the next step is to save it as part of a dashboard. Saving your work lets you reuse, share, and monitor key visualizations over time. In this section, you’ll learn how to name and describe your chart, and how to add it to a dashboard for easy access later.\n1. Saving a Chart To begin saving your chart, let’s give it a clear name and description.",
    "tags": [],
    "title": "Saving Charts and Dashboards",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-04-saving-charts/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "1 Creating a New Chart Now let’s create a new chart and add it to the dashboard we’ve been working on!\nTo get started, click the plus icon (1) in the top-middle part of the interface. From the dropdown, select Chart (2). Alternatively, you can click the + New Chart Button (3) button to create a new chart directly.\nYou’ll now see a blank chart template, ready for configuration:\nLet’s begin by adding a metric to visualize. For this example, we’ll continue working with demo.trans.latency, the same metric we used earlier.\nIn the Plot Editor (1), go to the Signal (2) section, and enter demo.trans.latency(3) into the input field. This will load the latency time series data into the chart, so we can start building and customizing our visualization.\nYou should now see a familiar line chart displaying 18 time series (4). To view recent activity, change the time range to Past 15 minutes by selecting it from the Time dropdown (1)",
    "description": "1 Creating a New Chart Now let’s create a new chart and add it to the dashboard we’ve been working on!\nTo get started, click the plus icon (1) in the top-middle part of the interface. From the dropdown, select Chart (2). Alternatively, you can click the + New Chart Button (3) button to create a new chart directly.",
    "tags": [],
    "title": "Create New Chart",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-05-new-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "1. Filtering and Analytics Splunk Observability makes it easy to explore large volumes of metric data by using filters and analytics functions. Filters help you focus on specific segments of your data—such as a particular service, host, or location—while analytics functions let you transform and analyze that data for deeper insights.\nNow let’s narrow down our data to focus on the Paris data center, which will allow us to apply more targeted analytics. We’ll do this by using a Filter.\nStart by returning to the Plot Editor tab. Click the Add Filter button (2) as shown in the screenshot below.\nWait a moment for the available dimensions to populate. Then:\nSelect demo_datacenter (1) as the filter dimension. Choose Paris (2) as the value to filter on. This filter will limit the chart to show only the time series coming from the Paris data center, making your analysis more focused and meaningful.\nIn the F(x) (1) column of the chart editor, click the Add Analytics button o apply an analytic function. From the dropdown list, select Percentile (2), and then choose the Percentile:Aggregation (3) option. In the follow-up panel, leave the percentile value set to 90, which tells the chart to display the 90th percentile of the selected time series.\nIn this context, the 90th percentile represents the value below which 90% of the latency measurements fall, in other words, only the highest 10% of values exceed this point. This is a useful way to understand “worst-case normal” performance—filtering out occasional spikes while still showing when latency is approaching unacceptable levels.\nTo apply the function, simply click anywhere outside the panel to confirm your selection (4).\nFor info on the Percentile function and the other functions see Chart Analytics.",
    "description": "1. Filtering and Analytics Splunk Observability makes it easy to explore large volumes of metric data by using filters and analytics functions. Filters help you focus on specific segments of your data—such as a particular service, host, or location—while analytics functions let you transform and analyze that data for deeper insights.\nNow let’s narrow down our data to focus on the Paris data center, which will allow us to apply more targeted analytics. We’ll do this by using a Filter.",
    "tags": [],
    "title": "Using Filters \u0026 Analytics",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-06-filtering/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "1. Using Timeshift Analytical Function In many cases, it’s useful to compare current performance against historical data—for example, to identify trends, regressions, or improvements over time. The Timeshift function makes this easy by shifting a time series backward in time, letting you see past values side by side with the present.\nTo get started, clone Signal A by clicking the ... menu (1) next to it and selecting Clone (2) from the dropdown.\nCloning a signal creates an identical copy with the same metric, filters, and settings. This duplicate—Signal B—can then be used for further calculations or comparisons, such as applying a Timeshift to visualize what the metric looked like one week ago, without altering the original signal. After cloning, you’ll see a new signal labeled Signal B (1). Since it’s an exact copy of Signal A, both signals display the same data over the same time range. As a result, they appear to overlap completely on the chart, making it look like there’s only one line.\nTo make the comparison meaningful, we’ll apply a Timeshift to Signal B, shifting its data one week into the past. This allows us to see how current latency trends compare to those from the same time last week.\nIn the F(x) column next to Signal B, click the + (2), then choose the Timeshift (3) function from the list. When prompted, enter 1w (or 7d for 7 days) (4) as the time shift value. Click anywhere outside the panel (5) to confirm the change.\nThis will shift Signal B’s data one week into the past, allowing you to visually compare current latency trends with those from the same time last week.\nTo change the color of Signal B, click the ⚙️ settings icon (1) on the far right of its row to open the settings menu. Then, select a Plot Color, such as pink (2), to visually distinguish Signal B from the original signal on the chart. When you are done, click on theClose (3) You should now see two plots on the chart: the p90 of current latency (Signal A) shown in blue, and the p90 from one week ago (Signal B) shown in pink.\nTo make the difference between them easier to interpret, click the Area chart icon (1) to change the visualization style. This highlights the areas under the curves, making it easier to spot when last week’s latency was higher than the current values.\nNext, update the time range by clicking the field next to Time (2) in the Override bar and selecting Past Hour (3) from the dropdown. 2. Using Formulas Now let’s take it a step further by calculating the difference between two time-shifted metric values—for example, comparing today’s data with data from exactly one week ago.\nClick the Enter Formula button one line C (1), then type in A - B (2) to subtract Signal B from Signal A. This creates a new calculated signal, labeled C.\nTo focus only on the result of this formula, hide the other signals by clicking the eye icon next to A (3) and B (4), leaving only C visible.\nYou should now see a single line that represents the difference between the current metric values (A) and those from a week ago (B). In the chart, some values may appear negative—this happens when the older metric (B) was higher than the current one (A) at that point in time.\nNow that we’ve explored visual chart analytics, let’s take a look under the hood at the SignalFlow powering our charts and detectors in the next section!",
    "description": "1. Using Timeshift Analytical Function In many cases, it’s useful to compare current performance against historical data—for example, to identify trends, regressions, or improvements over time. The Timeshift function makes this easy by shifting a time series backward in time, letting you see past values side by side with the present.\nTo get started, clone Signal A by clicking the ... menu (1) next to it and selecting Clone (2) from the dropdown.",
    "tags": [],
    "title": "Using TimeShift \u0026 Formula's",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-07-timeshift/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "1. Introduction to SignalFlow Now let’s take a closer look at SignalFlow, the powerful analytics language behind Splunk Observability Cloud. SignalFlow enables you to define monitoring logic as code, offering a flexible and real-time way to work with metrics and automate alerting.\nAt the core of Splunk Infrastructure Monitoring is the SignalFlow analytics engine, which processes streaming metric data in real time. SignalFlow is written in a Python-like syntax and allows you to build computations that take in metric time series (MTS), perform transformations or calculations, and output new MTS.\nSome common use cases for SignalFlow include:\nComparing current values with historical trends, such as week-over-week comparisons Creating population-level insights using distributed percentile charts Monitoring rates of change or thresholds, such as detecting when a Service Level Objective (SLO) is breached Identifying correlated dimensions, like pinpointing which service is linked to an increase in low disk space alerts You can create SignalFlow based computations directly in the Chart Builder interface by selecting metrics and applying analytical functions visually. For more advanced use cases, you can also write and execute SignalFlow programs directly using the SignalFlow API.\nSignalFlow includes a robust set of built-in functions that operate on time series data—making it ideal for dynamic, real-time monitoring across complex systems.\nInfo For more information on SignalFlow see Analyze incoming data using SignalFlow.\n2. View SignalFlow In the Chart Builder, click on View SignalFlow, to open the underlying code that powers your chart.\nWhen you click View SignalFlow, you’ll see the SignalFlow program (1) that defines the logic and transformations behind your chart. This view gives you direct access to the code powering your visualization, allowing you to fine-tune or extend it beyond what’s possible in the visual editor.\nBelow is an example of the SignalFlow code for the chart we just created. This snippet shows how we defined the two percentile signals (current and one week ago), applied a timeshift, and calculated the difference between them. Reviewing the code helps clarify how each step contributes to the final chart.\n​ SignalFlow A = data('demo.trans.latency', filter=filter('demo_datacenter', 'Paris')).percentile(pct=95).publish(label='A', enable=False) B = data('demo.trans.latency', filter=filter('demo_datacenter', 'Paris')).percentile(pct=95).timeshift('1w').publish(label='B', enable=False) C = (A-B).publish(label='C') Click on View Builder (2) to go back to the Chart Builder UI.\nLet’s save this new chart to our Dashboard!",
    "description": "1. Introduction to SignalFlow Now let’s take a closer look at SignalFlow, the powerful analytics language behind Splunk Observability Cloud. SignalFlow enables you to define monitoring logic as code, offering a flexible and real-time way to work with metrics and automate alerting.\nAt the core of Splunk Infrastructure Monitoring is the SignalFlow analytics engine, which processes streaming metric data in real time. SignalFlow is written in a Python-like syntax and allows you to build computations that take in metric time series (MTS), perform transformations or calculations, and output new MTS.",
    "tags": [],
    "title": "Using SignalFlow",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-08-signalflow/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "1. Saving to an Existing Dashboard Before saving your chart, check the top-left corner to confirm that YOUR_NAME-Dashboard: YOUR_NAME-Dashboard (1) is selected. This ensures that your chart will be saved to the correct dashboard.\nNext, give your chart a name. Enter Latency History (2), and if you’d like, add a brief description in the Chart Description (3) if you wish like in our example. When you’re ready, click the Save And Close button (4). You’ll be returned to your dashboard, which now contains two charts. 2. Copy and Paste a chart Now let’s quickly add another chart by duplicating the one we just created.\nIn your dashboard, locate the Latency History chart and click the three dots ... (1) in the upper-right corner of the chart. From the menu, select Copy (2).\nAfter copying, you’ll notice a small white 1 appear in front of the + icon (3) at the top of the page. This indicates that one chart is ready to be pasted. Click the + icon ()1 at the top of the page, and in the dropdown menu, select (2).\nYou should also see a 1 at the end of that line, confirming that the copied chart is ready to be added. This will place a copy of the previous chart in your dashboard. 3. Edit the pasted chart To edit the duplicated chart, click the three dots ... on one of the Latency History charts in your dashboard and then select Open. Alternatively, you can click directly on the chart’s title, Latency History, to open it in the editor.\nThis will bring you to the editor environment again.\nStart by adjusting the time range in the top-right corner of the chart. Set it to Past 1 Hour (1) to give you a broader view of recent data.\nNext, let’s customize the chart to make it unique. Click the eye icon next to Signal A (2) to make it visible again.\nThen hide Signal C (3) by clicking its eye icon.\nUpdate the chart title from Latency history to Latency vs Load (4), and optionally add or edit the chart description to reflect the updated focus (5). Click on the Add Metric Or Event button to create a new signal. In the input field that appears, type and select demo.trans.count (1) to add it as Signal D.\nThis adds a new signal, Signal D, to your chart. It represents the number of active requests being processed.\nTo focus on the Paris data center, add a filter for demo_datacenter: Paris (2). Then, click the Configure Plot ⚙️ (3) button to adjust how the data is displayed. Change the rollup type from Auto (Delta) to Rate/sec (4) to show the rate of incoming requests per second.\nFinally, rename the signal from demo.trans.count to Latency vs Load (5) to reflect its role in the chart more clearly.\nFinally press the Save And Close button. This returns you to your dashboard that now has three different charts!\nLet’s add an “instruction” note and arrange the charts!",
    "description": "1. Saving to an Existing Dashboard Before saving your chart, check the top-left corner to confirm that YOUR_NAME-Dashboard: YOUR_NAME-Dashboard (1) is selected. This ensures that your chart will be saved to the correct dashboard.\nNext, give your chart a name. Enter Latency History (2), and if you’d like, add a brief description in the Chart Description (3) if you wish like in our example.",
    "tags": [],
    "title": "Adding charts to dashboards",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-09-adding-charts/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 1. Installation",
    "content": "Confirm the Collector is running The collector should now be running. We will verify this as root using systemctl command. To exit the status just press q.\n​ Command Status Output sudo systemctl status otelcol-contrib ● otelcol-contrib.service - OpenTelemetry Collector Contrib Loaded: loaded (/lib/systemd/system/otelcol-contrib.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2024-10-07 10:27:49 BST; 52s ago Main PID: 17113 (otelcol-contrib) Tasks: 13 (limit: 19238) Memory: 34.8M CPU: 155ms CGroup: /system.slice/otelcol-contrib.service └─17113 /usr/bin/otelcol-contrib --config=/etc/otelcol-contrib/config.yaml Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: Descriptor: Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: -\u003e Name: up Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: -\u003e Description: The scraping was successful Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: -\u003e Unit: Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: -\u003e DataType: Gauge Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: NumberDataPoints #0 Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: StartTimestamp: 1970-01-01 00:00:00 +0000 UTC Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: Timestamp: 2024-10-07 09:28:36.942 +0000 UTC Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: Value: 1.000000 Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: {\"kind\": \"exporter\", \"data_type\": \"metrics\", \"name\": \"debug\"} Because we will be making multiple configuration file changes, setting environment variables and restarting the collector, we need to stop the collector service and disable it from starting on boot.\n​ Command sudo systemctl stop otelcol-contrib \u0026\u0026 sudo systemctl disable otelcol-contrib Ninja: Build your own collector using Open Telemetry Collector Builder (ocb) For this part we will require the following installed on your system:\nGolang (latest version)\ncd /tmp wget https://golang.org/dl/go1.20.linux-amd64.tar.gz sudo tar -C /usr/local -xzf go1.20.linux-amd64.tar.gz Edit .profile and add the following environment variables:\nexport GOROOT=/usr/local/go export GOPATH=$HOME/go export PATH=$GOPATH/bin:$GOROOT/bin:$PATH Renew your shell session:\nsource ~/.profile Check Go version:\ngo version ocb installed\nDownload the ocb binary from the project releases and run the following commands:\nmv ocb_0.80.0_darwin_arm64 /usr/bin/ocb chmod 755 /usr/bin/ocb An alternative approach would be to use the golang tool chain to build the binary locally by doing:\ngo install go.opentelemetry.io/collector/cmd/builder@v0.80.0 mv $(go env GOPATH)/bin/builder /usr/bin/ocb (Optional) Docker\nWhy build your own collector? The default distribution of the collector (core and contrib) either contains too much or too little in what they have to offer.\nIt is also not advised to run the contrib collector in your production environments due to the amount of components installed which more than likely are not needed by your deployment.\nBenefits of building your own collector? When creating your own collector binaries, (commonly referred to as distribution), means you build what you need.\nThe benefits of this are:\nSmaller sized binaries Can use existing go scanners for vulnerabilities Include internal components that can tie in with your organization Considerations for building your collector? Now, this would not be a 🥷 Ninja zone if it didn’t come with some drawbacks:\nGo experience is recommended if not required No Splunk support Responsibility for distribution and lifecycle management It is important to note that the project is working towards stability but it does not mean changes made will not break your workflow. The team at Splunk provides increased support and a higher level of stability so they can provide a curated experience helping you with your deployment needs.\nThe Ninja Zone Once you have all the required tools installed to get started, you will need to create a new file named otelcol-builder.yaml and we will follow this directory structure:\n. └── otelcol-builder.yaml Once we have the file created, we need to add a list of components for it to install with some additional metadata.\nFor this example, we are going to create a builder manifest that will install only the components we need for the introduction config:\ndist: name: otelcol-ninja description: A custom build of the Open Telemetry Collector output_path: ./dist extensions: - gomod: go.opentelemetry.io/collector/extension/ballastextension v0.80.0 - gomod: go.opentelemetry.io/collector/extension/zpagesextension v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/extension/httpforwarder v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/extension/healthcheckextension v0.80.0 exporters: - gomod: go.opentelemetry.io/collector/exporter/loggingexporter v0.80.0 - gomod: go.opentelemetry.io/collector/exporter/otlpexporter v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/exporter/splunkhecexporter v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/exporter/signalfxexporter v0.80.0 processors: - gomod: go.opentelemetry.io/collector/processor/batchprocessor v0.80.0 - gomod: go.opentelemetry.io/collector/processor/memorylimiterprocessor v0.80.0 receivers: - gomod: go.opentelemetry.io/collector/receiver/otlpreceiver v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/jaegerreceiver v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/prometheusreceiver v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/zipkinreceiver v0.80.0 Once the yaml file has been updated for the ocb, then run the following command:\nocb --config=otelcol-builder.yaml Which leave you with the following directory structure:\n├── dist │ ├── components.go │ ├── components_test.go │ ├── go.mod │ ├── go.sum │ ├── main.go │ ├── main_others.go │ ├── main_windows.go │ └── otelcol-ninja └── otelcol-builder.yaml References https://opentelemetry.io/docs/collector/custom-collector/ Default configuration OpenTelemetry is configured through YAML files. These files have default configurations that we can modify to meet our needs. Let’s look at the default configuration that is supplied:\n​ Command config.yaml cat /etc/otelcol-contrib/config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 # To limit exposure to denial of service attacks, change the host in endpoints below from 0.0.0.0 to a specific network interface. # See https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/security-best-practices.md#safeguards-against-denial-of-service-attacks extensions: health_check: pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 opencensus: endpoint: 0.0.0.0:55678 # Collect own metrics prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 10s static_configs: - targets: ['0.0.0.0:8888'] jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_binary: endpoint: 0.0.0.0:6832 thrift_compact: endpoint: 0.0.0.0:6831 thrift_http: endpoint: 0.0.0.0:14268 zipkin: endpoint: 0.0.0.0:9411 processors: batch: exporters: debug: verbosity: detailed service: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [otlp, opencensus, prometheus] processors: [batch] exporters: [debug] logs: receivers: [otlp] processors: [batch] exporters: [debug] extensions: [health_check, pprof, zpages] Congratulations! You have successfully downloaded and installed the OpenTelemetry Collector. You are well on your way to becoming an OTel Ninja. But first let’s walk through configuration files and different distributions of the OpenTelemetry Collector.\nNote Splunk does provide its own, fully supported, distribution of the OpenTelemetry Collector. This distribution is available to install from the Splunk GitHub Repository or via a wizard in Splunk Observability Cloud that will build out a simple installation script to copy and paste. This distribution includes many additional features and enhancements that are not available in the OpenTelemetry Collector Contrib distribution.\nThe Splunk Distribution of the OpenTelemetry Collector is production-tested; it is in use by the majority of customers in their production environments. Customers that use our distribution can receive direct help from official Splunk support within SLAs. Customers can use or migrate to the Splunk Distribution of the OpenTelemetry Collector without worrying about future breaking changes to its core configuration experience for metrics and traces collection (OpenTelemetry logs collection configuration is in beta). There may be breaking changes to the Collector’s metrics. We will now walk through each section of the configuration file and modify it to send host metrics to Splunk Observability Cloud.",
    "description": "Confirm the Collector is running The collector should now be running. We will verify this as root using systemctl command. To exit the status just press q.\n​ Command Status Output sudo systemctl status otelcol-contrib ● otelcol-contrib.service - OpenTelemetry Collector Contrib Loaded: loaded (/lib/systemd/system/otelcol-contrib.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2024-10-07 10:27:49 BST; 52s ago Main PID: 17113 (otelcol-contrib) Tasks: 13 (limit: 19238) Memory: 34.8M CPU: 155ms CGroup: /system.slice/otelcol-contrib.service └─17113 /usr/bin/otelcol-contrib --config=/etc/otelcol-contrib/config.yaml Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: Descriptor: Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: -\u003e Name: up Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: -\u003e Description: The scraping was successful Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: -\u003e Unit: Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: -\u003e DataType: Gauge Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: NumberDataPoints #0 Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: StartTimestamp: 1970-01-01 00:00:00 +0000 UTC Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: Timestamp: 2024-10-07 09:28:36.942 +0000 UTC Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: Value: 1.000000 Oct 07 10:28:36 petclinic-rum-testing otelcol-contrib[17113]: {\"kind\": \"exporter\", \"data_type\": \"metrics\", \"name\": \"debug\"} Because we will be making multiple configuration file changes, setting environment variables and restarting the collector, we need to stop the collector service and disable it from starting on boot.",
    "tags": [],
    "title": "Installing OpenTelemetry Collector Contrib",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/1-installation/1-confirmation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics \u003e 1. Uptime Test",
    "content": "Open Synthetics Click the Add new test button on the right side of the screen, then select Uptime and HTTP test. Name your test with your team name (provided by your workshop instructor), your initials, and any other details you’d like to include, like geographic region.\nFor now let’s test a GET request. Fill in the URL field. You can use one of your own, or one of ours like https://online-boutique-eu.splunko11y.com, https://online-boutique-us.splunko11y.com, or https://www.splunk.com.\nClick Try now to validate that the endpoint is accessible before the selected location before saving the test. Try now does not count against your subscription usage, so this is a good practice to make sure you’re not wasting real test runs on a misconfigured test. Tip A common reason for Try now to fail is that there is a non-2xx response code. If that is expected, add a Validation for the correct response code. Add any additional validations needed, for example: response code, response header, and response size. Add and remove any locations you’d like. Keep in mind where you expect your endpoint to be available.\nChange the frequency to test your more critical endpoints more often, up to one minute. Make sure “Round-robin” is on so the test will run from one location at a time, rather than from all locations at once.\nIf an endpoint is highly critical, think about if it is worth it to have all locations tested at the same time every single minute. If you have automations built in with a webhook from a detector, or if you have strict SLAs you need to track, this could be worth it to have as much coverage as possible. But if you are doing more manual investigation, or if this is a less critical endpoint, you could be wasting test runs that are executing while an issue is being investigated. Remember that your license is based on the number of test runs per month. Turning Round-robin off will multiply the number of test runs by the number of locations you have selected. When you are ready for the test to start running, make sure “Active” is on, then scroll down and click Submit to save the test configuration. Now the test will start running with your saved configuration. Take a water break, then we’ll look at the results!",
    "description": "Open Synthetics Click the Add new test button on the right side of the screen, then select Uptime and HTTP test. Name your test with your team name (provided by your workshop instructor), your initials, and any other details you’d like to include, like geographic region.\nFor now let’s test a GET request. Fill in the URL field. You can use one of your own, or one of ours like https://online-boutique-eu.splunko11y.com, https://online-boutique-us.splunko11y.com, or https://www.splunk.com.",
    "tags": [],
    "title": "Creating a test",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/1-uptime/1-create-uptime/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 1. Agent Configuration",
    "content": "The OpenTelemetry Gateway serves as a central hub for receiving, processing, and exporting telemetry data. It sits between your telemetry sources (such as applications and services) and your observability backends like Splunk Observability Cloud.\nBy centralizing telemetry traffic, the gateway enables advanced features such as data filtering, enrichment, transformation, and routing to one or more destinations. It helps reduce the burden on individual services by offloading telemetry processing and ensures consistent, standardized data across distributed systems.\nThis makes your observability pipeline easier to manage, scale, and analyze—especially in complex, multi-service environments.\nExercise Open or create your second terminal window and name it Gateway. Navigate to the first exercise directory [WORKSHOP]/1-agent-gateway then check the contents of the gateway.yaml file.\nThis file outlines the core structure of the OpenTelemetry Collector as deployed in Gateway mode.\nUnderstanding the Gateway Configuration Let’s explore the gateway.yaml file that defines how the OpenTelemetry Collector is configured in Gateway mode during this workshop. This Gateway is responsible for receiving telemetry from the Agent, then processing and exporting it for inspection or forwarding.\nOTLP Receiver (Custom Port)\nreceivers: otlp: protocols: http: endpoint: \"0.0.0.0:5318\" The port 5318 matches the otlphttp exporter in the Agent configuration, ensuring that all telemetry data sent by the Agent is accepted by the Gateway.\nNote This separation of ports avoids conflicts and keeps responsibilities clear between agent and gateway roles.\nFile Exporters\nThe Gateway uses three file exporters to output telemetry data to local files. These exporters are defined as:\nexporters: # List of exporters debug: # Debug exporter verbosity: detailed # Enable detailed debug output file/traces: # Exporter Type/Name path: \"./gateway-traces.out\" # Path for OTLP JSON output for traces append: false # Overwrite the file each time file/metrics: # Exporter Type/Name path: \"./gateway-metrics.out\" # Path for OTLP JSON output for metrics append: false # Overwrite the file each time file/logs: # Exporter Type/Name path: \"./gateway-logs.out\" # Path for OTLP JSON output for logs append: false # Overwrite the file each time Each exporter writes a specific signal type to its corresponding file.\nThese files are created once the gateway is started and will be populated with real telemetry as the agent sends data. You can monitor these files in real time to observe the flow of telemetry through your pipeline.",
    "description": "The OpenTelemetry Gateway serves as a central hub for receiving, processing, and exporting telemetry data. It sits between your telemetry sources (such as applications and services) and your observability backends like Splunk Observability Cloud.\nBy centralizing telemetry traffic, the gateway enables advanced features such as data filtering, enrichment, transformation, and routing to one or more destinations. It helps reduce the burden on individual services by offloading telemetry processing and ensures consistent, standardized data across distributed systems.",
    "tags": [],
    "title": "1.1 Verify Gateway Configuration",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/1-agent-gateway/1-1-gateway/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 1. Agent Configuration",
    "content": "The OpenTelemetry Gateway serves as a central hub for receiving, processing, and exporting telemetry data. It sits between your telemetry sources (such as applications and services) and your observability backends like Splunk Observability Cloud.\nBy centralizing telemetry traffic, the gateway enables advanced features such as data filtering, enrichment, transformation, and routing to one or more destinations. It helps reduce the burden on individual services by offloading telemetry processing and ensures consistent, standardized data across distributed systems.\nThis makes your observability pipeline easier to manage, scale, and analyze—especially in complex, multi-service environments.\nExercise Open or create your second terminal window and name it Gateway. Navigate to the first exercise directory [WORKSHOP]/1-agent-gateway then check the contents of the gateway.yaml file.\nThis file outlines the core structure of the OpenTelemetry Collector as deployed in Gateway mode.\nUnderstanding the Gateway Configuration Let’s explore the gateway.yaml file that defines how the OpenTelemetry Collector is configured in Gateway mode during this workshop. This Gateway is responsible for receiving telemetry from the Agent, then processing and exporting it for inspection or forwarding.\nOTLP Receiver (Custom Port)\nreceivers: otlp: protocols: http: endpoint: \"0.0.0.0:5318\" The port 5318 matches the otlphttp exporter in the Agent configuration, ensuring that all telemetry data sent by the Agent is accepted by the Gateway.\nNote This separation of ports avoids conflicts and keeps responsibilities clear between agent and gateway roles.\nFile Exporters\nThe Gateway uses three file exporters to output telemetry data to local files. These exporters are defined as:\nexporters: # List of exporters debug: # Debug exporter verbosity: detailed # Enable detailed debug output file/traces: # Exporter Type/Name path: \"./gateway-traces.out\" # Path for OTLP JSON output for traces append: false # Overwrite the file each time file/metrics: # Exporter Type/Name path: \"./gateway-metrics.out\" # Path for OTLP JSON output for metrics append: false # Overwrite the file each time file/logs: # Exporter Type/Name path: \"./gateway-logs.out\" # Path for OTLP JSON output for logs append: false # Overwrite the file each time Each exporter writes a specific signal type to its corresponding file.\nThese files are created once the gateway is started and will be populated with real telemetry as the agent sends data. You can monitor these files in real time to observe the flow of telemetry through your pipeline.",
    "description": "The OpenTelemetry Gateway serves as a central hub for receiving, processing, and exporting telemetry data. It sits between your telemetry sources (such as applications and services) and your observability backends like Splunk Observability Cloud.\nBy centralizing telemetry traffic, the gateway enables advanced features such as data filtering, enrichment, transformation, and routing to one or more destinations. It helps reduce the burden on individual services by offloading telemetry processing and ensures consistent, standardized data across distributed systems.",
    "tags": [],
    "title": "1.1 Verify Gateway Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/1-agent-gateway/1-1-gateway/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 2. API Test",
    "content": "Global Variables View the global variable that we’ll use to perform our API test. Click on Global Variables under the cog. The global variable named env.encoded_auth will be the one that we’ll use to build the spotify API transaction.",
    "description": "Global Variables View the global variable that we’ll use to perform our API test. Click on Global Variables under the cog. The global variable named env.encoded_auth will be the one that we’ll use to build the spotify API transaction.",
    "tags": [],
    "title": "Global Variables",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/2-api-test/1-global-varilables/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 1. Real Browser Test",
    "content": "Open the starting URL Open the starting URL for the workshop in Chrome. Click on the appropriate link below to open the site in a new tab.\nNote The starting URL for the workshop is different for EMEA and AMER/APAC. Please use the correct URL for your region.\n​ EMEA Workshop URL AMER/APAC Workshop URL https://online-boutique-eu.splunko11y.com/\nhttps://online-boutique-us.splunko11y.com/\nOpen the Chrome DevTools Recorder Next, open the Developer Tools (in the new tab that was opened above) by pressing Ctrl + Shift + I on Windows or Cmd + Option + I on a Mac, then select Recorder from the top-level menu or the More tools flyout menu.\nNote Site elements might change depending on viewport width. Before recording, set your browser window to the correct width for the test you want to create (Desktop, Tablet, or Mobile). Change the DevTools “dock side” to pop out as a separate window if it helps.\nCreate a new recording With the Recorder panel open in the DevTools window. Click on the Create a new recording button to start.\nFor the Recording Name use your initials to prefix the name of the recording e.g. \u003cyour initials\u003e - Online Boutique. Click on Start Recording to start recording your actions.\nNow that we are recording, complete the following actions on the site:\nClick on Vintage Camera Lens Click on Add to Cart Click on Place Order Click on End recording in the Recorder panel. Exporting the recording Click on the Export button:\nSelect JSON as the format, then click on Save\nCongratulations! You have successfully created a recording using the Chrome DevTools Recorder. Next, we will use this recording to create a Real Browser Test in Splunk Synthetic Monitoring.\nClick here to view the JSON file { \"title\": \"RWC - Online Boutique\", \"steps\": [ { \"type\": \"setViewport\", \"width\": 1430, \"height\": 1016, \"deviceScaleFactor\": 1, \"isMobile\": false, \"hasTouch\": false, \"isLandscape\": false }, { \"type\": \"navigate\", \"url\": \"https://online-boutique-eu.splunko11y.com/\", \"assertedEvents\": [ { \"type\": \"navigation\", \"url\": \"https://online-boutique-eu.splunko11y.com/\", \"title\": \"Online Boutique\" } ] }, { \"type\": \"click\", \"target\": \"main\", \"selectors\": [ [ \"div:nth-of-type(2) \u003e div:nth-of-type(2) a \u003e div\" ], [ \"xpath//html/body/main/div/div/div[2]/div[2]/div/a/div\" ], [ \"pierce/div:nth-of-type(2) \u003e div:nth-of-type(2) a \u003e div\" ] ], \"offsetY\": 170, \"offsetX\": 180, \"assertedEvents\": [ { \"type\": \"navigation\", \"url\": \"https://online-boutique-eu.splunko11y.com/product/66VCHSJNUP\", \"title\": \"\" } ] }, { \"type\": \"click\", \"target\": \"main\", \"selectors\": [ [ \"aria/ADD TO CART\" ], [ \"button\" ], [ \"xpath//html/body/main/div[1]/div/div[2]/div/form/div/button\" ], [ \"pierce/button\" ], [ \"text/Add to Cart\" ] ], \"offsetY\": 35.0078125, \"offsetX\": 46.4140625, \"assertedEvents\": [ { \"type\": \"navigation\", \"url\": \"https://online-boutique-eu.splunko11y.com/cart\", \"title\": \"\" } ] }, { \"type\": \"click\", \"target\": \"main\", \"selectors\": [ [ \"aria/PLACE ORDER\" ], [ \"div \u003e div \u003e div.py-3 button\" ], [ \"xpath//html/body/main/div/div/div[4]/div/form/div[4]/button\" ], [ \"pierce/div \u003e div \u003e div.py-3 button\" ], [ \"text/Place order\" ] ], \"offsetY\": 29.8125, \"offsetX\": 66.8203125, \"assertedEvents\": [ { \"type\": \"navigation\", \"url\": \"https://online-boutique-eu.splunko11y.com/cart/checkout\", \"title\": \"\" } ] } ] }",
    "description": "Open the starting URL Open the starting URL for the workshop in Chrome. Click on the appropriate link below to open the site in a new tab.\nNote The starting URL for the workshop is different for EMEA and AMER/APAC. Please use the correct URL for your region.\n​ EMEA Workshop URL AMER/APAC Workshop URL https://online-boutique-eu.splunko11y.com/",
    "tags": [],
    "title": "1.1 Recording a test",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/1-real-browser-test/1-recording-a-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk OnCall \u003e 1. Getting Started",
    "content": "Aim The aim of this module is for you to complete the first step of Team configuration by adding users to your Team.\n1. Find your Team Navigate to the Teams tab on the main toolbar, you should find you that a Team has been created for you as part of the workshop pre-setup and you would have been informed of your Team Name via e-mail.\nIf you have found your pre-configured Team, skip Step 2. and proceed to Step 3. Configure Your Team. However, if you cannot find your allocated Team, you will need to create a new one, so proceed with Step 2. Create Team\n2. Create Team Only complete this step if you cannot find your pre-allocated Team as detailed in your workshop e-mail. Select Add Team, then enter your allocated team name, this will typically be in the format of “AttendeeID Workshop” and then save by clicking the Add Team button.\n3. Configure Your Team You now need to add other users to your team. If you are running this workshop using the Splunk provided environment, the following accounts are available for testing. If you are running this lab in your own environment, you will have been provided a list of usernames you can use in place of the table below.\nThese users are dummy accounts who will not receive notifications when they are on call.\nName Username Shift Duane Chow duanechow Europe Steven Gomez gomez Europe Walter White heisenberg Europe Jim Halpert jimhalpert Asia Lydia Rodarte-Quayle lydia Asia Marie Schrader marie Asia Maximo Arciniega maximo West Coast Michael Scott michaelscott West Coast Tuco Salamanca tuco West Coast Jack Welker jackwelker 24/7 Hank Schrader hank 24/7 Pam Beesly pambeesly 24/7 Add the users to your team, using either the above list or the alternate one provided to you. The value in the Shift column can be ignored for now, but will be required for a later step.\nClick Invite User button on the right hand side, then either start typing the usernames (this will filter the list), or copy and paste them into the dialogue box. Once all users are added to the list click the Add User button.\nTo make a team member a Team Admin, simply click the :fontawesome-regular-edit: icon in the right hand column, pick any user and make them an Admin.\nTip For large team management you can use the APIs to streamline this process.\nContinue and also complete the Configure Rotations module.",
    "description": "Aim The aim of this module is for you to complete the first step of Team configuration by adding users to your Team.\n1. Find your Team Navigate to the Teams tab on the main toolbar, you should find you that a Team has been created for you as part of the workshop pre-setup and you would have been informed of your Team Name via e-mail.\nIf you have found your pre-configured Team, skip Step 2. and proceed to Step 3. Configure Your Team. However, if you cannot find your allocated Team, you will need to create a new one, so proceed with Step 2. Create Team",
    "tags": [],
    "title": "Teams",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/getting_started/team/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 1. Agent Setup",
    "content": "In this workshop, we’ll use https://otelbin.io to quickly validate YAML syntax and ensure your OpenTelemetry configurations are accurate. This step helps avoid errors before running tests during the session.\nExercise Here’s how to validate your configuration:\nOpen https://otelbin.io and replace the existing configuration by pasting your YAML into the left pane.\nInfo If are on a Mac and not using a Splunk Workshop instance, you can quickly copy the contents of the agent.yaml file to your clipboard by running the following command:\ncat agent.yaml | pbcopy At the top of the page, make sure Splunk OpenTelemetry Collector is selected as the validation target. If you don’t select this option, then you will see warnings in the UI stating Receiver \"hostmetrics\" is unused. (Line 8).\nOnce validated, refer to the image representation below to confirm your pipelines are set up correctly.\nIn most cases, we’ll display only the key pipeline. However, if all three pipelines (Traces, Metrics, and Logs) share the same structure, we’ll note this instead of showing each one individually.\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor EXP1(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-traces subgraph \" \" subgraph subID1[**Traces/Metrics/Logs**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e EXP1 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fff,stroke-width:1px, color:#fff,stroke-dasharray: 3 3; Load Generation Tool For this workshop we have specifically developed a loadgen tool. loadgen is a flexible load generator for simulating traces and logging activities. It supports base, health, and security traces by default, along with optional logging of random quotes to a file, either in plain text or JSON format.\nThe output generated by loadgen mimic those produced by an OpenTelemetry instrumentation library, allowing us to test the Collector’s processing logic and offers a simple yet powerful way to mimic real-world scenarios.",
    "description": "In this workshop, we’ll use https://otelbin.io to quickly validate YAML syntax and ensure your OpenTelemetry configurations are accurate. This step helps avoid errors before running tests during the session.\nExercise Here’s how to validate your configuration:\nOpen https://otelbin.io and replace the existing configuration by pasting your YAML into the left pane.\nInfo If are on a Mac and not using a Splunk Workshop instance, you can quickly copy the contents of the agent.yaml file to your clipboard by running the following command:",
    "tags": [],
    "title": "1.1 Validation \u0026 Load Generation",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/1-agent/1-1-validation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 1. Dashboards",
    "content": "1. Adding Notes Often on dashboards it makes sense to place a short “instruction” pane that helps users of a dashboard. Lets add one now by clicking on the New Text Note Button.\nThis will open the notes editor.\nTo allow you to add more then just text to you notes, Splunk is allowing you to use Markdown in these notes/panes. Markdown is a lightweight markup language for creating formatted text using plain-text often used in Webpages.\nThis includes (but not limited to):\nHeaders. (in various sizes) Emphasis styles. Lists and Tables. Links. These can be external webpages (for documentation for example) or directly to other Splunk IM Dashboards Below is an example of above Markdown options you can use in your note.\n​ Sample Markdown text # h1 Big headings ###### h6 To small headings ##### Emphasis **This is bold text**, *This is italic text* , ~~Strikethrough~~ ##### Lists Unordered + Create a list by starting a line with `+`, `-`, or `*` - Sub-lists are made by indenting 2 spaces: - Marker character change forces new list start: * Ac tristique libero volutpat at + Facilisis in pretium nisl aliquet * Very easy! Ordered 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa ##### Tables | Option | Description | | ------ | ----------- | | chart | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | #### Links [link to webpage](https://www.splunk.com) [link to dashboard with title](https://app.eu0.signalfx.com/#/dashboard/EaJHrbPAEAA?groupId=EaJHgrsAIAA\u0026configId=EaJHsHzAEAA \"Link to the Sample chart Dashboard!\") Copy the above by using the copy button and paste it in the Edit box. the preview will show you how it will look.\n2. Saving our chart Give the Note chart a name, in our example we used Example text chart, then press the Save And Close Button.\nThis will bring you back to you Dashboard, that now includes the note.\n3. Ordering \u0026 sizing of charts If you do not like the default order and sizes of your charts you can simply use window dragging technique to move and size them to the desired location.\nGrab the top border of a chart and you should see the mouse pointer change to a drag icon (see picture below).\nNow drag the Latency vs Load chart to sit between the Latency History Chart and the Example text chart.\nYou can also resize windows by dragging from the left, right and bottom edges.\nAs a last exercise reduce the width of the note chart to about a third of the other charts. The chart will automatically snap to one of the sizes it supports. Widen the 3 other charts to about a third of the Dashboard. Drag the notes to the right of the others and resize it to match it to the 3 others. Set the Time to -1h and you should have the following dashboard!",
    "description": "1. Adding Notes Often on dashboards it makes sense to place a short “instruction” pane that helps users of a dashboard. Lets add one now by clicking on the New Text Note Button.\nThis will open the notes editor.\nTo allow you to add more then just text to you notes, Splunk is allowing you to use Markdown in these notes/panes. Markdown is a lightweight markup language for creating formatted text using plain-text often used in Webpages.",
    "tags": [],
    "title": "Adding Notes and Dashboard Layout",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/dashboards/1-10-notes-and-layout/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 1. Agent Setup \u003e 1.3 File Exporter",
    "content": "Exercise Restart your agent: Find your Agent terminal window, and (re)start the agent using the modified configuration:\n​ Start the Agent Agent Output ../otelcol --config=agent.yaml 2025-01-13T12:43:51.747+0100 info service@v0.120.0/service.go:261 Everything is ready. Begin running and processing data. Send a Trace: From the Spans terminal window, send another span and verify you get the same output on the console as we saw previously:\n​ Start Load Generator ../loadgen -count 1 We can now stop the agent in the Agent terminal window using Ctrl-C so that we can verify the agent.out file was written.\nVerify that the agent.out file is written: Check that a file named agent.out is written in the current directory.\n​ Updated Directory Structure . ├── agent.out # OTLP/Json output created by the File Exporter └── agent.yaml Verify the span format:\nVerify the format used by the File Exporter to write the span to agent.out. The output will be a single line in OTLP/JSON format. To view the contents of agent.out, you can use the cat ./agent.out command. For a more readable formatted view, pipe the output to jq like this: cat ./agent.out | jq: ​ cat ./agent.out cat ./agent.out | jq {\"resourceSpans\":[{\"resource\":{\"attributes\":[{\"key\":\"service.name\",\"value\":{\"stringValue\":\"cinema-service\"}},{\"key\":\"deployment.environment\",\"value\":{\"stringValue\":\"production\"}},{\"key\":\"host.name\",\"value\":{\"stringValue\":\"workshop-instance\"}},{\"key\":\"os.type\",\"value\":{\"stringValue\":\"linux\"}},{\"key\":\"otelcol.service.mode\",\"value\":{\"stringValue\":\"agent\"}}]},\"scopeSpans\":[{\"scope\":{\"name\":\"cinema.library\",\"version\":\"1.0.0\",\"attributes\":[{\"key\":\"fintest.scope.attribute\",\"value\":{\"stringValue\":\"Starwars, LOTR\"}}]},\"spans\":[{\"traceId\":\"d824a28db5aa5f5a3011f19c452e5af0\",\"spanId\":\"ab4cde146f77eacf\",\"parentSpanId\":\"\",\"name\":\"/movie-validator\",\"kind\":2,\"startTimeUnixNano\":\"1741256991405300000\",\"endTimeUnixNano\":\"1741256992405300000\",\"attributes\":[{\"key\":\"user.name\",\"value\":{\"stringValue\":\"George Lucas\"}},{\"key\":\"user.phone_number\",\"value\":{\"stringValue\":\"+1555-867-5309\"}},{\"key\":\"user.email\",\"value\":{\"stringValue\":\"george@deathstar.email\"}},{\"key\":\"user.password\",\"value\":{\"stringValue\":\"LOTR\\u003eStarWars1-2-3\"}},{\"key\":\"user.visa\",\"value\":{\"stringValue\":\"4111 1111 1111 1111\"}},{\"key\":\"user.amex\",\"value\":{\"stringValue\":\"3782 822463 10005\"}},{\"key\":\"user.mastercard\",\"value\":{\"stringValue\":\"5555 5555 5555 4444\"}},{\"key\":\"payment.amount\",\"value\":{\"doubleValue\":56.24}}],\"status\":{\"message\":\"Success\",\"code\":1}}]}],\"schemaUrl\":\"https://opentelemetry.io/schemas/1.6.1\"}]} { \"resourceSpans\": [ { \"resource\": { \"attributes\": [ { \"key\": \"service.name\", \"value\": { \"stringValue\": \"cinema-service\" } }, { \"key\": \"deployment.environment\", \"value\": { \"stringValue\": \"production\" } }, { \"key\": \"host.name\", \"value\": { \"stringValue\": \"RCASTLEY-M-YQRY.local\" } }, { \"key\": \"os.type\", \"value\": { \"stringValue\": \"darwin\" } }, { \"key\": \"otelcol.service.mode\", \"value\": { \"stringValue\": \"agent\" } } ] }, \"scopeSpans\": [ { \"scope\": { \"name\": \"cinema.library\", \"version\": \"1.0.0\", \"attributes\": [ { \"key\": \"fintest.scope.attribute\", \"value\": { \"stringValue\": \"Starwars, LOTR\" } } ] }, \"spans\": [ { \"traceId\": \"d824a28db5aa5f5a3011f19c452e5af0\", \"spanId\": \"ab4cde146f77eacf\", \"parentSpanId\": \"\", \"name\": \"/movie-validator\", \"kind\": 2, \"startTimeUnixNano\": \"1741256991405300000\", \"endTimeUnixNano\": \"1741256992405300000\", \"attributes\": [ { \"key\": \"user.name\", \"value\": { \"stringValue\": \"George Lucas\" } }, { \"key\": \"user.phone_number\", \"value\": { \"stringValue\": \"+1555-867-5309\" } }, { \"key\": \"user.email\", \"value\": { \"stringValue\": \"george@deathstar.email\" } }, { \"key\": \"user.password\", \"value\": { \"stringValue\": \"LOTR\u003eStarWars1-2-3\" } }, { \"key\": \"user.visa\", \"value\": { \"stringValue\": \"4111 1111 1111 1111\" } }, { \"key\": \"user.amex\", \"value\": { \"stringValue\": \"3782 822463 10005\" } }, { \"key\": \"user.mastercard\", \"value\": { \"stringValue\": \"5555 5555 5555 4444\" } }, { \"key\": \"payment.amount\", \"value\": { \"doubleValue\": 56.24 } } ], \"status\": { \"message\": \"Success\", \"code\": 1 } } ] } ], \"schemaUrl\": \"https://opentelemetry.io/schemas/1.6.1\" } ] }",
    "description": "Exercise Restart your agent: Find your Agent terminal window, and (re)start the agent using the modified configuration:\n​ Start the Agent Agent Output ../otelcol --config=agent.yaml 2025-01-13T12:43:51.747+0100 info service@v0.120.0/service.go:261 Everything is ready. Begin running and processing data. Send a Trace: From the Spans terminal window, send another span and verify you get the same output on the console as we saw previously:",
    "tags": [],
    "title": "1.3.1 Test File Exporter",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/1-agent/1-3-fileexporter/1-test-fileexporter/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 1. Agent Setup \u003e 1.4 Resource Metadata",
    "content": "Exercise Restart your Agent: In your Agent terminal window, and restart the agent using the updated configuration to test the changes:\n​ Start the Agent ../otelcol --config=agent.yaml If everything is set up correctly, the last line of the output should confirm the collector is running:\n2025-01-13T12:43:51.747+0100 info service@v0.120.0/service.go:261 Everything is ready. Begin running and processing data. Send a Trace: From the Spans terminal window (making sure you are in the 1-agent directory), send spans again with the loadgen binary to create a new agent.out:\n​ Start Load Generator ../loadgen Check the Agent’s debug output: You should see three new lines in the resource attributes section: (host.name, os.type \u0026 otelcol.service.mode):\n\u003csnip\u003e Resource SchemaURL: https://opentelemetry.io/schemas/1.6.1 Resource attributes: -\u003e service.name: Str(cinema.service) -\u003e deployment.environment: Str(production) -\u003e host.name: Str([MY_HOST_NAME]) -\u003e os.type: Str([MY_OS]) -\u003e otelcol.service.mode: Str(agent) \u003c/snip\u003e Verify that metadata is added to spans: Stop loadgen using Ctrl-C. In the new agent.out file:\nCheck for the existence of theotelcol.service.mode attribute in the resourceSpans section and that it has a value of agent. Verify that the resourcedetection attributes (host.name and os.type) exist too. These values are automatically added based on your device by the processors configured in the pipeline.\n​ cat ./agent.out cat ./agent.out | jq {\"resourceSpans\":[{\"resource\":{\"attributes\":[{\"key\":\"service.name\",\"value\":{\"stringValue\":\"cinema-service\"}},{\"key\":\"deployment.environment\",\"value\":{\"stringValue\":\"production\"}},{\"key\":\"host.name\",\"value\":{\"stringValue\":\"RCASTLEY-M-YQRY.local\"}},{\"key\":\"os.type\",\"value\":{\"stringValue\":\"darwin\"}},{\"key\":\"otelcol.service.mode\",\"value\":{\"stringValue\":\"agent\"}}]},\"scopeSpans\":[{\"scope\":{\"name\":\"cinema.library\",\"version\":\"1.0.0\",\"attributes\":[{\"key\":\"fintest.scope.attribute\",\"value\":{\"stringValue\":\"Starwars, LOTR\"}}]},\"spans\":[{\"traceId\":\"ae921957a4d93fa11cee640cd7908eb8\",\"spanId\":\"f6b0f29825efe585\",\"parentSpanId\":\"\",\"name\":\"/movie-validator\",\"kind\":2,\"startTimeUnixNano\":\"1740994347431796000\",\"endTimeUnixNano\":\"1740994348431796000\",\"attributes\":[{\"key\":\"user.name\",\"value\":{\"stringValue\":\"George Lucas\"}},{\"key\":\"user.phone_number\",\"value\":{\"stringValue\":\"+1555-867-5309\"}},{\"key\":\"user.email\",\"value\":{\"stringValue\":\"george@deathstar.email\"}},{\"key\":\"user.account_password\",\"value\":{\"stringValue\":\"LOTR\\u003eStarWars1-2-3\"}},{\"key\":\"user.visa\",\"value\":{\"stringValue\":\"4111 1111 1111 1111\"}},{\"key\":\"user.amex\",\"value\":{\"stringValue\":\"3782 822463 10005\"}},{\"key\":\"user.mastercard\",\"value\":{\"stringValue\":\"5555 5555 5555 4444\"}}],\"status\":{\"message\":\"Success\",\"code\":1}}]}],\"schemaUrl\":\"https://opentelemetry.io/schemas/1.6.1\"}]} { \"resourceSpans\": [ { \"resource\": { \"attributes\": [ { \"key\": \"service.name\", \"value\": { \"stringValue\": \"cinema-service\" } }, { \"key\": \"deployment.environment\", \"value\": { \"stringValue\": \"production\" } }, { \"key\": \"host.name\", \"value\": { \"stringValue\": \"RCASTLEY-M-YQRY.local\" } }, { \"key\": \"os.type\", \"value\": { \"stringValue\": \"darwin\" } }, { \"key\": \"otelcol.service.mode\", \"value\": { \"stringValue\": \"agent\" } } ] }, \"scopeSpans\": [ { \"scope\": { \"name\": \"cinema.library\", \"version\": \"1.0.0\", \"attributes\": [ { \"key\": \"fintest.scope.attribute\", \"value\": { \"stringValue\": \"Starwars, LOTR\" } } ] }, \"spans\": [ { \"traceId\": \"ab984cd113463aa919ac200751fcfc1d\", \"spanId\": \"db651e116290a8f2\", \"parentSpanId\": \"\", \"name\": \"/movie-validator\", \"kind\": 2, \"startTimeUnixNano\": \"1740994462515044000\", \"endTimeUnixNano\": \"1740994463515044000\", \"attributes\": [ { \"key\": \"user.name\", \"value\": { \"stringValue\": \"George Lucas\" } }, { \"key\": \"user.phone_number\", \"value\": { \"stringValue\": \"+1555-867-5309\" } }, { \"key\": \"user.email\", \"value\": { \"stringValue\": \"george@deathstar.email\" } }, { \"key\": \"user.account_password\", \"value\": { \"stringValue\": \"LOTR\u003eStarWars1-2-3\" } }, { \"key\": \"user.visa\", \"value\": { \"stringValue\": \"4111 1111 1111 1111\" } }, { \"key\": \"user.amex\", \"value\": { \"stringValue\": \"3782 822463 10005\" } }, { \"key\": \"user.mastercard\", \"value\": { \"stringValue\": \"5555 5555 5555 4444\" } } ], \"status\": { \"message\": \"Success\", \"code\": 1 } } ] } ], \"schemaUrl\": \"https://opentelemetry.io/schemas/1.6.1\" } ] } Important Stop the agent and loadgen processes by using Ctrl-C in the respective terminal windows.",
    "description": "Exercise Restart your Agent: In your Agent terminal window, and restart the agent using the updated configuration to test the changes:\n​ Start the Agent ../otelcol --config=agent.yaml If everything is set up correctly, the last line of the output should confirm the collector is running:\n2025-01-13T12:43:51.747+0100 info service@v0.120.0/service.go:261 Everything is ready. Begin running and processing data. Send a Trace: From the Spans terminal window (making sure you are in the 1-agent directory), send spans again with the loadgen binary to create a new agent.out:",
    "tags": [],
    "title": "1.4.1 Test Resource Metadata",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/1-agent/1-4-metadata/1-4-1-test-metadata/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence",
    "content": "Setting Up Basic Alerts in Splunk Enterprise, AppDynamics, and Splunk Observability Cloud This section covers the creation of basic alerts in Splunk Enterprise, AppDynamics, and Splunk Observability Cloud. These examples focus on simplicity and demonstrating the core concepts. Remember that real-world alerting scenarios often require more complex configurations and thresholds.\n1. Splunk Enterprise Alerts Splunk alerts are triggered by search results that match specific criteria. We’ll create a real-time alert that notifies us when a certain condition is met.\nScenario: Alert when the number of “Invalid user” events in the “main” index exceeds 100 in the last 5 minutes.\nSteps:\nCreate a Search: Start by creating a Splunk search that identifies the events you want to alert on. For example:\nindex=main \"Invalid user\" Use the time picker to select “Last 15 minutes”\".\nConfigure the Alert:\nClick “Save As” and select “Alert.” Give your alert a descriptive name (e.g., “Numerous Invalid User Logins Attempted”). Alert type: Scheduled: Choose “Scheduled” to evaluate the search on a set schedule. Below scheduled will be the button to select the frequency, select “Run on Cron Schedule”. Cron Expression: */15 * * * * Triggered when: Select “Number of results” “is greater than” “100.” Time Range: Set to “15 minutes.” Trigger Actions: For this basic example, choose “Add to Triggered Alerts.” In a real-world scenario, you’d configure email notifications, Slack integrations, or other actions. Save: Save the alert. Explanation: This alert runs the search every 15 minutes and triggers if the search returns more than 100 results. The “Add to Triggered Alerts” action simply adds a Alert to the Splunk Triggered Alerts list.\nTime Ranges and Frequency: Since everything in Splunk core is a search, you need to consider the search timespan and frequency so that you are not a) searching the same data multiple times with an overlap timespan, b) missing events because of a gap between timespan and frequency, c) running too frequently and adding overhead or d) running too infrequently and experiencing delays in alerting.\n2. Splunk Observability Cloud Alerts (Detectors) Create a Detector:\nClick “Detectors \u0026 SLOs” in the lefthand menu Click “Create Detector -\u003e Custom Detector” Give the detector a descriptive name (e.g., “High CPU Utilization Alert - INITIALS”). Signal: Select the metric you want to monitor (“cpu.utilization”). Add any necessary filters to specify the host (service.name:otelshop-loadgenerator). Click “Proceed to Alert Condition” Condition: Select Static Threshold Set the threshold: “is above” “90” Notifications: For this example, choose a simple notification method (e.g., a test webhook). In a real-world scenario, you would configure integrations with PagerDuty, Slack, or other notification systems. Save: Save the detector. Explanation: This detector monitors the CPU utilization metric for the specified service. If the CPU utilization exceeds 90% for the configured “for” duration, the detector triggers the alert and sends a notification.\nImportant Considerations for All Platforms:\nThresholds: Carefully consider the thresholds you set for your alerts. Too sensitive thresholds can lead to alert fatigue, while thresholds that are too high might miss critical issues. Notification Channels: Integrate your alerting systems with appropriate notification channels (email, SMS, Slack, PagerDuty) to ensure that alerts are delivered to the right people at the right time. Alert Grouping and Correlation: For complex systems, implement alert grouping and correlation to reduce noise and focus on actionable insights. ITSI plays a critical role in this. Documentation: Document your alerts clearly, including the conditions that trigger them and the appropriate response procedures. These examples provide a starting point for creating basic alerts. As you become more familiar with these platforms, you can explore more advanced alerting features and configurations to meet your specific monitoring needs.",
    "description": "Setting Up Basic Alerts in Splunk Enterprise, AppDynamics, and Splunk Observability Cloud This section covers the creation of basic alerts in Splunk Enterprise, AppDynamics, and Splunk Observability Cloud. These examples focus on simplicity and demonstrating the core concepts. Remember that real-world alerting scenarios often require more complex configurations and thresholds.\n1. Splunk Enterprise Alerts Splunk alerts are triggered by search results that match specific criteria. We’ll create a real-time alert that notifies us when a certain condition is met.",
    "tags": [],
    "title": "Creating Basic Alerts",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/2-creating-basic-alerts/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud",
    "content": "System architecture The primary components of the Ingest Processor service include the Ingest Processor service and SPL2 pipelines that support data processing. The following diagram provides an overview of how the components of the Ingest Processor solution work together:\nIngest Processor service The Ingest Processor service is a cloud service hosted by Splunk. It is part of the data management experience, which is a set of services that fulfill a variety of data ingest and processing use cases.\nYou can use the Ingest Processor service to do the following:\nCreate and apply SPL2 pipelines that determine how each Ingest Processor processes and routes the data that it receives. Define source types to identify the kind of data that you want to process and determine how the Ingest Processor breaks and merges that data into distinct events. Create connections to the destinations that you want your Ingest Processor to send processed data to. Pipelines A pipeline is a set of data processing instructions written in SPL2. When you create a pipeline, you write a specialized SPL2 statement that specifies which data to process, how to process it, and where to send the results. When you apply a pipeline, the Ingest Processor uses those instructions to process all the data that it receives from data sources such as Splunk forwarders, HTTP clients, and logging agents.\nEach pipeline selects and works with a subset of all the data that the Ingest Processor receives. For example, you can create a pipeline that selects events with the source type cisco_syslog from the incoming data, and then sends them to a specified index in Splunk Cloud Platform. This subset of selected data is called a partition. For more information, see Partitions.\nThe Ingest Processor solution supports only the commands and functions that are part of the IngestProcessor profile. For information about the specific SPL2 commands and functions that you can use to write pipelines for Ingest Processor, see Ingest Processor pipeline syntax. For a summary of how the IngestProcessor profile supports different commands and functions compared to other SPL2 profiles, see the following pages in the SPL2 Search Reference:\nCompatibility Quick Reference for SPL2 commands Compatibility Quick Reference for SPL2 evaluation functions",
    "description": "System architecture The primary components of the Ingest Processor service include the Ingest Processor service and SPL2 pipelines that support data processing. The following diagram provides an overview of how the components of the Ingest Processor solution work together:\nIngest Processor service The Ingest Processor service is a cloud service hosted by Splunk. It is part of the data management experience, which is a set of services that fulfill a variety of data ingest and processing use cases.",
    "tags": [],
    "title": "How Ingest Processor Works",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/2-ingest-processor-pipelines/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk OnCall",
    "content": "Aim The aim of this module is for you to get more familiar with the Timeline Tab and the filtering features.\n1. Timeline The aim of Splunk On-Call is to make being on call more bearable, and it does this by getting the critical data, to the right people, at the right time.\nThe key to making it work for you is to centralize all your alerting sources, sending them all to the Splunk On-Call platform, then you have a single pane of glass in which to manage all of your alerting.\nLogin to the Splunk On-Call UI and select the Timeline tab on the main menu bar, you should have a screen similar to the following image:\n2. People On the left we have the People section with the Teams and Users sub tabs. On the Teams tab, click on All Teams then expand [Your Team name].\nUsers with the Splunk On-Call Logo against their name are currently on call. Here you can see who is on call within a particular Team, or across all Teams via Users → On-Call.\nIf you click into one of the currently on call users, you can see their status. It shows which Rotation they are on call for, when their current Shift ends and their next Shift starts (times are displayed in your time zone), what contact methods they have and which Teams they belong to (dummy users such as Hank do not have Contact Methods configured).\n3. Timeline In the centre Timeline section you get a realtime view of what is happening within your environment with the newest messages at the top. Here you can quickly post update messages to make your colleagues aware of important developments etc.\nYou can filter the view using the buttons on the top toolbar showing only update messages, GitHub integrations, or apply more advanced filters.\nLets change the Filters settings to streamline your view. Click the Filters button then within the Routing Keys tab change the Show setting from all routing keys to selected routing keys. Change the My Keys value to all and the Other Keys value to selected and deselect all keys under the Other Keys section.\nClick anywhere outside of the dialogue box to close it.\nYou will probably now have a much simpler view as you will not currently have Incidents created using your Routing Keys, so you are left with the other types of messages that the Timeline can display.\nClick on Filters again, but this time switch to the Message Types tab. Here you control the types of messages that are displayed.\nFor example, deselect On-call Changes and Escalations, this will reduce the amount of messages displayed.\n4. Incidents On the right we have the Incidents section. Here we get a list of all the incidents within the platform, or we can view a more specific list such as incidents you are specifically assigned to, or for any of the Teams you are a member of.\nSelect the Team Incidents tab you should find that the Triggered, Acknowledged \u0026 Resolved tabs are currently all empty as you have had no incidents logged.\nLet’s change that by generating your first incident!\nContinue with the Create Incidents module.",
    "description": "Aim The aim of this module is for you to get more familiar with the Timeline Tab and the filtering features.\n1. Timeline The aim of Splunk On-Call is to make being on call more bearable, and it does this by getting the critical data, to the right people, at the right time.\nThe key to making it work for you is to centralize all your alerting sources, sending them all to the Splunk On-Call platform, then you have a single pane of glass in which to manage all of your alerting.",
    "tags": [],
    "title": "Incident Lifecycle",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/incident_lifecycle/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring",
    "content": "Why Standards Matter As cloud adoption grows, we often face requests to support new technologies within a diverse landscape, posing challenges in delivering timely content. Take, for instance, a team containerizing five workloads on AWS requiring EKS visibility. Usually, this involves assisting with integration setup, configuring metadata, and creating dashboards and alerts—a process that’s both time-consuming and increases administrative overhead and technical debt.\nSplunk Observability Cloud was designed to handle customers with a diverse set of technical requirements and stacks – from monolithic to microservices architectures, from homegrown applications to Software-as-a-Service.\nSplunk offers a native experience for OpenTelemetry, which means OTel is the preferred way to get data into Splunk. Between Splunk’s integrations and the OpenTelemetry community, there are a number of integrations available to easily collect from diverse infrastructure and applications. This includes both on-prem systems like VMWare and as well as guided integrations with cloud vendors, centralizing these hybrid environments.\nFor someone like a Splunk admin, the OpenTelemetry Collector can additionally be deployed to a Splunk Universal Forwarder as a Technical Add-on. This enables fast roll-out and centralized configuration management using the Splunk Deployment Server. Let’s assume that the same team adopting Kubernetes is going to deploy a cluster for each one of our B2B customers. I’ll show you how to make a simple modification to the OpenTelemetry collector to add the customerID, and then use mirrored dashboards to allow any of our SRE teams to easily see the customer they care about.",
    "description": "Why Standards Matter As cloud adoption grows, we often face requests to support new technologies within a diverse landscape, posing challenges in delivering timely content. Take, for instance, a team containerizing five workloads on AWS requiring EKS visibility. Usually, this involves assisting with integration setup, configuring metadata, and creating dashboards and alerts—a process that’s both time-consuming and increases administrative overhead and technical debt.\nSplunk Observability Cloud was designed to handle customers with a diverse set of technical requirements and stacks – from monolithic to microservices architectures, from homegrown applications to Software-as-a-Service.",
    "tags": [],
    "title": "Standardize Data Collection",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/2-standardize-data-collection/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 2. Building Resilience",
    "content": "In this exercise, we will update the extensions: section of the agent.yaml file. This section is part of the OpenTelemetry configuration YAML and defines optional components that enhance or modify the OpenTelemetry Collector’s behavior.\nWhile these components do not process telemetry data directly, they provide valuable capabilities and services to improve the Collector’s functionality.\nExercise Important Change ALL terminal windows to the 2-building-resilience directory and run the clear command.\nYour directory structure will look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Update the agent.yaml: In the Agent terminal window, add the file_storage extension under the existing health_check extension:\nfile_storage/checkpoint: # Extension Type/Name directory: \"./checkpoint-dir\" # Define directory create_directory: true # Create directory timeout: 1s # Timeout for file operations compaction: # Compaction settings on_start: true # Start compaction at Collector startup # Define compaction directory directory: \"./checkpoint-dir/tmp\" max_transaction_size: 65536 # Max. size limit before compaction occurs Add file_storage to the exporter: Modify the otlphttp exporter to configure retry and queuing mechanisms, ensuring data is retained and resent if failures occur. Add the following under the endpoint: \"http://localhost:5318\" and make sure the indentation matches endpoint:\nretry_on_failure: enabled: true # Enable retry on failure sending_queue: # enabled: true # Enable sending queue num_consumers: 10 # No. of consumers queue_size: 10000 # Max. queue size storage: file_storage/checkpoint # File storage extension Update the services section: Add the file_storage/checkpoint extension to the existing extensions: section and the configuration needs to look like this:\nservice: extensions: - health_check - file_storage/checkpoint # Enabled extensions for this collector Update the metrics pipeline: For this exercise we are going to comment out the hostmetrics receiver from the Metric pipeline to reduce debug and log noise, again the configuration needs to look like this:\nmetrics: receivers: # - hostmetrics # Hostmetric reciever (cpu only) - otlp",
    "description": "In this exercise, we will update the extensions: section of the agent.yaml file. This section is part of the OpenTelemetry configuration YAML and defines optional components that enhance or modify the OpenTelemetry Collector’s behavior.\nWhile these components do not process telemetry data directly, they provide valuable capabilities and services to improve the Collector’s functionality.\nExercise Important Change ALL terminal windows to the 2-building-resilience directory and run the clear command.",
    "tags": [],
    "title": "2.1 File Storage Configuration",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/2-building-resilience/2-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 2. Building Resilience",
    "content": "In this exercise, we will update the extensions: section of the agent.yaml file. This section is part of the OpenTelemetry configuration YAML and defines optional components that enhance or modify the OpenTelemetry Collector’s behavior.\nWhile these components do not process telemetry data directly, they provide valuable capabilities and services to improve the Collector’s functionality.\nExercise Important Change ALL terminal windows to the 2-building-resilience directory and run the clear command.\nYour directory structure will look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Update the agent.yaml: In the Agent terminal window, add the file_storage extension under the existing health_check extension:\nfile_storage/checkpoint: # Extension Type/Name directory: \"./checkpoint-dir\" # Define directory create_directory: true # Create directory timeout: 1s # Timeout for file operations compaction: # Compaction settings on_start: true # Start compaction at Collector startup # Define compaction directory directory: \"./checkpoint-dir/tmp\" max_transaction_size: 65536 # Max. size limit before compaction occurs Add file_storage to the exporter: Modify the otlphttp exporter to configure retry and queuing mechanisms, ensuring data is retained and resent if failures occur. Add the following under the endpoint: \"http://localhost:5318\" and make sure the indentation matches endpoint:\nretry_on_failure: enabled: true # Enable retry on failure sending_queue: # enabled: true # Enable sending queue num_consumers: 10 # No. of consumers queue_size: 10000 # Max. queue size storage: file_storage/checkpoint # File storage extension Update the services section: Add the file_storage/checkpoint extension to the existing extensions: section and the configuration needs to look like this:\nservice: extensions: - health_check - file_storage/checkpoint # Enabled extensions for this collector Update the metrics pipeline: For this exercise we are going to comment out the hostmetrics receiver from the Metric pipeline to reduce debug and log noise, again the configuration needs to look like this:\nmetrics: receivers: # - hostmetrics # Hostmetric reciever (cpu only) - otlp Validate the Agent configuration using otelbin.io. For reference, the metrics: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver REC2(filelog\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor EXP1(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(otlphttp\u003cbr\u003efa:fa-upload):::exporter EXP3(\u0026ensp;file\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-metrics subgraph \" \" subgraph subID1[**Metrics**] direction LR REC1 --\u003e PRO1 REC2 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e EXP1 PRO3 --\u003e EXP3 PRO3 --\u003e EXP2 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-metrics stroke:#38bdf8,stroke-width:1px, color:#38bdf8,stroke-dasharray: 3 3;",
    "description": "In this exercise, we will update the extensions: section of the agent.yaml file. This section is part of the OpenTelemetry configuration YAML and defines optional components that enhance or modify the OpenTelemetry Collector’s behavior.\nWhile these components do not process telemetry data directly, they provide valuable capabilities and services to improve the Collector’s functionality.\nExercise Important Change ALL terminal windows to the 2-building-resilience directory and run the clear command.",
    "tags": [],
    "title": "2.1 File Storage Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/2-building-resilience/2-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk OnCall \u003e 2. Incident Lifecycle",
    "content": "Aim The aim of this module is for you to place yourself ‘On-Call’ then generate an Incident using the supplied EC2 Instance so you can then work through the lifecycle of an Incident.\n1. On-Call Before generating any incidents you should assign yourself to the current Shift within your Follow the Sun Support - Business Hours Rotation and also place yourself On-Call.\nClick on the Schedule link within your Team in the People section on the left, or navigate to Teams → [Your Team] → Rotations Expand the Follow the Sun Support - Business Hours Rotation Click on the Manage members icon (the figures) for the current active shift depending on your time zone Use the Select a user to add… dropdown to add yourself to the shift Then click on Set Current next to your name to make yourself the current on-call user within the shift You should now get a Push Notification to your phone informing you that You Are Now On-Call 2. Trigger Alert Switch back to your shell session connected to your EC2 Instance; all of the following commands will be executed from your Instance.\nForce the CPU to spike to 100% by running the following command:\nopenssl speed -multi $(grep -ci processor /proc/cpuinfo) Forked child 0 +DT:md4:3:16 +R:19357020:md4:3.000000 +DT:md4:3:64 +R:14706608:md4:3.010000 +DT:md4:3:256 +R:8262960:md4:3.000000 +DT:md4:3:1024 This will result in an Alert being generated by Splunk Infrastructure Monitoring which in turn will generate an Incident within Splunk On-Call within a maximum of 10 seconds. This is the default polling time for the OpenTelemetry Collector installed on your instance (note it can be reduced to 1 second).\nContinue with the Manage Incidents module.",
    "description": "Aim The aim of this module is for you to place yourself ‘On-Call’ then generate an Incident using the supplied EC2 Instance so you can then work through the lifecycle of an Incident.\n1. On-Call Before generating any incidents you should assign yourself to the current Shift within your Follow the Sun Support - Business Hours Rotation and also place yourself On-Call.\nClick on the Schedule link within your Team in the People section on the left, or navigate to Teams → [Your Team] → Rotations Expand the Follow the Sun Support - Business Hours Rotation Click on the Manage members icon (the figures) for the current active shift depending on your time zone Use the Select a user to add… dropdown to add yourself to the shift Then click on Set Current next to your name to make yourself the current on-call user within the shift You should now get a Push Notification to your phone informing you that You Are Now On-Call 2. Trigger Alert Switch back to your shell session connected to your EC2 Instance; all of the following commands will be executed from your Instance.",
    "tags": [],
    "title": "Create Incidents",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/incident_lifecycle/create_incidents/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Self-Service Observability \u003e 2 Collect Data with Standards",
    "content": "Gateway First we will deploy the OTel Gateway. The workshop instructor will deploy the gateway, but we will walk through the steps here if you wish to try this yourself on a second instance.\nThe steps:\nClick the Data Management icon in the toolbar Click the + Add integration button Click Deploy the Splunk OpenTelemetry Collector button Click Next Select Linux Change mode to Data forwarding (gateway) Set the environment to prod Choose the access token for this workshop Click Next Copy the installer script and run it in the provided linux environment. Once our gateway is started we will notice… Nothing. The gateway, by default, doesn’t send any data. It can be configured to send data, but it doesn’t by default.\nWe can review the config file with:\nsudo cat /etc/otel/collector/splunk-otel-collector.conf And see that the config being used is gateway_config.yaml.\nTip Diagrams created with otelbin.io. Click on them to see them in detail.\nDiagram What it Tells Us Metrics:\nThe gateway will receive metrics over otlp or signalfx protocols, and then send these metrics to Splunk Observability Cloud with the signalfx protocol.\nThere is also a pipeline for prometheus metrics to be sent to Splunk. That pipeline is labeled internal and is meant to be for the collector. (In other words if we want to receive prometheus directly we should add it to the main pipeline.) Traces:\nThe gateway will receive traces over jaeger, otlp, sapm, or zipkin and then send these traces to Splunk Observability Cloud with the sapm protocol. Logs:\nThe gateway will receive logs over otlp and then send these logs to 2 places: Splunk Enterprise (Cloud) (for logs) and Splunk Observability Cloud (for profiling data).\nThere is also a pipeline labeled signalfx that is sending signalfx to Splunk Observability Cloud; these are events that can be used to add events to charts, as well as the process list. We’re not going to see any host metrics, and we aren’t send any other data through the gateway yet. But we do have the internal metrics being sent in.\nYou can find it by creating a new chart and adding a metric:\nClick the + in the top-right Click Chart For the signal of Plot A, type otelcol_process_uptime Add a filter with the + to the right, and type: host.id:\u003cname of instance\u003e You should get a chart like the following: You can look at the Metric Finder to find other internal metrics to explore.\nAdd Metadata Before we deploy a collector (agent) let’s add some metada onto metrics and traces with the gateway. That’s how we will know data is passing through it.\nThe attributes processor let’s us add some metadata.\nsudo vi /etc/otel/collector/agent_config.yaml Here’s what we want to add to the processors section:\nprocessors: attributes/gateway_config: actions: - key: gateway value: oac action: insert And then to the pipelines (adding attributes/gateway_config to each):\nservice: pipelines: traces: receivers: [jaeger, otlp, smartagent/signalfx-forwarder, zipkin] processors: - memory_limiter - batch - resourcedetection - attributes/gateway_config #- resource/add_environment exporters: [sapm, signalfx] # Use instead when sending to gateway #exporters: [otlp, signalfx] metrics: receivers: [hostmetrics, otlp, signalfx, smartagent/signalfx-forwarder] processors: [memory_limiter, batch, resourcedetection, attributes/gateway_config] exporters: [signalfx] # Use instead when sending to gateway #exporters: [otlp] And finally we need to restart the gateway:\nsudo systemctl restart splunk-otel-collector.service We can make sure it is still running fine by checking the status:\nsudo systemctl status splunk-otel-collector.service Next Next, let’s deploy a collector and then configure it to this gateway.",
    "description": "Gateway First we will deploy the OTel Gateway. The workshop instructor will deploy the gateway, but we will walk through the steps here if you wish to try this yourself on a second instance.\nThe steps:\nClick the Data Management icon in the toolbar Click the + Add integration button Click Deploy the Splunk OpenTelemetry Collector button Click Next Select Linux Change mode to Data forwarding (gateway) Set the environment to prod Choose the access token for this workshop Click Next Copy the installer script and run it in the provided linux environment. Once our gateway is started we will notice… Nothing. The gateway, by default, doesn’t send any data. It can be configured to send data, but it doesn’t by default.",
    "tags": [],
    "title": "Deploy Gateway",
    "uri": "/observability-workshop/v6.5/en/scenarios/self-service-observability/2-collect-with-standards/1-deploy-gateway/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics \u003e 2. API Test",
    "content": "Global variables allow us to use stored strings in multiple tests, so we only need to update them in one place.\nView the global variable that we’ll use to perform our API test. Click on Global Variables under the cog icon. The global variable named env.encoded_auth will be the one that we’ll use to build the spotify API transaction.",
    "description": "Global variables allow us to use stored strings in multiple tests, so we only need to update them in one place.\nView the global variable that we’ll use to perform our API test. Click on Global Variables under the cog icon. The global variable named env.encoded_auth will be the one that we’ll use to build the spotify API transaction.",
    "tags": [],
    "title": "Global Variables",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/2-api-test/1-global-variables/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 2. Extensions",
    "content": "Health Check Extensions are configured in the same config.yaml file that we referenced in the installation step. Let’s edit the config.yaml file and configure the extensions. Note that the pprof and zpages extensions are already configured in the default config.yaml file. For the purpose of this workshop, we will only be updating the health_check extension to expose the port on all network interfaces on which we can access the health of the collector.\n​ Command sudo vi /etc/otelcol-contrib/config.yaml ​ Extensions Configuration extensions: health_check: endpoint: 0.0.0.0:13133 Start the collector:\n​ Command otelcol-contrib --config=file:/etc/otelcol-contrib/config.yaml This extension enables an HTTP URL that can be probed to check the status of the OpenTelemetry Collector. This extension can be used as a liveness and/or readiness probe on Kubernetes. To learn more about the curl command, check out the curl man page.\nOpen a new terminal session and SSH into your instance to run the following command:\n​ curl Command curl Output curl http://localhost:13133 {\"status\":\"Server available\",\"upSince\":\"2024-10-07T11:00:08.004685295+01:00\",\"uptime\":\"12.56420005s\"}",
    "description": "Health Check Extensions are configured in the same config.yaml file that we referenced in the installation step. Let’s edit the config.yaml file and configure the extensions. Note that the pprof and zpages extensions are already configured in the default config.yaml file. For the purpose of this workshop, we will only be updating the health_check extension to expose the port on all network interfaces on which we can access the health of the collector.",
    "tags": [],
    "title": "OpenTelemetry Collector Extensions",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/2-extensions/1-health/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 2. Gateway Setup",
    "content": "The configuration for the gateway does not need any additional configuration changes to function. This has been done to save time and focus on the core concepts of the Gateway.\nValidate the gateway configuration using otelbin.io. For reference, the logs: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO3(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(\u0026ensp;file\u0026ensp;\u003cbr\u003efa:fa-upload\u003cbr\u003elogs):::exporter EXP2(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-logs subgraph \" \" subgraph subID1[**Logs**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e EXP2 PRO3 --\u003e EXP1 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3; Exercise Start the Gateway: In the Gateway terminal window, run the following command to start the gateway:\n​ Start the Gateway ../otelcol --config=gateway.yaml If everything is configured correctly, the first and last lines of the output should look like:\n2025/01/15 15:33:53 settings.go:478: Set config to [gateway.yaml] \u003csnip to the end\u003e 2025-01-13T12:43:51.747+0100 info service@v0.120.0/service.go:261 Everything is ready. Begin running and processing data. Next, we will configure the agent to send data to the newly created gateway.",
    "description": "The configuration for the gateway does not need any additional configuration changes to function. This has been done to save time and focus on the core concepts of the Gateway.\nValidate the gateway configuration using otelbin.io. For reference, the logs: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO3(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(\u0026ensp;file\u0026ensp;\u003cbr\u003efa:fa-upload\u003cbr\u003elogs):::exporter EXP2(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-logs subgraph \" \" subgraph subID1[**Logs**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e EXP2 PRO3 --\u003e EXP1 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3; Exercise Start the Gateway: In the Gateway terminal window, run the following command to start the gateway:",
    "tags": [],
    "title": "2.1 Start Gateway",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/2-gateway/2-1-start-gateway/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud",
    "content": "Scenario Overview In this scenario you will be playing the role of a Splunk Admin responsible for managing your organizations Splunk Enterprise Cloud environment. You recently worked with an internal application team on instrumenting their Kubernetes environment with Splunk APM and Infrastructure monitoring using OpenTelemetry to monitor their critical microservice applications.\nThe logs from the Kubernetes environment are also being collected and sent to Splunk Enter Prize Cloud. These logs include:\nPod logs (application logs) Kubernetes Events Kubernetes Cluster Logs Control Plane Node logs Worker Node logs Audit Logs As a Splunk Admin you want to ensure that the data you are collecting is optimized, so it can be analyzed in the most efficient way possible. Taking this approach accelerates troubleshooting and ensures efficient license utilization.\nOne way to accomplish this is by using Ingest Processor to convert robust logs to metrics and use Splunk Observability Cloud as the destination for those metrics. Not only does this make collecting the logs more efficient, you have the added ability of using the newly created metrics in Splunk Observability which can then be correlated with Splunk APM data (traces) and Splunk Infrastructure Monitoring data providing additional troubleshooting context. Because Splunk Observability Cloud uses a streaming metrics pipeline, the metrics can be alerted on in real-time speeding up problem identification. Additionally, you can use the Metrics Pipeline Management functionality to further optimize the data by aggregating, dropping unnecessary fields, and archiving less important or unneeded metrics.\nIn the next step you’ll create an Ingest Processor Pipeline which will convert Kubernetes Audit Logs to metrics that will be sent to Observability Cloud.",
    "description": "Scenario Overview In this scenario you will be playing the role of a Splunk Admin responsible for managing your organizations Splunk Enterprise Cloud environment. You recently worked with an internal application team on instrumenting their Kubernetes environment with Splunk APM and Infrastructure monitoring using OpenTelemetry to monitor their critical microservice applications.\nThe logs from the Kubernetes environment are also being collected and sent to Splunk Enter Prize Cloud. These logs include:",
    "tags": [],
    "title": "Create an Ingest Pipeline",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/3-create-an-ingest-pipeline/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence",
    "content": "Creating Services in ITSI with Dependencies Based on Entity Type This workshop outlines how to create a service in Splunk IT Service Intelligence (ITSI) using an existing entity and establishing dependencies based on the entity’s type. We’ll differentiate between entities representing business workflows from Splunk Observability Cloud and those representing AppDynamics Business Transactions.\nScenario:\nWe have two existing services: “Online-Boutique-US” (representing an application running in Kubernetes and being monitored by Splunk Observability Cloud) and “AD.ECommerce” (representing an application monitored by AppDynamics). We want to create a new service and add it as a dependent of one of those services. It is not necessary to create a service for both during your first run through this workshop so pick one that you are more interested in to start with.\nReturn to your Splunk Environment and under Apps, select IT Service Intelligence\nIn the Default Analyzer update the Filter to “Buttercup Business Health”",
    "description": "Creating Services in ITSI with Dependencies Based on Entity Type This workshop outlines how to create a service in Splunk IT Service Intelligence (ITSI) using an existing entity and establishing dependencies based on the entity’s type. We’ll differentiate between entities representing business workflows from Splunk Observability Cloud and those representing AppDynamics Business Transactions.\nScenario:\nWe have two existing services: “Online-Boutique-US” (representing an application running in Kubernetes and being monitored by Splunk Observability Cloud) and “AD.ECommerce” (representing an application monitored by AppDynamics). We want to create a new service and add it as a dependent of one of those services. It is not necessary to create a service for both during your first run through this workshop so pick one that you are more interested in to start with.",
    "tags": [],
    "title": "Creating Services in ITSI",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/3-creating-services-in-itsi/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring",
    "content": "In today’s rapidly evolving technological landscape, where hybrid and cloud environments are becoming the norm, the need for effective monitoring and troubleshooting solutions has never been more critical. However, managing the elasticity and complexity of these modern infrastructures poses a significant challenge for teams across various industries. One of the primary pain points encountered in this endeavor is the inadequacy of existing monitoring and troubleshooting experiences.\nTraditional monitoring approaches often fall short in addressing the intricacies of hybrid and cloud environments. Teams frequently encounter slow data visualization and troubleshooting processes, compounded by the clutter of bespoke yet similar dashboards and the manual correlation of data from disparate sources. This cumbersome workflow is made worse by the absence of monitoring tools tailored to ephemeral technologies such as containers, orchestrators like Kubernetes, and serverless functions.\nIn this section, we’ll cover how Splunk Observability Cloud provides out-of-the-box content for every integration. Not only do the out-of-the-box dashboards provide rich visibility into the infrastructure that is being monitored they can also be mirrored. This is important because it enables you to create standard dashboards for use by teams throughout your organization. This allows all teams to see any changes to the charts in the dashboard, and members of each team can set dashboard variables and filter customizations relevant to their requirements.",
    "description": "In today’s rapidly evolving technological landscape, where hybrid and cloud environments are becoming the norm, the need for effective monitoring and troubleshooting solutions has never been more critical. However, managing the elasticity and complexity of these modern infrastructures poses a significant challenge for teams across various industries. One of the primary pain points encountered in this endeavor is the inadequacy of existing monitoring and troubleshooting experiences.\nTraditional monitoring approaches often fall short in addressing the intricacies of hybrid and cloud environments. Teams frequently encounter slow data visualization and troubleshooting processes, compounded by the clutter of bespoke yet similar dashboards and the manual correlation of data from disparate sources. This cumbersome workflow is made worse by the absence of monitoring tools tailored to ephemeral technologies such as containers, orchestrators like Kubernetes, and serverless functions.",
    "tags": [],
    "title": "Reuse Content Across Teams",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/3-reuse-content-across-teams/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 3. Dropping Spans",
    "content": "Exercise Switch to your Gateway terminal window and open the gateway.yaml file. Update the processors section with the following configuration:\nAdd a filter processor:\nConfigure the gateway to exclude spans with the name /_healthz. The error_mode: ignore directive ensures that any errors encountered during filtering are ignored, allowing the pipeline to continue running smoothly. The traces section defines the filtering rules, specifically targeting spans named /_healthz for exclusion.\nfilter/health: # Defines a filter processor error_mode: ignore # Ignore errors traces: # Filtering rules for traces span: # Exclude spans named \"/_healthz\" - 'name == \"/_healthz\"' Add the filter processor to the traces pipeline:\nInclude the filter/health processor in the traces pipeline. For optimal performance, place the filter as early as possible—right after the memory_limiter and before the batch processor. Here’s how the configuration should look:\ntraces: receivers: - otlp processors: - memory_limiter - filter/health # Filters data based on rules - resource/add_mode - batch exporters: - debug - file/traces This setup ensures that health check related spans (/_healthz) are filtered out early in the pipeline, reducing unnecessary noise in your telemetry data.",
    "description": "Exercise Switch to your Gateway terminal window and open the gateway.yaml file. Update the processors section with the following configuration:\nAdd a filter processor:\nConfigure the gateway to exclude spans with the name /_healthz. The error_mode: ignore directive ensures that any errors encountered during filtering are ignored, allowing the pipeline to continue running smoothly. The traces section defines the filtering rules, specifically targeting spans named /_healthz for exclusion.",
    "tags": [],
    "title": "3.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/3-dropping-spans/3-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 3. FileLog Setup",
    "content": "Exercise In the Agent terminal window edit the agent.yaml and configure the FileLog receiver.\nConfigure FileLog Receiver: The filelog receiver reads log data from a file and includes custom resource attributes in the log data:\nfilelog/quotes: # Receiver Type/Name include: ./quotes.log # The file to read log data from include_file_path: true # Include file path in the log data include_file_name: false # Exclude file name from the log data resource: # Add custom resource attributes com.splunk.source: ./quotes.log # Source of the log data com.splunk.sourcetype: quotes # Source type of the log data Update logs pipeline: Add thefilelog/quotes receiver to the logs pipeline only:\nlogs: receivers: - otlp - filelog/quotes # Filelog Receiver processors: - memory_limiter - resourcedetection - resource/add_mode - batch exporters: - debug - otlphttp Validate Configuration: Paste the updated agent.yaml into otelbin.io. For reference, the logs: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver REC2(filelog\u003cbr\u003efa:fa-download\u003cbr\u003equotes):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(otlphttp\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-logs subgraph \" \" subgraph subID1[**Logs**] direction LR REC1 --\u003e PRO1 REC2 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e PRO4 PRO4 --\u003e EXP1 PRO4 --\u003e EXP2 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3;",
    "description": "Exercise In the Agent terminal window edit the agent.yaml and configure the FileLog receiver.\nConfigure FileLog Receiver: The filelog receiver reads log data from a file and includes custom resource attributes in the log data:\nfilelog/quotes: # Receiver Type/Name include: ./quotes.log # The file to read log data from include_file_path: true # Include file path in the log data include_file_name: false # Exclude file name from the log data resource: # Add custom resource attributes com.splunk.source: ./quotes.log # Source of the log data com.splunk.sourcetype: quotes # Source type of the log data Update logs pipeline: Add thefilelog/quotes receiver to the logs pipeline only:",
    "tags": [],
    "title": "3.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/3-filelog/3-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 3. Dropping Spans",
    "content": "Exercise Switch to your Gateway terminal window and open the gateway.yaml file. Update the processors section with the following configuration:\nAdd a filter processor:\nConfigure the gateway to exclude spans with the name /_healthz. The error_mode: ignore directive ensures that any errors encountered during filtering are ignored, allowing the pipeline to continue running smoothly. The traces section defines the filtering rules, specifically targeting spans named /_healthz for exclusion.\nfilter/health: # Defines a filter processor error_mode: ignore # Ignore errors traces: # Filtering rules for traces span: # Exclude spans named \"/_healthz\" - 'name == \"/_healthz\"' Add the filter processor to the traces pipeline:\nInclude the filter/health processor in the traces pipeline. For optimal performance, place the filter as early as possible—right after the memory_limiter and before the batch processor. Here’s how the configuration should look:\ntraces: receivers: - otlp processors: - memory_limiter - filter/health # Filters data based on rules - resource/add_mode - batch exporters: - debug - file/traces This setup ensures that health check related spans (/_healthz) are filtered out early in the pipeline, reducing unnecessary noise in your telemetry data.\nValidate the agent configuration using otelbin.io. For reference, the traces: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(filter\u003cbr\u003efa:fa-microchip\u003cbr\u003ehealth):::processor PRO5(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026ensp;\u0026ensp;file\u0026ensp;\u0026ensp;\u003cbr\u003efa:fa-upload\u003cbr\u003etraces):::exporter %% Links subID1:::sub-traces subgraph \" \" subgraph subID1[**Traces**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PRO4 PRO4 --\u003e PRO3 PRO3 --\u003e PRO5 PRO5 --\u003e EXP1 PRO5 --\u003e EXP2 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3;",
    "description": "Exercise Switch to your Gateway terminal window and open the gateway.yaml file. Update the processors section with the following configuration:\nAdd a filter processor:\nConfigure the gateway to exclude spans with the name /_healthz. The error_mode: ignore directive ensures that any errors encountered during filtering are ignored, allowing the pipeline to continue running smoothly. The traces section defines the filtering rules, specifically targeting spans named /_healthz for exclusion.",
    "tags": [],
    "title": "3.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/3-dropping-spans/3-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 3. Dashboards",
    "content": "1. Editing a chart Select the SAMPLE CHARTS dashboard and then click on the three dots ... on the Latency histogram chart, then on Open (or you can click on the name of the chart which here is Latency histogram).\nYou will see the plot options, current plot and signal (metric) for the Latency histogram chart in the chart editor UI.\nIn the Plot Editor tab under Signal you see the metric demo.trans.latency we are currently plotting.\nYou will see a number of Line plots. The number 18 ts indicates that we are plotting 18 metric time series in the chart.\nClick on the different chart type icons to explore each of the visualizations. Notice their name while you swipe over them. For example, click on the Heat Map icon:\nSee how the chart changes to a heat map.\nNote You can use different charts to visualize your metrics - you choose which chart type fits best for the visualization you want to have.\nFor more info on the different chart types see Choosing a chart type.\nClick on the Line chart type and you will see the line plot.\n2. Changing the time window You can also increase the time window of the chart by changing the time to Past 15 minutes by selecting from the Time dropdown.\n3. Viewing the Data Table Click on the Data Table tab.\nYou now see 18 rows, each representing a metric time series with a number of columns. These columns represent the dimensions of the metric. The dimensions for demo.trans.latency are:\ndemo_datacenter demo_customer demo_host In the demo_datacenter column you see that there are two data centers, Paris and Tokyo, for which we are getting metrics.\nIf you move your cursor over the lines in the chart horizontally you will see the data table update accordingly. If you click on one of the lines in the chart you will see a pinned value appear in the data table.\nNow click on Plot editor again to close the Data Table and let’s save this chart into a dashboard for later use!",
    "description": "1. Editing a chart Select the SAMPLE CHARTS dashboard and then click on the three dots ... on the Latency histogram chart, then on Open (or you can click on the name of the chart which here is Latency histogram).\nYou will see the plot options, current plot and signal (metric) for the Latency histogram chart in the chart editor UI.",
    "tags": [],
    "title": "Editing charts",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/dashboards/editing/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring",
    "content": "Correlating infrastructure metrics and logs is often a challenging task, primarily due to inconsistencies in naming conventions across various data sources, including hosts operating on different systems. However, leveraging the capabilities of OpenTelemetry can significantly simplify this process. With OpenTelemetry’s robust framework, which offers rich metadata and attribution, metrics, traces, and logs can seamlessly correlate using standardized field names. This automated correlation not only alleviates the burden of manual effort but also enhances the overall observability of the system.\nBy aligning metrics and logs based on common field names, teams gain deeper insights into system performance, enabling more efficient troubleshooting, proactive monitoring, and optimization of resources. In this workshop section, we’ll explore the importance of correlating metrics with logs and demonstrate how Splunk Observability Cloud empowers teams to unlock additional value from their observability data.",
    "description": "Correlating infrastructure metrics and logs is often a challenging task, primarily due to inconsistencies in naming conventions across various data sources, including hosts operating on different systems. However, leveraging the capabilities of OpenTelemetry can significantly simplify this process. With OpenTelemetry’s robust framework, which offers rich metadata and attribution, metrics, traces, and logs can seamlessly correlate using standardized field names. This automated correlation not only alleviates the burden of manual effort but also enhances the overall observability of the system.",
    "tags": [],
    "title": "Correlate Metrics and Logs",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/4-correlate-metrics-logs/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence",
    "content": "Configuring a Basic Alert in Splunk ITSI This section guides you through configuring a basic alert in Splunk IT Service Intelligence (ITSI). We’ll set up an alert that triggers when our previously created Service breaches a KPI threshold.\nDepending on the Service You Created, the KPI we use for this alert will change. In the instruction steps below replace Service Name and KPI appropriately\nPaymentService2: Business Workflow Error Rate AD-Ecommerce2: Availability Steps:\nNavigate to the KPI:\nIn ITSI, go to “Configuration” -\u003e “Correlation Searches” Click “Create New Search” Configure the new search:\nSearch Title: Service Name KPI Critical Description: Service Name KPI Critical Search: index=itsi_summary kpi=\"*KPI*\" alert_severity=critical Time Range: Last 15 minutes Service: Service Name Entity Lookup Field: itsi_service_id Run Every: minutes Notable Event Title: Service Name KPI Critical Severity: Critical Notable Event Identified Fields: source After Creating the Alert:\nYou will need to wait 5-10 minutes for the alert to run The alert will be listed in the “Alerts and Episodes” Pane in ITSI. Important Considerations:\nAlert Fatigue: Avoid setting up too many alerts or alerts with overly sensitive thresholds. This can lead to alert fatigue, where people become desensitized to alerts and might miss critical issues.",
    "description": "Configuring a Basic Alert in Splunk ITSI This section guides you through configuring a basic alert in Splunk IT Service Intelligence (ITSI). We’ll set up an alert that triggers when our previously created Service breaches a KPI threshold.\nDepending on the Service You Created, the KPI we use for this alert will change. In the instruction steps below replace Service Name and KPI appropriately\nPaymentService2: Business Workflow Error Rate AD-Ecommerce2: Availability Steps:",
    "tags": [],
    "title": "Creating Alerts in ITSI",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/4-creating-alerts-in-itsi/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud",
    "content": "Context Matters In the previous section, you reviewed the raw Kubernetes audit logs and created an Ingest Processor Pipeline to convert them to metrics and send those metrics to Splunk Observability Cloud.\nNow that this pipeline is defined we are collecting the new metrics in Splunk Observability Cloud. This is a great start; however, you will only see a single metric showing the total number of Kubernetes audit events for a given time period. It would be much more valuable to add dimensions so that you can split the metric by the event type, user, response status, and so on.\nIn this section you will update the Ingest Processor Pipeline to include additional dimensions from the Kubernetes audit logs to the metrics that are being collected. This will allow you to further filter, group, visualize, and alert on specific aspects of the audit logs. After updating the metric, you will create a new dashboard showing the status of the different types of actions associated with the logs.",
    "description": "Context Matters In the previous section, you reviewed the raw Kubernetes audit logs and created an Ingest Processor Pipeline to convert them to metrics and send those metrics to Splunk Observability Cloud.\nNow that this pipeline is defined we are collecting the new metrics in Splunk Observability Cloud. This is a great start; however, you will only see a single metric showing the total number of Kubernetes audit events for a given time period. It would be much more valuable to add dimensions so that you can split the metric by the event type, user, response status, and so on.",
    "tags": [],
    "title": "Update Pipeline and Visualize Metrics",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/4-update-pipeline-and-visualize/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 4. Processors",
    "content": "Batch Processor By default, only the batch processor is enabled. This processor is used to batch up data before it is exported. This is useful for reducing the number of network calls made to exporters. For this workshop, we will inherit the following defaults which are hard-coded into the Collector:\nsend_batch_size (default = 8192): Number of spans, metric data points, or log records after which a batch will be sent regardless of the timeout. send_batch_size acts as a trigger and does not affect the size of the batch. If you need to enforce batch size limits sent to the next component in the pipeline see send_batch_max_size. timeout (default = 200ms): Time duration after which a batch will be sent regardless of size. If set to zero, send_batch_size is ignored as data will be sent immediately, subject to only send_batch_max_size. send_batch_max_size (default = 0): The upper limit of the batch size. 0 means no upper limit on the batch size. This property ensures that larger batches are split into smaller units. It must be greater than or equal to send_batch_size. For more information on the Batch processor, see the Batch Processor documentation.",
    "description": "Batch Processor By default, only the batch processor is enabled. This processor is used to batch up data before it is exported. This is useful for reducing the number of network calls made to exporters. For this workshop, we will inherit the following defaults which are hard-coded into the Collector:\nsend_batch_size (default = 8192): Number of spans, metric data points, or log records after which a batch will be sent regardless of the timeout. send_batch_size acts as a trigger and does not affect the size of the batch. If you need to enforce batch size limits sent to the next component in the pipeline see send_batch_max_size. timeout (default = 200ms): Time duration after which a batch will be sent regardless of size. If set to zero, send_batch_size is ignored as data will be sent immediately, subject to only send_batch_max_size. send_batch_max_size (default = 0): The upper limit of the batch size. 0 means no upper limit on the batch size. This property ensures that larger batches are split into smaller units. It must be greater than or equal to send_batch_size. For more information on the Batch processor, see the Batch Processor documentation.",
    "tags": [],
    "title": "OpenTelemetry Collector Processors",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/4-processors/1-batch-processor/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 4. Sensitive Data",
    "content": "In this step, we’ll modify agent.yaml to include the attributes and redaction processors. These processors will help ensure that sensitive data within span attributes is properly handled before being logged or exported.\nPreviously, you may have noticed that some span attributes displayed in the console contained personal and sensitive data. We’ll now configure the necessary processors to filter out and redact this information effectively.\nAttributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.account_password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\"} Exercise Switch to your Agent terminal window and open the agent.yaml file in your editor. We’ll add two processors to enhance the security and privacy of your telemetry data.\n1. Add an attributes Processor: The Attributes Processor allows you to modify span attributes (tags) by updating, deleting, or hashing their values. This is particularly useful for obfuscating sensitive information before it is exported.\nIn this step, we’ll:\nUpdate the user.phone_number attribute to a static value (\"UNKNOWN NUMBER\"). Hash the user.email attribute to ensure the original email is not exposed. Delete the user.password attribute to remove it entirely from the span. attributes/update: actions: # Actions - key: user.phone_number # Target key action: update # Update action value: \"UNKNOWN NUMBER\" # New value - key: user.email # Target key action: hash # Hash the email value - key: user.password # Target key action: delete # Delete the password 2. Add a redaction Processor: The Redaction Processor detects and redacts sensitive data in span attributes based on predefined patterns, such as credit card numbers or other personally identifiable information (PII).\nIn this step:\nWe set allow_all_keys: true to ensure all attributes are processed (if set to false, only explicitly allowed keys are retained).\nWe define blocked_values with regular expressions to detect and redact Visa and MasterCard credit card numbers.\nThe summary: debug option logs detailed information about the redaction process for debugging purposes.\nredaction/redact: allow_all_keys: true # If false, only allowed keys will be retained blocked_values: # List of regex patterns to block - '\\b4[0-9]{3}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}\\b' # Visa - '\\b5[1-5][0-9]{2}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}\\b' # MasterCard summary: debug # Show debug details about redaction Update the traces Pipeline: Integrate both processors into the traces pipeline. Make sure that you comment out the redaction processor at first (we will enable it later in a separate exercise). Your configuration should look like this:\ntraces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes #- redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp",
    "description": "In this step, we’ll modify agent.yaml to include the attributes and redaction processors. These processors will help ensure that sensitive data within span attributes is properly handled before being logged or exported.\nPreviously, you may have noticed that some span attributes displayed in the console contained personal and sensitive data. We’ll now configure the necessary processors to filter out and redact this information effectively.\nAttributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.account_password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\"} Exercise Switch to your Agent terminal window and open the agent.yaml file in your editor. We’ll add two processors to enhance the security and privacy of your telemetry data.",
    "tags": [],
    "title": "4.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/4-sensitive-data/4-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 4. Building Resilience",
    "content": "In this exercise, we will update the extensions: section of the agent.yaml file. This section is part of the OpenTelemetry configuration YAML and defines optional components that enhance or modify the OpenTelemetry Collector’s behavior.\nWhile these components do not process telemetry data directly, they provide valuable capabilities and services to improve the Collector’s functionality.\nExercise Update the agent.yaml: In the Agent terminal window, add the file_storage extension and name it checkpoint:\nfile_storage/checkpoint: # Extension Type/Name directory: \"./checkpoint-dir\" # Define directory create_directory: true # Create directory timeout: 1s # Timeout for file operations compaction: # Compaction settings on_start: true # Start compaction at Collector startup # Define compaction directory directory: \"./checkpoint-dir/tmp\" max_transaction_size: 65536 # Max. size limit before compaction occurs Add file_storage to existing otlphttp exporter: Modify the otlphttp: exporter to configure retry and queuing mechanisms, ensuring data is retained and resent if failures occur:\notlphttp: endpoint: \"http://localhost:5318\" retry_on_failure: enabled: true # Enable retry on failure sending_queue: # enabled: true # Enable sending queue num_consumers: 10 # No. of consumers queue_size: 10000 # Max. queue size storage: file_storage/checkpoint # File storage extension Update the services section: Add the file_storage/checkpoint extension to the existing extensions: section. This will cause the extension to be enabled:\nservice: extensions: - health_check - file_storage/checkpoint # Enabled extensions for this collector Update the metrics pipeline: For this exercise we are going to remove the hostmetrics receiver from the Metric pipeline to reduce debug and log noise:\nmetrics: receivers: - otlp # - hostmetrics # Hostmetrics Receiver Validate the agent configuration using otelbin.io. For reference, the metrics: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(otlphttp\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-metrics subgraph \" \" subgraph subID1[**Metrics**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e PRO4 PRO4 --\u003e EXP1 PRO4 --\u003e EXP2 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-metrics stroke:#38bdf8,stroke-width:1px, color:#38bdf8,stroke-dasharray: 3 3;",
    "description": "In this exercise, we will update the extensions: section of the agent.yaml file. This section is part of the OpenTelemetry configuration YAML and defines optional components that enhance or modify the OpenTelemetry Collector’s behavior.\nWhile these components do not process telemetry data directly, they provide valuable capabilities and services to improve the Collector’s functionality.\nExercise Update the agent.yaml: In the Agent terminal window, add the file_storage extension and name it checkpoint:",
    "tags": [],
    "title": "4.1 File Storage Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/4-building-resilience/4-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 4. Sensitive Data",
    "content": "In this step, we’ll modify agent.yaml to include the attributes and redaction processors. These processors will help ensure that sensitive data within span attributes is properly handled before being logged or exported.\nPreviously, you may have noticed that some span attributes displayed in the console contained personal and sensitive data. We’ll now configure the necessary processors to filter out and redact this information effectively.\nAttributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.account_password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\"} Exercise Switch to your Agent terminal window and open the agent.yaml file in your editor. We’ll add two processors to enhance the security and privacy of your telemetry data.\n1. Add an attributes Processor: The Attributes Processor allows you to modify span attributes (tags) by updating, deleting, or hashing their values. This is particularly useful for obfuscating sensitive information before it is exported.\nIn this step, we’ll:\nUpdate the user.phone_number attribute to a static value (\"UNKNOWN NUMBER\"). Hash the user.email attribute to ensure the original email is not exposed. Delete the user.password attribute to remove it entirely from the span. attributes/update: actions: # Actions - key: user.phone_number # Target key action: update # Update action value: \"UNKNOWN NUMBER\" # New value - key: user.email # Target key action: hash # Hash the email value - key: user.password # Target key action: delete # Delete the password 2. Add a redaction Processor: The Redaction Processor detects and redacts sensitive data in span attributes based on predefined patterns, such as credit card numbers or other personally identifiable information (PII).\nIn this step:\nWe set allow_all_keys: true to ensure all attributes are processed (if set to false, only explicitly allowed keys are retained).\nWe define blocked_values with regular expressions to detect and redact Visa and MasterCard credit card numbers.\nThe summary: debug option logs detailed information about the redaction process for debugging purposes.\nredaction/redact: allow_all_keys: true # If false, only allowed keys will be retained blocked_values: # List of regex patterns to block - '\\b4[0-9]{3}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}\\b' # Visa - '\\b5[1-5][0-9]{2}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}\\b' # MasterCard summary: debug # Show debug details about redaction Update the traces Pipeline: Integrate both processors into the traces pipeline. Make sure that you comment out the redaction processor at first (we will enable it later in a separate exercise). Your configuration should look like this:\ntraces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes #- redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp Validate the agent configuration using otelbin.io. For reference, the traces: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRML(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRRD(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRRS(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRUP(attributes\u003cbr\u003efa:fa-microchip\u003cbr\u003eupdate):::processor EXP1(otlphttp\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026ensp;\u0026ensp;debug\u0026ensp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP3(file\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-traces subgraph \" \" subgraph subID1[**Traces**] direction LR REC1 --\u003e PRML PRML --\u003e PRUP PRUP --\u003e PRRD PRRD --\u003e PRRS PRRS --\u003e EXP2 PRRS --\u003e EXP3 PRRS --\u003e EXP1 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3;",
    "description": "In this step, we’ll modify agent.yaml to include the attributes and redaction processors. These processors will help ensure that sensitive data within span attributes is properly handled before being logged or exported.\nPreviously, you may have noticed that some span attributes displayed in the console contained personal and sensitive data. We’ll now configure the necessary processors to filter out and redact this information effectively.\nAttributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.account_password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\"} Exercise Switch to your Agent terminal window and open the agent.yaml file in your editor. We’ll add two processors to enhance the security and privacy of your telemetry data.",
    "tags": [],
    "title": "4.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/4-sensitive-data/4-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 4. Detectors",
    "content": "Learn how to configure Muting Rules Learn how to resume notifications 1. Configuring Muting Rules There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in Splunk Observability Cloud. Let’s create one!\nClick on Alerts \u0026 Detectors in the sidebar and then click Detectors to see the list of active detectors.\nIf you created a detector in Creating a Detector you can click on the three dots ... on the far right for that detector; if not, do that for another detector.\nFrom the drop-down click on Create Muting Rule…\nIn the Muting Rule window check Mute Indefinitely and enter a reason.\nImportant This will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector.\nClick Next and in the new modal window confirm the muting rule setup.\nClick on Mute Indefinitely to confirm.\nYou won’t be receiving any email notifications from your detector until you resume notifications again. Let’s now see how to do that!\n2. Resuming notifications To Resume notifications, click on Muting Rules, you will see the name of the detector you muted notifications for under Detector heading.\nClick on the thee dots ... on the far right, and click on Resume Notifications.\nClick on Resume to confirm and resume notifications for this detector.\nCongratulations! You have now resumed your alert notifications!",
    "description": "Learn how to configure Muting Rules Learn how to resume notifications 1. Configuring Muting Rules There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in Splunk Observability Cloud. Let’s create one!\nClick on Alerts \u0026 Detectors in the sidebar and then click Detectors to see the list of active detectors.",
    "tags": [],
    "title": "Working with Muting Rules",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/detectors/muting/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud",
    "content": "In this workshop, you walked through the entire process of optimizing Kubernetes log management by converting detailed log events into actionable metrics using Splunk Ingest Pipelines. You started by defining a pipeline that efficiently converts Kubernetes audit logs into metrics, drastically reducing the data volume while retaining critical information. You then ensured the raw log events were securely stored in S3 for long-term retention and deeper analysis.\nNext, you demonstrated how to enhance these metrics by adding key dimensions from the raw events, enabling us to drill down into specific actions and resources. you created a chart that filtered the metrics to focus on errors, breaking them out by resource and action. This allowed us to pinpoint exactly where issues were occurring in real-time.\nThe real-time architecture of Splunk Observability Cloud means that these metrics can trigger alerts the moment an issue is detected, significantly reducing the Mean Time to Detection (MTTD). Additionally, you showed how this chart can be easily saved to new or existing dashboards, ensuring ongoing visibility and monitoring of critical metrics.\nThe value behind this approach is clear: by converting logs to metrics using Ingest Processor, you not only streamline data processing and reduce storage costs but also gain the ability to monitor and respond to issues in real-time using Splunk Observability Cloud. This results in faster problem resolution, improved system reliability, and more efficient resource utilization, all while maintaining the ability to retain and access the original logs for compliance or deeper analysis.\nHappy Splunking!",
    "description": "In this workshop, you walked through the entire process of optimizing Kubernetes log management by converting detailed log events into actionable metrics using Splunk Ingest Pipelines. You started by defining a pipeline that efficiently converts Kubernetes audit logs into metrics, drastically reducing the data volume while retaining critical information. You then ensured the raw log events were securely stored in S3 for long-term retention and deeper analysis.",
    "tags": [],
    "title": "Conclusion",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/5-workshop-conclusion/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence",
    "content": "Creating an Aggregation Policy in Splunk ITSI This section outlines the steps to create an aggregation policy in Splunk ITSI that matches the alerts we just set up. This policy will group related alerts, reducing noise and improving incident management.\nDepending on the Alert You Created, the title we use for this alert will change. In the instruction steps below replace AlertName with the Service Name used\nPaymentService2 or AD-Ecommerce2 Steps Navigate to Notable Event Aggregation Policies: In Splunk, go to “Configuration” -\u003e “Notable Event Aggregation Policies”.\nCreate New Policy: click the green “Create Notable Event Aggregation Policy” button in the upper right corner.\nFiltering Criteria: This is the most important part. You’ll define the criteria for alerts to be grouped by this policy. Click “Add Rule (OR)”\nField: Select “title” from the dropdown menu. Operator: Choose “matches”. Value: Enter the string “Service Name*”. (make sure to include the *) Splitting Events: Remove the “hosts” field that is provided by default and update it to use the “service” field. We want this generating new episodes for each Service that is found. In our example, it should only be 1.\nBreaking Criteria: Configure how Episodes are broken or ended. We’ll leave it as the default “If an event occurs for which severity = Normal”. Click Preview on the right to confirm it is picking up our Alert\nClick Next\nActions (Optional): Define actions to be taken on aggregated alerts. For example, you can automatically create a ticket in ServiceNow or send an email notification. We’re going to skip this part.\nClick Next\nPolicy Title and Description:\nPolicy Title: Service Name Alert Grouping Description: Grouping Service Name alerts together. Save Policy: Click the “Next” button to create the aggregation policy.\nVerification After saving the policy, navigate to the “Go to Episode Review” page and filter alerts for last 15 minutes and add a filter to status=New and search for our Service Name in the search box.\nThere may already be an episode named after our specific alert already, if so, close it out and wait for a new one to be generated with our new Title.",
    "description": "Creating an Aggregation Policy in Splunk ITSI This section outlines the steps to create an aggregation policy in Splunk ITSI that matches the alerts we just set up. This policy will group related alerts, reducing noise and improving incident management.\nDepending on the Alert You Created, the title we use for this alert will change. In the instruction steps below replace AlertName with the Service Name used\nPaymentService2 or AD-Ecommerce2 Steps Navigate to Notable Event Aggregation Policies: In Splunk, go to “Configuration” -\u003e “Notable Event Aggregation Policies”.",
    "tags": [],
    "title": "Creating Episodes in ITSI",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/5-episodes-in-itsi/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring",
    "content": "When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.\nIn this section, we’ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated.",
    "description": "When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.\nIn this section, we’ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated.",
    "tags": [],
    "title": "Improve Timeliness of Alerts",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/5-improve-alert-timeliness/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 5. Transform Data",
    "content": "Exercise Add a transform processor: Switch to your Gateway terminal window and edit the gateway.yaml and add the following transform processor:\ntransform/logs: # Processor Type/Name log_statements: # Log Processing Statements - context: resource # Log Context statements: # List of attribute keys to keep - keep_keys(attributes, [\"com.splunk.sourcetype\", \"host.name\", \"otelcol.service.mode\"]) By using the -context: resource key we are targeting the resourceLog attributes of logs.\nThis configuration ensures that only the relevant resource attributes (com.splunk.sourcetype, host.name, otelcol.service.mode) are retained, improving log efficiency and reducing unnecessary metadata.\nAdding a Context Block for Log Severity Mapping: To properly set the severity_text and severity_number fields of a log record, we add a log context block within log_statements. This configuration extracts the level value from the log body, maps it to severity_text, and assigns the corresponding severity_number based on the log level:\n- context: log # Log Context statements: # Transform Statements Array - set(cache, ParseJSON(body)) where IsMatch(body, \"^\\\\{\") # Parse JSON log body into a cache object - flatten(cache, \"\") # Flatten nested JSON structure - merge_maps(attributes, cache, \"upsert\") # Merge cache into attributes, updating existing keys - set(severity_text, attributes[\"level\"]) # Set severity_text from the \"level\" attribute - set(severity_number, 1) where severity_text == \"TRACE\" # Map severity_text to severity_number - set(severity_number, 5) where severity_text == \"DEBUG\" - set(severity_number, 9) where severity_text == \"INFO\" - set(severity_number, 13) where severity_text == \"WARN\" - set(severity_number, 17) where severity_text == \"ERROR\" - set(severity_number, 21) where severity_text == \"FATAL\" The merge_maps function is used to combine two maps (dictionaries) into one. In this case, it merges the cache object (containing parsed JSON data from the log body) into the attributes map.\nParameters: attributes: The target map where the data will be merged. cache: The source map containing the parsed JSON data. \"upsert\": This mode ensures that if a key already exists in the attributes map, its value will be updated with the value from cache. If the key does not exist, it will be inserted. This step is crucial because it ensures that all relevant fields from the log body (e.g., level, message, etc.) are added to the attributes map, making them available for further processing or exporting.\nSummary of Key Transformations:\nParse JSON: Extracts structured data from the log body. Flatten JSON: Converts nested JSON objects into a flat structure. Merge Attributes: Integrates extracted data into log attributes. Map Severity Text: Assigns severity_text from the log’s level attribute. Assign Severity Numbers: Converts severity levels into standardized numerical values. Important You should have a single transform processor containing two context blocks: one whose context is for resource and one whose context is for log.\nThis configuration ensures that log severity is correctly extracted, standardized, and structured for efficient processing.\nTip This method of mapping all JSON fields to top-level attributes should only be used for testing and debugging OTTL. It will result in high cardinality in a production scenario.\nUpdate the logs pipeline: Add the transform/logs: processor into the logs: pipeline so your configuration looks like this:\nlogs: # Logs pipeline receivers: - otlp # OTLP receiver processors: # Processors for logs - memory_limiter - resource/add_mode - transform/logs - batch exporters: - debug # Debug exporter - file/logs",
    "description": "Exercise Add a transform processor: Switch to your Gateway terminal window and edit the gateway.yaml and add the following transform processor:\ntransform/logs: # Processor Type/Name log_statements: # Log Processing Statements - context: resource # Log Context statements: # List of attribute keys to keep - keep_keys(attributes, [\"com.splunk.sourcetype\", \"host.name\", \"otelcol.service.mode\"]) By using the -context: resource key we are targeting the resourceLog attributes of logs.",
    "tags": [],
    "title": "5.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/5-transform-data/5-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 5. Dropping Spans",
    "content": "Exercise Switch to your Gateway terminal window and open the gateway.yaml file. Update the processors section with the following configuration:\nAdd a filter processor:\nConfigure the gateway to exclude spans with the name /_healthz. The error_mode: ignore directive ensures that any errors encountered during filtering are ignored, allowing the pipeline to continue running smoothly. The traces section defines the filtering rules, specifically targeting spans named /_healthz for exclusion.\nfilter/health: # Defines a filter processor error_mode: ignore # Ignore errors traces: # Filtering rules for traces span: # Exclude spans named \"/_healthz\" - 'name == \"/_healthz\"' Add the filter processor to the traces pipeline:\nInclude the filter/health processor in the traces pipeline. For optimal performance, place the filter as early as possible—right after the memory_limiter and before the batch processor. Here’s how the configuration should look:\ntraces: receivers: - otlp processors: - memory_limiter - filter/health # Filters data based on rules - resource/add_mode - batch exporters: - debug - file/traces This setup ensures that health check-related spans (/_healthz) are filtered out early in the pipeline, reducing unnecessary noise in your telemetry data.\nValidate the agent configuration using otelbin.io. For reference, the traces: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(filter\u003cbr\u003efa:fa-microchip\u003cbr\u003ehealth):::processor PRO5(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026ensp;\u0026ensp;file\u0026ensp;\u0026ensp;\u003cbr\u003efa:fa-upload\u003cbr\u003etraces):::exporter %% Links subID1:::sub-traces subgraph \" \" subgraph subID1[**Traces**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PRO4 PRO4 --\u003e PRO3 PRO3 --\u003e PRO5 PRO5 --\u003e EXP1 PRO5 --\u003e EXP2 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3;",
    "description": "Exercise Switch to your Gateway terminal window and open the gateway.yaml file. Update the processors section with the following configuration:\nAdd a filter processor:\nConfigure the gateway to exclude spans with the name /_healthz. The error_mode: ignore directive ensures that any errors encountered during filtering are ignored, allowing the pipeline to continue running smoothly. The traces section defines the filtering rules, specifically targeting spans named /_healthz for exclusion.",
    "tags": [],
    "title": "5.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/5-dropping-spans/5-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 5. Transform Data",
    "content": "Exercise Add a transform processor: Switch to your Gateway terminal window and edit the gateway.yaml and add the following transform processor:\ntransform/logs: # Processor Type/Name log_statements: # Log Processing Statements - context: resource # Log Context statements: # List of attribute keys to keep - keep_keys(attributes, [\"com.splunk.sourcetype\", \"host.name\", \"otelcol.service.mode\"]) By using the -context: resource key we are targeting the resourceLog attributes of logs.\nThis configuration ensures that only the relevant resource attributes (com.splunk.sourcetype, host.name, otelcol.service.mode) are retained, improving log efficiency and reducing unnecessary metadata.\nAdding a Context Block for Log Severity Mapping: To properly set the severity_text and severity_number fields of a log record, we add a log context block within log_statements. This configuration extracts the level value from the log body, maps it to severity_text, and assigns the corresponding severity_number based on the log level:\n- context: log # Log Context statements: # Transform Statements Array - set(cache, ParseJSON(body)) where IsMatch(body, \"^\\\\{\") # Parse JSON log body into a cache object - flatten(cache, \"\") # Flatten nested JSON structure - merge_maps(attributes, cache, \"upsert\") # Merge cache into attributes, updating existing keys - set(severity_text, attributes[\"level\"]) # Set severity_text from the \"level\" attribute - set(severity_number, 1) where severity_text == \"TRACE\" # Map severity_text to severity_number - set(severity_number, 5) where severity_text == \"DEBUG\" - set(severity_number, 9) where severity_text == \"INFO\" - set(severity_number, 13) where severity_text == \"WARN\" - set(severity_number, 17) where severity_text == \"ERROR\" - set(severity_number, 21) where severity_text == \"FATAL\" The merge_maps function is used to combine two maps (dictionaries) into one. In this case, it merges the cache object (containing parsed JSON data from the log body) into the attributes map.\nParameters: attributes: The target map where the data will be merged. cache: The source map containing the parsed JSON data. \"upsert\": This mode ensures that if a key already exists in the attributes map, its value will be updated with the value from cache. If the key does not exist, it will be inserted. This step is crucial because it ensures that all relevant fields from the log body (e.g., level, message, etc.) are added to the attributes map, making them available for further processing or exporting.\nSummary of Key Transformations:\nParse JSON: Extracts structured data from the log body. Flatten JSON: Converts nested JSON objects into a flat structure. Merge Attributes: Integrates extracted data into log attributes. Map Severity Text: Assigns severity_text from the log’s level attribute. Assign Severity Numbers: Converts severity levels into standardized numerical values. Important You should have a single transform processor containing two context blocks: one whose context is for resource and one whose context is for log.\nThis configuration ensures that log severity is correctly extracted, standardized, and structured for efficient processing.\nTip This method of mapping all JSON fields to top-level attributes should only be used for testing and debugging OTTL. It will result in high cardinality in a production scenario.\nUpdate the logs pipeline: Add the transform/logs: processor into the logs: pipeline so your configuration looks like this:\nlogs: # Logs pipeline receivers: - otlp # OTLP receiver processors: # Processors for logs - memory_limiter - resource/add_mode - transform/logs - batch exporters: - debug # Debug exporter - file/logs Validate the agent configuration using https://otelbin.io. For reference, the logs: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%%\rgraph LR\r%% Nodes\rREC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver\rPRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor\rPRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor\rPRO4(transform\u003cbr\u003efa:fa-microchip\u003cbr\u003elogs):::processor\rPRO5(batch\u003cbr\u003efa:fa-microchip):::processor\rEXP1(file\u003cbr\u003efa:fa-upload\u003cbr\u003elogs):::exporter\rEXP2(\u0026ensp;\u0026ensp;debug\u0026ensp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter\r%% Links\rsubID1:::sub-logs\rsubgraph \" \"\rsubgraph subID1[**Logs**]\rdirection LR\rREC1 --\u003e PRO1\rPRO1 --\u003e PRO3\rPRO3 --\u003e PRO4\rPRO4 --\u003e PRO5\rPRO5 --\u003e EXP2\rPRO5 --\u003e EXP1\rend\rend\rclassDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff;\rclassDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff;\rclassDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff;\rclassDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3;",
    "description": "Exercise Add a transform processor: Switch to your Gateway terminal window and edit the gateway.yaml and add the following transform processor:\ntransform/logs: # Processor Type/Name log_statements: # Log Processing Statements - context: resource # Log Context statements: # List of attribute keys to keep - keep_keys(attributes, [\"com.splunk.sourcetype\", \"host.name\", \"otelcol.service.mode\"]) By using the -context: resource key we are targeting the resourceLog attributes of logs.",
    "tags": [],
    "title": "5.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/5-transform-data/5-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 5. Exporters",
    "content": "OTLP HTTP Exporter To send metrics over HTTP to Splunk Observability Cloud, we will need to configure the otlphttp exporter.\nLet’s edit our /etc/otelcol-contrib/config.yaml file and configure the otlphttp exporter. Insert the following YAML under the exporters section, taking care to indent by two spaces e.g.\nWe will also change the verbosity of the logging exporter to prevent the disk from filling up. The default of detailed is very noisy.\nexporters: logging: verbosity: normal otlphttp/splunk: Next, we need to define the metrics_endpoint and configure the target URL.\nNote If you are an attendee at a Splunk-hosted workshop, the instance you are using has already been configured with a Realm environment variable. We will reference that environment variable in our configuration file. Otherwise, you will need to create a new environment variable and set the Realm e.g.\nexport REALM=\"us1\" The URL to use is https://ingest.${env:REALM}.signalfx.com/v2/datapoint/otlp. (Splunk has Realms in key geographical locations around the world for data residency).\nThe otlphttp exporter can also be configured to send traces and logs by defining a target URL for traces_endpoint and logs_endpoint respectively. Configuring these is outside the scope of this workshop.\nexporters: logging: verbosity: normal otlphttp/splunk: metrics_endpoint: https://ingest.${env:REALM}.signalfx.com/v2/datapoint/otlp By default, gzip compression is enabled for all endpoints. This can be disabled by setting compression: none in the exporter configuration. We will leave compression enabled for this workshop and accept the default as this is the most efficient way to send data.\nTo send metrics to Splunk Observability Cloud, we need to use an Access Token. This can be done by creating a new token in the Splunk Observability Cloud UI. For more information on how to create a token, see Create a token. The token needs to be of type INGEST.\nNote If you are an attendee at a Splunk-hosted workshop, the instance you are using has already been configured with an Access Token (which has been set as an environment variable). We will reference that environment variable in our configuration file. Otherwise, you will need to create a new token and set it as an environment variable e.g.\nexport ACCESS_TOKEN=\u003creplace-with-your-token\u003e The token is defined in the configuration file by inserting X-SF-TOKEN: ${env:ACCESS_TOKEN} under a headers: section:\nexporters: logging: verbosity: normal otlphttp/splunk: metrics_endpoint: https://ingest.${env:REALM}.signalfx.com/v2/datapoint/otlp headers: X-SF-TOKEN: ${env:ACCESS_TOKEN} Configuration Check-in Now that we’ve covered exporters, let’s check our configuration changes:\nCheck-inReview your configuration ​ config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 # To limit exposure to denial of service attacks, change the host in endpoints below from 0.0.0.0 to a specific network interface. # See https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/security-best-practices.md#safeguards-against-denial-of-service-attacks extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 receivers: hostmetrics: collection_interval: 10s scrapers: # CPU utilization metrics cpu: # Disk I/O metrics disk: # File System utilization metrics filesystem: # Memory utilization metrics memory: # Network interface I/O metrics \u0026 TCP connection metrics network: # CPU load metrics load: # Paging/Swap space utilization and I/O metrics paging: # Process count metrics processes: # Per process CPU, Memory and Disk I/O metrics. Disabled by default. # process: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 opencensus: endpoint: 0.0.0.0:55678 # Collect own metrics prometheus/internal: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 10s static_configs: - targets: ['0.0.0.0:8888'] jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_binary: endpoint: 0.0.0.0:6832 thrift_compact: endpoint: 0.0.0.0:6831 thrift_http: endpoint: 0.0.0.0:14268 zipkin: endpoint: 0.0.0.0:9411 processors: batch: resourcedetection/system: detectors: [system] system: hostname_sources: [os] resourcedetection/ec2: detectors: [ec2] attributes/conf: actions: - key: participant.name action: insert value: \"INSERT_YOUR_NAME_HERE\" exporters: debug: verbosity: normal otlphttp/splunk: metrics_endpoint: https://ingest.${env:REALM}.signalfx.com/v2/datapoint/otlp headers: X-SF-Token: ${env:ACCESS_TOKEN} service: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [otlp, opencensus, prometheus] processors: [batch] exporters: [debug] logs: receivers: [otlp] processors: [batch] exporters: [debug] extensions: [health_check, pprof, zpages] Of course, you can easily configure the metrics_endpoint to point to any other solution that supports the OTLP protocol.\nNext, we need to enable the receivers, processors and exporters we have just configured in the service section of the config.yaml.",
    "description": "OTLP HTTP Exporter To send metrics over HTTP to Splunk Observability Cloud, we will need to configure the otlphttp exporter.\nLet’s edit our /etc/otelcol-contrib/config.yaml file and configure the otlphttp exporter. Insert the following YAML under the exporters section, taking care to indent by two spaces e.g.\nWe will also change the verbosity of the logging exporter to prevent the disk from filling up. The default of detailed is very noisy.",
    "tags": [],
    "title": "OpenTelemetry Collector Exporters",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/5-exporters/1-otlphttp/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring",
    "content": "Today you’ve seen how Splunk Observability Cloud can help you overcome many of the challenges you face monitoring hybrid and cloud environments. You’ve demonstrated how Splunk Observability Cloud streamlines operations with standardized data collection and tags, ensuring consistency across all IT infrastructure. The Unified Service Telemetry has been a game-changer, providing in-context metrics, logs, and trace data that make troubleshooting swift and efficient. By enabling the reuse of content across teams, you’re minimizing technical debt and bolstering the performance of our monitoring systems.\nHappy Splunking!",
    "description": "Today you’ve seen how Splunk Observability Cloud can help you overcome many of the challenges you face monitoring hybrid and cloud environments. You’ve demonstrated how Splunk Observability Cloud streamlines operations with standardized data collection and tags, ensuring consistency across all IT infrastructure. The Unified Service Telemetry has been a game-changer, providing in-context metrics, logs, and trace data that make troubleshooting swift and efficient. By enabling the reuse of content across teams, you’re minimizing technical debt and bolstering the performance of our monitoring systems.",
    "tags": [],
    "title": "Conclusion",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/6-workshop-conclusion/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence",
    "content": "Part 2: Sending Alerts from Splunk Observability Cloud to Splunk ITSI Since we have a detector configured in Splunk Observability Cloud that we set up earlier, the next step is to ensure that when it triggers an alert, this alert is sent to Splunk IT Service Intelligence (ITSI). This integration allows ITSI to ingest these alerts as notable events, which can then be correlated with other events and contribute to service health scores. The most common method to achieve this is by using a webhook in Splunk Observability Cloud to send alert data to an HTTP Event Collector (HEC) endpoint configured in Splunk ITSI.\nStep 1: Configure an HTTP Event Collector (HEC) in Splunk (ITSI)\nBefore Splunk Observability Cloud can send alerts to ITSI, you need an HEC endpoint in your Splunk instance (where ITSI is running) to receive them.\nLog in to your Splunk Enterprise or Splunk Cloud instance that hosts ITSI. Navigate to Settings \u003e Data Inputs. Click on HTTP Event Collector. Click Global Settings. Ensure HEC is enabled. If not, enable it and specify a default port (e.g., 8088, though this might be managed differently in Splunk Cloud). Click New Token. Give your HEC token a descriptive name, for example, o11y_alerts_for_itsi. For Source name override, you can optionally specify a sourcetype, or leave it blank to specify it in Observability Cloud or let it default. For Default Index, select an appropriate index where ITSI can access these events. Often, there’s a dedicated index for ITSI events, or you might use a general events index like main or itsi_event_management. Ensure the token is enabled and click Submit. Copy the Token Value that is generated. You will need this for the webhook configuration in Splunk Observability Cloud. Step 2: Configure a Webhook Integration in Splunk Observability Cloud\nNow, return to Splunk Observability Cloud to set up the webhook that will use the HEC token you just created.\nIn Splunk Observability Cloud, navigate to Data Management \u003e Available Integrations. Look for an option to add a new Splunk platform. Give the Integration a name, for example, Splunk ITSI HEC. In the URL field, enter the HEC endpoint URI for your Splunk instance. This will typically be in the format https://\u003cyour-splunk-hec-host-or-ip\u003e:\u003chec-port\u003e/services/collector/event. You will need to add an HEC token that you created earlier. For the Payload, you need to construct a JSON payload that ITSI can understand. Splunk Observability Cloud provides an out of the box payload configured to include fields needed for ITSI event correlation. Review the Integration and click Save Step 3: Update the Detector to Use the Webhook\nNow, go back to the detector you created in Part 1 and update its notification settings to use the newly configured webhook.\nNavigate to Detectors \u0026 SLOs in Splunk Observability Cloud. Find and edit the detector you created for EC2 CPU utilization. Click the Alert rule that we created earlier Go to the Alert Recipients section. Click Add recipient \u003e Splunk platform and select the integration you just configured (Splunk ITSI HEC) for the desired alert severities (e.g., Critical, Warning). Save the changes to your detector. Step 4: Validate\nTo test the integration, you can wait for a genuine alert to trigger or, if your detector settings allow, you might be able to manually trigger a test alert or temporarily lower the threshold to force an alert. Once an alert triggers in Splunk Observability Cloud, it should send the payload via the webhook to your Splunk HEC endpoint.\nVerify in Splunk by searching your target index (e.g., index=itsi_event_management sourcetype=o11y:itsi:alert host=\u003cyour-ec2-instance-id\u003e). You should see the event data arriving from Splunk Observability Cloud.\nWith these steps, alerts from your Splunk Observability Cloud detector are now being sent to Splunk ITSI. Correlating Events and generating Notables all function exactly the same as we covered earlier in this workshop.",
    "description": "Part 2: Sending Alerts from Splunk Observability Cloud to Splunk ITSI Since we have a detector configured in Splunk Observability Cloud that we set up earlier, the next step is to ensure that when it triggers an alert, this alert is sent to Splunk IT Service Intelligence (ITSI). This integration allows ITSI to ingest these alerts as notable events, which can then be correlated with other events and contribute to service health scores. The most common method to achieve this is by using a webhook in Splunk Observability Cloud to send alert data to an HTTP Event Collector (HEC) endpoint configured in Splunk ITSI.",
    "tags": [],
    "title": "Using Observability Cloud Detectors in ITSI",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/6-detectors-to-itsi/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 6. Sensitive Data",
    "content": "In this step, we’ll modify agent.yaml to include the attributes and redaction processors. These processors will help ensure that sensitive data within span attributes is properly handled before being logged or exported.\nPreviously, you may have noticed that some span attributes displayed in the console contained personal and sensitive data. We’ll now configure the necessary processors to filter out and redact this information effectively.\n\u003csnip\u003e Attributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.account_password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\"} Exercise Switch to your Agent terminal window and open the agent.yaml file in your editor. We’ll add two processors to enhance the security and privacy of your telemetry data: the Attributes Processor and the Redaction Processor.\nAdd an attributes Processor: The Attributes Processor allows you to modify span attributes (tags) by updating, deleting, or hashing their values. This is particularly useful for obfuscating sensitive information before it is exported.\nIn this step, we’ll:\nUpdate the user.phone_number attribute to a static value (\"UNKNOWN NUMBER\"). Hash the user.email attribute to ensure the original email is not exposed. Delete the user.password attribute to remove it entirely from the span. attributes/update: actions: # Actions - key: user.phone_number # Target key action: update # Update action value: \"UNKNOWN NUMBER\" # New value - key: user.email # Target key action: hash # Hash the email value - key: user.password # Target key action: delete # Delete the password Add a redaction Processor: The The Redaction Processor detects and redacts sensitive data in span attributes based on predefined patterns, such as credit card numbers or other personally identifiable information (PII).\nIn this step:\nWe set allow_all_keys: true to ensure all attributes are processed (if set to false, only explicitly allowed keys are retained).\nWe define blocked_values with regular expressions to detect and redact Visa and MasterCard credit card numbers.\nThe summary: debug option logs detailed information about the redaction process for debugging purposes.\nredaction/redact: allow_all_keys: true # If false, only allowed keys will be retained blocked_values: # List of regex patterns to block - '\\b4[0-9]{3}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}\\b' # Visa - '\\b5[1-5][0-9]{2}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}[\\s-]?[0-9]{4}\\b' # MasterCard summary: debug # Show debug details about redaction Update the traces Pipeline: Integrate both processors into the traces pipeline. Make sure that you comment out the redaction processor at first (we will enable it later in a separate exercise):\nNote Leave the redaction/redact processor commented out in this exercise. We will enable it in an upcoming exercise.\ntraces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes #- redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp Validate the agent configuration using otelbin.io. For reference, the traces: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRML(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRRD(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRRS(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRBA(batch\u003cbr\u003efa:fa-microchip):::processor PRUP(attributes\u003cbr\u003efa:fa-microchip\u003cbr\u003eupdate):::processor EXP1(otlphttp\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026ensp;\u0026ensp;debug\u0026ensp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP3(file\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-traces subgraph \" \" subgraph subID1[**Traces**] direction LR REC1 --\u003e PRML PRML --\u003e PRUP PRUP --\u003e PRRD PRRD --\u003e PRRS PRRS --\u003e PRBA PRBA --\u003e EXP2 PRBA --\u003e EXP3 PRBA --\u003e EXP1 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3;",
    "description": "In this step, we’ll modify agent.yaml to include the attributes and redaction processors. These processors will help ensure that sensitive data within span attributes is properly handled before being logged or exported.\nPreviously, you may have noticed that some span attributes displayed in the console contained personal and sensitive data. We’ll now configure the necessary processors to filter out and redact this information effectively.\n\u003csnip\u003e Attributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.account_password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\"} Exercise Switch to your Agent terminal window and open the agent.yaml file in your editor. We’ll add two processors to enhance the security and privacy of your telemetry data: the Attributes Processor and the Redaction Processor.",
    "tags": [],
    "title": "6.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/6-sensitive-data/6-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 6. Service",
    "content": "Hostmetrics Receiver If you recall from the Receivers portion of the workshop, we defined the Host Metrics Receiver to generate metrics about the host system, which are scraped from various sources. To enable the receiver, we must include the hostmetrics receiver in the metrics pipeline.\nIn the metrics pipeline, add hostmetrics to the metrics receivers section.\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus] processors: [batch] exporters: [debug]",
    "description": "Hostmetrics Receiver If you recall from the Receivers portion of the workshop, we defined the Host Metrics Receiver to generate metrics about the host system, which are scraped from various sources. To enable the receiver, we must include the hostmetrics receiver in the metrics pipeline.\nIn the metrics pipeline, add hostmetrics to the metrics receivers section.\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus] processors: [batch] exporters: [debug]",
    "tags": [],
    "title": "OpenTelemetry Collector Service",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/6-service/1-hostmetrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 6. Routing Data",
    "content": "In this exercise, you will configure the Routing Connector in the gateway.yaml. The Routing Connector can route metrics, traces, and logs based on any attributes, we will focus exclusively on trace routing based on the deployment.environment attribute (though any span/log/metirc attribute can be used).\nExercise Add new file exporters: The routing connector requires different targets for routing. Create two new file exporters, file/traces/route1-regular and file/traces/route2-security, to ensure data is directed correctly in the exporters section of the gateway.yaml:\nfile/traces/route1-regular: # Exporter for regular traces path: \"./gateway-traces-route1-regular.out\" # Path for saving trace data append: false # Overwrite the file each time file/traces/route2-security: # Exporter for security traces path: \"./gateway-traces-route2-security.out\" # Path for saving trace data append: false # Overwrite the file each time Enable Routing by adding the routing connector. In OpenTelemetry configuration files, connectors have their own dedicated section, similar to receivers and processors.\nIn the Gateway terminal window, edit gateway.yaml and find and uncomment the #connectors: section. Then, add the following below the connectors: section:\nrouting: default_pipelines: [traces/route1-regular] # Default pipeline if no rule matches error_mode: ignore # Ignore errors in routing table: # Define routing rules # Routes spans to a target pipeline if the resourceSpan attribute matches the rule - statement: route() where attributes[\"deployment.environment\"] == \"security-applications\" pipelines: [traces/route2-security] # Security target pipeline The default pipeline in the configuration file works at a Catch all. It will be the routing target for any data (spans in our case) that do not match a rule in the routing rules table, In this table you find the pipeline that is the target for any span that matches the following rule: [\"deployment.environment\"] == \"security-applications\"\nWith the routing configuration complete, the next step is to configure the pipelines to apply these routing rules.",
    "description": "In this exercise, you will configure the Routing Connector in the gateway.yaml. The Routing Connector can route metrics, traces, and logs based on any attributes, we will focus exclusively on trace routing based on the deployment.environment attribute (though any span/log/metirc attribute can be used).\nExercise Add new file exporters: The routing connector requires different targets for routing. Create two new file exporters, file/traces/route1-regular and file/traces/route2-security, to ensure data is directed correctly in the exporters section of the gateway.yaml:",
    "tags": [],
    "title": "6.1 Configure the Routing Connector",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/6-routing-data/6-1-connector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 6. Routing Data",
    "content": "In this exercise, you will configure the Routing Connector in the gateway.yaml. The Routing Connector can route metrics, traces, and logs based on any attributes, we will focus exclusively on trace routing based on the deployment.environment attribute (though any span/log/metirc attribute can be used).\nExercise Add new file exporters: The routing connector requires different targets for routing. In the Gateway terminal create two new file exporters, file/traces/route1-regular and file/traces/route2-security, to ensure data is directed correctly in the exporters section of the gateway.yaml:\nfile/traces/route1-regular: # Exporter for regular traces path: \"./gateway-traces-route1-regular.out\" # Path for saving trace data append: false # Overwrite the file each time file/traces/route2-security: # Exporter for security traces path: \"./gateway-traces-route2-security.out\" # Path for saving trace data append: false # Overwrite the file each time Enable Routing by adding the routing connector. In OpenTelemetry configuration files, connectors have their own dedicated section, similar to receivers and processors.\nFind and uncomment the #connectors: section. Then, add the following below the connectors: section:\nrouting: default_pipelines: [traces/route1-regular] # Default pipeline if no rule matches error_mode: ignore # Ignore errors in routing table: # Define routing rules # Routes spans to a target pipeline if the resourceSpan attribute matches the rule - statement: route() where attributes[\"deployment.environment\"] == \"security-applications\" pipelines: [traces/route2-security] # Security target pipeline The default pipeline in the configuration file works at a Catch all. It will be the routing target for any data (spans in our case) that do not match a rule in the routing rules table, In this table you find the pipeline that is the target for any span that matches the following rule: [\"deployment.environment\"] == \"security-applications\"\nWith the routing configuration complete, the next step is to configure the pipelines to apply these routing rules.",
    "description": "In this exercise, you will configure the Routing Connector in the gateway.yaml. The Routing Connector can route metrics, traces, and logs based on any attributes, we will focus exclusively on trace routing based on the deployment.environment attribute (though any span/log/metirc attribute can be used).\nExercise Add new file exporters: The routing connector requires different targets for routing. In the Gateway terminal create two new file exporters, file/traces/route1-regular and file/traces/route2-security, to ensure data is directed correctly in the exporters section of the gateway.yaml:",
    "tags": [],
    "title": "6.1 Configure the Routing Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/6-routing-data/6-1-connector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 6. Service Bureau",
    "content": "Introduction to Teams Create a Team and add members to the Team 1. Introduction to Teams To make sure that users see the dashboards and alerts that are relevant to them when using Observability Cloud, most organizations will use Observability Cloud’s Teams feature to assign a member to one or more Teams.\nIdeally, this matches work-related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in Observability Cloud.\nWhen a user logs into Observability Cloud, they can choose which Team Dashboard will be their home page and they will typically select the page for their primary role.\nIn the example below, the user is a member of the Development, Operations and Product Management Teams, and is currently viewing the Dashboard for the Operations Team.\nThis Dashboard has specific Dashboard Groups for Usage, SaaS and APM Business Workflows assigned but any Dashboard Group can be linked to a Teams Dashboard.\nThey can use the menu along the top left to quickly navigate between their allocated teams, or they can use the ALL TEAMS dropdown on the right to select specific Team Dashboards, as well as quickly access ALL Dashboards** using the adjacent link.\nAlerts can be linked to specific Teams so the Team can monitor only the Alerts they are interested in, and in the above example, they currently have 1 active Critical Alert.\nThe Description for the Team Dashboard can be customized and can include links to team-specific resources (using Markdown).\n2. Creating a new Team To work with Splunk’s Team UI click on the hamburger icon top left and select the Organizations Settings → Teams.\nWhen the Team UI is selected you will be presented with the list of current Teams.\nTo add a new Team click on the Create New Team button. This will present you with the Create New Team dialog.\nCreate your own team by naming it [YOUR-INITIALS]-Team and add yourself by searching for your name and selecting the Add link next to your name. This should result in a dialog similar to the one below:\nYou can remove selected users by pressing Remove or the small x.\nMake sure you have your group created with your initials and with yourself added as a member, then click Done\nThis will bring you back to the Teams list that will now show your Team and the ones created by others.\nNote The Teams(s) you are a member of have a grey Member icon in front of it.\nIf no members are assigned to your Team, you should see a blue Add Members link instead of the member count, clicking on that link will get you to the Edit Team dialog where you can add yourself.\nThis is the same dialog you get when pressing the 3 dots … at the end of the line with your Team and selecting Edit Team\nThe … menu gives you the option to Edit, Join, Leave or Delete a Team (leave and join will depend on if you are currently a member).\n3. Adding Notification Rules You can set up specific Notification rules per team, by clicking on the Notification Policy tab, this will open the notification edit menu.\nBy default, the system offers you the ability to set up a general notification rule for your team.\nNote The Email all team members option means all members of this Team will receive an email with the Alert information, regardless of the alert type.\n3.1 Adding recipients You can add other recipients, by clicking Add Recipient. These recipients do not need to be Observability Cloud users.\nHowever, if you click on the link Configure separate notification tiers for different severity alerts you can configure every alert level independently.\nDifferent alert rules for the different alert levels can be configured, as shown in the above image.\nCritical and Major are using Splunk's On-Call Incident Management solution. For the Minor alerts, we send it to the Teams Slack channel and for Warning and Info we send an email.\n3.2 Notification Integrations In addition to sending alert notifications via email, you can configure Observability Cloud to send alert notifications to the services shown below.\nTake a moment to create some notification rules for your Team.",
    "description": "Introduction to Teams Create a Team and add members to the Team 1. Introduction to Teams To make sure that users see the dashboards and alerts that are relevant to them when using Observability Cloud, most organizations will use Observability Cloud’s Teams feature to assign a member to one or more Teams.\nIdeally, this matches work-related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in Observability Cloud.",
    "tags": [],
    "title": "Teams",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/servicebureau/teams/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 7. Transform Data",
    "content": "Exercise Add a transform processor: Switch to your Agent terminal window and edit the agent.yaml and add the following transform processor:\ntransform/logs: # Processor Type/Name log_statements: # Log Processing Statements - context: resource # Log Context statements: # List of attribute keys to keep - keep_keys(attributes, [\"com.splunk.sourcetype\", \"host.name\", \"otelcol.service.mode\"]) By using the -context: resource key we are targeting the resourceLog attributes of logs.\nThis configuration ensures that only the relevant resource attributes (com.splunk.sourcetype, host.name, otelcol.service.mode) are retained, improving log efficiency and reducing unnecessary metadata.\nAdding a Context Block for Log Severity Mapping: To properly set the severity_text and severity_number fields of a log record, we add a log context block within log_statements. This configuration extracts the level value from the log body, maps it to severity_text, and assigns the corresponding severity_number based on the log level:\n- context: log # Log Context statements: # Transform Statements Array - set(cache, ParseJSON(body)) where IsMatch(body, \"^\\\\{\") # Parse JSON log body into a cache object - flatten(cache, \"\") # Flatten nested JSON structure - merge_maps(attributes, cache, \"upsert\") # Merge cache into attributes, updating existing keys - set(severity_text, attributes[\"level\"]) # Set severity_text from the \"level\" attribute - set(severity_number, 1) where severity_text == \"TRACE\" # Map severity_text to severity_number - set(severity_number, 5) where severity_text == \"DEBUG\" - set(severity_number, 9) where severity_text == \"INFO\" - set(severity_number, 13) where severity_text == \"WARN\" - set(severity_number, 17) where severity_text == \"ERROR\" - set(severity_number, 21) where severity_text == \"FATAL\" The merge_maps function is used to combine two maps (dictionaries) into one. In this case, it merges the cache object (containing parsed JSON data from the log body) into the attributes map.\nParameters: attributes: The target map where the data will be merged. cache: The source map containing the parsed JSON data. \"upsert\": This mode ensures that if a key already exists in the attributes map, its value will be updated with the value from cache. If the key does not exist, it will be inserted. This step is crucial because it ensures that all relevant fields from the log body (e.g., level, message, etc.) are added to the attributes map, making them available for further processing or exporting.\nSummary of Key Transformations:\nParse JSON: Extracts structured data from the log body. Flatten JSON: Converts nested JSON objects into a flat structure. Merge Attributes: Integrates extracted data into log attributes. Map Severity Text: Assigns severity_text from the log’s level attribute. Assign Severity Numbers: Converts severity levels into standardized numerical values. You should have a single transform processor containing two context blocks: one whose context is for resource and one whose context is for log.\nThis configuration ensures that log severity is correctly extracted, standardized, and structured for efficient processing.\nTip This method of mapping all JSON fields to top-level attributes should only be used for testing and debugging OTTL. It will result in high cardinality in a production scenario.\nUpdate the logs pipeline: Add the transform/logs: processor into the logs: pipeline:\nlogs: receivers: - otlp - filelog/quotes processors: - memory_limiter - resourcedetection - resource/add_mode - transform/logs # Transform logs processor - batch exporters: - debug - otlphttp Validate the agent configuration using https://otelbin.io. For reference, the logs: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%%\rgraph LR\r%% Nodes\rREC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver\rREC2(filelog\u003cbr\u003efa:fa-download\u003cbr\u003equotes):::receiver\rPRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor\rPRO2(resourcedetection\u003cbr\u003efa:fa-microchip):::processor\rPRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor\rPRO4(transform\u003cbr\u003efa:fa-microchip\u003cbr\u003elogs):::processor\rPRO5(batch\u003cbr\u003efa:fa-microchip):::processor\rEXP1(otlphttp\u003cbr\u003efa:fa-upload):::exporter\rEXP2(\u0026ensp;\u0026ensp;debug\u0026ensp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter\r%% Links\rsubID1:::sub-logs\rsubgraph \" \"\rsubgraph subID1[**Logs**]\rdirection LR\rREC1 --\u003e PRO1\rREC2 --\u003e PRO1\rPRO1 --\u003e PRO2\rPRO2 --\u003e PRO3\rPRO3 --\u003e PRO4\rPRO4 --\u003e PRO5\rPRO5 --\u003e EXP2\rPRO5 --\u003e EXP1\rend\rend\rclassDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff;\rclassDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff;\rclassDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff;\rclassDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3;",
    "description": "Exercise Add a transform processor: Switch to your Agent terminal window and edit the agent.yaml and add the following transform processor:\ntransform/logs: # Processor Type/Name log_statements: # Log Processing Statements - context: resource # Log Context statements: # List of attribute keys to keep - keep_keys(attributes, [\"com.splunk.sourcetype\", \"host.name\", \"otelcol.service.mode\"]) By using the -context: resource key we are targeting the resourceLog attributes of logs.",
    "tags": [],
    "title": "7.1 Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/7-transform-data/7-1-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 7. Count \u0026 Sum Connector",
    "content": "Exercise Start the Gateway:\nIn the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent:\nIn the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml Send 12 Logs lines with the Loadgen:\nIn the Spans terminal window send 12 log lines, they should be read in two intervals. Do this with the following loadgen command:\n​ Loadgen ../loadgen -logs -json -count 12 Both the Agent and Gateway will display debug information, showing they are processing data. Wait until the loadgen completes.\nVerify metrics have been generated\nAs the logs are processed, the Agent generates metrics and forwards them to the Gateway, which then writes them to gateway-metrics.out.\nTo check if the metrics logs.full.count, logs.sw.count, logs.lotr.count, and logs.error.count are present in the output, run the following jq query:\n​ jq query command jq example output jq '.resourceMetrics[].scopeMetrics[].metrics[] | select(.name == \"logs.full.count\" or .name == \"logs.sw.count\" or .name == \"logs.lotr.count\" or .name == \"logs.error.count\") | {name: .name, value: (.sum.dataPoints[0].asInt // \"-\")}' gateway-metrics.out { \"name\": \"logs.sw.count\", \"value\": \"2\" } { \"name\": \"logs.lotr.count\", \"value\": \"2\" } { \"name\": \"logs.full.count\", \"value\": \"4\" } { \"name\": \"logs.error.count\", \"value\": \"2\" } { \"name\": \"logs.error.count\", \"value\": \"1\" } { \"name\": \"logs.sw.count\", \"value\": \"2\" } { \"name\": \"logs.lotr.count\", \"value\": \"6\" } { \"name\": \"logs.full.count\", \"value\": \"8\" } Tip Note: the logs.full.count normally is equal to logs.sw.count + logs.lotr.count, while the logs.error.count will be a random number.\nImportant Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "Exercise Start the Gateway:\nIn the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent:\nIn the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml Send 12 Logs lines with the Loadgen:\nIn the Spans terminal window send 12 log lines, they should be read in two intervals. Do this with the following loadgen command:",
    "tags": [],
    "title": "7.1 Testing the Count Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/7-sum-count/7-1-count-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 8. Routing Data",
    "content": "In this exercise, you will configure the Routing Connector in the gateway.yaml file. This setup enables the gateway to route traces based on the deployment.environment attribute in the spans you send. By implementing this, you can process and handle traces differently depending on their attributes.\nExercise In OpenTelemetry configuration files, connectors have their own dedicated section, similar to receivers and processors.\nAdd the routing connector: In the Gateway terminal window, edit gateway.yaml and uncomment the #connectors: section. Then, add the following below the connectors: section:\nconnectors: routing: default_pipelines: [traces/standard] # Default pipeline if no rule matches error_mode: ignore # Ignore errors in routing table: # Define routing rules # Routes spans to a target pipeline if the resourceSpan attribute matches the rule - statement: route() where attributes[\"deployment.environment\"] == \"security-applications\" pipelines: [traces/security] # Target pipeline The rules above apply to traces, but this approach also applies to metrics and logs, allowing them to be routed based on attributes in resourceMetrics or resourceLogs.\nConfigure file: exporters: The routing connector requires separate targets for routing. In the exporters: section, add two file exporters, file/traces/security and file/traces/standard, to ensure data is directed correctly.\nfile/traces/standard: # Exporter for regular traces path: \"./gateway-traces-standard.out\" # Path for saving trace data append: false # Overwrite the file each time file/traces/security: # Exporter for security traces path: \"./gateway-traces-security.out\" # Path for saving trace data append: false # Overwrite the file each time With the routing configuration complete, the next step is to configure the pipelines to apply these routing rules.",
    "description": "In this exercise, you will configure the Routing Connector in the gateway.yaml file. This setup enables the gateway to route traces based on the deployment.environment attribute in the spans you send. By implementing this, you can process and handle traces differently depending on their attributes.\nExercise In OpenTelemetry configuration files, connectors have their own dedicated section, similar to receivers and processors.\nAdd the routing connector: In the Gateway terminal window, edit gateway.yaml and uncomment the #connectors: section. Then, add the following below the connectors: section:",
    "tags": [],
    "title": "8.1 Configure the Routing Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/8-routing-data/8-1-connector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 9. Count \u0026 Sum Connector",
    "content": "Exercise Start the Gateway:\nIn the Gateway terminal window navigate to the [WORKSHOP]/9-sum-count directory and run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent:\nIn the Agent terminal window navigate to the [WORKSHOP]/9-sum-count directory and run:\n​ Start the Agent ../otelcol --config=agent.yaml Send 12 Logs lines with the Loadgen:\nIn the Spans terminal window navigate to the [WORKSHOP]/9-sum-count directory.\nSend 12 log lines, they should be read in two intervals. Do this with the following loadgen command:\n​ Loadgen ../loadgen -logs -json -count 12 Both the agent and gateway will display debug information, showing they are processing data. Wait until the loadgen completes.\nVerify metrics have been generated\nAs the logs are processed, the Agent generates metrics and forwards them to the Gateway, which then writes them to gateway-metrics.out.\nTo check if the metrics logs.full.count, logs.sw.count, logs.lotr.count, and logs.error.count are present in the output, run the following jq query:\n​ jq query command jq example output jq '.resourceMetrics[].scopeMetrics[].metrics[] | select(.name == \"logs.full.count\" or .name == \"logs.sw.count\" or .name == \"logs.lotr.count\" or .name == \"logs.error.count\") | {name: .name, value: (.sum.dataPoints[0].asInt // \"-\")}' gateway-metrics.out { \"name\": \"logs.sw.count\", \"value\": \"2\" } { \"name\": \"logs.lotr.count\", \"value\": \"2\" } { \"name\": \"logs.full.count\", \"value\": \"4\" } { \"name\": \"logs.error.count\", \"value\": \"2\" } { \"name\": \"logs.error.count\", \"value\": \"1\" } { \"name\": \"logs.sw.count\", \"value\": \"2\" } { \"name\": \"logs.lotr.count\", \"value\": \"6\" } { \"name\": \"logs.full.count\", \"value\": \"8\" } Tip Note: the logs.full.count normally is equal to logs.sw.count + logs.lotr.count, while the logs.error.count will be a random number.\nImportant Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "Exercise Start the Gateway:\nIn the Gateway terminal window navigate to the [WORKSHOP]/9-sum-count directory and run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent:\nIn the Agent terminal window navigate to the [WORKSHOP]/9-sum-count directory and run:\n​ Start the Agent ../otelcol --config=agent.yaml Send 12 Logs lines with the Loadgen:\nIn the Spans terminal window navigate to the [WORKSHOP]/9-sum-count directory.\nSend 12 log lines, they should be read in two intervals. Do this with the following loadgen command:",
    "tags": [],
    "title": "9.1 Testing the Count Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/9-sum-count/9-1-count-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "PetClinic Monolith WorkshopA workshop using automatic discovery and configuration for Java.\nPetClinic Kubernetes WorkshopLearn how to enable automatic discovery and configuration for your Java-based application running in Kubernetes. Experience real-time monitoring to help you maximize application behavior with end-to-end visibility.",
    "description": "Automatic Discovery Workshops",
    "tags": [],
    "title": "Automatic Discovery Workshops",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources \u003e Local Hosting",
    "content": "Install Multipass and Terraform for your operating system. On a Mac (Intel), you can also install via Homebrew e.g.\nbrew install multipass terraform jq Clone workshop repository:\ngit clone https://github.com/splunk/observability-workshop Change into Multipass directory:\ncd observability-workshop/local-hosting/multipass Log Observer Connect:\nIf you plan to use your own Splunk Observability Cloud Suite Org and or Splunk instance, you may need to create a new Log Observer Connect connection: Follow the instructions found in the documentation for Splunk Cloud or Splunk Enterprize.\nAdditional requirements for running your own Log Observer Connect connection are:\nCreate an index called splunk4rookies-workshop Make sure the Service account user used in the Log observer Connect connection has access to the splunk4rookies-workshop index (you can remove all other indexes, as all workshop log data should go to this index). Initialize Terraform:\n​ Command Example Output terraform init -upgrade ```text Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/random... - Finding latest version of hashicorp/local... - Finding larstobi/multipass versions matching \"~\u003e 1.4.1\"... - Installing hashicorp/random v3.5.1... - Installed hashicorp/random v3.5.1 (signed by HashiCorp) - Installing hashicorp/local v2.4.0... - Installed hashicorp/local v2.4.0 (signed by HashiCorp) - Installing larstobi/multipass v1.4.2... - Installed larstobi/multipass v1.4.2 (self-signed, key ID 797707331BF3549C) ``` Create Terraform variables file. Variables are kept in file terrform.tfvars and a template is provided, terraform.tfvars.template, to copy and edit:\ncp terraform.tfvars.template terraform.tfvars The following Terraform variables are required:\nswipe_id: SWiPE ID for the instance splunk_index: Splunk Index to send logs to. Defaults to splunk4rookies-workshop. Instance type variables:\nsplunk_presetup: Provide a preconfigured instance (OTel Collector and Online Boutique deployed with RUM enabled). The default is false. splunk_diab: Install and run Demo-in-a-Box. The default is false. tagging_workshop: Install and configure the Tagging Workshop. The default is false. otel_demo : Install and configure the OpenTelemetry Astronomy Shop Demo. This requires that splunk_presetup is set to false. The default is false. Optional advanced variables:\nwsversion: Set this to main if working on the development of the workshop, otherwise this can be omitted. architecture: Set this to the correct architecture, arm64 or amd64. Defaults to arm64 which is appropriate for Apple Silicon. Run terraform plan to check that all configuration is OK. Once happy run terraform apply to create the instance.\n​ Command Example Output terraform apply random_string.hostname: Creating... random_string.hostname: Creation complete after 0s [id=cynu] local_file.user_data: Creating... local_file.user_data: Creation complete after 0s [id=46a5c50e396a1a7820c3999c131a09214db903dd] multipass_instance.ubuntu: Creating... multipass_instance.ubuntu: Still creating... [10s elapsed] multipass_instance.ubuntu: Still creating... [20s elapsed] ... multipass_instance.ubuntu: Still creating... [1m30s elapsed] multipass_instance.ubuntu: Creation complete after 1m38s [name=cynu] data.multipass_instance.ubuntu: Reading... data.multipass_instance.ubuntu: Read complete after 1s [name=cynu] Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Outputs: instance_details = [ { \"image\" = \"Ubuntu 22.04.2 LTS\" \"image_hash\" = \"345fbbb6ec82 (Ubuntu 22.04 LTS)\" \"ipv4\" = \"192.168.205.185\" \"name\" = \"cynu\" \"state\" = \"Running\" }, ] Once the instance has been successfully created (this can take several minutes), exec into it using the name from the output above. The password for Multipass instance is Splunk123!.\n​ Command Example Output multipass exec cynu -- su -l splunk $ multipass exec kdhl -- su -l splunk Password: Waiting for cloud-init status... Your instance is ready! Validate the instance:\nkubectl version --output=yaml To delete the instance, first make sure you have exited from instance and then run the following command:\nterraform destroy",
    "description": "Learn how to create a local hosting environment with Multipass - Windows/Linux/Mac(Intel)",
    "tags": [],
    "title": "Local Hosting with Multipass",
    "uri": "/observability-workshop/v6.5/en/resources/local-hosting/multipass/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops",
    "content": "In this workshop, we’ll demonstrate how Splunk Observability Cloud delivers instant visibility into the user experience—covering everything from front-end applications to back-end services. You’ll have the opportunity to explore some of the platform’s most powerful features, which set it apart from other observability solutions:\nInfrastructure Monitoring Full-fidelity Real User Monitoring (RUM) Complete end-to-end trace visibility with NoSample Full-fidelity Application Performance Monitoring (APM) No-code log querying Synthetic Monitoring Root cause analysis with tag analytics and error stacks Related Content for seamless navigation between components One of the core strengths of Splunk Observability Cloud is its ability to unify telemetry data, creating a comprehensive picture of both the end-user experience and your entire application stack.\nThe workshop will focus on a microservices-based e-commerce application deployed on AWS EC2 instances. Users can browse products, add items to a cart, and complete purchases. This application is fully instrumented with OpenTelemetry to capture detailed performance data.\nWhat is OpenTelemetry?\nOpenTelemetry is an open-source collection of tools, APIs, and software development kits (SDKs) designed to help you instrument, generate, collect, and export telemetry data—such as metrics, traces, and logs. This data enables in-depth analysis of your software’s performance and behavior.\nThe OpenTelemetry community is growing rapidly, supported by leading companies like Splunk, Google, Microsoft, and Amazon. It currently has the second-largest number of contributors within the Cloud Native Computing Foundation, following only Kubernetes.",
    "description": "In this workshop, we will be showing how Splunk Observability Cloud provides instant visibility of the user experience – from the perspective of the front-end application to its back-end services – Letting you experience some of the most compelling product features and differentiators of Splunk Observability Cloud.",
    "tags": [],
    "title": "Observability Cloud",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops",
    "content": "Abstract Organizations getting started with OpenTelemetry may begin by sending data directly to an observability backend. While this works well for initial testing, using the OpenTelemetry collector as part of your observability architecture provides numerous benefits and is recommended for any production deployment.\nIn this workshop, we will be focusing on using the OpenTelemetry collector and starting with the fundamentals of configuring the receivers, processors, and exporters ready to use with Splunk Observability Cloud. The journey will take attendees from novices to being able to start adding custom components to help solve for their business observability needs for their distributed platform.\nNinja Sections Throughout the workshop there will be expandable Ninja Sections, these will be more hands on and go into further technical detail that you can explore within the workshop or in your own time.\nPlease note that the content in these sections may go out of date due to the frequent development being made to the OpenTelemetry project. Links will be provided in the event details are out of sync, please let us know if you spot something that needs updating.\nNinja: Test Me! By completing this workshop you will officially be an OpenTelemetry Collector Ninja!\nTarget Audience This interactive workshop is for developers and system administrators who are interested in learning more about architecture and deployment of the OpenTelemetry Collector.\nPrerequisites Attendees should have a basic understanding of data collection Command line and vim/vi experience. A instance/host/VM running Ubuntu 20.04 LTS or 22.04 LTS. Minimum requirements are an AWS/EC2 t2.micro (1 CPU, 1GB RAM, 8GB Storage) Learning Objectives By the end of this talk, attendees will be able to:\nUnderstand the components of OpenTelemetry Use receivers, processors, and exporters to collect and analyze data Identify the benefits of using OpenTelemetry Build a custom component to solve their business needs OpenTelemetry Architecture %%{ init:{ \"theme\":\"base\", \"themeVariables\": { \"primaryColor\": \"#ffffff\", \"clusterBkg\": \"#eff2fb\", \"defaultLinkColor\": \"#333333\" } } }%% flowchart LR; subgraph Collector A[OTLP] --\u003e M(Receivers) B[JAEGER] --\u003e M(Receivers) C[Prometheus] --\u003e M(Receivers) end subgraph Processors M(Receivers) --\u003e H(Filters, Attributes, etc) E(Extensions) end subgraph Exporters H(Filters, Attributes, etc) --\u003e S(OTLP) H(Filters, Attributes, etc) --\u003e T(JAEGER) H(Filters, Attributes, etc) --\u003e U(Prometheus) end",
    "description": "Learn the concepts of the OpenTelemetry Collector and how to use it to send data to Splunk Observability Cloud.",
    "tags": [],
    "title": "Making Your Observability Cloud Native With OpenTelemetry",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios",
    "content": "The elasticity of cloud architectures means that monitoring artifacts must scale elastically as well, breaking the paradigm of purpose-built monitoring assets. As a result, administrative overhead, visibility gaps, and tech debt skyrocket while MTTR slows. This typically happens for three reasons:\nComplex and Inefficient Data Management: Infrastructure data is scattered across multiple tools with inconsistent naming conventions, leading to fragmented views and poor metadata and labelling. Managing multiple agents and data flows adds to the complexity. Inadequate Monitoring and Troubleshooting Experience: Slow data visualization and troubleshooting, cluttered with bespoke dashboards and manual correlations, are hindered further by the lack of monitoring tools for ephemeral technologies like Kubernetes and serverless functions. Operational and Scaling Challenges: Manual onboarding, user management, and chargeback processes, along with the need for extensive data summarization, slow down operations and inflate administrative tasks, complicating data collection and scalability. To address these challenges you need a way to:\nStandardize Data Collection and Tags: Centralized monitoring with a single, open-source agent to apply uniform naming standards and ensure metadata for visibility. Optimize data collection and use a monitoring-as-code approach for consistent collection and tagging. Reuse Content Across Teams: Streamline new IT infrastructure onboarding and user management with templates and automation. Utilize out-of-the-box dashboards, alerts, and self-service tools to enable content reuse, ensuring uniform monitoring and reducing manual effort. Improve Timeliness of Alerts: Utilize highly performant open source data collection, combined with real-time streaming-based data analytics and alerting, to enhance the timeliness of notifications. Automatically configured alerts for common problem patterns (AutoDetect) and minimal yet effective monitoring dashboards and alerts will ensure rapid response to emerging issues, minimizing potential disruptions. Correlate Infrastructure Metrics and Logs: Achieve full monitoring coverage of all IT infrastructure by enabling seamless correlation between infrastructure metrics and logs. High-performing data visualization and a single source of truth for data, dashboards, and alerts will simplify the correlation process, allowing for more effective troubleshooting and analysis of the IT environment. In this workshop, we’ll explore:\nHow to standardize data collection and tags using OpenTelemetry. How to reuse content across teams. How to improve timelines of alerts. How to correlate infrastructure metrics and logs. Tip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "This scenario is for ITOps teams managing a hybrid infrastructure that need to troubleshoot cloud-native performance issues, by correlating real-time metrics with logs to troubleshoot faster, improve MTTD/MTTR, and optimize costs.",
    "tags": [],
    "title": "Optimize Cloud Monitoring",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 6.2 Optional Exercise",
    "content": "This is the first section of our optimal Kubernetes Navigator exercise. Below is some high-level information regarding Kubernetes, just in case you’re not familiar with it.\nKubernetes Terminology K8s, short for Kubernetes, is an open-source container orchestration platform. It manages the deployment, scaling, and maintenance of containerized applications, and we use it in this workshop to host our e-commerce application\nSome terminology:\nA Kubernetes cluster is a group of machines, called nodes, that work together to run containerized applications. Nodes are individual servers or VMs in the cluster. Typically, you would have several nodes in a cluster but you may have just one node, just like in this workshop. Pods are the smallest deployable units in Kubernetes, representing one or more containers that share the same network and storage, enabling efficient application scaling and management Applications are a collection of one or more Pods interacting together to provide a service. Namespaces help you keep your applications organized and separate within the cluster, by providing a logical separation for multiple teams or projects within a cluster. Workloads are like a task list and define how many instances of your application should run, how they should be created, and how they should respond to failures Please select the K8s nodes tile from the Tile pane if you have not yet done so. (Select Kubernetes as your Technology). This will bring you to the Kubernetes Navigator Page.\nThe screenshot above shows the main part of the Kubernetes navigator. It will show all the clusters \u0026 their nodes that send metrics to Splunk Observability Cloud, and the first row of charts that show cluster-based Metrics. In the workshop, you will mostly see single-node Kubernetes clusters.\nBefore we dive deeper, let’s make sure we are looking at our cluster.\nExercise First, use the option to pick your cluster. This can be done by selecting k8s.cluster.name from the filter drop-down box. You then can start typing the name of your cluster, (as provided by your instructor). The name should also appear in the drop-down values. Select yours and make sure just the one for your workshop is highlighted with a . Click the Apply Filter button to focus on our Cluster We now should have a single cluster visible. Let’s move on to the next page of this exercise and look at your cluster in detail.",
    "description": "This is the first section of our optimal Kubernetes Navigator exercise. Below is some high-level information regarding Kubernetes, just in case you’re not familiar with it.\nKubernetes Terminology K8s, short for Kubernetes, is an open-source container orchestration platform. It manages the deployment, scaling, and maintenance of containerized applications, and we use it in this workshop to host our e-commerce application\nSome terminology:\nA Kubernetes cluster is a group of machines, called nodes, that work together to run containerized applications. Nodes are individual servers or VMs in the cluster. Typically, you would have several nodes in a cluster but you may have just one node, just like in this workshop. Pods are the smallest deployable units in Kubernetes, representing one or more containers that share the same network and storage, enabling efficient application scaling and management Applications are a collection of one or more Pods interacting together to provide a service. Namespaces help you keep your applications organized and separate within the cluster, by providing a logical separation for multiple teams or projects within a cluster. Workloads are like a task list and define how many instances of your application should run, how they should be created, and how they should respond to failures Please select the K8s nodes tile from the Tile pane if you have not yet done so. (Select Kubernetes as your Technology). This will bring you to the Kubernetes Navigator Page.",
    "tags": [],
    "title": "Infrastructure Exercise - Part 1",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/30-im-exercise/1-im-exercise/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6.2 Optional Exercise",
    "content": "This is the first section of our optimal Kubernetes Navigator exercise. Below is some high-level information regarding Kubernetes, just in case you’re not familiar with it.\nKubernetes Terminology K8s, short for Kubernetes, is an open-source container orchestration platform. It manages the deployment, scaling, and maintenance of containerized applications, and we use it in this workshop to host our e-commerce application\nSome terminology:\nA Kubernetes cluster is a group of machines, called nodes, that work together to run containerized applications. Nodes are individual servers or VMs in the cluster. Typically, you would have several nodes in a cluster but you may have just one node, just like in this workshop. Pods are the smallest deployable units in Kubernetes, representing one or more containers that share the same network and storage, enabling efficient application scaling and management Applications are a collection of one or more Pods interacting together to provide a service. Namespaces help you keep your applications organized and separate within the cluster, by providing a logical separation for multiple teams or projects within a cluster. Workloads are like a task list and define how many instances of your application should run, how they should be created, and how they should respond to failures Please select the K8s nodes tile from the Tile pane if you have not yet done so. (Select Kubernetes as your Technology). This will bring you to the Kubernetes Navigator Page.\nThe screenshot above shows the main part of the Kubernetes navigator. It will show all the clusters \u0026 their nodes that send metrics to Splunk Observability Cloud, and the first row of charts that show cluster-based Metrics. In the workshop, you will mostly see single-node Kubernetes clusters.\nBefore we dive deeper, let’s make sure we are looking at our cluster.\nExercise First, use the option to pick your cluster. This can be done by selecting k8s.cluster.name from the filter drop-down box. You then can start typing the name of your cluster, (as provided by your instructor). The name should also appear in the drop-down values. Select yours and make sure just the one for your workshop is highlighted with a . Click the Apply Filter button to focus on our Cluster We now should have a single cluster visible. Let’s move on to the next page of this exercise and look at your cluster in detail.",
    "description": "This is the first section of our optimal Kubernetes Navigator exercise. Below is some high-level information regarding Kubernetes, just in case you’re not familiar with it.\nKubernetes Terminology K8s, short for Kubernetes, is an open-source container orchestration platform. It manages the deployment, scaling, and maintenance of containerized applications, and we use it in this workshop to host our e-commerce application\nSome terminology:\nA Kubernetes cluster is a group of machines, called nodes, that work together to run containerized applications. Nodes are individual servers or VMs in the cluster. Typically, you would have several nodes in a cluster but you may have just one node, just like in this workshop. Pods are the smallest deployable units in Kubernetes, representing one or more containers that share the same network and storage, enabling efficient application scaling and management Applications are a collection of one or more Pods interacting together to provide a service. Namespaces help you keep your applications organized and separate within the cluster, by providing a logical separation for multiple teams or projects within a cluster. Workloads are like a task list and define how many instances of your application should run, how they should be created, and how they should respond to failures Please select the K8s nodes tile from the Tile pane if you have not yet done so. (Select Kubernetes as your Technology). This will bring you to the Kubernetes Navigator Page.",
    "tags": [],
    "title": "Infrastructure Exercise - Part 1",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/30-im-exercise/1-im-exercise/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops",
    "content": "The goal is to walk through the basic steps to configure the following components of the Splunk Observability Cloud platform:\nSplunk Infrastructure Monitoring (IM) Splunk Automatic Discovery for Java (APM) Database Query Performance AlwaysOn Profiling Splunk Real User Monitoring (RUM) RUM to APM Correlation Splunk Log Observer (LO) We will also show the steps about how to clone (download) a sample Java application (Spring PetClinic), as well as how to compile, package and run the application.\nOnce the application is up and running, we will instantly start seeing metrics, traces and logs via the automatic discovery and configuration for Java 2.x that will be used by the Splunk APM product.\nAfter that, we will instrument PetClinic’s end user interface (HTML pages rendered by the application) with the Splunk OpenTelemetry Javascript Libraries (RUM) that will generate RUM traces around all the individual clicks and page loads executed by an end user.\nLastly, we will view the logs generated by the automatic injection of trace metadata into the PetClinic application logs.\nPrerequisites Outbound SSH access to port 2222. Outbound HTTP access to port 8083. Familiarity with the bash shell and vi/vim editor.",
    "description": "A workshop using automatic discovery and configuration for Java.",
    "tags": [],
    "title": "PetClinic Monolith Workshop",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/1-petclinic-monolith/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops",
    "content": "During this technical Splunk Observability Cloud Infrastructure Monitoring and APM Workshop, you will build out an environment based on a lightweight Kubernetes1 cluster.\nTo simplify the workshop modules, a pre-configured AWS/EC2 instance is provided.\nThe instance is pre-configured with all the software required to deploy the Splunk OpenTelemetry Connector2 in Kubernetes, deploy an NGINX^3 ReplicaSet^4 and finally deploy a microservices-based application which has been instrumented using OpenTelemetry to send metrics, traces, spans and logs3.\nThe workshops also introduce you to dashboards, editing and creating charts, creating detectors to fire alerts, Monitoring as Code and the Service Bureau4\nBy the end of these technical workshops, you will have a good understanding of some of the key features and capabilities of the Splunk Observability Cloud.\nHere are the instructions on how to access your pre-configured AWS/EC2 instance\nKubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. ↩︎\nOpenTelemetry Collector offers a vendor-agnostic implementation on how to receive, process and export telemetry data. In addition, it removes the need to run, operate and maintain multiple agents/collectors to support open-source telemetry data formats (e.g. Jaeger, Prometheus, etc.) sending to multiple open-source or commercial back-ends. ↩︎\nJaeger, inspired by Dapper and OpenZipkin, is a distributed tracing system released as open source by Uber Technologies. It is used for monitoring and troubleshooting microservices-based distributed systems ↩︎\nMonitoring as Code and Service Bureau ↩︎",
    "description": "Splunk delivers real-time monitoring and troubleshooting to help you maximize infrastructure performance with complete visibility.",
    "tags": [],
    "title": "Splunk IM",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Welcome Get insights into your applications and infrastructure in real-time with the help of the monitoring, analytics and response tools of the Splunk Observability Cloud\nThese workshops are going to take you through the best-in-class observability platform for ingesting, monitoring, visualizing and analyzing metrics, traces and logs.\nSplunk4Rookies WorkshopsThe following workshops are for Rookies.\nObservability CloudIn this workshop, we will be showing how Splunk Observability Cloud provides instant visibility of the user experience – from the perspective of the front-end application to its back-end services – Letting you experience some of the most compelling product features and differentiators of Splunk Observability Cloud.\nFinancial Services Observability CloudThis workshop, tailored for the Financial Services sector, will demonstrate how Splunk Observability Cloud delivers real-time insights into user experience, spanning from front-end applications to back-end services. You'll explore key product features and unique advantages that set Splunk Observability Cloud apart.\nSplunk4Ninjas WorkshopsThe following workshops require Ninja skills, wax on, wax off.\nAutomatic Discovery WorkshopsAutomatic Discovery Workshops\nHorizontal Pod AutoscalingThis workshop will equip you with the basic understanding of monitoring Kubernetes using the Splunk OpenTelemetry Collector\nOpenTelemetry Collector Workshops OpenTelemetry Collector ConceptsLearn the concepts of the OpenTelemetry Collector and how to use it to send data to Splunk Observability Cloud.\nAdvanced OpenTelemetry CollectorPractice setting up the OpenTelemetry Collector configuration from scratch and go though several advanced configuration scenarios's.\nSplunk Synthetic ScriptingProactively find and fix performance issues across user flows, business transactions and APIs to deliver better digital experiences.\nLambda TracingThis workshop will enable you to build a distributed trace for a small serverless application that runs on AWS Lambda, producing and consuming a message via AWS Kinesis\nDashboard WorkshopDashboard\nHands-On OpenTelemetry, Docker, and K8sBy the end of this workshop you'll have gotten hands-on experience instrumenting a .NET application with OpenTelemetry, then Dockerizing the application and deploying it to Kubernetes. You’ll also gain experience deploying the OpenTelemetry collector using Helm, customizing the collector configuration, and troubleshooting collector configuration issues.\nSolving Problems with O11y CloudBy the end of this workshop you'll have gotten hands-on experience deploying the OpenTelemetry Collector, instrumenting an application with OpenTelemetry, capturing tags from the application, and using Troubleshooting MetricSets and Tag Spotlight to determine the root cause of an issue.\nIngest Processor for Observability CloudScenario Description\nScenariosLearn how to build observability solutions with Splunk\nOptimize Cloud MonitoringThis scenario is for ITOps teams managing a hybrid infrastructure that need to troubleshoot cloud-native performance issues, by correlating real-time metrics with logs to troubleshoot faster, improve MTTD/MTTR, and optimize costs.\nDebug Problems in MicroservicesThis scenario helps engineering teams identify and fix issues caused by planned and unplanned changes to their microservices-based applications.\nOptimize End User ExperiencesUse Splunk Real User Monitoring (RUM) and Synthetics to get insight into end user experience, and proactively test scenarios to improve that experience.\nResourcesResources for learning about Splunk Observability Cloud\nDimension, Properties and TagsOne conversation that frequently comes up is Dimensions vs Properties and when you should use one verus the other.\nOpenTelemetry TaggingWhen deploying OpenTelemetry in a large organization, it’s critical to define a standardized naming convention for tagging, and a governance process to ensure the convention is adhered to.\nSPLUNK ARCADE - PLAY. LEARN. OBSERVE!SPLUNK ARCADE - Where Retro Gaming Meets Real-World Observability\nLocal HostingResources for setting up a locally hosted workshop environment.\nUnsupported Field WorkshopsWorkshops that use unsupported fields in Splunk Observability Cloud\nSplunk IMSplunk delivers real-time monitoring and troubleshooting to help you maximize infrastructure performance with complete visibility.\nNodeJS Zero-Config WorkshopA workshop using Zero Configuration Auto-Instrumentation for NodeJS.\nGDI (OTel \u0026 UF)Learn how to get data into Splunk Observability Cloud with OpenTelemetry and the Splunk Universal Forwarder.\nSplunk OnCallMake expensive service outages a thing of the past. Remediate issues faster, reduce on-call burnout and keep your services up and running.",
    "description": "Learn how to build observability solutions with Splunk",
    "tags": [],
    "title": "Splunk Observability Workshops",
    "uri": "/observability-workshop/v6.5/en/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "Observability CloudIn this workshop, we will be showing how Splunk Observability Cloud provides instant visibility of the user experience – from the perspective of the front-end application to its back-end services – Letting you experience some of the most compelling product features and differentiators of Splunk Observability Cloud.\nFinancial Services Observability CloudThis workshop, tailored for the Financial Services sector, will demonstrate how Splunk Observability Cloud delivers real-time insights into user experience, spanning from front-end applications to back-end services. You'll explore key product features and unique advantages that set Splunk Observability Cloud apart.",
    "description": "The following workshops are for Rookies.",
    "tags": [],
    "title": "Splunk4Rookies Workshops",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences",
    "content": "Let’s quickly set up some tests in Synthetics to immediately start understanding our end user experience, without waiting for real users to interact with our app.\nWe can capture not only the performance and availability of our own apps and endpoints, but also those third parties we rely on any time of the day or night.\nTip If you find that your tests are being bot-blocked, see the docs for tips on how to allow Synthetic testing. if you need to test something that is not accessible externally, see private location instructions.",
    "description": "Let’s quickly set up some tests in Synthetics to immediately start understanding our end user experience, without waiting for real users to interact with our app.\nWe can capture not only the performance and availability of our own apps and endpoints, but also those third parties we rely on any time of the day or night.\nTip If you find that your tests are being bot-blocked, see the docs for tips on how to allow Synthetic testing. if you need to test something that is not accessible externally, see private location instructions.",
    "tags": [],
    "title": "Synthetics",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "The Spring PetClinic Java application is a simple microservices application that consists of frontend and backend services. The frontend service is a Spring Boot application that serves a web interface to interact with backend services. The backend services are Spring Boot applications that serve RESTful API’s to interact with a MySQL database.\nBy the end of this workshop, you will have a better understanding of how to enable automatic discovery and configuration for your Java-based applications running in Kubernetes.\nThe diagram below details the architecture of the Spring PetClinic Java application running in Kubernetes with the Splunk OpenTelemetry Operator and automatic discovery and configuration enabled.\nBased on the example Josh Voravong created.",
    "description": "The Spring PetClinic Java application is a simple microservices application that consists of frontend and backend services. The frontend service is a Spring Boot application that serves a web interface to interact with backend services. The backend services are Spring Boot applications that serve RESTful API’s to interact with a MySQL database.\nBy the end of this workshop, you will have a better understanding of how to enable automatic discovery and configuration for your Java-based applications running in Kubernetes.",
    "tags": [],
    "title": "Architecture",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/1-architecture/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 7. Log Observer",
    "content": "In order to see logs click on the Log Observer in the left-hand menu. Once in Log Observer please ensure Index on the filter bar is set to splunk4rookies-workshop. (1)\nNext, click Add Filter and search, using the Fields (2) option for the field deployment.environment (3). Then from the dropdown list, select your workshop instance, (4) and click = (to include). You will now see only the log messages from your PetClinic application.\nNext, search for the field service.name, selecting the value customers-service and click = (to include). This should now appear in the filter bar (1). Next click on the Button Run Search (2).\nThis wil refresh the log entries and they will now be reduced to show the entries from your customers-service only.\nClick on an entry that starts with “Saving pet” (1). A side pane will open where you can see the detailed information, including the relevant trace and span IDs (2).",
    "description": "In order to see logs click on the Log Observer in the left-hand menu. Once in Log Observer please ensure Index on the filter bar is set to splunk4rookies-workshop. (1)\nNext, click Add Filter and search, using the Fields (2) option for the field deployment.environment (3). Then from the dropdown list, select your workshop instance, (4) and click = (to include). You will now see only the log messages from your PetClinic application.",
    "tags": [],
    "title": "Viewing the Logs",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/7-log-observer-connect/1-view-logs/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 1. Getting Started",
    "content": "How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty1 or your web browser. Verify your connection to your AWS/EC2 cloud instance. Using Putty (Optional) Using Multipass (Optional) 1. AWS/EC2 IP Address In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2.\nTo get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader.\nSearch for your AWS/EC2 instance by looking for your first and last name, as provided during registration for this workshop.\nFind your allocated IP address, SSH command (for Mac OS, Linux and the latest Windows versions) and password to enable you to connect to your workshop instance.\nIt also has the Browser Access URL that you can use in case you cannot connect via SSH or Putty - see EC2 access via Web browser\nImportant Please use SSH or Putty to gain access to your EC2 instance if possible and make a note of the IP address as you will need this during the workshop.\n2. SSH (Mac OS/Linux) Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device, or on Windows 10 and above.\nTo use SSH, open a terminal on your system and type ssh splunk@x.x.x.x (replacing x.x.x.x with the IP address found in Step #1).\nWhen prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes.\nEnter the password provided in the Google Sheet from Step #1.\nUpon successful login, you will be presented with the Splunk logo and the Linux prompt.\n3. SSH (Windows 10 and above) The procedure described above is the same on Windows 10, and the commands can be executed either in the Windows Command Prompt or PowerShell. However, Windows regards its SSH Client as an “optional feature”, which might need to be enabled.\nYou can verify if SSH is enabled by simply executing ssh\nIf you are shown a help text on how to use the SSH command (like shown in the screenshot below), you are all set.\nIf the result of executing the command looks something like the screenshot below, you want to enable the “OpenSSH Client” feature manually.\nTo do that, open the “Settings” menu, and click on “Apps”. While in the “Apps \u0026 features” section, click on “Optional features”.\nHere, you are presented with a list of installed features. On the top, you see a button with a plus icon to “Add a feature”. Click it. In the search input field, type “OpenSSH”, and find a feature called “OpenSSH Client”, or respectively, “OpenSSH Client (Beta)”, click on it, and click the “Install”-button.\nNow you are set! In case you are not able to access the provided instance despite enabling the OpenSSH feature, please do not shy away from reaching out to the course instructor, either via chat or directly.\nAt this point you are ready to continue and start the workshop\n4. Putty (For Windows Versions prior to Windows 10) If you do not have SSH pre-installed or if you are on a Windows system, the best option is to install Putty which you can find here.\nImportant If you cannot install Putty, please go to Web Browser (All).\nOpen Putty and enter in the Host Name (or IP address) field the IP address provided in the Google Sheet.\nYou can optionally save your settings by providing a name and pressing Save.\nTo then login to your instance click on the Open button as shown above.\nIf this is the first time connecting to your AWS/EC2 workshop instance, you will be presented with a security dialogue, please click Yes.\nOnce connected, login in as splunk and the password is the one provided in the Google Sheet.\nOnce you are connected successfully you should see a screen similar to the one below:\nAt this point, you are ready to continue and start the workshop\n5. Web Browser (All) If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser.\nNote This assumes that access to port 6501 is not restricted by your company’s firewall.\nOpen your web browser and type http://x.x.x.x:6501 (where X.X.X.X is the IP address from the Google Sheet).\nOnce connected, login in as splunk and the password is the one provided in the Google Sheet.\nOnce you are connected successfully you should see a screen similar to the one below:\nUnlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions.\nWhen the workshop asks you to copy instructions into your terminal, please do the following:\nCopy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from the browser as show below:\nThis will open a dialogue box asking for the text to be pasted into the web terminal:\nPaste the text in the text box as shown, then press OK to complete the copy and paste process.\nNote Unlike regular SSH connection, the web browser has a 60-second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal.\nSimply click the Connect button and you will be reconnected and will be able to continue.\nAt this point, you are ready to continue and start the workshop.\n6. Multipass (All) If you are unable to access AWS but want to install software locally, follow the instructions for using Multipass.\nDownload Putty ↩︎",
    "description": "How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty1 or your web browser. Verify your connection to your AWS/EC2 cloud instance. Using Putty (Optional) Using Multipass (Optional) 1. AWS/EC2 IP Address In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2.\nTo get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader.",
    "tags": [],
    "title": "How to connect to your workshop environment",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/1-getting-started/1-access-ec2/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour \u003e 1. Getting Started",
    "content": "After you have registered and logged into Splunk Observability Cloud you will be taken to the home or landing page. Here, you will find several useful features to help you get started.\nExplore your data pane: Displays which integrations are enabled and allows you to add additional integrations if you are an Administrator. Documentation pane: Training videos and links to documentation to get you started with Splunk Observability Cloud. Recents pane: Recently created/visited dashboards and/or detectors for quick access. Main Menu pane: Navigate the components of Splunk Observability Cloud. Org Switcher: Easily switch between Organizations (if you are a member of more than one Organization). Expand/Contract Main Menu: Expand » / Collapse « the main menu if space is at a premium. Let’s start with our first exercise:\nExercise Expand the Main Menu and click on Settings. Check in the Org Switcher if you have access to more than one Organization. Tip If you have used Splunk Observability before, you may be placed in an Organization you have used previously. Make sure you are in the correct workshop organization. Verify this with your instructor if you have access to multiple Organizations.\nExercise Click Onboarding Guidance (Here you can toggle the visibility of the onboarding panes. This is useful if you know the product well enough, and can use the space to show more information). Hide the Onboarding Content for the Home Page. At the bottom of the menu, select your preferred appearance: Light, Dark or Auto mode. Did you also notice this is where the Sign Out option is? Please don’t 😊 ! Click \u003c to get back to the main menu. Next, let’s check out Splunk Real User Monitoring (RUM).",
    "description": "After you have registered and logged into Splunk Observability Cloud you will be taken to the home or landing page. Here, you will find several useful features to help you get started.\nExplore your data pane: Displays which integrations are enabled and allows you to add additional integrations if you are an Administrator. Documentation pane: Training videos and links to documentation to get you started with Splunk Observability Cloud. Recents pane: Recently created/visited dashboards and/or detectors for quick access. Main Menu pane: Navigate the components of Splunk Observability Cloud. Org Switcher: Easily switch between Organizations (if you are a member of more than one Organization). Expand/Contract Main Menu: Expand » / Collapse « the main menu if space is at a premium. Let’s start with our first exercise:",
    "tags": [],
    "title": "Home Page",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/1-homepage/1-home-page/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour \u003e 1. Getting Started",
    "content": "After you have registered and logged into Splunk Observability Cloud you will be taken to the home or landing page. Here, you will find several useful features to help you get started.\nExpand/Contract Main Menu: Expand » / Collapse « the main menu if space is at a premium. Org Switcher: Switch between Organizations (if you are a member of more than one Organization). Teams: Focus on assets of interest to the Teams you belong to. Active Alerts: Quickly see what might require your immediate attention. AI Assistant: Accelerate root cause analysis and get expert guidance to find and fix issues faster. Let’s start with our first exercise:\nExercise Click on your name in the top right and check the Organization name. If you are not in Observability Workshop AMER, select it now. At the bottom of the menu, select your preferred appearance: Light, Dark or Auto mode. Did you also notice this is where the Log Out option is? Please don’t 😊 ! Tip If you have used Splunk Observability before, you may be placed in an Organization you have used previously. Make sure you are in the correct workshop organization. Verify this with your instructor if you have access to multiple Organizations.\nExercise Click the help icon and select Onboarding Guidance. (You can toggle the visibility of the onboarding panes. This is useful if you know the product well enough, and can use the space to show more information).\nHide the Onboarding Content for the Home Page.\nClick \u003c to get back to the main menu.\nNext, let’s check out Splunk Real User Monitoring (RUM).",
    "description": "After you have registered and logged into Splunk Observability Cloud you will be taken to the home or landing page. Here, you will find several useful features to help you get started.\nExpand/Contract Main Menu: Expand » / Collapse « the main menu if space is at a premium. Org Switcher: Switch between Organizations (if you are a member of more than one Organization). Teams: Focus on assets of interest to the Teams you belong to. Active Alerts: Quickly see what might require your immediate attention. AI Assistant: Accelerate root cause analysis and get expert guidance to find and fix issues faster. Let’s start with our first exercise:",
    "tags": [],
    "title": "Home Page",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/1-homepage/1-home-page/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud \u003e 1. Getting Started",
    "content": "How to retrieve the URL for your Splunk Enterprise Cloud instances. How to access the Splunk Observability Cloud workshop organization. 1. Splunk Cloud Instances There are three instances that will be used throughout this workshop which have already been provisioned for you:\nSplunk Enterprise Cloud Splunk Ingest Processor (SCS Tenant) Splunk Observability Cloud The Splunk Enterprise Cloud and Ingest Processor instances are hosted in Splunk Show. If you were invited to the workshop, you should have received an email with an invite to the event in Splunk Show or a link to the event will have been provided at the beginning of the workshop.\nLogin to Splunk Show using your splunk.com credentials. You should see the event for this workshop. Open the event to see the instance details for your Splunk Cloud and Ingest Processor instances.\nNote Take note of the User Id provided in your Splunk Show event details. This number will be included in the sourcetype that you will use for searching and filtering the Kubernetes data. Because this is a shared environment only use the participant number provided so that other participants data is not effected.\n2. Splunk Observability Cloud Instances You should have also received an email to access the Splunk Observability Cloud workshop organization (You may need to check your spam folder). If you have not received an email, let your workshop instructor know. To access the environment click the Join Now button.\nImportant If you access the event before the workshop start time, your instances may not be available yet. Don’t worry, they will be provided once the workshop begins.\nAdditionally, you have been invited to a Splunk Observability Cloud workshop organization. The invitation includes a link to the environment. If you don’t have a Splunk Observability Cloud account already, you will be asked to create one. If you already have one, you can log in to the instance and, you will see the workshop organization in your available organizations.",
    "description": "How to retrieve the URL for your Splunk Enterprise Cloud instances. How to access the Splunk Observability Cloud workshop organization. 1. Splunk Cloud Instances There are three instances that will be used throughout this workshop which have already been provisioned for you:\nSplunk Enterprise Cloud Splunk Ingest Processor (SCS Tenant) Splunk Observability Cloud The Splunk Enterprise Cloud and Ingest Processor instances are hosted in Splunk Show. If you were invited to the workshop, you should have received an email with an invite to the event in Splunk Show or a link to the event will have been provided at the beginning of the workshop.",
    "tags": [],
    "title": "How to connect to your workshop environment",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/1-getting-started/1-access-cloud-instances/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence \u003e 1. Getting Started",
    "content": "Starting up your Workshop This workshop is available on Splunk Show and will take some time to start up all of your resources. It contains a Splunk environment with IT Service Intelligence, the Splunk Infrastructure Monitoring Add-On, as well as the recently updated AppDynamics Add-on all preconfigured.\nThe Workshop is titled “Tech Summit 2025: OBS-122” or you can go directly to it’s entry on Splunk Show. It takes approximately 15 minutes to start up however data generation and ingestion will take up to a half hour.\nSplunk Observability Cloud Access Creating an alert in Observability Cloud should be done in the Observability Cloud US1 Show Playground Org.",
    "description": "Starting up your Workshop This workshop is available on Splunk Show and will take some time to start up all of your resources. It contains a Splunk environment with IT Service Intelligence, the Splunk Infrastructure Monitoring Add-On, as well as the recently updated AppDynamics Add-on all preconfigured.\nThe Workshop is titled “Tech Summit 2025: OBS-122” or you can go directly to it’s entry on Splunk Show. It takes approximately 15 minutes to start up however data generation and ingestion will take up to a half hour.",
    "tags": [],
    "title": "How to connect to your workshop environment",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/1-getting-started/1-access-cloud-instances/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk APM \u003e 1. The Online Boutique",
    "content": "1. RED Metrics Go to Dashboards → All Dashboards → APM Services → Service. Here we can view the RED metrics (Rate, Error \u0026 Duration) for the Online Boutique application.\nFor this, we need to know the name of your application environment. In this workshop all the environments use: \u003cinstance\u003e-workshop.\nTo find the instance, on the AWS/EC2 instance run the following command:\n​ Echo Instance Output Example echo $INSTANCE-workshop bdzx-workshop Select the environment you found in the previous step then select the frontend service and set the time to Past 15 minutes.\nWith this automatically generated dashboard, you can keep an eye out for the health of your service(s) using RED (Rate, Error \u0026 Duration) metrics. It provides various performance-related charts as well as correlated information on the underlying host and Kubernetes pods (if applicable).\nTake some time to explore the various charts in this dashboard\n2. APM Metrics In the left-hand menu card click on APM this will bring you to the APM Overview dashboard:\nSelect Explore on the right-hand side select the environment you found before and set the time to 15 minutes. This will show you the automatically generated Dependency/Service Map for the Online Boutique application.\nIt should look similar to the screenshot below:\nThe legend at the bottom of the page explains the different visualizations in the Dependency/Service Map.\nService requests, error rate and root error rate. Request rate, latency and error rate Also in this view, you can see the overall Error and Latency rates over time charts.\n3. OpenTelemetry Dashboard Once the Open Telemetery Collector is deployed the platform will automatically provide a built in dashboard display OpenTelemetry Collector metrics.\nFrom the top left hamburger menu Dashboards → OpenTelemetry Collector, scroll all the way to the bottom of the page and validate metrics and spans are being sent:\n4. OpenTelemetry zpages To debug the traces being sent you can use the zpages extension. zpages are part of the OpenTelemetry collector and provide live data for troubleshooting and statistics.\nNinja - Access zPages on your EC2 instance Details zPages is available on port 55679 of the EC2 instance’s IP address. Open a new tab in your web browser and enter in http://{==EC2-IP==}:55679/debug/tracez, you will then be able to see the zpages output.\nAlternatively, from your shell prompt you can run a text based browser:\nlynx http://localhost:55679/debug/tracez Your Workshop instructor will provide you with a URL to access zPages. Enter this URL into your browser and you will see the zPages output.",
    "description": "1. RED Metrics Go to Dashboards → All Dashboards → APM Services → Service. Here we can view the RED metrics (Rate, Error \u0026 Duration) for the Online Boutique application.\nFor this, we need to know the name of your application environment. In this workshop all the environments use: \u003cinstance\u003e-workshop.\nTo find the instance, on the AWS/EC2 instance run the following command:\n​ Echo Instance Output Example echo $INSTANCE-workshop bdzx-workshop Select the environment you found in the previous step then select the frontend service and set the time to Past 15 minutes.",
    "tags": [],
    "title": "1.1 Validate APM data",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/2-apm/online-boutique/validate-apm/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 2. API Test",
    "content": "Create a new API test Create a new API test by clicking on the Add new test button and select API test from the dropdown. Name the test using your initials followed by Spotify API e.g. RWC - Spotify API",
    "description": "Create a new API test Create a new API test by clicking on the Add new test button and select API test from the dropdown. Name the test using your initials followed by Spotify API e.g. RWC - Spotify API",
    "tags": [],
    "title": "Create new API test",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/2-api-test/2-create-new-check/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 1. Real Browser Test",
    "content": "In Splunk Observability Cloud, navigate to Synthetics and click on Add new test.\nFrom the dropdown select Browser test.\nYou will then be presented with the Browser test content configuration page.",
    "description": "In Splunk Observability Cloud, navigate to Synthetics and click on Add new test.\nFrom the dropdown select Browser test.\nYou will then be presented with the Browser test content configuration page.",
    "tags": [],
    "title": "1.2 Create Real Browser Test",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/1-real-browser-test/2-create-real-browser-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk OnCall \u003e 1. Getting Started",
    "content": "Aim A rotation is a recurring schedule, that consists of one or more shifts, with members who rotate through a shift.\nThe aim of this module is for you to configure two example Rotations, and assign Team Members to the Rotations.\nNavigate to the Rotations tab on the Teams sub menu, you should have no existing Rotations so we need to create some.\nThe 1st Rotation you will create is for a follow the sun support pattern where the members of each shift provide cover during their normal working hours within their time zone.\nThe 2nd will be a Rotation used to provide escalation support by more experienced senior members of the team, based on a 24/7, 1 week shift pattern.\n1. Follow the Sun Support - Business Hours Click Add Rotation\nEnter a name of “Follow the Sun Support - Business Hours” and Select Partial day from the three available shift templates.\nEnter a Shift name of “Asia” Time Zone set to “Asia/Tokyo” Each user is on duty from “Monday through Friday from 9.00am to 5.00pm” Handoff happens every “5 days” The next handoff happens - Select the next Monday using the calendar Click Save Rotation You will now be prompted to add Members to this shift; add the Asia members who are Jim Halpert, Lydie Rodarte-Quayle and Marie Schrader, but only if you’re using the Splunk provided environment for this workshop.\nIf you’re using your own Organisation refer to the specific list provided separately.\nNow add an 2nd shift for Europe by again clicking +Add a shift → Partial Day\nEnter a Shift name of “Europe” Time Zone set to “Europe/London” Each user is on duty from “Monday through Friday from 9.00am to 5.00pm” Handoff happens every “5 days” The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the Europe members who are Duane Chow, Steven Gomez and Walter White, but only if you’re using the Observability Workshop Org for this workshop.\nIf you’re using your own Organisation refer to the specific list provided separately.\nNow add a 3rd shift for West Coast USA by again clicking +Add a shift - Partial Day\nEnter a Shift name of “West Coast” Time Zone set to “US/Pacific” Each user is on duty from “Monday through Friday from 9.00am to 5.00pm” Handoff happens every “5 days” The next handoff happens - Select the next Monday using the calendar Click Save Shift You will again be prompted to add Members to this shift; add the West Coast members who are Maximo Arciniega, Michael Scott and Tuco Salamanca, but only if you’re using the Observability Workshop Org for this workshop.\nIf you’re using your own Organisation refer to the specific list provided separately.\nThe first user added will be the ‘current’ user for that shift.\nYou can re-order the shifts by simply dragging the users up and down, and you can change the current user by clicking Set Current on an alternate user\nYou will now have three different Shift patterns, that provide cover 24hr hours, Mon - Fri, but with no cover at weekends.\nWe will now add another Rotation for our Senior SRE Escalation cover.\n2. Senior SRE Escalation Click Add Rotation Enter a name of “Senior SRE Escalation” Select 24/7 from the three available shift templates Enter a Shift name of “Senior SRE Escalation” Time Zone set to “Asia/Tokyo” Handoff happens every “7 days at 9.00am” The next handoff happens [select the next Monday from the date picker] Click Save Rotation You will again be prompted to add Members to this shift; add the 24/7 members who are Jack Welker, Hank Schrader and Pam Beesly, but only if you’re using the Observability Workshop Org for this workshop.\nIf you’re using your own Organisation refer to the specific list provided separately.\nPlease wait for the instructor before proceeding to the Configuring Escalation Policies module.",
    "description": "Aim A rotation is a recurring schedule, that consists of one or more shifts, with members who rotate through a shift.\nThe aim of this module is for you to configure two example Rotations, and assign Team Members to the Rotations.\nNavigate to the Rotations tab on the Teams sub menu, you should have no existing Rotations so we need to create some.\nThe 1st Rotation you will create is for a follow the sun support pattern where the members of each shift provide cover during their normal working hours within their time zone.",
    "tags": [],
    "title": "Configure Rotations",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/getting_started/rotations/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 1. Agent Setup",
    "content": "You’re ready to start the OpenTelemetry Collector with the newly created agent.yaml. This exercise sets the foundation for understanding how data flows through the OpenTelemetry Collector.\nExercise Start the Agent: In the Agent terminal window run the following command:\n​ Start Collector ../otelcol --config=agent.yaml Verify debug output: If everything is configured correctly, the first and last lines of the output will look like:\n2025/01/13T12:43:51 settings.go:478: Set config to [agent.yaml] \u003csnip to the end\u003e 2025-01-13T12:43:51.747+0100 info service@v0.120.0/service.go:261 Everything is ready. Begin running and processing data. Send Test Span: Instead of instrumenting an application, we’ll simulate sending trace data to the OpenTelemetry Collector using the loadgen tool.\nIn the Spans terminal window, change into the 1-agent directory and run the following command to send a single span:\n​ Start Load Generator Load Generator Output ../loadgen -count 1 Sending traces. Use Ctrl-C to stop. Response: {\"partialSuccess\":{}} Base trace sent with traceId: 1aacb1db8a6d510f10e52f154a7fdb90 and spanId: 7837a3a2d3635d9f {\"partialSuccess\":{}}: Indicates 100% success, as the partialSuccess field is empty. In case of a partial failure, this field will include details about any failed parts.\nVerify Debug Output:\nIn the Agent terminal window check the collector’s debug output:\n2025-03-06T10:11:35.174Z info Traces {\"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"Exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1} 2025-03-06T10:11:35.174Z info ResourceSpans #0 Resource SchemaURL: https://opentelemetry.io/schemas/1.6.1 Resource attributes: -\u003e service.name: Str(cinema-service) -\u003e deployment.environment: Str(production) -\u003e host.name: Str(workshop-instance) -\u003e os.type: Str(linux) -\u003e otelcol.service.mode: Str(agent) ScopeSpans #0 ScopeSpans SchemaURL: InstrumentationScope cinema.library 1.0.0 InstrumentationScope attributes: -\u003e fintest.scope.attribute: Str(Starwars, LOTR) Span #0 Trace ID : 0ef4daa44a259a7199a948231bc383c0 Parent ID : ID : e8fdd442c36cbfb1 Name : /movie-validator Kind : Server Start time : 2025-03-06 10:11:35.163557 +0000 UTC End time : 2025-03-06 10:11:36.163557 +0000 UTC Status code : Ok Status message : Success Attributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(86.48) {\"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"Exporter\", \"otelcol.signal\": \"traces\"} Important Stop the agent in the Agent terminal window using Ctrl-C.",
    "description": "You’re ready to start the OpenTelemetry Collector with the newly created agent.yaml. This exercise sets the foundation for understanding how data flows through the OpenTelemetry Collector.\nExercise Start the Agent: In the Agent terminal window run the following command:\n​ Start Collector ../otelcol --config=agent.yaml Verify debug output: If everything is configured correctly, the first and last lines of the output will look like:",
    "tags": [],
    "title": "1.2 Test Agent Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/1-agent/1-2-test-agent/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics \u003e 1. Uptime Test",
    "content": "From the Synthetics landing page, click into a test to see its summary view and play with the Performance KPIs chart filters to see how you can slice and dice your data. This is a good place to get started understanding trends. Later, we will see what custom charts look like, so you can tailor dashboards to the KPIs you care about most. Workshop Question: Using the Performance KPIs chart What metrics are available? Is your data consistent across time and locations? Do certain locations run slower than others? Are there any spikes or failures?\nClick into a recent run either in the chart or in the table below. If there are failures, look at the response to see if you need to add a response code assertion (302 is a common one), if there is some authorization needed, or different request headers added. Here we have information about this particular test run including if it succeeded or failed, the location, timestamp, and duration in addition to the other Uptime test metrics. Click through to see the response, request, and connection info as well. If you need to edit the test for it to run successfully, click the test name in the top left breadcrumb on this run result page, then click Edit test on the top right of the test overview page. Remember to scroll down and click Submit to save your changes after editing the test configuration.\nIn addition to the test running successfully, there are other metrics to measure the health of your endpoints. For example, Time to First Byte(TTFB) is a great indicator of performance, and you can optimize TTFB to improve end user experience.\nGo back to the test overview page and change the Performance KPIs chart to display First Byte time. Once the test has run for a long enough time, expanding the time frame will draw the data points as lines to better see trends and anomalies, like in the example below. In the example above, we can see that TTFB varies consistently between locations. Knowing this, we can keep location in mind when reporting on metrics. We could also improve the experience, for example by serving users in those locations an endpoint hosted closer to them, which should reduce network latency. We can also see some slight variations in the results over time, but overall we already have a good idea of our baseline for this endpoint’s KPIs. When we have a baseline, we can alert on worsening metrics as well as visualize improvements. Tip We are not setting a detector on this test yet, to make sure it is running consistently and successfully. If you are testing a highly critical endpoint and want to be alerted on it ASAP (and have tolerance for potential alert noise), jump to Single Test Detectors.\nOnce you have your Uptime test running successfully, let’s move on to the next test type.",
    "description": "From the Synthetics landing page, click into a test to see its summary view and play with the Performance KPIs chart filters to see how you can slice and dice your data. This is a good place to get started understanding trends. Later, we will see what custom charts look like, so you can tailor dashboards to the KPIs you care about most. Workshop Question: Using the Performance KPIs chart What metrics are available? Is your data consistent across time and locations? Do certain locations run slower than others? Are there any spikes or failures?",
    "tags": [],
    "title": "Understanding results",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/1-uptime/2-understand-uptime-results/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 1. Agent Setup",
    "content": "To capture more than just debug output on the screen, we also want to generate output during the export phase of the pipeline. For this, we’ll add a File Exporter to write OTLP data to files for comparison.\nThe difference between the OpenTelemetry debug exporter and the file exporter lies in their purpose and output destination:\nFeature Debug Exporter File Exporter Output Location Console/Log File on disk Purpose Real-time debugging Persistent offline analysis Best for Quick inspection during testing Temporary storage and sharing Production Use No Rare, but possible Persistence No Yes In summary, the Debug Exporter is great for real-time, in-development troubleshooting, while the File Exporter is better suited for storing telemetry data locally for later use.\nExercise In the Agent terminal window ensure the collector is not running then edit the agent.yaml and configure the File Exporter:\nConfiguring a file exporter: The File Exporter writes telemetry data to files on disk.\nfile: # File Exporter path: \"./agent.out\" # Save path (OTLP/JSON) append: false # Overwrite the file each time Update the Pipelines Section: Add the file exporter to the traces pipeline only:\npipelines: traces: receivers: - otlp # OTLP Receiver processors: - memory_limiter # Memory Limiter processor - resourcedetection # Add system attributes to the data - resource/add_mode # Add collector mode metadata exporters: - debug # Debug Exporter - file # File Exporter metrics: receivers: - otlp processors: - memory_limiter - resourcedetection - resource/add_mode exporters: - debug logs: receivers: - otlp processors: - memory_limiter - resourcedetection - resource/add_mode exporters: - debug Validate the agent configuration using https://otelbin.io:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor EXP1(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026ensp;file\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-traces subgraph \" \" subgraph subID1[**Traces**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e EXP1 PRO3 --\u003e EXP2 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3;",
    "description": "To capture more than just debug output on the screen, we also want to generate output during the export phase of the pipeline. For this, we’ll add a File Exporter to write OTLP data to files for comparison.\nThe difference between the OpenTelemetry debug exporter and the file exporter lies in their purpose and output destination:\nFeature Debug Exporter File Exporter Output Location Console/Log File on disk Purpose Real-time debugging Persistent offline analysis Best for Quick inspection during testing Temporary storage and sharing Production Use No Rare, but possible Persistence No Yes In summary, the Debug Exporter is great for real-time, in-development troubleshooting, while the File Exporter is better suited for storing telemetry data locally for later use.",
    "tags": [],
    "title": "1.3 File Exporter",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/1-agent/1-3-fileexporter/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Self-Service Observability",
    "content": "Introduction For this workshop, we’ll be doing things that only a central tools or administration would do.\nThe workshop uses scripts to help with steps that aren’t part of the focus of this workshop – like how to change a kubernetes app, or start an application from a host.\nTip It can be useful to review what the scripts are doing.\nSo along the way it is advised to run cat \u003cfilename\u003e from time to time to see what that step just did.\nThe workshop won’t call this out, so do it when you are curious.\nWe’ll also be running some scripts to simulate data that we want to deal with.\nA simplified version of the architecture (leaving aside the specifics of kubernetes) will look something like the following:\nThe App sends metrics and traces to the Otel Collector The Otel Collector also collects metrics of its own The Otel Collector adds metadata to its own metrics and data that passes through it The OTel Gateway offers another opportunity to add metadata Let’s start by deploying the gateway.",
    "description": "Introduction For this workshop, we’ll be doing things that only a central tools or administration would do.\nThe workshop uses scripts to help with steps that aren’t part of the focus of this workshop – like how to change a kubernetes app, or start an application from a host.\nTip It can be useful to review what the scripts are doing.\nSo along the way it is advised to run cat \u003cfilename\u003e from time to time to see what that step just did.",
    "tags": [],
    "title": "Collect Data with Standards",
    "uri": "/observability-workshop/v6.5/en/scenarios/self-service-observability/2-collect-with-standards/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Profiling Workshop",
    "content": "Let’s use Splunk Observability Cloud to determine why the game started so slowly.\nView your application in Splunk Observability Cloud Note: when the application is deployed for the first time, it may take a few minutes for the data to appear.\nNavigate to APM, then use the Environment dropdown to select your environment (i.e. profiling-workshop-name).\nIf everything was deployed correctly, you should see doorgame displayed in the list of services:\nClick on Explore on the right-hand side to view the service map. We should the doorgame application on the service map:\nNotice how the majority of the time is being spent in the MySQL database. We can get more details by clicking on Database Query Performance on the right-hand side.\nThis view shows the SQL queries that took the most amount of time. Ensure that the Compare to dropdown is set to None, so we can focus on current performance.\nWe can see that one query in particular is taking a long time:\nselect * from doorgamedb.users, doorgamedb.organizations (do you notice anything unusual about this query?)\nLet’s troubleshoot further by clicking on one of the spikes in the latency graph. This brings up a list of example traces that include this slow query:\nClick on one of the traces to see the details:\nIn the trace, we can see that the DoorGame.startNew operation took 25.8 seconds, and 17.6 seconds of this was associated with the slow SQL query we found earlier.\nWhat did we accomplish? To recap what we’ve done so far:\nWe’ve deployed our application and are able to access it successfully. The application is sending traces to Splunk Observability Cloud successfully. We started troubleshooting the slow application startup time, and found a slow SQL query that seems to be the root cause. To troubleshoot further, it would be helpful to get deeper diagnostic data that tells us what’s happening inside our JVM, from both a memory (i.e. JVM heap) and CPU perspective. We’ll tackle that in the next section of the workshop.",
    "description": "Let’s use Splunk Observability Cloud to determine why the game started so slowly.\nView your application in Splunk Observability Cloud Note: when the application is deployed for the first time, it may take a few minutes for the data to appear.\nNavigate to APM, then use the Environment dropdown to select your environment (i.e. profiling-workshop-name).\nIf everything was deployed correctly, you should see doorgame displayed in the list of services:",
    "tags": [],
    "title": "Troubleshoot Game Startup",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/profiling/2-troubleshoot-game-startup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop \u003e 7. Use Tags for Monitoring",
    "content": "Alerts It’s great that we have a dashboard to monitor the response times of the credit check service by credit score, but we don’t want to stare at a dashboard all day.\nLet’s create an alert so we can be notified proactively if customers with exceptional credit scores encounter slow requests.\nTo create this alert, click on the little bell on the top right-hand corner of the chart, then select New detector from chart:\nLet’s call the detector Latency by Credit Score Category. Set the environment to your environment name (i.e. tagging-workshop-yourname) then select creditcheckservice as the service. Since we only want to look at performance for customers with exceptional credit scores, add a filter using the credit_score_category dimension and select exceptional:\nAs an alert condition instead of “Static threshold” we want to select “Sudden Change” to make the example more vivid.\nWe can then set the remainder of the alert details as we normally would. The key thing to remember here is that without capturing a tag with the credit score category and indexing it, we wouldn’t be able to alert at this granular level, but would instead be forced to bucket all customers together, regardless of their importance to the business.\nUnless you want to get notified, we do not need to finish this wizard. You can just close the wizard by clicking the X on the top right corner of the wizard pop-up.",
    "description": "Alerts It’s great that we have a dashboard to monitor the response times of the credit check service by credit score, but we don’t want to stare at a dashboard all day.\nLet’s create an alert so we can be notified proactively if customers with exceptional credit scores encounter slow requests.\nTo create this alert, click on the little bell on the top right-hand corner of the chart, then select New detector from chart:",
    "tags": [],
    "title": "Use Tags with Alerting",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/7-alerting-dashboards-slos/2-alerting/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 6. Service Health Dashboard",
    "content": "In this section, we are going to use the Copy and Paste functionality to extend our dashboard. Remember we copied some charts during the APM Service Dashboard section, we will now add those charts to our dashboard.\nExercise Select the 2+ at the top of the page and select Paste charts, this will create the charts in your custom dashboard. The chart currently shows data for all Environments and Services, so let’s add a filter for our environment and the paymentservice. Click on the 3 dots … at the top right side of the Request Rate single value chart. This will open the chart in edit mode. In the new screen, click on the x in the sf_environment:* x button (1) in the middle of the screen to close it. Click on the + to add a new filter and select sf_environment then pick the [WORKSHOPNAME] from the drop-down and hit Apply. The button will change to sf_environment:[WORKSHOPNAME] Do the same with for the sf_service. button (2), close it and create a new filter for sf_service. Only this time change it to paymentservice. Click the Save and close button (3). Repeat the previous 4 steps for the Request Rate text chart Click Save after you have update the two charts. As the new pasted charts appeared at the bottom of our dashboard, we need to re-organize our dashboard again. Using the drag and drop and resizing skills you learned earlier, make your dashboard look like the image below. Next, we are going to create a custom chart based on our Synthetic test that is running.",
    "description": "In this section, we are going to use the Copy and Paste functionality to extend our dashboard. Remember we copied some charts during the APM Service Dashboard section, we will now add those charts to our dashboard.\nExercise Select the 2+ at the top of the page and select Paste charts, this will create the charts in your custom dashboard. The chart currently shows data for all Environments and Services, so let’s add a filter for our environment and the paymentservice. Click on the 3 dots … at the top right side of the Request Rate single value chart. This will open the chart in edit mode. In the new screen, click on the x in the sf_environment:* x button (1) in the middle of the screen to close it. Click on the + to add a new filter and select sf_environment then pick the [WORKSHOPNAME] from the drop-down and hit Apply. The button will change to sf_environment:[WORKSHOPNAME] Do the same with for the sf_service. button (2), close it and create a new filter for sf_service. Only this time change it to paymentservice. Click the Save and close button (3). Repeat the previous 4 steps for the Request Rate text chart Click Save after you have update the two charts. As the new pasted charts appeared at the bottom of our dashboard, we need to re-organize our dashboard again. Using the drag and drop and resizing skills you learned earlier, make your dashboard look like the image below. Next, we are going to create a custom chart based on our Synthetic test that is running.",
    "tags": [],
    "title": "Adding Copied Charts",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/6-custom-dashboard/2-add-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 9. Service Health Dashboard",
    "content": "In this section, we are going to use the Copy and Paste functionality to extend our dashboard. Remember we copied some charts during the APM Service Dashboard section, we will now add those charts to our dashboard.\nExercise Select the 2+ at the top of the page and select Paste charts, this will create the charts in your custom dashboard. The chart currently shows data for all Environments and Services, so let’s add a filter for our environment and the paymentservice. Click on the 3 dots … at the top right side of the Request Rate single value chart. This will open the chart in edit mode. In the new screen, click on the x in the sf_environment:* x button (1) in the middle of the screen to close it. Click on the + to add a new filter and select sf_environment then pick the [WORKSHOPNAME] from the drop-down and hit Apply. The button will change to sf_environment:[WORKSHOPNAME] Do the same with for the sf_service. button (2), close it and create a new filter for sf_service. Only this time change it to paymentservice. Click the Save and close button (3). Repeat the previous 4 steps for the Request Rate text chart Click Save after you have update the two charts. As the new pasted charts appeared at the bottom of our dashboard, we need to re-organize our dashboard again. Using the drag and drop and resizing skills you learned earlier, make your dashboard look like the image below. Next, we are going to create a custom chart based on our Synthetic test that is running.",
    "description": "In this section, we are going to use the Copy and Paste functionality to extend our dashboard. Remember we copied some charts during the APM Service Dashboard section, we will now add those charts to our dashboard.\nExercise Select the 2+ at the top of the page and select Paste charts, this will create the charts in your custom dashboard. The chart currently shows data for all Environments and Services, so let’s add a filter for our environment and the paymentservice. Click on the 3 dots … at the top right side of the Request Rate single value chart. This will open the chart in edit mode. In the new screen, click on the x in the sf_environment:* x button (1) in the middle of the screen to close it. Click on the + to add a new filter and select sf_environment then pick the [WORKSHOPNAME] from the drop-down and hit Apply. The button will change to sf_environment:[WORKSHOPNAME] Do the same with for the sf_service. button (2), close it and create a new filter for sf_service. Only this time change it to paymentservice. Click the Save and close button (3). Repeat the previous 4 steps for the Request Rate text chart Click Save after you have update the two charts. As the new pasted charts appeared at the bottom of our dashboard, we need to re-organize our dashboard again. Using the drag and drop and resizing skills you learned earlier, make your dashboard look like the image below. Next, we are going to create a custom chart based on our Synthetic test that is running.",
    "tags": [],
    "title": "Adding Copied Charts",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/9-custom-dashboard/2-add-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting",
    "content": "The API Test provides a flexible way to check the functionality and performance of API endpoints. The shift toward API-first development has magnified the necessity to monitor the back-end services that provide your core front-end functionality.\nWhether you’re interested in testing the multi-step API interactions or you want to gain visibility into the performance of your endpoints, the API Test can help you accomplish your goals.",
    "description": "The API Test provides a flexible way to check the functionality and performance of API endpoints. The shift toward API-first development has magnified the necessity to monitor the back-end services that provide your core front-end functionality.\nWhether you’re interested in testing the multi-step API interactions or you want to gain visibility into the performance of your endpoints, the API Test can help you accomplish your goals.",
    "tags": [],
    "title": "API Test",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/2-api-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics",
    "content": "The API test provides a flexible way to check the functionality and performance of API endpoints. The shift toward API-first development has magnified the necessity to monitor the back-end services that provide your core front-end functionality.\nWhether you’re interested in testing multi-step API interactions or you want to gain visibility into the performance of your endpoints, the API Test can help you accomplish your goals.\nThis excercise will walk through a multi-step test on the Spotify API. You can also use it as a reference to build tests on your own APIs or on those of your critical third parties.",
    "description": "The API test provides a flexible way to check the functionality and performance of API endpoints. The shift toward API-first development has magnified the necessity to monitor the back-end services that provide your core front-end functionality.\nWhether you’re interested in testing multi-step API interactions or you want to gain visibility into the performance of your endpoints, the API Test can help you accomplish your goals.\nThis excercise will walk through a multi-step test on the Spotify API. You can also use it as a reference to build tests on your own APIs or on those of your critical third parties.",
    "tags": [],
    "title": "API Test",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/2-api-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 4. Splunk APM",
    "content": "Service View As a service owners you can use the service view in Splunk APM to get a complete view of your service health in a single pane of glass. The service view includes a service-level indicator (SLI) for availability, dependencies, request, error, and duration (RED) metrics, runtime metrics, infrastructure metrics, Tag Spotlight, endpoints, and logs for a selected service. You can also quickly navigate to code profiling and memory profiling for your service from the service view.\nExercise In the Time box change the timeframe to -1h. Note how the charts update. These charts are very useful to quickly identify performance issues. You can use this dashboard to keep an eye on the health of your service. Scroll down the page and expand Infrastructure Metrics. Here you will see the metrics for the Host and Pod. Runtime Metrics are not available as profiling data is not available for services written in Node.js. Now let’s go back to the explore view, you can hit the back button in your Browser Exercise ​ Question Answer In the Service Map hover over the wire-transfer-service. What can you conclude from the popup service chart?\nThe error percentage is very high.\nWe need to understand if there is a pattern to this error rate. We have a handy tool for that, Tag Spotlight.",
    "description": "Service View As a service owners you can use the service view in Splunk APM to get a complete view of your service health in a single pane of glass. The service view includes a service-level indicator (SLI) for availability, dependencies, request, error, and duration (RED) metrics, runtime metrics, infrastructure metrics, Tag Spotlight, endpoints, and logs for a selected service. You can also quickly navigate to code profiling and memory profiling for your service from the service view.",
    "tags": [],
    "title": "2. APM Service View",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/4-apm/2-apm-service-view/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6. Splunk APM",
    "content": "Service View As a service owners you can use the service view in Splunk APM to get a complete view of your service health in a single pane of glass. The service view includes a service-level indicator (SLI) for availability, dependencies, request, error, and duration (RED) metrics, runtime metrics, infrastructure metrics, Tag Spotlight, endpoints, and logs for a selected service. You can also quickly navigate to code profiling and memory profiling for your service from the service view.\nExercise Check the Time box, you can see that the dashboards only show data relevant to the time it took for the APM trace we previosuly selected to complete (note that the charts are static). In the Time box change the timeframe to -1h. These charts are very useful to quickly identify performance issues. You can use this dashboard to keep an eye on the health of your service. Scroll down the page and expand Infrastructure Metrics. Here you will see the metrics for the Host and Pod. Runtime Metrics are not available as profiling data is not available for services written in Node.js. Now let’s go back to the explore view, you can hit the back button in your Browser Exercise ​ Question Answer In the Service Map hover over the paymentservice. What can you conclude from the popup service chart?\nThe error percentage is very high.\nWe need to understand if there is a pattern to this error rate. We have a handy tool for that, Tag Spotlight.",
    "description": "Service View As a service owners you can use the service view in Splunk APM to get a complete view of your service health in a single pane of glass. The service view includes a service-level indicator (SLI) for availability, dependencies, request, error, and duration (RED) metrics, runtime metrics, infrastructure metrics, Tag Spotlight, endpoints, and logs for a selected service. You can also quickly navigate to code profiling and memory profiling for your service from the service view.",
    "tags": [],
    "title": "2. APM Service View",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/6-apm/2-apm-service-view/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 5. APM Features",
    "content": "To pick a trace, select a line in the Service Requests \u0026 Errors chart (1). A selection of related traces will appear.\nOnce you have the list of related traces, click on the blue (2) Trace ID Link, making sure the trace you select has the same three services mentioned in the Services Column.\nThis brings us to the selected Trace in the Waterfall view:\nHere we find several sections:\nThe Waterfall Pane (1), where you see the trace and all the instrumented functions visible as spans, with their duration representation and order/relationship showing. The Trace Info Pane (2), which shows the selected Span information (highlighted with a box around the Span in the Waterfall Pane). The Span Pane (3) where you can find all the Tags that have been sent in the selected Span. You can scroll down to see all of them. The process Pane, with tags related to the process that created the Span (scroll down to see as it is not in the screenshot). The Trace Properties, located at the top right-hand side of the pane is collapsed by default.",
    "description": "To pick a trace, select a line in the Service Requests \u0026 Errors chart (1). A selection of related traces will appear.\nOnce you have the list of related traces, click on the blue (2) Trace ID Link, making sure the trace you select has the same three services mentioned in the Services Column.\nThis brings us to the selected Trace in the Waterfall view:",
    "tags": [],
    "title": "APM Trace",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/5-traces/2-trace/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Lambda Tracing",
    "content": "The first part of our workshop will demonstrate how auto-instrumentation with OpenTelemetry allows the OpenTelemetry Collector to auto-detect what language your function is written in, and start capturing traces for those functions.\nThe Auto-Instrumentation Workshop Directory \u0026 Contents First, let us take a look at the workshop/lambda/auto directory, and some of its files. This is where all the content for the auto-instrumentation portion of our workshop resides.\nThe auto Directory Run the following command to get into the workshop/lambda/auto directory:\ncd ~/workshop/lambda/auto Inspect the contents of this directory:\nls The output should include the following files and directories:\nhandler outputs.tf terraform.tf variables.tf main.tf send_message.py terraform.tfvars The output should include the following files and directories:\nget_logs.py main.tf send_message.py handler outputs.tf terraform.tf The main.tf file Take a closer look at the main.tf file: cat main.tf Workshop Questions Can you identify which AWS resources are being created by this template? Can you identify where OpenTelemetry instrumentation is being set up? Hint: study the lambda function definitions Can you determine which instrumentation information is being provided by the environment variables we set earlier? You should see a section where the environment variables for each lambda function are being set.\nenvironment { variables = { SPLUNK_ACCESS_TOKEN = var.o11y_access_token SPLUNK_REALM = var.o11y_realm OTEL_SERVICE_NAME = \"producer-lambda\" OTEL_RESOURCE_ATTRIBUTES = \"deployment.environment=${var.prefix}-lambda-shop\" AWS_LAMBDA_EXEC_WRAPPER = \"/opt/nodejs-otel-handler\" KINESIS_STREAM = aws_kinesis_stream.lambda_streamer.name } } By using these environment variables, we are configuring our auto-instrumentation in a few ways:\nWe are setting environment variables to inform the OpenTelemetry collector of which Splunk Observability Cloud organization we would like to have our data exported to.\nSPLUNK_ACCESS_TOKEN = var.o11y_access_token SPLUNK_ACCESS_TOKEN = var.o11y_realm We are also setting variables that help OpenTelemetry identify our function/service, as well as the environment/application it is a part of.\nOTEL_SERVICE_NAME = \"producer-lambda\" # consumer-lambda in the case of the consumer function OTEL_RESOURCE_ATTRIBUTES = \"deployment.environment=${var.prefix}-lambda-shop\" We are setting an environment variable that lets OpenTelemetry know what wrappers it needs to apply to our function’s handler so as to capture trace data automatically, based on our code language.\nAWS_LAMBDA_EXEC_WRAPPER - \"/opt/nodejs-otel-handler\" In the case of the producer-lambda function, we are setting an environment variable to let the function know what Kinesis Stream to put our record to.\nKINESIS_STREAM = aws_kinesis_stream.lambda_streamer.name These values are sourced from the environment variables we set in the Prerequisites section, as well as resources that will be deployed as a part of this Terraform configuration file.\nYou should also see an argument for setting the Splunk OpenTelemetry Lambda layer on each function\nlayers = var.otel_lambda_layer The OpenTelemetry Lambda layer is a package that contains the libraries and dependencies necessary to collector, process and export telemetry data for Lambda functions at the moment of invocation.\nWhile there is a general OTel Lambda layer that has all the libraries and dependencies for all OpenTelemetry-supported languages, there are also language-specific Lambda layers, to help make your function even more lightweight.\nYou can see the relevant Splunk OpenTelemetry Lambda layer ARNs (Amazon Resource Name) and latest versions for each AWS region HERE The producer.mjs file Next, let’s take a look at the producer-lambda function code:\nRun the following command to view the contents of the producer.mjs file: cat ~/workshop/lambda/auto/handler/producer.mjs This NodeJS module contains the code for the producer function. Essentially, this function receives a message, and puts that message as a record to the targeted Kinesis Stream Deploying the Lambda Functions \u0026 Generating Trace Data Now that we are familiar with the contents of our auto directory, we can deploy the resources for our workshop, and generate some trace data from our Lambda functions.\nInitialize Terraform in the auto directory In order to deploy the resources defined in the main.tf file, you first need to make sure that Terraform is initialized in the same folder as that file.\nEnsure you are in the auto directory:\npwd The expected output would be ~/workshop/lambda/auto If you are not in the auto directory, run the following command:\ncd ~/workshop/lambda/auto Run the following command to initialize Terraform in this directory\nterraform init This command will create a number of elements in the same folder: .terraform.lock.hcl file: to record the providers it will use to provide resources .terraform directory: to store the provider configurations In addition to the above files, when terraform is run using the apply subcommand, the terraform.tfstate file will be created to track the state of your deployed resources. These enable Terraform to manage the creation, state and destruction of resources, as defined within the main.tf file of the auto directory Deploy the Lambda functions and other AWS resources Once we’ve initialized Terraform in this directory, we can go ahead and deploy our resources.\nFirst, run the terraform plan command to ensure that Terraform will be able to create your resources without encountering any issues.\nterraform plan This will result in a plan to deploy resources and output some data, which you can review to ensure everything will work as intended. Do note that a number of the values shown in the plan will be known post-creation, or are masked for security purposes. Next, run the terraform apply command to deploy the Lambda functions and other supporting resources from the main.tf file:\nterraform apply Respond yes when you see the Enter a value: prompt\nThis will result in the following outputs:\nOutputs: base_url = \"https://______.amazonaws.com/serverless_stage/producer\" consumer_function_name = \"_____-consumer\" consumer_log_group_arn = \"arn:aws:logs:us-east-1:############:log-group:/aws/lambda/______-consumer\" consumer_log_group_name = \"/aws/lambda/______-consumer\" environment = \"______-lambda-shop\" lambda_bucket_name = \"lambda-shop-______-______\" producer_function_name = \"______-producer\" producer_log_group_arn = \"arn:aws:logs:us-east-1:############:log-group:/aws/lambda/______-producer\" producer_log_group_name = \"/aws/lambda/______-producer\" Terraform outputs are defined in the outputs.tf file. These outputs will be used programmatically in other parts of our workshop, as well. Send some traffic to the producer-lambda URL (base_url) To start getting some traces from our deployed Lambda functions, we would need to generate some traffic. We will send a message to our producer-lambda function’s endpoint, which should be put as a record into our Kinesis Stream, and then pulled from the Stream by the consumer-lambda function.\nEnsure you are in the auto directory:\npwd The expected output would be ~/workshop/lambda/auto If you are not in the auto directory, run the following command\ncd ~/workshop/lambda/auto The send_message.py script is a Python script that will take input at the command line, add it to a JSON dictionary, and send it to your producer-lambda function’s endpoint repeatedly, as part of a while loop.\nRun the send_message.py script as a background process\nIt requires the --name and --superpower arguments nohup ./send_message.py --name CHANGEME --superpower CHANGEME \u0026 You should see an output similar to the following if your message is successful [1] 79829 user@host manual % appending output to nohup.out The two most import bits of information here are: The process ID on the first line (79829 in the case of my example), and The appending output to nohup.out message The nohup command ensures the script will not hang up when sent to the background. It also captures the curl output from our command in a nohup.out file in the same folder as the one you’re currently in. The \u0026 tells our shell process to run this process in the background, thus freeing our shell to run other commands. Next, check the contents of the response.logs file, to ensure your output confirms your requests to your producer-lambda endpoint are successful:\ncat response.logs You should see the following output among the lines printed to your screen if your message is successful: {\"message\": \"Message placed in the Event Stream: {prefix}-lambda_stream\"} If unsuccessful, you will see: {\"message\": \"Internal server error\"} Important If this occurs, ask one of the workshop facilitators for assistance.\nView the Lambda Function Logs Next, let’s take a look at the logs for our Lambda functions.\nTo view your producer-lambda logs, check the producer.logs file:\ncat producer.logs To view your consumer-lambda logs, check the consumer.logs file:\ncat consumer.logs Examine the logs carefully.\nWorkshop Question Do you see OpenTelemetry being loaded? Look out for the lines with splunk-extension-wrapper Consider running head -n 50 producer.logs or head -n 50 consumer.logs to see the splunk-extension-wrapper being loaded.",
    "description": "The first part of our workshop will demonstrate how auto-instrumentation with OpenTelemetry allows the OpenTelemetry Collector to auto-detect what language your function is written in, and start capturing traces for those functions.\nThe Auto-Instrumentation Workshop Directory \u0026 Contents First, let us take a look at the workshop/lambda/auto directory, and some of its files. This is where all the content for the auto-instrumentation portion of our workshop resides.\nThe auto Directory Run the following command to get into the workshop/lambda/auto directory:",
    "tags": [],
    "title": "Auto-Instrumentation",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/6-lambda-kinesis/2-auto-instrumentation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Lambda Tracing and Kinesis",
    "content": "Auto-Instrumentation Navigate to the auto directory that contains auto-instrumentation code.\n​ Command cd ~/o11y-lambda-lab/auto Inspect the contents of the files in this directory. Take a look at the serverless.yml template.\n​ Command cat serverless.yml Workshop Question Can you identify which AWS entities are being created by this template? Can you identify where OpenTelemetry instrumentation is being set up? Can you determine which instrumentation information is being provided by the Environment Variables? You should see the Splunk OpenTelemetry Lambda layer being added to each fuction.\nlayers: - arn:aws:lambda:us-east-1:254067382080:layer:splunk-apm:70 You can see the relevant layer ARNs (Amazon Resource Name) and latest versions for each AWS region here: https://github.com/signalfx/lambda-layer-versions/blob/main/splunk-apm/splunk-apm.md\nYou should also see a section where the Environment variables that are being set.\nenvironment: AWS_LAMBDA_EXEC_WRAPPER: /opt/nodejs-otel-handler OTEL_RESOURCE_ATTRIBUTES: deployment.environment=${self:custom.prefix}-apm-lambda OTEL_SERVICE_NAME: consumer-lambda SPLUNK_ACCESS_TOKEN: ${self:custom.accessToken} SPLUNK_REALM: ${self:custom.realm} Using the environment variables we are configuring and enriching our auto-instrumentation.\nHere we provide minimum information, such as NodeJS wrapper location in the Splunk APM Layer, environment name, service name, and our Splunk Org credentials. We are sending trace data directly to Splunk Observability Cloud. You could alternatively export traces to an OpenTelemetry Collector set up in Gateway mode.\nTake a look at the function code.\n​ Command cat handler.js Workshop Question Can you identify the code for producer function? Can you identify the code for consumer function? Notice there is no mention of Splunk or OpenTelemetry in the code. We are adding the instrumentation using the Lambda layer and Environment Variables only.\nDeploy your Lambdas Run the following command to deploy your Lambda Functions:\n​ Deploy Command Expected Output sls deploy Deploying hostname-lambda-lab to stage dev (us-east-1) ... ... endpoint: POST - https://randomstring.execute-api.us-east-1.amazonaws.com/dev/producer functions: producer: hostname-lambda-lab-dev-producer (1.6 kB) consumer: hostname-lambda-lab-dev-consumer (1.6 kB) This command will follow the instructions in your serverless.yml template to create your Lambda functions and your Kinesis stream. Note it may take a 1-2 minutes to execute.\nNote serverless.yml is in fact a CloudFormation template. CloudFormation is an infrastructure as code service from AWS. You can read more about it here - https://aws.amazon.com/cloudformation/ Check the details of your serverless functions:\n​ Command sls info Take note of your endpoint value: Send some Traffic Use the curl command to send a payload to your producer function. Note the command option -d is followed by your message payload.\nTry changing the value of name to your name and telling the Lambda function about your superpower. Replace YOUR_ENDPOINT with the endpoint from your previous step.\n​ Command curl -d '{ \"name\": \"CHANGE_ME\", \"superpower\": \"CHANGE_ME\" }' YOUR_ENDPOINT For example:\ncurl -d '{ \"name\": \"Kate\", \"superpower\": \"Distributed Tracing\" }' https://xvq043lj45.execute-api.us-east-1.amazonaws.com/dev/producer You should see the following output if your message is successful:\n{\"message\":\"Message placed in the Event Stream: hostname-eventSteam\"} If unsuccessful, you will see:\n{\"message\": \"Internal server error\"} If this occurs, ask one of the lab facilitators for assistance.\nIf you see a success message, generate more load: re-send that messate 5+ times. You should keep seeing a success message after each send.\nCheck the lambda logs output:\nProducer function logs:\n​ Producer Function Logs sls logs -f producer Consumer function logs:\n​ Consumer Function Logs sls logs -f consumer Examine the logs carefully.\nWorkshop Question Do you see OpenTelemetry being loaded? Look out for lines with splunk-extension-wrapper.",
    "description": "Auto-Instrumentation Navigate to the auto directory that contains auto-instrumentation code.\n​ Command cd ~/o11y-lambda-lab/auto Inspect the contents of the files in this directory. Take a look at the serverless.yml template.\n​ Command cat serverless.yml Workshop Question Can you identify which AWS entities are being created by this template? Can you identify where OpenTelemetry instrumentation is being set up? Can you determine which instrumentation information is being provided by the Environment Variables? You should see the Splunk OpenTelemetry Lambda layer being added to each fuction.",
    "tags": [],
    "title": "Auto-Instrumentation",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/6-lambda-kinesis/2-auto-instrumentation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Monolith Workshop",
    "content": "The first thing we need to set up APM is… well, an application. For this exercise, we will use the Spring PetClinic application. This is a very popular sample Java application built with the Spring framework (Springboot).\nFirst, clone the PetClinic GitHub repository, and then we will compile, build, package and test the application:\ngit clone https://github.com/spring-projects/spring-petclinic Change into the spring-petclinic directory:\ncd spring-petclinic Using Docker, start a MySQL database for PetClinic to use:\ndocker run -d -e MYSQL_USER=petclinic -e MYSQL_PASSWORD=petclinic -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=petclinic -p 3306:3306 docker.io/biarms/mysql:5.7 Next, we will start another container running Locust that will generate some simple traffic to the PetClinic application. Locust is a simple load-testing tool that can be used to generate traffic to a web application.\ndocker run --network=\"host\" -d -p 8090:8090 -v ~/workshop/petclinic:/mnt/locust docker.io/locustio/locust -f /mnt/locust/locustfile.py --headless -u 1 -r 1 -H http://127.0.0.1:8083 Next, compile, build and package PetClinic using maven:\n./mvnw package -Dmaven.test.skip=true Info This will take a few minutes the first time you run and will download a lot of dependencies before it compiles the application. Future builds will be a lot quicker.\nOnce the build completes, you need to obtain the public IP address of the instance you are running on. You can do this by running the following command:\ncurl http://ifconfig.me You will see an IP address returned, make a note of this as we will need it to validate that the application is running.",
    "description": "The first thing we need to set up APM is… well, an application. For this exercise, we will use the Spring PetClinic application. This is a very popular sample Java application built with the Spring framework (Springboot).\nFirst, clone the PetClinic GitHub repository, and then we will compile, build, package and test the application:\ngit clone https://github.com/spring-projects/spring-petclinic Change into the spring-petclinic directory:\ncd spring-petclinic Using Docker, start a MySQL database for PetClinic to use:",
    "tags": [],
    "title": "Building the Spring PetClinic Application",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/1-petclinic-monolith/2-building-petclinic/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop \u003e 3. Capture Tags with OpenTelemetry",
    "content": "Let’s add some tags to our traces, so we can find out why some customers receive a poor experience from our application.\nIdentify Useful Tags We’ll start by reviewing the code for the credit_check function of creditcheckservice (which can be found in the /home/splunk/workshop/tagging/creditcheckservice-py/main.py file):\n@app.route('/check') def credit_check(): customerNum = request.args.get('customernum') # Get Credit Score creditScoreReq = requests.get(\"http://creditprocessorservice:8899/getScore?customernum=\" + customerNum) creditScoreReq.raise_for_status() creditScore = int(creditScoreReq.text) creditScoreCategory = getCreditCategoryFromScore(creditScore) # Run Credit Check creditCheckReq = requests.get(\"http://creditprocessorservice:8899/runCreditCheck?customernum=\" + str(customerNum) + \"\u0026score=\" + str(creditScore)) creditCheckReq.raise_for_status() checkResult = str(creditCheckReq.text) return checkResult We can see that this function accepts a customer number as an input. This would be helpful to capture as part of a trace. What else would be helpful?\nWell, the credit score returned for this customer by the creditprocessorservice may be interesting (we want to ensure we don’t capture any PII data though). It would also be helpful to capture the credit score category, and the credit check result.\nGreat, we’ve identified four tags to capture from this service that could help with our investigation. But how do we capture these?\nCapture Tags We start by adding importing the trace module by adding an import statement to the top of the creditcheckservice-py/main.py file:\nimport requests from flask import Flask, request from waitress import serve from opentelemetry import trace # \u003c--- ADDED BY WORKSHOP ... Next, we need to get a reference to the current span so we can add an attribute (aka tag) to it:\ndef credit_check(): current_span = trace.get_current_span() # \u003c--- ADDED BY WORKSHOP customerNum = request.args.get('customernum') current_span.set_attribute(\"customer.num\", customerNum) # \u003c--- ADDED BY WORKSHOP ... That was pretty easy, right? Let’s capture some more, with the final result looking like this:\ndef credit_check(): current_span = trace.get_current_span() # \u003c--- ADDED BY WORKSHOP customerNum = request.args.get('customernum') current_span.set_attribute(\"customer.num\", customerNum) # \u003c--- ADDED BY WORKSHOP # Get Credit Score creditScoreReq = requests.get(\"http://creditprocessorservice:8899/getScore?customernum=\" + customerNum) creditScoreReq.raise_for_status() creditScore = int(creditScoreReq.text) current_span.set_attribute(\"credit.score\", creditScore) # \u003c--- ADDED BY WORKSHOP creditScoreCategory = getCreditCategoryFromScore(creditScore) current_span.set_attribute(\"credit.score.category\", creditScoreCategory) # \u003c--- ADDED BY WORKSHOP # Run Credit Check creditCheckReq = requests.get(\"http://creditprocessorservice:8899/runCreditCheck?customernum=\" + str(customerNum) + \"\u0026score=\" + str(creditScore)) creditCheckReq.raise_for_status() checkResult = str(creditCheckReq.text) current_span.set_attribute(\"credit.check.result\", checkResult) # \u003c--- ADDED BY WORKSHOP return checkResult Redeploy Service Once these changes are made, let’s run the following script to rebuild the Docker image used for creditcheckservice and redeploy it to our Kubernetes cluster:\n./5-redeploy-creditcheckservice.sh Confirm Tag is Captured Successfully After a few minutes, return to Splunk Observability Cloud and load one of the latest traces to confirm that the tags were captured successfully (hint: sort by the timestamp to find the latest traces):\nWell done, you’ve leveled up your OpenTelemetry game and have added context to traces using tags.\nNext, we’re ready to see how you can use these tags with Splunk Observability Cloud!",
    "description": "Let’s add some tags to our traces, so we can find out why some customers receive a poor experience from our application.\nIdentify Useful Tags We’ll start by reviewing the code for the credit_check function of creditcheckservice (which can be found in the /home/splunk/workshop/tagging/creditcheckservice-py/main.py file):\n@app.route('/check') def credit_check(): customerNum = request.args.get('customernum') # Get Credit Score creditScoreReq = requests.get(\"http://creditprocessorservice:8899/getScore?customernum=\" + customerNum) creditScoreReq.raise_for_status() creditScore = int(creditScoreReq.text) creditScoreCategory = getCreditCategoryFromScore(creditScore) # Run Credit Check creditCheckReq = requests.get(\"http://creditprocessorservice:8899/runCreditCheck?customernum=\" + str(customerNum) + \"\u0026score=\" + str(creditScore)) creditCheckReq.raise_for_status() checkResult = str(creditCheckReq.text) return checkResult We can see that this function accepts a customer number as an input. This would be helpful to capture as part of a trace. What else would be helpful?",
    "tags": [],
    "title": "2. Capture Tags - Python",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/3-capture-tags/2-capture-tags-python/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Advanced Synthetics",
    "content": "In Splunk Observability Cloud, navigate to Synthetics and click on Add new test.\nFrom the dropdown select Browser test.\nYou will then be presented with the Browser test content configuration page.",
    "description": "In Splunk Observability Cloud, navigate to Synthetics and click on Add new test.\nFrom the dropdown select Browser test.\nYou will then be presented with the Browser test content configuration page.",
    "tags": [],
    "title": "Create a Browser Test",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/3-advanced-synthetics/2-create-real-browser-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "Uninstall the OpenTelemetry Collector Our EC2 instance may already have an older version the Splunk Distribution of the OpenTelemetry Collector installed. Before proceeding further, let’s uninstall it using the following command:\n​ Script Example Output curl -sSL https://dl.signalfx.com/splunk-otel-collector.sh \u003e /tmp/splunk-otel-collector.sh; sudo sh /tmp/splunk-otel-collector.sh --uninstall Reading package lists... Done Building dependency tree... Done Reading state information... Done The following packages will be REMOVED: splunk-otel-collector* 0 upgraded, 0 newly installed, 1 to remove and 167 not upgraded. After this operation, 766 MB disk space will be freed. (Reading database ... 157441 files and directories currently installed.) Removing splunk-otel-collector (0.92.0) ... (Reading database ... 147373 files and directories currently installed.) Purging configuration files for splunk-otel-collector (0.92.0) ... Scanning processes... Scanning candidates... Scanning linux images... Running kernel seems to be up-to-date. Restarting services... systemctl restart fail2ban.service falcon-sensor.service Service restarts being deferred: systemctl restart networkd-dispatcher.service systemctl restart unattended-upgrades.service No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. Successfully removed the splunk-otel-collector package Deploy the OpenTelemetry Collector Let’s deploy the latest version of the Splunk Distribution of the OpenTelemetry Collector on our Linux EC2 instance.\nWe can do this by downloading the collector binary using curl, and then running it\nwith specific arguments that tell the collector which realm to report data into, which access token to use, and which deployment environment to report into.\nA deployment environment in Splunk Observability Cloud is a distinct deployment of your system or application that allows you to set up configurations that don’t overlap with configurations in other deployments of the same application.\n​ Script Example Output curl -sSL https://dl.signalfx.com/splunk-otel-collector.sh \u003e /tmp/splunk-otel-collector.sh; \\ sudo sh /tmp/splunk-otel-collector.sh \\ --realm $REALM \\ --mode agent \\ --without-instrumentation \\ --deployment-environment otel-$INSTANCE \\ -- $ACCESS_TOKEN Splunk OpenTelemetry Collector Version: latest Memory Size in MIB: 512 Realm: us1 Ingest Endpoint: https://ingest.us1.signalfx.com API Endpoint: https://api.us1.signalfx.com HEC Endpoint: https://ingest.us1.signalfx.com/v1/log etc. Refer to Install the Collector for Linux with the installer script for further details on how to install the collector.\nConfirm the Collector is Running Let’s confirm that the collector is running successfully on our instance.\nPress Ctrl + C to exit out of the status command.\n​ Script Example Output sudo systemctl status splunk-otel-collector ● splunk-otel-collector.service - Splunk OpenTelemetry Collector Loaded: loaded (/lib/systemd/system/splunk-otel-collector.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/splunk-otel-collector.service.d └─service-owner.conf Active: active (running) since Fri 2024-12-20 00:13:14 UTC; 45s ago Main PID: 14465 (otelcol) Tasks: 9 (limit: 19170) Memory: 117.4M CPU: 681ms CGroup: /system.slice/splunk-otel-collector.service └─14465 /usr/bin/otelcol How do we view the collector logs? We can view the collector logs using journalctl:\nPress Ctrl + C to exit out of tailing the log.\n​ Script Example Output sudo journalctl -u splunk-otel-collector -f -n 100 Dec 20 00:13:14 derek-1 systemd[1]: Started Splunk OpenTelemetry Collector. Dec 20 00:13:14 derek-1 otelcol[14465]: 2024/12/20 00:13:14 settings.go:483: Set config to /etc/otel/collector/agent_config.yaml Dec 20 00:13:14 derek-1 otelcol[14465]: 2024/12/20 00:13:14 settings.go:539: Set memory limit to 460 MiB Dec 20 00:13:14 derek-1 otelcol[14465]: 2024/12/20 00:13:14 settings.go:524: Set soft memory limit set to 460 MiB Dec 20 00:13:14 derek-1 otelcol[14465]: 2024/12/20 00:13:14 settings.go:373: Set garbage collection target percentage (GOGC) to 400 Dec 20 00:13:14 derek-1 otelcol[14465]: 2024/12/20 00:13:14 settings.go:414: set \"SPLUNK_LISTEN_INTERFACE\" to \"127.0.0.1\" etc. Collector Configuration Where do we find the configuration that is used by this collector?\nIt’s available in the /etc/otel/collector directory. Since we installed the collector in agent mode, the collector configuration can be found in the agent_config.yaml file.",
    "description": "Uninstall the OpenTelemetry Collector Our EC2 instance may already have an older version the Splunk Distribution of the OpenTelemetry Collector installed. Before proceeding further, let’s uninstall it using the following command:\n​ Script Example Output curl -sSL https://dl.signalfx.com/splunk-otel-collector.sh \u003e /tmp/splunk-otel-collector.sh; sudo sh /tmp/splunk-otel-collector.sh --uninstall Reading package lists... Done Building dependency tree... Done Reading state information... Done The following packages will be REMOVED: splunk-otel-collector* 0 upgraded, 0 newly installed, 1 to remove and 167 not upgraded. After this operation, 766 MB disk space will be freed. (Reading database ... 157441 files and directories currently installed.) Removing splunk-otel-collector (0.92.0) ... (Reading database ... 147373 files and directories currently installed.) Purging configuration files for splunk-otel-collector (0.92.0) ... Scanning processes... Scanning candidates... Scanning linux images... Running kernel seems to be up-to-date. Restarting services... systemctl restart fail2ban.service falcon-sensor.service Service restarts being deferred: systemctl restart networkd-dispatcher.service systemctl restart unattended-upgrades.service No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. Successfully removed the splunk-otel-collector package Deploy the OpenTelemetry Collector Let’s deploy the latest version of the Splunk Distribution of the OpenTelemetry Collector on our Linux EC2 instance.",
    "tags": [],
    "title": "Deploy the OpenTelemetry Collector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/2-deploy-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Solving Problems with O11y Cloud",
    "content": "The first step to “getting data in” is to deploy an OpenTelemetry collector, which receives and processes telemetry data in our environment before exporting it to Splunk Observability Cloud.\nWe’ll be using Kubernetes for this workshop, and will deploy the collector in our K8s cluster using Helm.\nWhat is Helm? Helm is a package manager for Kubernetes which provides the following benefits:\nManage Complexity deal with a single values.yaml file rather than dozens of manifest files Easy Updates in-place upgrades Rollback support Just use helm rollback to roll back to an older version of a release Install the Collector using Helm Let’s change into the correct directory and run a script to install the collector:\n​ Script Example Output cd /home/splunk/workshop/tagging ./1-deploy-otel-collector.sh \"splunk-otel-collector-chart\" has been added to your repositories Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"splunk-otel-collector-chart\" chart repository Update Complete. ⎈Happy Helming!⎈ NAME: splunk-otel-collector LAST DEPLOYED: Mon Dec 23 18:47:38 2024 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Observability realm us1. Note that the script may take a minute or so to run.\nHow did this script install the collector? It first ensured that the environment variables set in the ~./profile file are read:\nImportant: There is no need to run the following commands, as they were already run by the 1-deploy-otel-collector.sh script.\nsource ~/.profile It then installed the splunk-otel-collector-chart Helm chart and ensured it’s up-to-date:\nhelm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart helm repo update And finally, it used helm install to install the collector:\nhelm install splunk-otel-collector --version 0.132.0 \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-k3s-cluster\" \\ --set=\"environment=tagging-workshop-$INSTANCE\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f otel/values.yaml Note that the helm install command references a values.yaml file, which is used to customize the collector configuration. We’ll explore this is more detail below.\nConfirm the Collector is Running We can confirm whether the collector is running with the following command:\n​ Script Example Output kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-agent-kfvjb 1/1 Running 0 2m33s splunk-otel-collector-certmanager-7d89558bc9-2fqnx 1/1 Running 0 2m33s splunk-otel-collector-certmanager-cainjector-796cc6bd76-hz4sp 1/1 Running 0 2m33s splunk-otel-collector-certmanager-webhook-6959cd5f8-qd5b6 1/1 Running 0 2m33s splunk-otel-collector-k8s-cluster-receiver-57569b58c8-8ghds 1/1 Running 0 2m33s splunk-otel-collector-operator-6fd9f9d569-wd5mn 2/2 Running 0 2m33s Confirm your K8s Cluster is in O11y Cloud In Splunk Observability Cloud, navigate to Infrastructure → Kubernetes → Kubernetes Clusters, and then search for your Cluster Name (which is $INSTANCE-k3s-cluster):\nGet the Collector Configuration Before we customize the collector config, how do we determine what the current configuration looks like?\nIn a Kubernetes environment, the collector configuration is stored using a Config Map.\nWe can see which config maps exist in our cluster with the following command:\n​ Script Example Output kubectl get cm -l app=splunk-otel-collector NAME DATA AGE splunk-otel-collector-otel-k8s-cluster-receiver 1 3h37m splunk-otel-collector-otel-agent 1 3h37m We can then view the config map of the collector agent as follows:\n​ Script Example Output kubectl describe cm splunk-otel-collector-otel-agent Name: splunk-otel-collector-otel-agent Namespace: default Labels: app=splunk-otel-collector app.kubernetes.io/instance=splunk-otel-collector app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=splunk-otel-collector app.kubernetes.io/version=0.113.0 chart=splunk-otel-collector-0.113.0 helm.sh/chart=splunk-otel-collector-0.113.0 heritage=Helm release=splunk-otel-collector Annotations: meta.helm.sh/release-name: splunk-otel-collector meta.helm.sh/release-namespace: default Data ==== relay: ---- exporters: otlphttp: headers: X-SF-Token: ${SPLUNK_OBSERVABILITY_ACCESS_TOKEN} metrics_endpoint: https://ingest.us1.signalfx.com/v2/datapoint/otlp traces_endpoint: https://ingest.us1.signalfx.com/v2/trace/otlp (followed by the rest of the collector config in yaml format) How to Update the Collector Configuration in K8s We can customize the collector configuration in K8s using the values.yaml file.\nSee this file for a comprehensive list of customization options that are available in the values.yaml file.\nLet’s look at an example.\nAdd the Debug Exporter Suppose we want to see the traces that are sent to the collector. We can use the debug exporter for this purpose, which can be helpful for troubleshooting OpenTelemetry related issues.\nYou can use vi or nano to edit the values.yaml file. We will show an example using vi:\nvi /home/splunk/workshop/tagging/otel/values.yaml Add the debug exporter by copying and pasting the following text to the bottom of the values.yaml file:\nPress ‘i’ to enter into insert mode in vi before adding the text below.\n# NEW CONTENT exporters: debug: verbosity: detailed service: pipelines: traces: exporters: - sapm - signalfx - debug After these changes, the values.yaml file should include the following contents:\nsplunkObservability: logsEnabled: false profilingEnabled: true infrastructureMonitoringEventsEnabled: true certmanager: enabled: true operator: enabled: true agent: config: receivers: kubeletstats: insecure_skip_verify: true auth_type: serviceAccount endpoint: ${K8S_NODE_IP}:10250 metric_groups: - container - pod - node - volume k8s_api_config: auth_type: serviceAccount extra_metadata_labels: - container.id - k8s.volume.type extensions: zpages: endpoint: 0.0.0.0:55679 # NEW CONTENT exporters: debug: verbosity: detailed service: pipelines: traces: exporters: - sapm - signalfx - debug To save your changes in vi, press the esc key to enter command mode, then type :wq! followed by pressing the enter/return key.\nOnce the file is saved, we can apply the changes with:\n​ Script Example Output cd /home/splunk/workshop/tagging helm upgrade splunk-otel-collector --version 0.132.0 \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-k3s-cluster\" \\ --set=\"environment=tagging-workshop-$INSTANCE\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f otel/values.yaml Release \"splunk-otel-collector\" has been upgraded. Happy Helming! NAME: splunk-otel-collector LAST DEPLOYED: Mon Dec 23 19:08:08 2024 NAMESPACE: default STATUS: deployed REVISION: 2 NOTES: Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Observability realm us1. Whenever a change to the collector config is made via a values.yaml file, it’s helpful to review the actual configuration applied to the collector by looking at the config map:\nkubectl describe cm splunk-otel-collector-otel-agent We can see that the debug exporter was added to the traces pipeline, as desired:\ntraces: exporters: - sapm - signalfx - debug We’ll explore the output of the debug exporter once we deploy an application in our cluster and start capturing traces.",
    "description": "The first step to “getting data in” is to deploy an OpenTelemetry collector, which receives and processes telemetry data in our environment before exporting it to Splunk Observability Cloud.\nWe’ll be using Kubernetes for this workshop, and will deploy the collector in our K8s cluster using Helm.\nWhat is Helm? Helm is a package manager for Kubernetes which provides the following benefits:\nManage Complexity deal with a single values.yaml file rather than dozens of manifest files Easy Updates in-place upgrades Rollback support Just use helm rollback to roll back to an older version of a release Install the Collector using Helm Let’s change into the correct directory and run a script to install the collector:",
    "tags": [],
    "title": "Deploy the OpenTelemetry Collector and Customize Config",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/9-solving-problems-with-o11y-cloud/2-deploy-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e GDI (OTel \u0026 UF)",
    "content": "Objective: Learn how to efficiently deploy complex infrastructure components such as Kafka and MongoDB to demonstrate metrics collection with Splunk O11y IM integrations\nDuration: 15 Minutes\nScenario A prospect uses Kafka and MongoDB in their environment. Since there are integrations for these services, you’d like to demonstrate this to the prospect. What is a quick and efficient way to set up a live environment with these services and have metrics collected?\n1. Where can I find helm charts? Google “myservice helm chart” https://artifacthub.io/ (Note: Look for charts from trusted organizations, with high star count and frequent updates) 2. Review Apache Kafka packaged by Bitnami We will deploy the helm chart with these options enabled:\nreplicaCount=3 metrics.jmx.enabled=true metrics.kafka.enabled=true deleteTopicEnable=true 3. Review MongoDB(R) packaged by Bitnami We will deploy the helm chart with these options enabled:\nversion 12.1.31 metrics.enabled=true global.namespaceOverride=default auth.rootUser=root auth.rootPassword=splunk auth.enabled=false 4. Install Kafka and MongoDB with helm charts helm repo add bitnami https://charts.bitnami.com/bitnami helm install kafka --set replicaCount=3 --set metrics.jmx.enabled=true --set metrics.kafka.enabled=true --set deleteTopicEnable=true bitnami/kafka helm install mongodb --set metrics.enabled=true bitnami/mongodb --set global.namespaceOverride=default --set auth.rootUser=root --set auth.rootPassword=splunk --set auth.enabled=false --version 12.1.31 Verify the helm chart installation\n​ helm list helm list output helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION kafka default 1 2022-11-14 11:21:36.328956822 -0800 PST deployed kafka-19.1.3 3.3.1 mongodb default 1 2022-11-14 11:19:36.507690487 -0800 PST deployed mongodb-12.1.31 5.0.10 Verify the helm chart installation\n​ kubectl get pods kubectl get pods Output kubectl get pods NAME READY STATUS RESTARTS AGE kafka-exporter-595778d7b4-99ztt 0/1 ContainerCreating 0 17s mongodb-b7c968dbd-jxvsj 0/2 Pending 0 6s kafka-1 0/2 ContainerCreating 0 16s kafka-2 0/2 ContainerCreating 0 16s kafka-zookeeper-0 0/1 Pending 0 17s kafka-0 0/2 Pending 0 17s Use information for each Helm chart and Splunk O11y Data Setup to generate values.yaml for capturing metrics from Kafka and MongoDB.\nNote values.yaml for the different services will be passed to the Splunk Helm Chart at installation time. These will configure the OTEL collector to capture metrics from these services.\nReferences: Apache Kafka packaged by Bitnami Configure application receivers for databases » Apache Kafka Kafkametricsreceiver 4.1 Example kafka.values.yaml otelAgent: config: receivers: receiver_creator: receivers: smartagent/kafka: rule: type == \"pod\" \u0026\u0026 name matches \"kafka\" config: #endpoint: '`endpoint`:5555' port: 5555 type: collectd/kafka clusterName: sl-kafka otelK8sClusterReceiver: k8sEventsEnabled: true config: receivers: kafkametrics: brokers: kafka:9092 protocol_version: 2.0.0 scrapers: - brokers - topics - consumers service: pipelines: metrics: receivers: #- prometheus - k8s_cluster - kafkametrics 4.2 Example mongodb.values.yaml otelAgent: config: receivers: receiver_creator: receivers: smartagent/mongodb: rule: type == \"pod\" \u0026\u0026 name matches \"mongo\" config: type: collectd/mongodb host: mongodb.default.svc.cluster.local port: 27017 databases: [\"admin\", \"O11y\", \"local\", \"config\"] sendCollectionMetrics: true sendCollectionTopMetrics: true 4.3 Example zookeeper.values.yaml otelAgent: config: receivers: receiver_creator: receivers: smartagent/zookeeper: rule: type == \"pod\" \u0026\u0026 name matches \"kafka-zookeeper\" config: type: collectd/zookeeper host: kafka-zookeeper port: 2181 5. Install the Splunk OTEL helm chart cd /home/splunk/realtime_enrichment/otel_yamls/ helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart helm repo update helm install --set provider=' ' --set distro=' ' --set splunkObservability.accessToken=$ACCESS_TOKEN --set clusterName=$clusterName --set splunkObservability.realm=$REALM --set otelCollector.enabled='false' --set splunkObservability.logsEnabled='true' --set gateway.enabled='false' --values kafka.values.yaml --values mongodb.values.yaml --values zookeeper.values.yaml --values alwayson.values.yaml --values k3slogs.yaml --generate-name splunk-otel-collector-chart/splunk-otel-collector 6. Verify installation Verify that the Kafka, MongoDB and Splunk OTEL Collector helm charts are installed, note that names may differ.\n​ helm list helm list Output helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION kafka default 1 2021-12-07 12:48:47.066421971 -0800 PST deployed kafka-14.4.1 2.8.1 mongodb default 1 2021-12-07 12:49:06.132771625 -0800 PST deployed mongodb-10.29.2 4.4.10 splunk-otel-collector-1638910184 default 1 2021-12-07 12:49:45.694013749 -0800 PST deployed splunk-otel-collector-0.37.1 0.37.1 ​ kubectl get pods kubectl get pods Output kubectl get pods NAME READY STATUS RESTARTS AGE kafka-zookeeper-0 1/1 Running 0 18m kafka-2 2/2 Running 1 18m mongodb-79cf87987f-gsms8 2/2 Running 0 18m kafka-1 2/2 Running 1 18m kafka-exporter-7c65fcd646-dvmtv 1/1 Running 3 18m kafka-0 2/2 Running 1 18m splunk-otel-collector-1638910184-agent-27s5c 2/2 Running 0 17m splunk-otel-collector-1638910184-k8s-cluster-receiver-8587qmh9l 1/1 Running 0 17m 7. Verify dashboards Verify that out of the box dashboards for Kafka, MongoDB and Zookeeper are populated in the Infrastructure Monitor landing page. Drill down into each component to view granular details for each service.\nTip: You can use the filter k8s.cluster.name with your cluster name to find your instance.\nInfrastructure Monitoring Landing page: K8 Navigator: MongoDB Dashboard: Kafka Dashboard:",
    "description": "Objective: Learn how to efficiently deploy complex infrastructure components such as Kafka and MongoDB to demonstrate metrics collection with Splunk O11y IM integrations\nDuration: 15 Minutes\nScenario A prospect uses Kafka and MongoDB in their environment. Since there are integrations for these services, you’d like to demonstrate this to the prospect. What is a quick and efficient way to set up a live environment with these services and have metrics collected?",
    "tags": [],
    "title": "Deploy Complex Environments and Capture Metrics",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/8-gdi/2-deploy/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop",
    "content": "Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules 1. Introduction Splunk Observability Cloud uses detectors, events, alerts, and notifications to keep you informed when certain criteria are met. For example, you might want a message sent to a Slack channel or an email address for the Ops team when CPU Utilization has reached 95%, or when the number of concurrent users is approaching a limit that might require you to spin up an additional AWS instance.\nThese conditions are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical.\n2. Creating a Detector In Dashboards click on your Custom Dashboard Group (that you created in the previous module) and then click on the dashboard name.\nWe are now going to create a new detector from a chart on this dashboard. Click on the bell icon on the Latency vs Load chart, and then click New Detector From Chart.\nIn the text field next to Detector Name, ADD YOUR INITIALS before the proposed detector name.\nNaming the detector It’s important that you add your initials in front of the proposed detector name.\nIt should be something like this: XYZ’s Latency Chart Detector.\nClick on Create Alert Rule\nIn the Detector window, inside Alert signal, the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert.\nClick on Proceed to Alert Condition\n3. Setting Alert condition In Alert condition, click on Static Threshold and then on Proceed to Alert Settings\nIn Alert Settings, enter the value 290 in the Threshold field. In the same window change Time on top right to past day (-1d).\n4. Alert pre-flight check A pre-flight check will take place after 5 seconds. See the Estimated alert count. Based on the current alert settings, the amount of alerts we would have received in 1 day would have been 3.\nAbout pre-flight checks Once you set an alert condition, the UI estimates how many alerts you might get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day.\nImmediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guesswork from configuring alerts in a simple but very powerful way, only available using the Splunk Observability Cloud.\nTo read more about detector previewing, please visit this link Preview detector alerts.\nClick on Proceed to Alert Message\n5. Alert message In Alert message, under Severity choose Major.\nClick on Proceed to Alert Recipients\nClick on Add Recipient and then on your email address displayed as the first option.\nNotification Services That’s the same as entering that email address OR you can enter another email address by clicking on E-mail….\nThis is just one example of the many Notification Services the platform has available. You can check this out by going to the Integrations tab of the top menu, and see Notification Services.\n6. Alert Activation Click on Proceed to Alert Activation\nIn Activate… click on Activate Alert Rule\nIf you want to get alerts quicker you edit the rule and lower the value from 290 to say 280.\nIf you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metrics from the last 1 hour.\nClick on the in the navbar and then click on Detectors. You can optionally filter for your initials. You will see you detector listed here. If you don’t then please refresh your browser.\nCongratulations! You have created your first detector and activated it!",
    "description": "Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules 1. Introduction Splunk Observability Cloud uses detectors, events, alerts, and notifications to keep you informed when certain criteria are met. For example, you might want a message sent to a Slack channel or an email address for the Ops team when CPU Utilization has reached 95%, or when the number of concurrent users is approaching a limit that might require you to spin up an additional AWS instance.",
    "tags": [],
    "title": "Working with Detectors",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/detectors/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Frontend Dashboards",
    "content": "Seeing the visualization of our KPIs is great. What’s better? KPIs in context with events! Overlaying events on a dashboard can help us more quickly understand if an event like a deployment caused a change in metrics, for better or worse.\nYour instructor will push a condition change to the workshop application. Click the event marker on any of your dashboard charts to see more details. In the dimensions, we can see more details about this specific event. If we click the event record, we can mark for deletion if needed. We can also see a history of events in the event feed by clicking the icon on the top right of the screen, and selecting Event feed. Again, we can see details about recent events in this feed. We can also add new events in the GUI or via API. To add a new event in the GUI, click the New event button. Name your event with your team name, initials, and what kind of event it is (deployment, campaign start, etc). Choose a timestamp, or leave as-is to use the current time, and click “Create”. Now, we need to make sure our new event is overlaid in this dashboard. Wait a minute or so (refresh the page if needed) and then search for the event in the Event overlay field. If your event is within the dashboard time window, you should now see it overlaid in your charts. Click “Save” to make sure your event overlay is saved to your dashboard! Keep in mind Want to add context to that bug ticket, or show your manager how your change improved app performance? Seeing observability data in context with events not only helps with troubleshooting, but also helps us communicate with other teams.",
    "description": "Seeing the visualization of our KPIs is great. What’s better? KPIs in context with events! Overlaying events on a dashboard can help us more quickly understand if an event like a deployment caused a change in metrics, for better or worse.\nYour instructor will push a condition change to the workshop application. Click the event marker on any of your dashboard charts to see more details.",
    "tags": [],
    "title": "Events in context with chart data",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/4-dashboards/2-event-overlay/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts",
    "content": "Now that we have the OpenTelemetry Collector installed, let’s take a look at extensions for the OpenTelemetry Collector. Extensions are optional and available primarily for tasks that do not involve processing telemetry data. Examples of extensions include health monitoring, service discovery, and data forwarding.\n%%{ init:{ \"theme\": \"base\", \"themeVariables\": { \"primaryColor\": \"#ffffff\", \"clusterBkg\": \"#eff2fb\", \"defaultLinkColor\": \"#333333\" } } }%% flowchart LR; style E fill:#e20082,stroke:#333,stroke-width:4px,color:#fff subgraph Collector A[OTLP] --\u003e M(Receivers) B[JAEGER] --\u003e M(Receivers) C[Prometheus] --\u003e M(Receivers) end subgraph Processors M(Receivers) --\u003e H(Filters, Attributes, etc) E(Extensions) end subgraph Exporters H(Filters, Attributes, etc) --\u003e S(OTLP) H(Filters, Attributes, etc) --\u003e T(JAEGER) H(Filters, Attributes, etc) --\u003e U(Prometheus) end",
    "description": "Now that we have the OpenTelemetry Collector installed, let’s take a look at extensions for the OpenTelemetry Collector. Extensions are optional and available primarily for tasks that do not involve processing telemetry data. Examples of extensions include health monitoring, service discovery, and data forwarding.\n%%{ init:{ \"theme\": \"base\", \"themeVariables\": { \"primaryColor\": \"#ffffff\", \"clusterBkg\": \"#eff2fb\", \"defaultLinkColor\": \"#333333\" } } }%% flowchart LR; style E fill:#e20082,stroke:#333,stroke-width:4px,color:#fff subgraph Collector A[OTLP] --\u003e M(Receivers) B[JAEGER] --\u003e M(Receivers) C[Prometheus] --\u003e M(Receivers) end subgraph Processors M(Receivers) --\u003e H(Filters, Attributes, etc) E(Extensions) end subgraph Exporters H(Filters, Attributes, etc) --\u003e S(OTLP) H(Filters, Attributes, etc) --\u003e T(JAEGER) H(Filters, Attributes, etc) --\u003e U(Prometheus) end",
    "tags": [],
    "title": "OpenTelemetry Collector Extensions",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/2-extensions/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 8. Real User Monitoring",
    "content": "In the TAG Spotlight view, you are presented with all the tags associated with the RUM data. Tags are key-value pairs that are used to identify the data. In this case, the tags are automatically generated by the OpenTelemetry instrumentation. The tags are used to filter the data and to create the charts and tables. The Tag Spotlight view allows you detect trends in behavior and to drill down into a user session.\nClick on User Sessions (1), this will show you the list of user session that occurred during the time window.\nWe want to look at one of the session, so click on Duration (2) to sort on duration, and make sure you click on the link of one of the longer ones (3):",
    "description": "In the TAG Spotlight view, you are presented with all the tags associated with the RUM data. Tags are key-value pairs that are used to identify the data. In this case, the tags are automatically generated by the OpenTelemetry instrumentation. The tags are used to filter the data and to create the charts and tables. The Tag Spotlight view allows you detect trends in behavior and to drill down into a user session.",
    "tags": [],
    "title": "RUM trace Waterfall view \u0026 linking to APM",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/8-rum/2-rum-tour/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM",
    "content": "Use the Splunk Helm chart to install the OpenTelemetry Collector in K3s Explore your cluster in the Kubernetes Navigator 1. Installation using Helm Install the OpenTelemetry Collector using the Splunk Helm chart. First, add the Splunk Helm chart repository to Helm and update:\n​ Helm Repo Add Helm Repo Add Output helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 “splunk-otel-collector-chart” has been added to your repositories Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 Hang tight while we grab the latest from your chart repositories… …Successfully got an update from the “splunk-otel-collector-chart” chart repository Update Complete. ⎈Happy Helming!⎈\nInstall the OpenTelemetry Collector Helm chart with the following commands, do NOT edit this:\n​ Helm Install Helm Install Output helm install splunk-otel-collector --version 0.132.0 \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-k3s-cluster\" \\ --set=\"logsEngine=otel\" \\ --set=\"splunkObservability.profilingEnabled=true\" \\ --set=\"environment=$INSTANCE-workshop\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 NAME: splunk-otel-collector LAST DEPLOYED: Fri May 7 11:19:01 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None You can monitor the progress of the deployment by running kubectl get pods which should typically report a new pod is up and running after about 30 seconds.\nEnsure the status is reported as Running before continuing.\n​ Kubectl Get Pods Kubectl Get Pods Output kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-agent-2sk6k 0/1 Running 0 10s splunk-otel-collector-k8s-cluster-receiver-6956d4446f-gwnd7 0/1 Running 0 10s Ensure there are no errors by tailing the logs from the OpenTelemetry Collector pod. The output should look similar to the log output shown in the Output tab below.\nUse the label set by the helm install to tail logs (You will need to press ctrl+c to exit). Or use the installed k9s terminal UI for bonus points!\n​ Kubectl Logs Kubectl Logs Output kubectl logs -l app=splunk-otel-collector -f --container otel-collector 2021-03-21T16:11:10.900Z INFO service/service.go:364 Starting receivers… 2021-03-21T16:11:10.900Z INFO builder/receivers_builder.go:70 Receiver is starting… {“component_kind”: “receiver”, “component_type”: “prometheus”, “component_name”: “prometheus”} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {“component_kind”: “receiver”, “component_type”: “prometheus”, “component_name”: “prometheus”} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:70 Receiver is starting… {“component_kind”: “receiver”, “component_type”: “k8s_cluster”, “component_name”: “k8s_cluster”} 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/watcher.go:195 Configured Kubernetes MetadataExporter {“component_kind”: “receiver”, “component_type”: “k8s_cluster”, “component_name”: “k8s_cluster”, “exporter_name”: “signalfx”} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {“component_kind”: “receiver”, “component_type”: “k8s_cluster”, “component_name”: “k8s_cluster”} 2021-03-21T16:11:11.009Z INFO healthcheck/handler.go:128 Health Check state change {“component_kind”: “extension”, “component_type”: “health_check”, “component_name”: “health_check”, “status”: “ready”} 2021-03-21T16:11:11.009Z INFO service/service.go:267 Everything is ready. Begin running and processing data. 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/receiver.go:59 Starting shared informers and wait for initial cache sync. {“component_kind”: “receiver”, “component_type”: “k8s_cluster”, “component_name”: “k8s_cluster”} 2021-03-21T16:11:11.281Z INFO k8sclusterreceiver@v0.21.0/receiver.go:75 Completed syncing shared informer caches. {“component_kind”: “receiver”, “component_type”: “k8s_cluster”, “component_name”: “k8s_cluster”}\nDeleting a failed installation If you make an error installing the OpenTelemetry Collector you can start over by deleting the installation using:\nhelm delete splunk-otel-collector 2. Validate metrics in the UI In the Splunk UI, click the » bottom left and click on Infrastructure.\nUnder Containers click on Kubernetes to open the Kubernetes Navigator Cluster Map to ensure metrics are being sent in.\nValidate that your cluster is discovered and reported by finding your cluster (in the workshop you will see many other clusters). To find your cluster name run the following command and copy the output to your clipboard:\n​ Echo Cluster Name echo $INSTANCE-k3s-cluster Then in the UI, click on the “Cluster: - \" menu just below the Splunk Logo, paste the Cluster name you just copied into the search box, click the box to select your cluster, and finally click off the menu into white space to apply the filter.\nTo examine the health of your node, hover over the pale blue background of your cluster, then click on the blue magnifying glass that appears in the top left-hand corner.\nThis will drill down to the node level. Next, open the Metrics sidebar by clicking on the sidebar button.\nOnce it is open, you can use the slider on the side to explore the various charts relevant to your cluster/node: CPU, Memory, Network, Events etc.",
    "description": "Use the Splunk Helm chart to install the OpenTelemetry Collector in K3s Explore your cluster in the Kubernetes Navigator 1. Installation using Helm Install the OpenTelemetry Collector using the Splunk Helm chart. First, add the Splunk Helm chart repository to Helm and update:\n​ Helm Repo Add Helm Repo Add Output helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 “splunk-otel-collector-chart” has been added to your repositories Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 Hang tight while we grab the latest from your chart repositories… …Successfully got an update from the “splunk-otel-collector-chart” chart repository Update Complete. ⎈Happy Helming!⎈",
    "tags": [],
    "title": "Deploying the OpenTelemetry Collector in Kubernetes",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/gdi/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e NodeJS Zero-Config Workshop",
    "content": "1. Introduction Delete any existing OpenTelemetry Collectors If you have completed any other Observability workshops, please ensure you delete the collector running in Kubernetes before continuing. This can be done by running the following command:\nhelm delete splunk-otel-collector 2. Confirm environment variables To ensure your instance is configured correctly, we need to confirm that the required environment variables for this workshop are set correctly. In your terminal run the following command:\nenv In the output check the following environment variables are present and have values set:\nACCESS_TOKEN REALM RUM_TOKEN HEC_TOKEN HEC_URL For this workshop, all of the above are required. If any are missing, please contact your instructor.\n3. Install the OpenTelemetry Collector We can then go ahead and install the Collector. Some additional parameters are passed to the helm install command, they are:\n--set=\"operator.enabled=true\" - Enabled the Splunk OpenTelemetry Collector Operator for Kubernetes. --set=\"splunkObservability.profilingEnabled=true\" - Enables CPU/Memory profiling for supported languages. helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update helm install splunk-otel-collector --version 0.132.0 \\ --set=\"operatorcrds.install=true\", \\ --set=\"operator.enabled=true\", \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-k3s-cluster\" \\ --set=\"logsEngine=otel\" \\ --set=\"splunkObservability.profilingEnabled=true\" \\ --set=\"environment=$INSTANCE-workshop\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml Once the installation is completed, you can navigate to the Kubernetes Navigator to see the data from your host.\nClick on Add filters select k8s.cluster.name and select the cluster of your workshop instance. You can determine your instance name from the command prompt in your terminal session:\necho $INSTANCE Once you see data flowing for your host, we are then ready to get started with the APM component.",
    "description": "1. Introduction Delete any existing OpenTelemetry Collectors If you have completed any other Observability workshops, please ensure you delete the collector running in Kubernetes before continuing. This can be done by running the following command:\nhelm delete splunk-otel-collector 2. Confirm environment variables To ensure your instance is configured correctly, we need to confirm that the required environment variables for this workshop are set correctly. In your terminal run the following command:",
    "tags": [],
    "title": "Installing the OpenTelemetry Collector",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/3-nodejs-kubernetes/2-otel-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Horizontal Pod Autoscaling",
    "content": "1. Cluster vs Workload View The Kubernetes Navigator offers you two separate use cases to view your Kubernetes data.\nThe K8s workloads are focusing on providing information in regards to workloads a.k.a. your deployments. The K8s nodes are focusing on providing insight into the performance of clusters, nodes, pods and containers. You will initially select either view depending on your need (you can switch between the view on the fly if required). The most common one we will use in this workshop is the workload view and we will focus on that specifically.\n1.1 Finding your K8s Cluster Name Your first task is to identify and find your cluster. The cluster will be named as determined by the preconfigured environment variable INSTANCE. To confirm the cluster name enter the following command in your terminal:\necho $INSTANCE-k3s-cluster Please make a note of your cluster name as you will need this later in the workshop for filtering.\n2. Workloads \u0026 Workload Details Pane Go to the Infrastructure page in the Observability UI and select Kubernetes, this will offer you a set of Kubernetes services, one of them being the Kubernetes workloads pane. The pane will show a tiny graph giving you a bird’s eye view of the load being handled across all workloads. Click on the Kubernetes workloads pane and you will be taken to the workload view.\nInitially, you will see all the workloads for all clusters that are reported into your Observability Cloud Org. If an alert has fired for any of the workloads, it will be highlighted on the top right in the image below.\nNow, let’s find your cluster by filtering on Cluster in the filter toolbar.\nNote You can enter a partial name into the search box, such as emea-ws-7*, to quickly find your Cluster.\nAlso, it’s a very good idea to switch the default time from the default -4h back to the last 15 minutes (-15m).\nYou will now just see data just for your own cluster.\nWorkshop Question How many workloads are running \u0026 how many namespaces are in your Cluster?\n2.1 Using the Navigator Selection Chart By default, the Kubernetes Workloads table filters by # Pods Failed grouped by k8s.namespace.name. Go ahead and expand the default namespace to see the workloads in the namespace.\nNow, let’s change the list view to a heatmap view by selecting Map icon (next to the Table icon). Changing this option will result in the following visualization (or similar):\nIn this view, you will note that each workload is now a colored square. These squares will change color according to the Color by option you select. The colors give a visual indication of health and/or usage. You can check the meaning by hovering over the legend exclamation icon bottom right of the heatmaps.\nAnother valuable option in this screen is Find outliers which provides historical analytics of your clusters based on what is selected in the Color by dropdown.\nNow, let’s select the Network transferred (bytes) from the Color by drop-down box, then click on the Find outliers and change the Scope in the dialog to Per k8s.namespace.name and Deviation from Median as below:\nThe Find Outliers view is very useful when you need to view a selection of your workloads (or any service depending on the Navigator used) and quickly need to figure out if something has changed.\nIt will give you fast insight into items (workloads in our case) that are performing differently (both increased or decreased) which helps to make it easier to spot problems.\n2.2 The Deployment Overview pane The Deployment Overview pane gives you a quick insight into the status of your deployments. You can see at once if the pods of your deployments are Pending, Running, Succeeded, Failed or in an Unknown state.\nRunning: Pod is deployed and in a running state Pending: Waiting to be deployed Succeeded: Pod has been deployed and completed its job and is finished Failed: Containers in the pod have run and returned some kind of error Unknown: Kubernetes isn’t reporting any of the known states. (This may be during the starting or stopping of pods, for example). You can expand the Workload name by hovering your mouse on it, in case the name is longer than the chart allows.\nTo filter to a specific workload, you can click on three dots … next to the workload name in the k8s.workload.name column and choose Filter from the dropdown box:\nThis will add the selected workload to your filters. It would then list a single workload in the default namespace:\nFrom the Heatmap above find the splunk-otel-collector-k8s-cluster-receiver in the default namespace and click on the square to see more information about the workload:\nWorkshop Question What are the CPU request \u0026 CPU limit units for the otel-collector?\nAt this point, you can drill into the information of the pods, but that is outside the scope of this workshop.\n3. Navigator Sidebar Later in the workshop, you will deploy an Apache server into your cluster which will display an icon in the Navigator Sidebar.\nIn navigators for Kubernetes, you can track dependent services and containers in the navigator sidebar. To get the most out of the navigator sidebar you configure the services you want to track by configuring an extra dimension called service.name. For this workshop, we have already configured the extraDimensions in the collector configuration for monitoring Apache e.g.\nextraDimensions: service.name: php-apache The Navigator Sidebar will expand and a link to the discovered service will be added as seen in the image below:\nThis will allow for easy switching between Navigators. The same applies to your Apache server instance, it will have a Navigator Sidebar allowing you to quickly jump back to the Kubernetes Navigator.",
    "description": "1. Cluster vs Workload View The Kubernetes Navigator offers you two separate use cases to view your Kubernetes data.\nThe K8s workloads are focusing on providing information in regards to workloads a.k.a. your deployments. The K8s nodes are focusing on providing insight into the performance of clusters, nodes, pods and containers. You will initially select either view depending on your need (you can switch between the view on the fly if required). The most common one we will use in this workshop is the workload view and we will focus on that specifically.",
    "tags": [],
    "title": "Tour of the Kubernetes Navigator",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/2-hpa/2-kubernetes-navigator/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud",
    "content": "OpenTelemetry With the rise of cloud computing, microservices architectures, and ever-more complex business requirements, the need for Observability has never been greater. Observability is the ability to understand the internal state of a system by examining its outputs. In the context of software, this means being able to understand the internal state of a system by examining its telemetry data, which includes metrics, traces, and logs.\nTo make a system observable, it must be instrumented. That is, the code must emit traces, metrics, and logs. The instrumented data must then be sent to an Observability back-end such as Splunk Observability Cloud.\nMetrics Traces Logs Do I have a problem? Where is the problem? What is the problem? OpenTelemetry does two important things:\nAllows you to own the data that you generate rather than be stuck with a proprietary data format or tool. Allows you to learn a single set of APIs and conventions These two things combined enable teams and organizations the flexibility they need in today’s modern computing world.\nThere are a lot of variables to consider when getting started with Observability, including the all-important question: “How do I get my data into an Observability tool?”. The industry-wide adoption of OpenTelemetry makes this question easier to answer than ever.\nWhy Should You Care? OpenTelemetry is completely open-source and free to use. In the past, monitoring and Observability tools relied heavily on proprietary agents meaning that the effort required to change or set up additional tooling required a large amount of changes across systems, from the infrastructure level to the application level.\nSince OpenTelemetry is vendor-neutral and supported by many industry leaders in the Observability space, adopters can switch between supported Observability tools at any time with minor changes to their instrumentation. This is true regardless of which distribution of OpenTelemetry is used – like with Linux, the various distributions bundle settings and add-ons but are all fundamentally based on the community-driven OpenTelemetry project.\nSplunk has fully committed to OpenTelemetry so that our customers can collect and use ALL their data, in any type, any structure, from any source, on any scale, and all in real-time. OpenTelemetry is fundamentally changing the monitoring landscape, enabling IT and DevOps teams to bring data to every question and every action. You will experience this during these workshops.",
    "description": "Learn about OpenTelemetry and why you should care about it.",
    "tags": [],
    "title": "What is OpenTelemetry \u0026 why should you care?",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/2-opentelemetry/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "OpenTelemetry With the rise of cloud computing, microservices architectures, and ever-more complex business requirements, the need for Observability has never been greater. Observability is the ability to understand the internal state of a system by examining its outputs. In the context of software, this means being able to understand the internal state of a system by examining its telemetry data, which includes metrics, traces, and logs.\nTo make a system observable, it must be instrumented. That is, the code must emit traces, metrics, and logs. The instrumented data must then be sent to an Observability back-end such as Splunk Observability Cloud.\nMetrics Traces Logs Do I have a problem? Where is the problem? What is the problem? OpenTelemetry does two important things:\nAllows you to own the data that you generate rather than be stuck with a proprietary data format or tool. Allows you to learn a single set of APIs and conventions These two things combined enable teams and organizations the flexibility they need in today’s modern computing world.\nThere are a lot of variables to consider when getting started with Observability, including the all-important question: “How do I get my data into an Observability tool?”. The industry-wide adoption of OpenTelemetry makes this question easier to answer than ever.\nWhy Should You Care? OpenTelemetry is completely open-source and free to use. In the past, monitoring and Observability tools relied heavily on proprietary agents meaning that the effort required to change or set up additional tooling required a large amount of changes across systems, from the infrastructure level to the application level.\nSince OpenTelemetry is vendor-neutral and supported by many industry leaders in the Observability space, adopters can switch between supported Observability tools at any time with minor changes to their instrumentation. This is true regardless of which distribution of OpenTelemetry is used – like with Linux, the various distributions bundle settings and add-ons but are all fundamentally based on the community-driven OpenTelemetry project.\nSplunk has fully committed to OpenTelemetry so that our customers can collect and use ALL their data, in any type, any structure, from any source, on any scale, and all in real-time. OpenTelemetry is fundamentally changing the monitoring landscape, enabling IT and DevOps teams to bring data to every question and every action. You will experience this during these workshops.",
    "description": "Learn about OpenTelemetry and why you should care about it.",
    "tags": [],
    "title": "What is OpenTelemetry \u0026 why should you care?",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/2-opentelemetry/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 3. Receivers",
    "content": "Prometheus Receiver You will also notice another receiver called prometheus. Prometheus is an open-source toolkit used by the OpenTelemetry Collector. This receiver is used to scrape metrics from the OpenTelemetry Collector itself. These metrics can then be used to monitor the health of the collector.\nLet’s modify the prometheus receiver to clearly show that it is for collecting metrics from the collector itself. By changing the name of the receiver from prometheus to prometheus/internal, it is now much clearer as to what that receiver is doing. Update the configuration file to look like this:\n​ Prometheus Receiver Configuration prometheus/internal: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 10s static_configs: - targets: ['0.0.0.0:8888'] Example Dashboard - Prometheus metrics The following screenshot shows an example dashboard of some of the metrics the Prometheus internal receiver collects from the OpenTelemetry Collector. Here, we can see accepted and sent spans, metrics and log records.\nNote The following screenshot is an out-of-the-box (OOTB) dashboard from Splunk Observability Cloud that allows you to easily monitor your Splunk OpenTelemetry Collector install base.",
    "description": "Prometheus Receiver You will also notice another receiver called prometheus. Prometheus is an open-source toolkit used by the OpenTelemetry Collector. This receiver is used to scrape metrics from the OpenTelemetry Collector itself. These metrics can then be used to monitor the health of the collector.\nLet’s modify the prometheus receiver to clearly show that it is for collecting metrics from the collector itself. By changing the name of the receiver from prometheus to prometheus/internal, it is now much clearer as to what that receiver is doing. Update the configuration file to look like this:",
    "tags": [],
    "title": "OpenTelemetry Collector Receivers",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/3-receivers/2-prometheus/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Detectors",
    "content": "Let’s say we want to know about an issue in production without waiting for a ticket from our support center. This is where creating detectors in RUM will be helpful for us.\nGo to the RUM overview of our App. Scroll to the LCP chart, click the chart menu icon, and click Create Detector. Rename the detector to include your team name and initials, and change the scope of the detector to App so we are not limited to a single URL or page. Change the threshold and sensitivity until there is at least one alert event in the time frame. Change the alert severity and add a recipient if you’d like, and click Activate to save the Detector.\nExercise Now, your workshop instructor will change something on the website. How do you find out about the issue, and how do you investigate it?\nTip Wait a few minutes, and take a look at the online store homepage in your browser. How is the experience in an incognito browser window? How is it different when you refresh the page?",
    "description": "Let’s say we want to know about an issue in production without waiting for a ticket from our support center. This is where creating detectors in RUM will be helpful for us.\nGo to the RUM overview of our App. Scroll to the LCP chart, click the chart menu icon, and click Create Detector.",
    "tags": [],
    "title": "RUM Detectors",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/5-detectors/2-rum-detector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e RUM",
    "content": "Check the HEAD section of the Online-boutique webpage in your browser Find the code that instruments RUM 1. Browse to the Online Boutique Your workshop instructor will provide you with the Online Boutique URL that has RUM installed so that you can complete the next steps.\n2. Inspecting the HTML source The changes needed for RUM are placed in the \u003chead\u003e section of the hosts Web page. Right click to view the page source or to inspect the code. Below is an example of the \u003chead\u003e section with RUM:\nThis code enables RUM Tracing, Session Replay, and Custom Events to better understand performance in the context of user workflows:\nThe first part is to indicate where to download the Splunk Open Telemetry Javascript file from: https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js (this can also be hosted locally if so required). The next section defines the location where to send the traces to in the beacon url: {beaconUrl: \"https://rum-ingest.eu0.signalfx.com/v1/rum\" The RUM Access Token: rumAuth: \"\u003credacted\u003e\". Identification tags app and environment to indentify in the SPLUNK RUM UI e.g. app: \"online-boutique-us-store\", environment: \"online-boutique-us\"} (these values will be different in your workshop) The above lines 21 and 23-30 are all that is required to enable RUM on your website!\nLines 22 and 31-34 are optional if you want Session Replay instrumented.\nLine 36-39 var tracer=Provider.getTracer('appModuleLoader'); will add a Custom Event for every page change, allowing you to better track your website conversions and usage. This may or may not be instrumented for this workshop.\nExercise Time to shop! Take a minute to open the workshop store URL in as many browsers and devices as you’d like, shop around, add items to cart, checkout, and feel free to close the shopping browsers when you’re finished. Keep in mind this is a lightweight demo shop site, so don’t be alarmed if the cart doesn’t match the item you picked!",
    "description": "Check the HEAD section of the Online-boutique webpage in your browser Find the code that instruments RUM 1. Browse to the Online Boutique Your workshop instructor will provide you with the Online Boutique URL that has RUM installed so that you can complete the next steps.\n2. Inspecting the HTML source The changes needed for RUM are placed in the \u003chead\u003e section of the hosts Web page. Right click to view the page source or to inspect the code. Below is an example of the \u003chead\u003e section with RUM:",
    "tags": [],
    "title": "RUM instrumentation in a browser app",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/2-rum/2-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "Splunk RUM is the industry’s only end-to-end, NoSample RUM solution - providing visibility into the full user experience of every web and mobile session to uniquely combine all front-end traces with back-end metrics, traces, and logs as they happen. IT Operations and Engineering teams can quickly scope, prioritize and isolate errors, measure how performance impacts real users and optimize end-user experiences by correlating performance metrics alongside video reconstructions of all user interactions.\nFull user session analysis: Streaming analytics capture full user sessions from single and multi-page apps, measuring the customer impact of every resource, image, route change and API call.\nCorrelate issues faster: Infinite cardinality and full transaction analysis help you pinpoint and correlate issues faster across complex distributed systems.\nIsolate latency and errors: Easily identify latency, errors and poor performance for each code change and deployment. Measure how content, images and third-party dependencies impact your customers.\nBenchmark and improve page performance: Leverage core web vitals to measure and improve your page load experience, interactivity and visual stability. Find and fix impactful JavaScript errors, and easily understand which pages to improve first.\nExplore meaningful metrics: Instantly visualize the customer impact with metrics on specific workflows, custom tags and auto-suggest un-indexed tags to quickly find the root cause of issues.\nOptimize end-user experience: Correlate performance metrics alongside video reconstructions of all user interactions to optimize end-user experiences.",
    "description": "Learn about Splunk RUM and how it can help you monitor the full user experience of every web and mobile session.",
    "tags": [],
    "title": "Real User Monitoring Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/2-rum-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Environment Setup - Application Clone the workshop repository: git clone -b distributed-tracing-for-development-teams https://github.com/shabuhabs/javashop-otel.git Access the workshop directory: cd javashop-otel Set Environment Variables nano .env Note NO spaces in \u003cyour_name\u003e The shop_user provides us an Environment Tag. To get your access token, go to your Splunk O11y UI -\u003e Settings -\u003eAccess Tokens. It is assumed in this workshop that you can send traces to the org and token you are using. Set the following values: SHOP_USER=\u003cyour_name\u003e SPLUNK_ACCESS_TOKEN=\u003cyour_token\u003e SPLUNK_REALM=\u003cyour_realm\u003e Save in nano with [CTRL]-o [ENTER] Exit nano with [CTRL]-x Build and Deploy Application Let’s get started by building and deploying our Application, the Splunk Instrument Shop. Run the commands below to begin and start reading ahead as your traces are coming up!\nBuild the app ./BuildAndDeploy.sh",
    "description": "Environment Setup - Application Clone the workshop repository: git clone -b distributed-tracing-for-development-teams https://github.com/shabuhabs/javashop-otel.git Access the workshop directory: cd javashop-otel Set Environment Variables nano .env Note NO spaces in \u003cyour_name\u003e The shop_user provides us an Environment Tag. To get your access token, go to your Splunk O11y UI -\u003e Settings -\u003eAccess Tokens. It is assumed in this workshop that you can send traces to the org and token you are using. Set the following values: SHOP_USER=\u003cyour_name\u003e SPLUNK_ACCESS_TOKEN=\u003cyour_token\u003e SPLUNK_REALM=\u003cyour_realm\u003e Save in nano with [CTRL]-o [ENTER] Exit nano with [CTRL]-x Build and Deploy Application Let’s get started by building and deploying our Application, the Splunk Instrument Shop. Run the commands below to begin and start reading ahead as your traces are coming up!",
    "tags": [],
    "title": "Setting up your Application",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/2-setup-app/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 8. Splunk Synthetics",
    "content": "Right now we are looking at the result of a single Synthetic Browser Test. This test is split up into Business Transactions, think of this as a group of one or more logically related interactions that represent a business-critical user flow.\nInfo The screenshot below doesn’t contain a red banner with an error in it however you might be seeing one in your run results. This is expected as in some cases the test run fails and does not impact the workshop.\nFilmstrip: Offers a set of screenshots of site performance so that you can see how the page responds in real-time. Video: This lets you see exactly what a user trying to load your site from the location and device of a particular test run would experience. Browser test metrics: A View that offers you a picture of website performance. Synthetic transactions: List of the Synthetic transactions that made up the interaction with the site Waterfall chart The waterfall chart is a visual representation of the interaction between the test runner and the site being tested. By default, Splunk Synthetics provides screenshots and video capture of the test. This is useful for debugging issues. You can see, for example, the slow loading of large images, the slow rendering of a page etc.\nExercise Use your mouse to scroll left and right through the filmstrip to see how the site was being rendered during the test run. In the Video pane, press on the play button ▶ to see the test playback. If you click the ellipses ⋮ you can change the playback speed, view it Picture in Picture and even Download the video. In the Synthetic Transaction pane, under the header Business Transactions, click on the first button Home The waterfall below will show all the objects that make up the page. The first line is the HTML page itself. The next lines are the objects that make up the page (HTML, CSS, JavaScript, Images, Fonts, etc.). In the waterfall find the line GET splunk-otel-web.js. Click on the \u003e button to open the metadata section to see the Request/Response Header information. In the Synthetic Transaction pane, click on the second Business Transaction Shop. Note that the filmstrip adjusts and moves to the beginning of the new transaction. Repeat this for all the other Transactions, then finally select thePlaceOrder transaction.",
    "description": "Right now we are looking at the result of a single Synthetic Browser Test. This test is split up into Business Transactions, think of this as a group of one or more logically related interactions that represent a business-critical user flow.\nInfo The screenshot below doesn’t contain a red banner with an error in it however you might be seeing one in your run results. This is expected as in some cases the test run fails and does not impact the workshop.",
    "tags": [],
    "title": "2. Synthetics Test Detail",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/8-synthetics/2-synthetics-detail/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 5. Splunk RUM",
    "content": "Exercise Make sure you are on the Custom Events tab by selecting it.\nHave a look at the Custom Event Latency chart. The metrics shown here show the application latency. The comparison metrics to the side show the latency compared to 1 hour ago (which is selected in the top filter bar).\nClick on the see all link under the chart title.\nIn this dashboard view, you are presented with all the tags associated with the RUM data. Tags are key-value pairs that are used to identify the data. In this case, the tags are automatically generated by the OpenTelemetry instrumentation. The tags are used to filter the data and to create the charts and tables. The Tag Spotlight view allows you to drill down into a user session.\nExercise Change the timeframe to Last 1 hour. Click Add Filters, select OS Version, click != and select Synthetics and RUMLoadGen then click the Apply Filter button. Find the Custom Event Name chart, locate PlaceOrder in the list, click on it and select Add to filter. Notice the large spikes in the graph across the top. Click on the User Sessions tab. Click on the Duration heading twice to sort the sessions by duration (longest at the top). Click on the above the table and select Sf Geo City from the list of additional columns and click Save We now have a User Session table that is sorted by longest duration descending and includes the cities of all the users that have been shopping on the site. We could apply more filters to further narrow down the data e.g. OS version, browser version, etc.",
    "description": "Exercise Make sure you are on the Custom Events tab by selecting it.\nHave a look at the Custom Event Latency chart. The metrics shown here show the application latency. The comparison metrics to the side show the latency compared to 1 hour ago (which is selected in the top filter bar).\nClick on the see all link under the chart title.",
    "tags": [],
    "title": "2. Tag Spotlight",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/5-rum/2-tag-spotlight/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 6. Advanced Features",
    "content": "Make sure you have your original (or similar) Trace \u0026 Span (1) selected in the APM Waterfall view and select Memory Stack Traces (2) from the right-hand pane:\nThe pane should show you the Memory Stack Trace Flame Graph (3). You can scroll down and/or expand by dragging the right side of the pane.\nAlwaysOn Profiling is constantly taking snapshots, or stack traces, of your application’s code. Imagine having to read through thousands of stack traces! It is simply not practical. To assist with this, AlwaysOn Profiling aggregates and summarizes profiling data, providing a convenient way to explore Call Stacks in a view called the Flame Graph. It represents a summary of all stack traces captured from your application. You can use the Flame Graph to discover which lines of code might be causing performance issues and to confirm whether changes you make to the code have the intended effect.\nTo dive deeper into Always-on Profiling, select Span (3) (as referenced in the above image) in the Profiling Pane under Memory Stack Traces. This will open the Always-on Profiling main screen, with the Memory view pre-selected:\nThe Time filter will be set to the time frame of the span we selected (1) Java Memory Metric Charts (2) allow you to Monitor Heap Memory, Application Activity like Memory Allocation Rate and Garbage Collecting Metrics. Ability to focus/see metrics and Stack Traces only related to the Span (3), This will filter out background activities running in the Java application if required. Java Function calls identified (4), allowing you to drill down into the Methods called from that function. The Flame Graph (5), with the visualization of hierarchy based on the stack traces of the profiled service. Ability to select the Service instance (6) in case the service spins up multiple version of itself. For further investigation, the UI allows you to click a stack trace so that you can see the called function and the relevant line from the flame chart that you can then use in your coding platform to view the actual lines of code (depending of course on your preferred Coding platform).",
    "description": "Make sure you have your original (or similar) Trace \u0026 Span (1) selected in the APM Waterfall view and select Memory Stack Traces (2) from the right-hand pane:\nThe pane should show you the Memory Stack Trace Flame Graph (3). You can scroll down and/or expand by dragging the right side of the pane.\nAlwaysOn Profiling is constantly taking snapshots, or stack traces, of your application’s code. Imagine having to read through thousands of stack traces! It is simply not practical. To assist with this, AlwaysOn Profiling aggregates and summarizes profiling data, providing a convenient way to explore Call Stacks in a view called the Flame Graph. It represents a summary of all stack traces captured from your application. You can use the Flame Graph to discover which lines of code might be causing performance issues and to confirm whether changes you make to the code have the intended effect.",
    "tags": [],
    "title": "Always-On Profiling in the Trace Waterfall",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/6-profiling-db-query/2-waterfall/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "Check the original HEAD section of your Online-boutique webpage (or use the examples here) in your browser Find the Web address of your workshop hosts Online Boutique Compare the changes made to the hosts Online-Boutique and compare with the base one. 1. Browse to the Online Boutique If you have access to an EC2 instance and have previously installed the Online Boutique as part of the APM session, you can view it on port 81 of the EC2 instance’s IP address.\nIf you have not got access to an EC2 instance with the Online Boutique installed then your workshop instructor will provide you with the Online Boutique URL that does not have RUM installed so that you can complete the next steps.\n2. Inspecting the HTML source The changes needed for RUM are placed in the \u003chead\u003e section of the hosts Web page. Below is the updated \u003chead\u003e section with the changes required to enable RUM:\nThe first three lines (marked in red) have been added to the \u003chead\u003e section of the host Web page to enable RUM Tracing, the last three (marked in blue) are optional and used to enable Custom RUM events.\nThe first part is to indicate where to download the Splunk Open Telemetry Javascript file from: https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js (this can also be hosted locally if so required). The second line defines the location where to send the traces to in the beacon url: {beaconUrl: \"https://rum-ingest.eu0.signalfx.com/v1/rum\" The RUM Access Token: rumAuth: \"\u003credacted\u003e\". Identification tags app and environment to indentify in the SPLUNK RUM UI e.g. app: \"ksnq-store\", environment: \"ksnq-workshop\"} Info In this example the app name is ksnq-store, this will be different in the Workshop. Check with your host what the app name and environment to use in the RUM session will be and make a note of it!\nThe above two lines are all that is required to enable RUM on your website!\nThe (blue) optional section that uses var tracer=Provider.getTracer('appModuleLoader'); will add a custom event for every page change allowing you to better track your website conversions and usage.",
    "description": "Check the original HEAD section of your Online-boutique webpage (or use the examples here) in your browser Find the Web address of your workshop hosts Online Boutique Compare the changes made to the hosts Online-Boutique and compare with the base one. 1. Browse to the Online Boutique If you have access to an EC2 instance and have previously installed the Online Boutique as part of the APM session, you can view it on port 81 of the EC2 instance’s IP address.",
    "tags": [],
    "title": "2. Example of RUM enablement in your Website",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/2-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Workshop Setup",
    "content": "Observability Workshop Templates Splunk Show offers two templates for Observability workshops. Select the one that best fits your needs:\nWorkshop Type Description Template Splunk4Rookies - Observability This workshop template is designed for beginners. The template pre-deploys the OpenTelemetry Collector and the Online Boutique application. Attendees only need a browser to participate and complete the workshop. Rookies Splunk4Ninjas - Observability This workshop template is tailored for advanced users. The template sets up an instance with all necessary tools for hands-on activities. No resources are deployed or running at launch, allowing attendees to configure and deploy as needed. Ninja Ninja Workshops Only Set the Estimated Participants to the number of attendees you expect, and match this value in the O11y Shop Quantity field.\nTip Tip: Over-provision by 10% - 20% to accommodate last-minute attendees.\nImportant Configuration Tips When provisioning your instances, keep the following in mind:\nOperating Hours\nSet Operating hours to Run always (24/7). This prevents the instance from being suspended, which could result in a new IP address and break RUM and Synthetics configurations. DNS Name\nAssign a DNS name to the instance(s). This makes it easier to locate and manage environments in Splunk Observability Cloud during the workshop. Splunk Observability Cloud Realm and SWiPE ID\nSelect your Splunk Observability Cloud Realm. Enter the SWiPE ID generated for your workshop. Select the appropriate event type: Normal Workshop, Private Event, or Public Event.\nAdditional Resources For more detailed guidance on using Splunk Show, refer to the Splunk Show User Guide.",
    "description": "Observability Workshop Templates Splunk Show offers two templates for Observability workshops. Select the one that best fits your needs:\nWorkshop Type Description Template Splunk4Rookies - Observability This workshop template is designed for beginners. The template pre-deploys the OpenTelemetry Collector and the Online Boutique application. Attendees only need a browser to participate and complete the workshop. Rookies Splunk4Ninjas - Observability This workshop template is tailored for advanced users. The template sets up an instance with all necessary tools for hands-on activities. No resources are deployed or running at launch, allowing attendees to configure and deploy as needed. Ninja Ninja Workshops Only Set the Estimated Participants to the number of attendees you expect, and match this value in the O11y Shop Quantity field.",
    "tags": [],
    "title": "2. Using Splunk Show",
    "uri": "/observability-workshop/v6.5/en/workshop-setup/2-splunk-show/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 4. Automatic discovery and configuration",
    "content": "In the left-hand menu click on APM to see the data generated by the traces from the newly instrumented services. Change the Environment filter (1) to the name of your workshop instance in the dropdown box. (Note: this will be \u003cINSTANCE\u003e-workshop where INSTANCE is the value from the shell script you ran earlier. Ensure it is the only one selected.)\nIf you see a Critical Alert, you can ignore it as it is caused by the sudden request increase generated by the load generator (2). Click on the Service Map (3) pane to get ready for the next section.",
    "description": "In the left-hand menu click on APM to see the data generated by the traces from the newly instrumented services. Change the Environment filter (1) to the name of your workshop instance in the dropdown box. (Note: this will be \u003cINSTANCE\u003e-workshop where INSTANCE is the value from the shell script you ran earlier. Ensure it is the only one selected.)\nIf you see a Critical Alert, you can ignore it as it is caused by the sudden request increase generated by the load generator (2). Click on the Service Map (3) pane to get ready for the next section.",
    "tags": [],
    "title": "Viewing the data in Splunk APM",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/4-apm/2-apm-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 5. Splunk Log Observer",
    "content": "Before we look at a specific log line, let’s quickly recap what we have done so far and why we are here based on the 3 pillars of Observability:\nMetrics Traces Logs Do I have a problem? Where is the problem? What is the problem? Using metrics we identified we have a problem with our application. This was obvious from the error rate in the Service Dashboards as it was higher than it should be. Using traces and span tags we found where the problem is. The wire-transfer-service comprises of two versions, v350.9 and v350.10, and the error rate was 100% for v350.10. We did see that this error from the wire-transfer-service v350.10 caused multiple retries and a long delay in the response back from the compliance check service. From the trace, using the power of Related Content, we arrived at the log entries for the failing wire-transfer-service version. Now, we can determine what the problem is. Exercise Click on an error entry in the log table (make sure it says hostname: \"wire-transfer-service-xxxx\" in case there is a rare error from a different service in the list too. ​ Question Answer Based on the message, what would you tell the development team to do to resolve the issue?\nThe development team needs to rebuild and deploy the container with a valid API Token or rollback to v350.9.\nClick on the X in the log message pane to close it. Congratulations You have successfully used Splunk Observability Cloud to understand why users are experiencing issues while using the wire transfer service. You used Splunk APM and Splunk Log Observer to understand what happened in your service landscape and subsequently, found the underlying cause, all based on the 3 pillars of Observability, metrics, traces and logs\nYou also learned how to use Splunk’s intelligent tagging and analysis with Tag Spotlight to detect patterns in your applications’ behavior and to use the full stack correlation power of Related Content to quickly move between the different components and telemetry while keeping in context of the issue.\nIn the next part of the workshop, we will move from problem-finding mode into mitigation, prevention and process improvement mode.\nNext up, creating log charts in a custom dashboard.",
    "description": "Before we look at a specific log line, let’s quickly recap what we have done so far and why we are here based on the 3 pillars of Observability:\nMetrics Traces Logs Do I have a problem? Where is the problem? What is the problem? Using metrics we identified we have a problem with our application. This was obvious from the error rate in the Service Dashboards as it was higher than it should be. Using traces and span tags we found where the problem is. The wire-transfer-service comprises of two versions, v350.9 and v350.10, and the error rate was 100% for v350.10. We did see that this error from the wire-transfer-service v350.10 caused multiple retries and a long delay in the response back from the compliance check service. From the trace, using the power of Related Content, we arrived at the log entries for the failing wire-transfer-service version. Now, we can determine what the problem is. Exercise Click on an error entry in the log table (make sure it says hostname: \"wire-transfer-service-xxxx\" in case there is a rare error from a different service in the list too. ​ Question Answer Based on the message, what would you tell the development team to do to resolve the issue?",
    "tags": [],
    "title": "2. Viewing Log Entries",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/5-log-observer/2-log-entry/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 7. Splunk Log Observer",
    "content": "Before we look at a specific log line, let’s quickly recap what we have done so far and why we are here based on the 3 pillars of Observability:\nMetrics Traces Logs Do I have a problem? Where is the problem? What is the problem? Using metrics we identified we have a problem with our application. This was obvious from the error rate in the Service Dashboards as it was higher than it should be. Using traces and span tags we found where the problem is. The paymentservice comprises of two versions, v350.9 and v350.10, and the error rate was 100% for v350.10. We did see that this error from the paymentservice v350.10 caused multiple retries and a long delay in the response back from the Online Boutique checkout. From the trace, using the power of Related Content, we arrived at the log entries for the failing paymentservice version. Now, we can determine what the problem is. Exercise Click on an error entry in the log table (make sure it says hostname: \"paymentservice-xxxx\" in case there is a rare error from a different service in the list too. ​ Question Answer Based on the message, what would you tell the development team to do to resolve the issue?\nThe development team needs to rebuild and deploy the container with a valid API Token or rollback to v350.9.\nClick on the X in the log message pane to close it. Congratulations You have successfully used Splunk Observability Cloud to understand why you experienced a poor user experience whilst shopping at the Online Boutique. You used RUM, APM and logs to understand what happened in your service landscape and subsequently, found the underlying cause, all based on the 3 pillars of Observability, metrics, traces and logs\nYou also learned how to use Splunk’s intelligent tagging and analysis with Tag Spotlight to detect patterns in your applications’ behavior and to use the full stack correlation power of Related Content to quickly move between the different components whilst keeping in context of the issue.\nIn the next part of the workshop, we will move from problem-finding mode into mitigation, prevention and process improvement mode.\nNext up, creating log charts in a custom dashboard.",
    "description": "Before we look at a specific log line, let’s quickly recap what we have done so far and why we are here based on the 3 pillars of Observability:\nMetrics Traces Logs Do I have a problem? Where is the problem? What is the problem? Using metrics we identified we have a problem with our application. This was obvious from the error rate in the Service Dashboards as it was higher than it should be. Using traces and span tags we found where the problem is. The paymentservice comprises of two versions, v350.9 and v350.10, and the error rate was 100% for v350.10. We did see that this error from the paymentservice v350.10 caused multiple retries and a long delay in the response back from the Online Boutique checkout. From the trace, using the power of Related Content, we arrived at the log entries for the failing paymentservice version. Now, we can determine what the problem is. Exercise Click on an error entry in the log table (make sure it says hostname: \"paymentservice-xxxx\" in case there is a rare error from a different service in the list too. ​ Question Answer Based on the message, what would you tell the development team to do to resolve the issue?",
    "tags": [],
    "title": "2. Viewing Log Entries",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/7-log-observer/2-log-entry/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop",
    "content": "To understand why some requests have errors or slow performance, we’ll need to add context to our traces. We’ll do this by adding tags. But first, let’s take a moment to discuss what tags are, and why they’re so important for observability.\nWhat are tags? Tags are key-value pairs that provide additional metadata about spans in a trace, allowing you to enrich the context of the spans you send to Splunk APM.\nFor example, a payment processing application would find it helpful to track:\nThe payment type used (i.e. credit card, gift card, etc.) The ID of the customer that requested the payment This way, if errors or performance issues occur while processing the payment, we have the context we need for troubleshooting.\nWhile some tags can be added with the OpenTelemetry collector, the ones we’ll be working with in this workshop are more granular, and are added by application developers using the OpenTelemetry API.\nAttributes vs. Tags A note about terminology before we proceed. While this workshop is about tags, and this is the terminology we use in Splunk Observability Cloud, OpenTelemetry uses the term attributes instead. So when you see tags mentioned throughout this workshop, you can treat them as synonymous with attributes.\nWhy are tags so important? Tags are essential for an application to be truly observable. As we saw with our credit check service, some users are having a great experience: fast with no errors. But other users get a slow experience or encounter errors.\nTags add the context to the traces to help us understand why some users get a great experience and others don’t. And powerful features in Splunk Observability Cloud utilize tags to help you jump quickly to root cause.\nSneak Peak: Tag Spotlight Tag Spotlight uses tags to discover trends that contribute to high latency or error rates:\nThe screenshot above provides an example of Tag Spotlight from another application.\nSplunk has analyzed all of the tags included as part of traces that involve the payment service.\nIt tells us very quickly whether some tag values have more errors than others.\nIf we look at the version tag, we can see that version 350.10 of the service has a 100% error rate, whereas version 350.9 of the service has no errors at all:\nWe’ll be using Tag Spotlight with the credit check service later on in the workshop, once we’ve captured some tags of our own.",
    "description": "To understand why some requests have errors or slow performance, we’ll need to add context to our traces. We’ll do this by adding tags. But first, let’s take a moment to discuss what tags are, and why they’re so important for observability.\nWhat are tags? Tags are key-value pairs that provide additional metadata about spans in a trace, allowing you to enrich the context of the spans you send to Splunk APM.",
    "tags": [],
    "title": "What are Tags?",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/2-what-are-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour \u003e 2. APM Overview",
    "content": "Click APM in the main menu, the APM Home Page is made up of 3 distinct sections:\nOnboarding Pane Pane: Training videos and links to documentation to get you started with Splunk APM. APM Overview Pane: Real-time metrics for the Top Services and Top Business Workflows. Functions Pane: Links for deeper analysis of your services, tags, traces, database query performance and code profiling. The APM Overview pan provides a high-level view of the health of your application. It includes a summary of the services, latency and errors in your application. It also includes a list of the top services by error rate and the top business workflows by error rate (a business workflow is the start-to-finish journey of the collection of traces associated with a given activity or transaction and enables monitoring of end-to-end KPIs and identifying root causes and bottlenecks).\nAbout Environments To easily differentiate between multiple applications, Splunk uses environments. The naming convention for workshop environments is [NAME OF WORKSHOP]-workshop. Your instructor will provide you with the correct one to select.\nExercise Verify that the time window we are working with is set to the last 15 minutes (-15m). Change the environment to the workshop one by selecting its name from the drop-down box and make sure that is the only one selected. ​ Question Answer What can you conclude from the Top Services by Error Rate chart?\nThe wire-transfer-service has a high error rate\nIf you scroll down the Overview Page you will notice some services listed have Inferred Service next to them.\nSplunk APM can infer the presence of the remote service, or inferred service if the span calling the remote service has the necessary information. Examples of possible inferred services include databases, HTTP endpoints, and message queues. Inferred services are not instrumented, but they are displayed on the service map and the service list.\nNext, let’s check out Splunk Log Observer (LO).",
    "description": "Click APM in the main menu, the APM Home Page is made up of 3 distinct sections:\nOnboarding Pane Pane: Training videos and links to documentation to get you started with Splunk APM. APM Overview Pane: Real-time metrics for the Top Services and Top Business Workflows. Functions Pane: Links for deeper analysis of your services, tags, traces, database query performance and code profiling. The APM Overview pan provides a high-level view of the health of your application. It includes a summary of the services, latency and errors in your application. It also includes a list of the top services by error rate and the top business workflows by error rate (a business workflow is the start-to-finish journey of the collection of traces associated with a given activity or transaction and enables monitoring of end-to-end KPIs and identifying root causes and bottlenecks).",
    "tags": [],
    "title": "Application Performance Monitoring Home page",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/2-apm-home/1-apm-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Dashboard Workshop \u003e 2. Detectors",
    "content": "Learn how to configure Muting Rules Learn how to resume notifications 1. Configuring Muting Rules There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in Splunk Observability Cloud. Let’s create one!\nClick on Alerts \u0026 Detectors in the sidebar and then click Detectors to see the list of active detectors.\nIf you created a detector in Creating a Detector you can click on the three dots ... on the far right for that detector; if not, do that for another detector.\nFrom the drop-down click on Create Muting Rule…\nIn the Muting Rule window check Mute Indefinitely and enter a reason.\nImportant This will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector.\nClick Next and in the new modal window confirm the muting rule setup.\nClick on Mute Indefinitely to confirm.\nYou won’t be receiving any email notifications from your detector until you resume notifications again. Let’s now see how to do that!\n2. Resuming notifications To Resume notifications, click on Muting Rules, you will see the name of the detector you muted notifications for under Detector heading.\nClick on the thee dots ... on the far right, and click on Resume Notifications.\nClick on Resume to confirm and resume notifications for this detector.\nCongratulations! You have now resumed your alert notifications!",
    "description": "Learn how to configure Muting Rules Learn how to resume notifications 1. Configuring Muting Rules There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in Splunk Observability Cloud. Let’s create one!\nClick on Alerts \u0026 Detectors in the sidebar and then click Detectors to see the list of active detectors.",
    "tags": [],
    "title": "Working with Muting Rules",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/detectors/muting/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour \u003e 2. RUM Overview",
    "content": "Click RUM in the main menu, this will bring you to the main RUM Home or Landing page. The main concept of this page is to provide you at a glance, the overall status of all selected RUM applications, either in a full dashboard or the compact view.\nIndependent of the type of Status Dashboard used, the RUM Home Page is made up of 3 distinct sections:\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk RUM. (You can hide this pane in case you need the screen real estate). Filter Pane: Filter on the time frame, environment, application and source type. Application Summary Pane: Summary of all your applications that send RUM data. RUM Environments \u0026 Application and Source Type Splunk Observability uses the environments Tag that is sent as part of the RUM trace, (created with every interaction with your website or Mobile App), to separate data coming from different environments like “Production” or “Development”. A further separation can be made by the Applications Tag. This allows you to distinguish between separate browser/mobile applications running in the same environment. Splunk RUM is available for both browser and mobile applications, you could use Source Type to distinguish between them, however for this workshop, we will only use browser-based RUM. Exercise Ensure the time window is set to -15m Select the environment for your workshop from the drop-down box. The naming convention is [NAME OF WORKSHOP]-workshop (Selecting this will make sure the workshop RUM application is visible) Select the App name. There the naming convention is [NAME OF WORKSHOP]-store and leave Source set to All In the JavaScript Errors tile click on the TypeError entry that says: Cannot read properties of undefined (reading ‘Prcie’) to see more details. Note that you are given a quick indication of what part of the website the error occurred, allowing you to fix this quickly. Close the pane. The 3rd tile reports Web Vitals, a metric that focuses on three important aspects of the user experience: loading, interactivity, and visual stability. ​ Question Answer Based on the Web Vitals metrics, how do you rate the current web performance of the site?\nAccording to the Web Vitals Metrics, the initial load of the site is OK and is rated Good\nThe last tile, Most recent detectors tile, will show if any alerts have been triggered for the application. Click on the down ⌵ arrow in front of the Application name to toggle the view to the compact style. Note that you have all the main information available in this view as well. Click anywhere in the compact view to go back to the full view. Next, let’s check out Splunk Application Performance Monitoring (APM).",
    "description": "Click RUM in the main menu, this will bring you to the main RUM Home or Landing page. The main concept of this page is to provide you at a glance, the overall status of all selected RUM applications, either in a full dashboard or the compact view.\nIndependent of the type of Status Dashboard used, the RUM Home Page is made up of 3 distinct sections:",
    "tags": [],
    "title": "Real User Monitoring Home Page",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/2-rum-home/1-rum-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk APM \u003e 2. Using Splunk APM",
    "content": "Service Map Click on paymentservice in the service map and select version from the breakdown drop down filter underneath paymentservice. This will filter our service map by the custom span tag version.\nYou will now see the service map has been updated like the below screenshot to show the different versions of the paymentservice.\nSplunk Observability shows that not only is paymentservice experiencing errors (you can see request rate vs error rate) but that this service is the root cause.\nThis happens automatically with our AI-directed triage capabilities once distributed tracing is enabled in your services. You don’t have to set a threshold or anything for it to populate just like this.\nThis is one example of how customers can detect issues faster and know where to look for errors and hence helps reduce the MTTD and MTTR.",
    "description": "Service Map Click on paymentservice in the service map and select version from the breakdown drop down filter underneath paymentservice. This will filter our service map by the custom span tag version.\nYou will now see the service map has been updated like the below screenshot to show the different versions of the paymentservice.\nSplunk Observability shows that not only is paymentservice experiencing errors (you can see request rate vs error rate) but that this service is the root cause.",
    "tags": [],
    "title": "2.1 Service Map",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/2-apm/using-splunk-apm/service_map/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 2. Standardize Data Collection",
    "content": "Tags are key-value pairs that provide additional metadata about metrics, spans in a trace, or logs allowing you to enrich the context of the data you send to Splunk Observability Cloud. Many tags are collected by default such as hostname or OS type. Custom tags can be used to provide environment or application-specific context. Examples of custom tags include:\nInfrastructure specific attributes What data center a host is in What services are hosted on an instance What team is responsible for a set of hosts Application specific attributes What Application Version is running Feature flags or experimental identifiers Tenant ID in multi-tenant applications User related attributes User ID User role (e.g. admin, guest, subscriber) User geographical location (e.g. country, city, region) There are two ways to add tags to your data Add tags as OpenTelemetry attributes to metrics, traces, and logs when you send data to the Splunk Distribution of OpenTelemetry Collector. This option lets you add spans in bulk. Instrument your application to create span tags. This option gives you the most flexibility at the per-application level. Why are tags so important? Tags are essential for an application to be truly observable. Tags add context to the traces to help us understand why some users get a great experience and others don’t. Powerful features in Splunk Observability Cloud utilize tags to help you jump quickly to the root cause.\nContextual Information: Tags provide additional context to the metrics, traces, and logs allowing developers and operators to understand the behavior and characteristics of infrastructure and traced operations.\nFiltering and Aggregation: Tags enable filtering and aggregation of collected data. By attaching tags, users can filter and aggregate data based on specific criteria. This filtering and aggregation help in identifying patterns, diagnosing issues, and gaining insights into system behavior.\nCorrelation and Analysis: Tags facilitate correlation between metrics and other telemetry data, such as traces and logs. By including common identifiers or contextual information as tags, users can correlate metrics, traces, and logs enabling comprehensive analysis and troubleshooting of distributed systems.\nCustomization and Flexibility: OpenTelemetry allows developers to define custom tags based on their application requirements. This flexibility enables developers to capture domain-specific metadata or contextual information that is crucial for understanding the behavior of their applications.\nAttributes vs. Tags A note about terminology before we proceed. While this workshop is about tags, and this is the terminology we use in Splunk Observability Cloud, OpenTelemetry uses the term attributes instead. So when you see tags mentioned throughout this workshop, you can treat them as synonymous with attributes.",
    "description": "Tags are key-value pairs that provide additional metadata about metrics, spans in a trace, or logs allowing you to enrich the context of the data you send to Splunk Observability Cloud. Many tags are collected by default such as hostname or OS type. Custom tags can be used to provide environment or application-specific context. Examples of custom tags include:\nInfrastructure specific attributes What data center a host is in What services are hosted on an instance What team is responsible for a set of hosts Application specific attributes What Application Version is running Feature flags or experimental identifiers Tenant ID in multi-tenant applications User related attributes User ID User role (e.g. admin, guest, subscriber) User geographical location (e.g. country, city, region) There are two ways to add tags to your data Add tags as OpenTelemetry attributes to metrics, traces, and logs when you send data to the Splunk Distribution of OpenTelemetry Collector. This option lets you add spans in bulk. Instrument your application to create span tags. This option gives you the most flexibility at the per-application level. Why are tags so important? Tags are essential for an application to be truly observable. Tags add context to the traces to help us understand why some users get a great experience and others don’t. Powerful features in Splunk Observability Cloud utilize tags to help you jump quickly to the root cause.",
    "tags": [],
    "title": "What Are Tags?",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/2-standardize-data-collection/1-what-are-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 2. Gateway Setup",
    "content": "Exercise Add the otlphttp exporter: The OTLP/HTTP Exporter is used to send data from the agent to the gateway using the OTLP/HTTP protocol.\nSwitch to your Agent terminal window. Validate that the newly generated gateway-logs.out, gateway-metrics.out, and gateway-traces.out are present in the directory. Open the agent.yaml file in your editor. Add the otlphttp exporter configuration to the exporters: section: otlphttp: # Exporter Type endpoint: \"http://localhost:5318\" # Gateway OTLP endpoint Add a Batch Processor configuration: The Batch Processor will accept spans, metrics, or logs and place them into batches. Batching helps better compress the data and reduce the number of outgoing connections required to transmit the data. It is highly recommended configuring the batch processor on every collector.\nAdd the batch processor configuration to the processors: section: batch: # Processor Type Update the pipelines:\nEnable Hostmetrics Receiver: Add hostmetrics to the metrics pipeline. The HostMetrics Receiver will generate host CPU metrics once per hour with the current configuration. Enable Batch Processor: Add the batch processor (after the resource/add_mode processor) to the traces, metrics, and logs pipelines. Enable OTLPHTTP Exporter: Add the otlphttp exporter to the traces, metrics, and logs pipelines. pipelines: traces: receivers: - otlp # OTLP Receiver processors: - memory_limiter # Memory Limiter processor - resourcedetection # Add system attributes to the data - resource/add_mode # Add collector mode metadata - batch # Batch processor exporters: - debug # Debug Exporter - file # File Exporter - otlphttp # OTLP/HTTP Exporter metrics: receivers: - otlp - hostmetrics # Host Metrics Receiver processors: - memory_limiter - resourcedetection - resource/add_mode - batch exporters: - debug - otlphttp logs: receivers: - otlp processors: - memory_limiter - resourcedetection - resource/add_mode - batch exporters: - debug - otlphttp Validate the agent configuration using otelbin.io. For reference, the metrics: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver REC2(hostmetrics\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(otlphttp\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-metrics subgraph \" \" subgraph subID1[**Metrics**] direction LR REC1 --\u003e PRO1 REC2 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e PRO4 PRO4 --\u003e EXP2 PRO4 --\u003e EXP1 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-metrics stroke:#38bdf8,stroke-width:1px, color:#38bdf8,stroke-dasharray: 3 3;",
    "description": "Exercise Add the otlphttp exporter: The OTLP/HTTP Exporter is used to send data from the agent to the gateway using the OTLP/HTTP protocol.\nSwitch to your Agent terminal window. Validate that the newly generated gateway-logs.out, gateway-metrics.out, and gateway-traces.out are present in the directory. Open the agent.yaml file in your editor. Add the otlphttp exporter configuration to the exporters: section: otlphttp: # Exporter Type endpoint: \"http://localhost:5318\" # Gateway OTLP endpoint Add a Batch Processor configuration: The Batch Processor will accept spans, metrics, or logs and place them into batches. Batching helps better compress the data and reduce the number of outgoing connections required to transmit the data. It is highly recommended configuring the batch processor on every collector.",
    "tags": [],
    "title": "2.2 Configure Agent",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/2-gateway/2-2-configure-agent/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics \u003e 2. API Test",
    "content": "Create a new API test by clicking on the Add new test button and select API test from the dropdown. Name the test using your team name, your initials, and Spotify API e.g. [Daisy] RWC - Spotify API",
    "description": "Create a new API test by clicking on the Add new test button and select API test from the dropdown. Name the test using your team name, your initials, and Spotify API e.g. [Daisy] RWC - Spotify API",
    "tags": [],
    "title": "Create new API test",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/2-api-test/2-create-new-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Self-Service Observability \u003e 2 Collect Data with Standards",
    "content": "Collector (Agent) Now we will deploy a collector. At first this will be configured to go directly to the back-end, but we will change the configuration and restart the collector to use the gateway.\nThe steps:\nClick the Data Management icon in the toolbar Click the + Add integration button Click Deploy the Splunk OpenTelemetry Collector button Click Next Select Linux Leave the mode as Host monitoring (agent) Set the environment to prod Leave the rest as defaults Choose the access token for this workshop Click Next Copy the installer script and run it in the provided linux environment. This collector is sending host metrics, so you can find it in common navigators:\nClick the Infrastructure icon in the toolbar Click the EC2 panel under Amazon Web Services The AWSUniqueId is the easiest thing to find; add a filter and look for it with a wildcard (i.e. i-0ba6575181cb05226*) We can also simply look at the cpu.utilization metric. Create a new chart to display it, filtered on the AWSUniqueId:\nThe reason we wanted to do that is so we can easily see the new dimension added on once we send the collector through the gateway. You can click on the Data table to see the dimensions currently being sent:\nNext Next we’ll reconfigure the collector to send to the gateway.",
    "description": "Collector (Agent) Now we will deploy a collector. At first this will be configured to go directly to the back-end, but we will change the configuration and restart the collector to use the gateway.\nThe steps:\nClick the Data Management icon in the toolbar Click the + Add integration button Click Deploy the Splunk OpenTelemetry Collector button Click Next Select Linux Leave the mode as Host monitoring (agent) Set the environment to prod Leave the rest as defaults Choose the access token for this workshop Click Next Copy the installer script and run it in the provided linux environment. This collector is sending host metrics, so you can find it in common navigators:",
    "tags": [],
    "title": "Deploy Collector (Agent)",
    "uri": "/observability-workshop/v6.5/en/scenarios/self-service-observability/2-collect-with-standards/2-deploy-collector-agent/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk OnCall \u003e 2. Incident Lifecycle",
    "content": "1. Acknowledge Use your Splunk On-Call App on your phone to acknowledge the Incident by clicking on the push notification …\n…to open the alert in the Splunk On-Call mobile app, then clicking on either the single tick in the top right hand corner, or the Acknowledge link to acknowledge the incident and stop the escalation process.\nThe :fontawesome-solid-check: will then transform into a :fontawesome-solid-check::fontawesome-solid-check:, and the status will change from TRIGGERED to ACKNOWLEDGED.\nTriggered Incident Acknowledge Incident 2. Details and Annotations Still on your phone, select the Alert Details tab. Then on the Web UI, navigate back to Timeline, select Team Incidents on the right, then select Acknowledged and click into the new Incident, this will open up the War Room Dashboard view of the Incident.\nYou should now have the Details tab displayed on both your Phone and the Web UI. Notice how they both show the exact same information.\nNow select the Annotations tab on both the Phone and the Web UI, you should have a Graph displayed in the UI which is generated by Splunk Infrastructure Monitoring.\nOn your phone you should get the same image displayed (sometimes it’s a simple hyperlink depending on the image size)\nSplunk On-Call is a ‘Mobile First’ platform meaning the phone app is full functionality and you can manage an incident directly from your phone.\nFor the remainder of this module we will focus on the Web UI however please spend some time later exploring the phone app features.\n3. Link to Alerting System Sticking with the Web UI, click the 2. Alert Details in SignalFx link.\nThis will open a new browser tab and take you directly to the Alert within Splunk Infrastructure Monitoring where you could then progress your troubleshooting using the powerful tools built into its UI.\nHowever, we are focussing on Splunk On-Call so close this tab and return to the Splunk On-Call UI.\n4. Similar Incidents What if Splunk On-Call could identify previous incidents within the system which may give you a clue to the best way to tackle this incident.\nThe Similar Incidents tab does exactly that, surfacing previous incidents allowing you to look at them and see what actions were taken to resolve them, actions which could be easily repeated for this incident.\n5 Timeline On right we have a Time Line view where you can add messages and see the history of previous alerts and interactions.\n6 Add Responders On the far left you have the option of allocating additional resources to this incident by clicking on the Add Responders link.\nThis allows you build a virtual team specific to this incident by adding other Teams or individual Users, and also share details of a Conference Bridge where you can all get together and collaborate.\nOnce the system has built up some incident data history, it will use Machine Learning to suggest Teams and Users who have historically worked on similar incidents, as they may be best placed to help resolve this incident quickly.\nYou can select different Teams and/or Users and also choose from a pre-configured conference bridge, or populate the details of a new bridge from your preferred provider.\nWe do not need to add any Responders in this exercise so close the Add Responders dialogue by clicking Cancel.\n7 Reroute If it’s decided that maybe the incident could be better dealt with by a different Team, the call can be Rerouted by clicking the Reroute Button at the top of the left hand panel.\nIn a similar method to that used in the Add Responders dialogue, you can select Teams or Users to Reroute the Incident to.\nWe do not need to actually Reroute in this exercise so close the Reroute Incident dialogue by clicking Cancel.\n8 Snooze You can also snooze this incident by clicking on the alarm clock Button at the top of the left hand panel.\nYou can enter an amount of time upto 24 hours to snooze the incident. This action will be tracked in the Timeline, and when the time expires the paging will restart.\nThis is useful for low priority incidents, enabling you to put them on a back burner for a few hours, but it ensures they do not get forgotten thanks to the paging process starting again.\nWe do not need to actually Snooze in this exercise so close the Snooze Incident dialogue by clicking Cancel.\n9 Action Tracking Now lets fix this issue and update the Incident with what we did. Add a new message at the top of the right hand panel such as Discovered rogue process, terminated it.\nAll the actions related to the Incident will be recorded here, and can then be summarized is a Post Incident Review Report available from the Reports tab\n10 Resolution Now kill off the process we started in the VM to max out the CPU by switching back the Shell session for the VM and pressing ctrl+c\nWithin no more than 10 seconds SignalFx should detect the new CPU value, clear the alert state in SignalFx, then automatically update the Incident in VictorOps marking it as Resolved.\nAs we have two way integration between Splunk Infrastructure Monitoring and Splunk On-Call we could have also marked the incident as Resolved in Splunk On-Call, and this would have resulted in the alert in Splunk Infrastructure Monitoring being resolved as well.\nThat completes this introduction to Splunk On-Call!",
    "description": "1. Acknowledge Use your Splunk On-Call App on your phone to acknowledge the Incident by clicking on the push notification …\n…to open the alert in the Splunk On-Call mobile app, then clicking on either the single tick in the top right hand corner, or the Acknowledge link to acknowledge the incident and stop the escalation process.\nThe :fontawesome-solid-check: will then transform into a :fontawesome-solid-check::fontawesome-solid-check:, and the status will change from TRIGGERED to ACKNOWLEDGED.",
    "tags": [],
    "title": "Manage Incidents",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/incident_lifecycle/manage_incidents/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 2. Extensions",
    "content": "Performance Profiler Performance Profiler extension enables the golang net/http/pprof endpoint. This is typically used by developers to collect performance profiles and investigate issues with the service. We will not be covering this in this workshop.",
    "description": "Performance Profiler Performance Profiler extension enables the golang net/http/pprof endpoint. This is typically used by developers to collect performance profiles and investigate issues with the service. We will not be covering this in this workshop.",
    "tags": [],
    "title": "OpenTelemetry Collector Extensions",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/2-extensions/2-performance/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 2. Building Resilience",
    "content": "Next, we will configure our environment to be ready for testing the File Storage configuration.\nExercise Start the Gateway: In the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml Send five test spans: In the Loadgen terminal window run:\n​ Start Load Generator ../loadgen -count 5 Both the Agent and Gateway should display debug logs, and the Gateway should create a ./gateway-traces.out file.\nIf everything functions correctly, we can proceed with testing system resilience.",
    "description": "Next, we will configure our environment to be ready for testing the File Storage configuration.\nExercise Start the Gateway: In the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml Send five test spans: In the Loadgen terminal window run:",
    "tags": [],
    "title": "2.2 Setup environment for Resilience Testing",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/2-building-resilience/2-2-test-environment/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 2. Building Resilience",
    "content": "Next, we will configure our environment to be ready for testing the File Storage configuration.\nExercise Start the Gateway: In the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml Send five test spans: In the Loadgen terminal window run:\n​ Start Load Generator ../loadgen -count 5 Both the Agent and Gateway should display debug logs, and the Gateway should create a ./gateway-traces.out file.\nIf everything functions correctly, we can proceed with testing system resilience.",
    "description": "Next, we will configure our environment to be ready for testing the File Storage configuration.\nExercise Start the Gateway: In the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml Send five test spans: In the Loadgen terminal window run:",
    "tags": [],
    "title": "2.2 Setup environment for Resilience Testing",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/2-building-resilience/2-2-test-environment/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour \u003e 3. APM Overview",
    "content": "Click APM in the main menu, the APM Home Page is made up of 3 distinct sections:\nOnboarding Pane Pane: Training videos and links to documentation to get you started with Splunk APM. APM Overview Pane: Real-time metrics for the Top Services and Top Business Workflows. Functions Pane: Links for deeper analysis of your services, tags, traces, database query performance and code profiling. The APM Overview pan provides a high-level view of the health of your application. It includes a summary of the services, latency and errors in your application. It also includes a list of the top services by error rate and the top business workflows by error rate (a business workflow is the start-to-finish journey of the collection of traces associated with a given activity or transaction and enables monitoring of end-to-end KPIs and identifying root causes and bottlenecks).\nAbout Environments To easily differentiate between multiple applications, Splunk uses environments. The naming convention for workshop environments is [NAME OF WORKSHOP]-workshop. Your instructor will provide you with the correct one to select.\nExercise Verify that the time window we are working with is set to the last 15 minutes (-15m). Change the environment to the workshop one by selecting its name from the drop-down box and make sure that is the only one selected. ​ Question Answer What can you conclude from the Top Services by Error Rate chart?\nThe paymentservice has a high error rate\nIf you scroll down the Overview Page you will notice some services listed have Inferred Service next to them.\nSplunk APM can infer the presence of the remote service, or inferred service if the span calling the remote service has the necessary information. Examples of possible inferred services include databases, HTTP endpoints, and message queues. Inferred services are not instrumented, but they are displayed on the service map and the service list.\nNext, let’s check out Splunk Log Observer (LO).",
    "description": "Click APM in the main menu, the APM Home Page is made up of 3 distinct sections:\nOnboarding Pane Pane: Training videos and links to documentation to get you started with Splunk APM. APM Overview Pane: Real-time metrics for the Top Services and Top Business Workflows. Functions Pane: Links for deeper analysis of your services, tags, traces, database query performance and code profiling. The APM Overview pan provides a high-level view of the health of your application. It includes a summary of the services, latency and errors in your application. It also includes a list of the top services by error rate and the top business workflows by error rate (a business workflow is the start-to-finish journey of the collection of traces associated with a given activity or transaction and enables monitoring of end-to-end KPIs and identifying root causes and bottlenecks).",
    "tags": [],
    "title": "Application Performance Monitoring Home page",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/3-apm-home/1-apm-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence \u003e 3. Creating Services in ITSI",
    "content": "Starting with an Observability Cloud Based Service Access Services: In ITSI click “Configuration”, click on “Services”.\nCreate New Service: PaymentService2: Click “Create New Service”.\nService Details (PaymentService2):\nTitle: “PaymentService2” Description (Optional): e.g., “Payment Service for Hipster Shop - version 2” Select Template: Choose “Link service to a service template” and search for “Splunk APM Business Workflow KPIs” from the template dropdown. Click Create to save the new service.\nEntity Assignment:\nThe page will load and display the new Service and you will be on the Entities page. This demo defaults to selecting the paymentservice:grpc.hipstershop.PaymentService/Charge entity. In a real world situation you would need to match the workflow to the entity name manually. Direct Entity Selection (If Available): Search for the entity using sf_workflow=\"paymentservice:grpc.hipstershop.PaymentService/Charge\" and select it. Save Service (PaymentService2): Click “Save” to create “PaymentService2”.\nSettings: Click the “Settings” tab, enable Backfill and keep that standard 7 days. Enable the Service, and click “Save”\nSetting PaymentService2’s Service Health as a Dependency for Online-Boutique-US Locate Online-Boutique-US: Find the “Online-Boutique-US” service in the service list.\nEdit Online-Boutique-US: Click “Edit”.\nService Dependencies: Look for the “Service Dependencies” section.\nAdd Dependency: There should be an option to add a dependent service. Search for “PaymentService2”.\nSelect KPI: Check the box next to ServiceHealthScore for PaymentService2.\nSave Changes: Save the changes to the “Online-Boutique-US” service.\nVerification Click on “Service Analyzer” and select the “Default Analyzer” Filter the service to just “Buttercup Business Health” Verify that PaymentService2 is now present below Online-Boutique-US and should be in a grey status.",
    "description": "Starting with an Observability Cloud Based Service Access Services: In ITSI click “Configuration”, click on “Services”.\nCreate New Service: PaymentService2: Click “Create New Service”.\nService Details (PaymentService2):\nTitle: “PaymentService2” Description (Optional): e.g., “Payment Service for Hipster Shop - version 2” Select Template: Choose “Link service to a service template” and search for “Splunk APM Business Workflow KPIs” from the template dropdown. Click Create to save the new service.\nEntity Assignment:",
    "tags": [],
    "title": "Creating an O11y Based Service",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/3-creating-services-in-itsi/1-creating-o11y-service/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 3. Reuse Content Across Teams",
    "content": "Splunk Infrastructure Monitoring (IM) is a market-leading monitoring and observability service for hybrid cloud environments. Built on a patented streaming architecture, it provides a real-time solution for engineering teams to visualize and analyze performance across infrastructure, services, and applications in a fraction of the time and with greater accuracy than traditional solutions.\n300+ Easy-to-use OOTB content: Pre-built navigators and dashboards, deliver immediate visualizations of your entire environment so that you can interact with all your data in real time.\nKubernetes navigator: Provides an instant, comprehensive out-of-the-box hierarchical view of nodes, pods, and containers. Ramp up even the most novice Kubernetes user with easy-to-understand interactive cluster maps.\nAutoDetect alerts and detectors: Automatically identify the most important metrics, out-of-the-box, to create alert conditions for detectors that accurately alert from the moment telemetry data is ingested and use real-time alerting capabilities for important notifications in seconds.\nLog views in dashboards: Combine log messages and real-time metrics on one page with common filters and time controls for faster in-context troubleshooting.\nMetrics pipeline management: Control metrics volume at the point of ingest without re-instrumentation with a set of aggregation and data-dropping rules to store and analyze only the needed data. Reduce metrics volume and optimize observability spend.\nExercise: Find your Kubernetes Cluster From the Splunk Observability Cloud homepage, click the button -\u003e Kubernetes -\u003e K8s nodes First, use the option to pick your cluster. From the filter drop-down box, use the store.location value you entered when deploying the application. You then can start typing the city you used which should also appear in the drop-down values. Select yours and make sure just the one for your workshop is highlighted with a . Click the Apply Filter button to focus on our Cluster. You should now have your Kubernetes Cluster visible Here we can see all of the different components of the cluster (Nodes, Pods, etc), each of which has relevant metrics associated with it. On the right side, you can also see what services are running in the cluster. Before moving to the next section, take some time to explore the Kubernetes Navigator to see the data that is available Out of the Box.",
    "description": "Splunk Infrastructure Monitoring (IM) is a market-leading monitoring and observability service for hybrid cloud environments. Built on a patented streaming architecture, it provides a real-time solution for engineering teams to visualize and analyze performance across infrastructure, services, and applications in a fraction of the time and with greater accuracy than traditional solutions.\n300+ Easy-to-use OOTB content: Pre-built navigators and dashboards, deliver immediate visualizations of your entire environment so that you can interact with all your data in real time.\nKubernetes navigator: Provides an instant, comprehensive out-of-the-box hierarchical view of nodes, pods, and containers. Ramp up even the most novice Kubernetes user with easy-to-understand interactive cluster maps.\nAutoDetect alerts and detectors: Automatically identify the most important metrics, out-of-the-box, to create alert conditions for detectors that accurately alert from the moment telemetry data is ingested and use real-time alerting capabilities for important notifications in seconds.\nLog views in dashboards: Combine log messages and real-time metrics on one page with common filters and time controls for faster in-context troubleshooting.\nMetrics pipeline management: Control metrics volume at the point of ingest without re-instrumentation with a set of aggregation and data-dropping rules to store and analyze only the needed data. Reduce metrics volume and optimize observability spend.",
    "tags": [],
    "title": "Infrastrcuture Navigators",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/3-reuse-content-across-teams/1-infrastructure-dashboards/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour \u003e 3. Log Observer Overview",
    "content": "Click Log Observer in the main menu, the Log Observer Home Page is made up of 4 distinct sections:\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Log Observer. Filter Bar: Filter on time, indexes, and fields and also Save Queries. Logs Table Pane: List of log entries that match the current filter criteria. Fields Pane: List of fields available in the currently selected index. Splunk indexes Generally, in Splunk, an “index” refers to a designated place where your data is stored. It’s like a folder or container for your data. Data within a Splunk index is organized and structured in a way that makes it easy to search and analyze. Different indexes can be created to store specific types of data. For example, you might have one index for web server logs, another for application logs, and so on.\nTip If you have used Splunk Enterprise or Splunk Cloud before, you are probably used to starting investigations with logs. As you will see in the following exercise, you can do that with Splunk Observability Cloud as well. This workshop, however, will use all the OpenTelemetry signals for investigations.\nLet’s run a little search exercise:\nExercise Set the time frame to -15m.\nClick on Add Filter in the filter bar then click on Fields in the dialog.\nType in cardType and select it.\nUnder Top values click on visa, then click on = to add it to the filter.\nClick Run search\nClick on one of the log entries in the Logs table to validate that the entry contains cardType: \"visa\".\nLet’s find all the wire transfer orders that have been compelted. Click on Clear All in the filter bar to remove the previous filter.\nClick again on Add Filter in the filter bar, then select Keyword. Next just type order in the Enter Keyword… box and press enter.\nClick Run search\nYou should now only have log lines that contain the word order. There are still a lot of log lines – some of which may not be our service – so let’s filter some more.\nAdd another filter, this time select the Fields box, then type severity in the Find a field … search box and select it. Under Top values click on error, then click on = to add it to the filter.\nClick Run search\nYou should now have a list of wire transfer orders that failed to complete for the last 15 minutes.\nNext, let’s check out Splunk Synthetics.",
    "description": "Click Log Observer in the main menu, the Log Observer Home Page is made up of 4 distinct sections:\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Log Observer. Filter Bar: Filter on time, indexes, and fields and also Save Queries. Logs Table Pane: List of log entries that match the current filter criteria. Fields Pane: List of fields available in the currently selected index. Splunk indexes Generally, in Splunk, an “index” refers to a designated place where your data is stored. It’s like a folder or container for your data. Data within a Splunk index is organized and structured in a way that makes it easy to search and analyze. Different indexes can be created to store specific types of data. For example, you might have one index for web server logs, another for application logs, and so on.",
    "tags": [],
    "title": "Log Observer Home Page",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/3-log-observer-home/1-log-observer-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud \u003e 3. Create an Ingest Pipeline",
    "content": "In this section you will create an Ingest Pipeline which will convert Kubernetes Audit Logs to metrics which are sent to the Splunk Observability Cloud workshop organization. Before getting started you will need to access the Splunk Cloud and Ingest Processor SCS Tenant environments provided in the Splunk Show event details.\nPre-requisite: Login to Splunk Enterprise Cloud 1. Open the Ingest Processor Cloud Stack URL provided in the Splunk Show event details.\n2. In the Connection info click on the Stack URL link to open your Splunk Cloud stack.\n3. Use the admin username and password to login to Splunk Cloud.\n4. After logging in, if prompted, accept the Terms of Service and click OK\n5. Navigate back to the Splunk Show event details and select the Ingest Processor SCS Tenant\n6. Click on the Console URL to access the Ingest Processor SCS Tenant\nNote Single Sign-On (SSO) Single Sign-on (SSO) is configured between the Splunk Data Management service (‘SCS Tenant’) and Splunk Cloud environments, so if you already logged in to your Splunk Cloud stack you should automatically be logged in to Splunk Data Management service. If you are prompted for credentials, use the credentials provided in the Splunk Cloud Stack on Splunk Show event (listed under the ‘Splunk Cloud Stack’ section.)",
    "description": "In this section you will create an Ingest Pipeline which will convert Kubernetes Audit Logs to metrics which are sent to the Splunk Observability Cloud workshop organization. Before getting started you will need to access the Splunk Cloud and Ingest Processor SCS Tenant environments provided in the Splunk Show event details.\nPre-requisite: Login to Splunk Enterprise Cloud 1. Open the Ingest Processor Cloud Stack URL provided in the Splunk Show event details.",
    "tags": [],
    "title": "Login to Splunk Cloud",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/3-create-an-ingest-pipeline/1-login-to-splunk/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 3. Dashboards",
    "content": "1. Saving a chart To start saving your chart, lets give it a name and description. Click the name of the chart Copy of Latency Histogram and rename it to “Active Latency”.\nTo change the description click on Spread of latency values across time. and change this to Overview of latency values in real-time.\nClick the Save As button. Make sure your chart has a name, it will use the name Active Latency the you defined in the previous step, but you can edit it here if needed.\nPress the Ok button to continue.\n2. Creating a dashboard In the Choose dashboard dialog, we need to create a new dashboard, click on the New Dashboard button.\nYou will now see the New Dashboard Dialog. In here you can give you dashboard a name and description, and set Read and Write Permissions.\nPlease use your own name in the following format to give your dashboard a name e.g. YOUR_NAME-Dashboard.\nPlease replace YOUR_NAME with your own name, change the dashboard permissions to Restricted Read and Write access, and verify your user can read/write.\nYou should see you own login information displayed, meaning you are now the only one who can edit this dashboard. Of course you have the option to add other users or teams from the drop box below that may edit your dashboard and charts, but for now make sure you change it back to Everyone can Read or Write to remove any restrictions and press the Save Button to continue.\nYour new dashboard is now available and selected so you can save your chart in your new dashboard.\nMake sure you have your dashboard selected and press the Ok button.\nYou will now be taken to your dashboard like below. You can see at the top left that your YOUR_NAME-DASHBOARD is part of a Dashboard Group YOUR_NAME-Dashboard. You can add other dashboards to this dashboard group.\n3. Add to Team page It is common practice to link dashboards that are relevant to a Team to a teams page. So let’s add your dashboard to the team page for easy access later. Use the from the navbar again.\nThis will bring you to your teams dashboard, We use the team Example Team as an example here, the workshop one will be different.\nPress the + Add Dashboard Group button to add you dashboard to the team page.\nThis will bring you to the Select a dashboard group to link to this team dialog. Type your name (that you used above) in the search box to find your Dashboard. Select it so its highlighted and click the Ok button to add your dashboard.\nYour dashboard group will appear as part of the team page. Please note during the course of the workshop many more will appear here.\nNow click on the link for your Dashboard to add more charts!",
    "description": "1. Saving a chart To start saving your chart, lets give it a name and description. Click the name of the chart Copy of Latency Histogram and rename it to “Active Latency”.\nTo change the description click on Spread of latency values across time. and change this to Overview of latency values in real-time.\nClick the Save As button. Make sure your chart has a name, it will use the name Active Latency the you defined in the previous step, but you can edit it here if needed.",
    "tags": [],
    "title": "Saving charts",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/dashboards/savingcharts/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 3. Dropping Spans",
    "content": "To test your configuration, you’ll need to generate some trace data that includes a span named \"/_healthz\".\nExercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config ./gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config ./agent.yaml Start the Loadgen: In the Loadgen terminal window, execute the following command to start the load generator with health check spans enabled:\n../loadgen -health -count 5 The debug output in the Agent terminal will show _healthz spans:\nInstrumentationScope healthz 1.0.0 Span #0 Trace ID : 0cce8759b5921c8f40b346b2f6e2f4b6 Parent ID : ID : bc32bd0e4ddcb174 Name : /_healthz Kind : Server Start time : 2025-07-11 08:47:50.938703979 +0000 UTC End time : 2025-07-11 08:47:51.938704091 +0000 UTC Status code : Ok Status message : Success They will not be present in the Gateway debug as they are dropped by the filter processor that was configured earlier.\nVerify agent.out: Using jq, in the Test terminal, confirm the name of the spans received by the Agent:\n​ Check spans in agent.out Example output jq -c '.resourceSpans[].scopeSpans[].spans[] | \"Span \\(input_line_number) found with name \\(.name)\"' ./agent.out \"Span 1 found with name /movie-validator\" \"Span 2 found with name /_healthz\" \"Span 3 found with name /movie-validator\" \"Span 4 found with name /_healthz\" \"Span 5 found with name /movie-validator\" \"Span 6 found with name /_healthz\" \"Span 7 found with name /movie-validator\" \"Span 8 found with name /_healthz\" \"Span 9 found with name /movie-validator\" \"Span 10 found with name /_healthz\" Check the Gateway Debug output: Using jq confirm the name of the spans received by the Gateway:\n​ Check spans in gateway-traces.out Example output jq -c '.resourceSpans[].scopeSpans[].spans[] | \"Span \\(input_line_number) found with name \\(.name)\"' ./gateway-traces.out The gateway-metrics.out file will not contain any spans named /_healthz.\n\"Span 1 found with name /movie-validator\" \"Span 2 found with name /movie-validator\" \"Span 3 found with name /movie-validator\" \"Span 4 found with name /movie-validator\" \"Span 5 found with name /movie-validator\" Tip To ensure optimal performance with the Filter processor, thoroughly understand your incoming data format and rigorously test your configuration. Use the most specific filtering criteria possible to minimize the risk of inadvertently dropping important data.\nThis configuration can be extended to filter spans based on various attributes, tags, or custom criteria, enhancing the OpenTelemetry Collector’s flexibility and efficiency for your specific observability requirements.\nImportant Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "To test your configuration, you’ll need to generate some trace data that includes a span named \"/_healthz\".\nExercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config ./gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config ./agent.yaml Start the Loadgen: In the Loadgen terminal window, execute the following command to start the load generator with health check spans enabled:",
    "tags": [],
    "title": "3.2 Test Filter Processor",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/3-dropping-spans/3-2-test-filter/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 3. Dropping Spans",
    "content": "To test your configuration, you’ll need to generate some trace data that includes a span named \"/_healthz\".\nExercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config ./gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config ./agent.yaml Start the Loadgen: In the Loadgen terminal window, execute the following command to start the load generator with health check spans enabled:\n../loadgen -health -count 5 The debug output in the Agent terminal will show _healthz spans:\nInstrumentationScope healthz 1.0.0 Span #0 Trace ID : 0cce8759b5921c8f40b346b2f6e2f4b6 Parent ID : ID : bc32bd0e4ddcb174 Name : /_healthz Kind : Server Start time : 2025-07-11 08:47:50.938703979 +0000 UTC End time : 2025-07-11 08:47:51.938704091 +0000 UTC Status code : Ok Status message : Success They will not be present in the Gateway debug as they are dropped by the filter processor that was configured earlier.\nVerify agent.out: Using jq, in the Test terminal, confirm the name of the spans received by the Agent:\n​ Check spans in agent.out Example output jq -c '.resourceSpans[].scopeSpans[].spans[] | \"Span \\(input_line_number) found with name \\(.name)\"' ./agent.out \"Span 1 found with name /movie-validator\" \"Span 2 found with name /_healthz\" \"Span 3 found with name /movie-validator\" \"Span 4 found with name /_healthz\" \"Span 5 found with name /movie-validator\" \"Span 6 found with name /_healthz\" \"Span 7 found with name /movie-validator\" \"Span 8 found with name /_healthz\" \"Span 9 found with name /movie-validator\" \"Span 10 found with name /_healthz\" Check the Gateway Debug output: Using jq confirm the name of the spans received by the Gateway:\n​ Check spans in gateway-traces.out Example output jq -c '.resourceSpans[].scopeSpans[].spans[] | \"Span \\(input_line_number) found with name \\(.name)\"' ./gateway-traces.out The gateway-metrics.out file will not contain any spans named /_healthz.\n\"Span 1 found with name /movie-validator\" \"Span 2 found with name /movie-validator\" \"Span 3 found with name /movie-validator\" \"Span 4 found with name /movie-validator\" \"Span 5 found with name /movie-validator\" Tip To ensure optimal performance with the Filter processor, thoroughly understand your incoming data format and rigorously test your configuration. Use the most specific filtering criteria possible to minimize the risk of inadvertently dropping important data.\nThis configuration can be extended to filter spans based on various attributes, tags, or custom criteria, enhancing the OpenTelemetry Collector’s flexibility and efficiency for your specific observability requirements.\nImportant Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "To test your configuration, you’ll need to generate some trace data that includes a span named \"/_healthz\".\nExercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config ./gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config ./agent.yaml Start the Loadgen: In the Loadgen terminal window, execute the following command to start the load generator with health check spans enabled:",
    "tags": [],
    "title": "3.2 Test Filter Processor",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/3-dropping-spans/3-2-test-filter/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour \u003e 4. Infrastructure Overview",
    "content": "Click on Infrastructure in the main menu, the Infrastructure Home Page is made up of 4 distinct sections.\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Infrastructure Monitoring. Time \u0026 Filter Pane: Time window (not configurable at the top level) Integrations Pane: List of all the technologies that are sending metrics to Splunk Observability Cloud. Tile Pane: Total number of services being monitored broken down by integration. Using the Infrastructure pane, we can select the infrastructure/technology we are interested in, let’s do that now.\nExercise Under the Containers section in the Integrations Pane (3), select Kubernetes as the technology you wish to examine.\nThis should show you two tiles, K8s Nodes and K8s Workloads.\nThe bottom part of each tile will have a history graph and the top part will show notifications for alerts that fired. Across all tiles, this additional information on each of the tiles will give you a good overview of the health of your infrastructure.\nClick on the K8s Nodes tile.\nYou will be presented with one or more representations of a Kubernetes Cluster.\nClick on the Add filters button. Type in k8s.cluster.name and click on the search result.\nFrom the list, select [NAME OF WORKSHOP]-k3s-cluster then click on the Apply Filter button.\nThe Kubernetes Navigator uses color to indicate health. As you can see there are two pods or services that are unhealthy and in a Failed state (1). The rest are healthy and running. This is not uncommon in shared Kubernetes environments, so we replicated that for the workshop.\nNote the tiles to the side, under Nodes dependencies (2), specifically the MySQL and Redis tiles. These are the two databases used by our e-commerce application.\nNode Dependencies The UI will show services that are running on the node you have selected if they have been configured to be monitored by the OpenTelemetry Collector.\nExercise Click on the Redis tile and this will take you to the Redis instances navigator. Under REDIS INSTANCE click on redis-[NAME OF WORKSHOP]. This will bring you to the Redis instance. This navigator will show charts with metric data from the active Redis instance from our e-commerce site. ​ Question Answer Can you name the Instance dependencies tile in this view?\nYes, there is one for Kubernetes.\nClick the tile, it will bring us back into the Kubernetes Navigator, this time at the Pod level showing the Pod that runs the Redis Service. To return to the Cluster level, simply click on the link Cluster (1) at the top of the screen. This completes the tour of Splunk Observability Cloud.\nHere, have some virtual 💶 and let’s go and look at our e-commerce site, the ‘Online Boutique’ and do some shopping.",
    "description": "Click on Infrastructure in the main menu, the Infrastructure Home Page is made up of 4 distinct sections.\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Infrastructure Monitoring. Time \u0026 Filter Pane: Time window (not configurable at the top level) Integrations Pane: List of all the technologies that are sending metrics to Splunk Observability Cloud. Tile Pane: Total number of services being monitored broken down by integration. Using the Infrastructure pane, we can select the infrastructure/technology we are interested in, let’s do that now.",
    "tags": [],
    "title": "Infrastructure Navigators",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/4-infrastructure-home/1-infrastructure-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour \u003e 4. Log Observer Overview",
    "content": "Click Log Observer in the main menu, the Log Observer Home Page is made up of 4 distinct sections:\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Log Observer. Filter Bar: Filter on time, indexes, and fields and also Save Queries. Logs Table Pane: List of log entries that match the current filter criteria. Fields Pane: List of fields available in the currently selected index. Splunk indexes Generally, in Splunk, an “index” refers to a designated place where your data is stored. It’s like a folder or container for your data. Data within a Splunk index is organized and structured in a way that makes it easy to search and analyze. Different indexes can be created to store specific types of data. For example, you might have one index for web server logs, another for application logs, and so on.\nTip If you have used Splunk Enterprise or Splunk Cloud before, you are probably used to starting investigations with logs. As you will see in the following exercise, you can do that with Splunk Observability Cloud as well. This workshop, however, will use all the OpenTelemetry signals for investigations.\nLet’s run a little search exercise:\nExercise Set the time frame to -15m.\nClick on Add Filter in the filter bar then click on Fields in the dialog.\nType in cardType and select it.\nUnder Top values click on visa, then click on = to add it to the filter.\nClick Run search\nClick on one of the log entries in the Logs table to validate that the entry contains cardType: \"visa\".\nLet’s find all the orders that have been shipped. Click on Clear All in the filter bar to remove the previous filter.\nClick again on Add Filter in the filter bar, then select Keyword. Next just type order in the Enter Keyword… box and press enter.\nClick Run search\nYou should now only have log lines that contain the word order. There are still a lot of log lines, so let’s filter some more.\nAdd another filter, this time select the Fields box, then type severity in the Find a field … search box and select it. Under Top values click on error, then click on = to add it to the filter.\nClick Run search\nYou should now have a list of orders that failed to complete for the last 15 minutes.\nNext, let’s check out Splunk Synthetics.",
    "description": "Click Log Observer in the main menu, the Log Observer Home Page is made up of 4 distinct sections:\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Log Observer. Filter Bar: Filter on time, indexes, and fields and also Save Queries. Logs Table Pane: List of log entries that match the current filter criteria. Fields Pane: List of fields available in the currently selected index. Splunk indexes Generally, in Splunk, an “index” refers to a designated place where your data is stored. It’s like a folder or container for your data. Data within a Splunk index is organized and structured in a way that makes it easy to search and analyze. Different indexes can be created to store specific types of data. For example, you might have one index for web server logs, another for application logs, and so on.",
    "tags": [],
    "title": "Log Observer Home Page",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/4-log-observer-home/1-log-observer-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud \u003e 4. Update Pipeline and Visualize Metrics",
    "content": "Exercise: Update Ingest Pipeline 1. Navigate back to the configuration page for the Ingest Pipeline you created in the previous step.\n2. To add dimensions to the metric from the raw Kubernetes audit logs update the SPL2 query you created for the pipeline by replacing the logs_to_metrics portion of the query with the following:\nNote Be sure to update the metric name field (name=\"k8s_audit_UNIQUE_FIELD\") to the name you provided in the original pipeline\n| logs_to_metrics name=\"k8s_audit_UNIQUE_FIELD\" metrictype=\"counter\" value=1 time=_time dimensions={\"level\": _raw.level, \"response_status\": _raw.responseStatus.code, \"namespace\": _raw.objectRef.namespace, \"resource\": _raw.objectRef.resource, \"user\": _raw.user.username, \"action\": _raw.verb} Note Using the dimensions field in the SPL2 query you can add dimensions from the raw events to the metrics that will be sent to Splunk Observability Cloud. In this case you are adding the event response status, namespace, Kubernetes resource, user, and verb (action that was performed). These dimensions can be used to create more granular dashboards and alerts.\nYou should consider adding any common tags across your services so that you can take advantage of context propagation and related content in Splunk Observability Cloud.\nThe updated pipeline should now be the following:\n/*A valid SPL2 statement for a pipeline must start with \"$pipeline\", and include \"from $source\" and \"into $destination\".*/ /* Import logs_to_metrics */ import logs_to_metrics from /splunk/ingest/commands $pipeline = | from $source | thru [ //define the metric name, type, and value for the Kubernetes Events // // REPLACE UNIQUE_FIELD WITH YOUR INITIALS // | logs_to_metrics name=\"k8s_audit_UNIQUE_FIELD\" metrictype=\"counter\" value=1 time=_time dimensions={\"level\": _raw.level, \"response_status\": _raw.responseStatus.code, \"namespace\": _raw.objectRef.namespace, \"resource\": _raw.objectRef.resource, \"user\": _raw.user.username, \"action\": _raw.verb} | into $metrics_destination ] | eval index = \"kube_logs\" | into $destination; 3. In the upper-right corner click the Preview button or press CTRL+Enter (CMD+Enter on Mac). From the Previewing $pipeline dropdown select $metrics_destination. Confirm you are seeing a preview of the metrics that will be sent to Splunk Observability Cloud.\n4. Confirm you are seeing the dimensions in the dimensions column of the preview table. You can view the entire dimensions object by clicking into the table.\n5. In the upper-right corner click the Save pipeline button . On the “You are editing an active pipeline modal” click Save.\nNote Because this pipeline is already active, the changes you made will take effect immediately. Your metric should now be split into multiple metric timeseries using the dimensions you added.\nIn the next step you will create a visualization using different dimensions from the Kubernetes audit events.",
    "description": "Exercise: Update Ingest Pipeline 1. Navigate back to the configuration page for the Ingest Pipeline you created in the previous step.\n2. To add dimensions to the metric from the raw Kubernetes audit logs update the SPL2 query you created for the pipeline by replacing the logs_to_metrics portion of the query with the following:",
    "tags": [],
    "title": "Update Ingest Pipeline",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/4-update-pipeline-and-visualize/1-update-ingest-pipeline/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 4. Processors",
    "content": "Resource Detection Processor The resourcedetection processor can be used to detect resource information from the host and append or override the resource value in telemetry data with this information.\nBy default, the hostname is set to the FQDN if possible, otherwise, the hostname provided by the OS is used as a fallback. This logic can be changed from using using the hostname_sources configuration option. To avoid getting the FQDN and use the hostname provided by the OS, we will set the hostname_sources to os.\n​ System Resource Detection Processor Configuration processors: batch: resourcedetection/system: detectors: [system] system: hostname_sources: [os] If the workshop instance is running on an AWS/EC2 instance we can gather the following tags from the EC2 metadata API (this is not available on other platforms).\ncloud.provider (\"aws\") cloud.platform (\"aws_ec2\") cloud.account.id cloud.region cloud.availability_zone host.id host.image.id host.name host.type We will create another processor to append these tags to our metrics.\n​ EC2 Resource Detection Processor Configuration processors: batch: resourcedetection/system: detectors: [system] system: hostname_sources: [os] resourcedetection/ec2: detectors: [ec2]",
    "description": "Resource Detection Processor The resourcedetection processor can be used to detect resource information from the host and append or override the resource value in telemetry data with this information.\nBy default, the hostname is set to the FQDN if possible, otherwise, the hostname provided by the OS is used as a fallback. This logic can be changed from using using the hostname_sources configuration option. To avoid getting the FQDN and use the hostname provided by the OS, we will set the hostname_sources to os.",
    "tags": [],
    "title": "OpenTelemetry Collector Processors",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/4-processors/2-resource-detection/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 4. Building Resilience",
    "content": "Next, we will configure our environment to be ready for testing the File Storage configuration.\nExercise Start the Gateway: In the Gateway terminal window navigate to the [WORKSHOP]/4-resilience directory and run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal window navigate to the [WORKSHOP]/4-resilience directory and run:\n​ Start the Agent ../otelcol --config=agent.yaml Send five test spans: In the Spans terminal window navigate to the [WORKSHOP]/4-resilience directory and run:\n​ Start Load Generator ../loadgen -count 5 Both the agent and gateway should display debug logs, and the gateway should create a ./gateway-traces.out file.\nIf everything functions correctly, we can proceed with testing system resilience.",
    "description": "Next, we will configure our environment to be ready for testing the File Storage configuration.\nExercise Start the Gateway: In the Gateway terminal window navigate to the [WORKSHOP]/4-resilience directory and run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal window navigate to the [WORKSHOP]/4-resilience directory and run:\n​ Start the Agent ../otelcol --config=agent.yaml Send five test spans: In the Spans terminal window navigate to the [WORKSHOP]/4-resilience directory and run:",
    "tags": [],
    "title": "4.2 Setup environment for Resilience Testing",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/4-building-resilience/4-2-test-environment/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 4. Sensitive Data",
    "content": "In this exercise, we will delete the user.account_password, update the user.phone_number attribute and hash the user.email in the span data before it is exported by the Agent.\nExercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config=gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window start the loadgen:\n../loadgen -count 1 Check the debug output: For both the Agent and Gateway confirm that user.account_password has been removed, and both user.phone_number \u0026 user.email have been updated:\n​ New Debug Output Original Debug Output -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(UNKNOWN NUMBER) -\u003e user.email: Str(62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287) -\u003e payment.amount: Double(51.71) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(95.22) Check file output: Using jq validate that user.account_password has been removed, and user.phone_number \u0026 user.email have been updated in gateway-taces.out:\n​ Validate attribute changes Output jq '.resourceSpans[].scopeSpans[].spans[].attributes[] | select(.key == \"user.password\" or .key == \"user.phone_number\" or .key == \"user.email\") | {key: .key, value: .value.stringValue}' ./gateway-traces.out Notice that the user.account_password has been removed, and the user.phone_number \u0026 user.email have been updated:\n{ \"key\": \"user.phone_number\", \"value\": \"UNKNOWN NUMBER\" } { \"key\": \"user.email\", \"value\": \"62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287\" } Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "In this exercise, we will delete the user.account_password, update the user.phone_number attribute and hash the user.email in the span data before it is exported by the Agent.\nExercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config=gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window start the loadgen:",
    "tags": [],
    "title": "4.2 Test Attribute Processor",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/4-sensitive-data/4-2-test-delete-tag/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 4. Sensitive Data",
    "content": "In this exercise, we will delete the user.account_password, update the user.phone_number attribute and hash the user.email in the span data before it is exported by the Agent.\nExercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config=gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window start the loadgen:\n../loadgen -count 1 Check the debug output: For both the Agent and Gateway confirm that user.account_password has been removed, and both user.phone_number \u0026 user.email have been updated:\n​ New Debug Output Original Debug Output -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(UNKNOWN NUMBER) -\u003e user.email: Str(62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287) -\u003e payment.amount: Double(51.71) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(95.22) Check file output: Using jq validate that user.account_password has been removed, and user.phone_number \u0026 user.email have been updated in gateway-taces.out:\n​ Validate attribute changes Output jq '.resourceSpans[].scopeSpans[].spans[].attributes[] | select(.key == \"user.password\" or .key == \"user.phone_number\" or .key == \"user.email\") | {key: .key, value: .value.stringValue}' ./gateway-traces.out Notice that the user.account_password has been removed, and the user.phone_number \u0026 user.email have been updated:\n{ \"key\": \"user.phone_number\", \"value\": \"UNKNOWN NUMBER\" } { \"key\": \"user.email\", \"value\": \"62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287\" } Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "In this exercise, we will delete the user.account_password, update the user.phone_number attribute and hash the user.email in the span data before it is exported by the Agent.\nExercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config=gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window start the loadgen:",
    "tags": [],
    "title": "4.2 Test Attribute Processor",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/4-sensitive-data/4-2-test-delete-tag/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour \u003e 5. Synthetics Overview",
    "content": "Click on Synthetics in the main menu. This will bring us to the Synthetics Home Page. It has 3 distinct sections that provide either useful information or allow you to pick or create a Synthetic Test.\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Synthetics. Test Pane: List of all the tests that are configured (Browser, API and Uptime) Create Test Pane: Drop-down for creating new Synthetic tests. Info As part of the workshop we have created a default browser test against the application we are running. You find it in the Test Pane (2). It will have the following name Workshop Browser Test for, followed by the name of your Workshop (your instructor should have provided that to you).\nTo continue our tour, let’s look at the result of our workshop’s automatic browser test.\nExercise In the Test Pane, click on the line that contains the name of your workshop. The result should look like this: Note, On the Synthetic Tests Page, the first pane will show the performance of your site for the last day, 8 days and 30 days. As shown in the screenshot above, only if a test started far enough in the past, the corresponding chart will contain valid data. For the workshop, this depends on when it was created. In the Performance KPI drop-down, change the time from the default 4 hours to the 1 last hour. ​ Question Answer How often is the test run, and from where?\nThe test runs at a 1-minute round-robin interval from Frankfurt, London and Paris\nNext, let’s examine the infrastructure our application is running on using Splunk Infrastructure Monitoring (IM).",
    "description": "Click on Synthetics in the main menu. This will bring us to the Synthetics Home Page. It has 3 distinct sections that provide either useful information or allow you to pick or create a Synthetic Test.\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Synthetics. Test Pane: List of all the tests that are configured (Browser, API and Uptime) Create Test Pane: Drop-down for creating new Synthetic tests. Info As part of the workshop we have created a default browser test against the application we are running. You find it in the Test Pane (2). It will have the following name Workshop Browser Test for, followed by the name of your Workshop (your instructor should have provided that to you).",
    "tags": [],
    "title": "Synthetics Home Page",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/5-synthetics-home/1-synthetics-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 5. Transform Data",
    "content": "Exercise Start the Gateway: In the Gateway terminal run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window, execute the following command to start the load generator with JSON enabled:\n​ Log Generator ../loadgen -logs -json -count 5 The loadgen will write 5 log lines to ./quotes.log in JSON format.",
    "description": "Exercise Start the Gateway: In the Gateway terminal run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window, execute the following command to start the load generator with JSON enabled:",
    "tags": [],
    "title": "5.2 Setup Environment",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/5-transform-data/5-2-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 5. Transform Data",
    "content": "Exercise Start the Gateway: In the Gateway terminal run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window, execute the following command to start the load generator with JSON enabled:\n​ Log Generator ../loadgen -logs -json -count 5 The loadgen will write 5 log lines to ./quotes.log in JSON format.",
    "description": "Exercise Start the Gateway: In the Gateway terminal run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window, execute the following command to start the load generator with JSON enabled:",
    "tags": [],
    "title": "5.2 Setup Environment",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/5-transform-data/5-2-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 5. Dropping Spans",
    "content": "To test your configuration, you’ll need to generate some trace data that includes a span named \"/_healthz\".\nExercise Start the Gateway: In your Gateway terminal window start the gateway.\n../otelcol --config ./gateway.yaml Start the Agent: In your Agent terminal window start the agent.\n../otelcol --config ./agent.yaml Start the Loadgen: In the Spans terminal window run the loadgen with the flag to also send healthz spans along with base spans:\n../loadgen -health -count 5 Verify agent.out: Using jq confirm the name of the spans received by the agent:\n​ Check spans in agent.out Example output jq -c '.resourceSpans[].scopeSpans[].spans[] | \"Span \\(input_line_number) found with name \\(.name)\"' ./agent.out \"Span 1 found with name /movie-validator\" \"Span 2 found with name /_healthz\" \"Span 3 found with name /movie-validator\" \"Span 4 found with name /_healthz\" \"Span 5 found with name /movie-validator\" \"Span 6 found with name /_healthz\" \"Span 7 found with name /movie-validator\" \"Span 8 found with name /_healthz\" \"Span 9 found with name /movie-validator\" \"Span 10 found with name /_healthz\" Check the Gateway Debug output: Using jq confirm the name of the spans received by the gateway:\n​ Check spans in gateway-traces.out Example output ​ Check spans in gateway-traces.out jq -c '.resourceSpans[].scopeSpans[].spans[] | \"Span \\(input_line_number) found with name \\(.name)\"' ./gateway-traces.out The gateway-metrics.out file will not contain any spans named /_healthz.\n\"Span 1 found with name /movie-validator\" \"Span 2 found with name /movie-validator\" \"Span 3 found with name /movie-validator\" \"Span 4 found with name /movie-validator\" \"Span 5 found with name /movie-validator\" Tip When using the Filter processor, make sure you understand the look of your incoming data and test the configuration thoroughly. In general, use as specific a configuration as possible to lower the risk of the wrong data being dropped.\nYou can further extend this configuration to filter out spans based on different attributes, tags, or other criteria, making the OpenTelemetry Collector more customizable and efficient for your observability needs.\nImportant Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "To test your configuration, you’ll need to generate some trace data that includes a span named \"/_healthz\".\nExercise Start the Gateway: In your Gateway terminal window start the gateway.\n../otelcol --config ./gateway.yaml Start the Agent: In your Agent terminal window start the agent.\n../otelcol --config ./agent.yaml Start the Loadgen: In the Spans terminal window run the loadgen with the flag to also send healthz spans along with base spans:",
    "tags": [],
    "title": "5.2 Test Filter Processor",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/5-dropping-spans/5-2-test-filter/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour \u003e 6. Infrastructure Overview",
    "content": "Click on Infrastructure in the main menu, the Infrastructure Home Page is made up of 4 distinct sections.\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Infrastructure Monitoring. Time \u0026 Filter Pane: Time window (not configurable at the top level) Integrations Pane: List of all the technologies that are sending metrics to Splunk Observability Cloud. Tile Pane: Total number of services being monitored broken down by integration. Using the Infrastructure pane, we can select the infrastructure/technology we are interested in, let’s do that now.\nExercise Under the Containers section in the Integrations Pane (3), select Kubernetes as the technology you wish to examine.\nThis should show you two tiles, K8s Nodes and K8s Workloads.\nThe bottom part of each tile will have a history graph and the top part will show notifications for alerts that fired. Across all tiles, this additional information on each of the tiles will give you a good overview of the health of your infrastructure.\nClick on the K8s Nodes tile.\nYou will be presented with one or more representations of a Kubernetes Cluster.\nClick on the Add filters button. Type in k8s.cluster.name and click on the search result.\nFrom the list, select [NAME OF WORKSHOP]-k3s-cluster then click on the Apply Filter button.\nThe Kubernetes Navigator uses color to indicate health. As you can see there are two pods or services that are unhealthy and in a Failed state (1). The rest are healthy and running. This is not uncommon in shared Kubernetes environments, so we replicated that for the workshop.\nNote the tiles to the side, under Nodes dependencies (2), specifically the MySQL and Redis tiles. These are the two databases used by our e-commerce application.\nNode Dependencies The UI will show services that are running on the node you have selected if they have been configured to be monitored by the OpenTelemetry Collector.\nExercise Click on the Redis tile and this will take you to the Redis instances navigator. Under REDIS INSTANCE click on redis-[NAME OF WORKSHOP]. This will bring you to the Redis instance. This navigator will show charts with metric data from the active Redis instance from our e-commerce site. ​ Question Answer Can you name the Instance dependencies tile in this view?\nYes, there is one for Kubernetes.\nClick the tile, it will bring us back into the Kubernetes Navigator, this time at the Pod level showing the Pod that runs the Redis Service. To return to the Cluster level, simply click on the link Cluster (1) at the top of the screen. This completes the tour of Splunk Observability Cloud.\nHere, have some virtual 💶 and let’s go and look at our e-commerce site, the ‘Online Boutique’ and do some shopping.",
    "description": "Click on Infrastructure in the main menu, the Infrastructure Home Page is made up of 4 distinct sections.\nOnboarding Pane: Training videos and links to documentation to get you started with Splunk Infrastructure Monitoring. Time \u0026 Filter Pane: Time window (not configurable at the top level) Integrations Pane: List of all the technologies that are sending metrics to Splunk Observability Cloud. Tile Pane: Total number of services being monitored broken down by integration. Using the Infrastructure pane, we can select the infrastructure/technology we are interested in, let’s do that now.",
    "tags": [],
    "title": "Infrastructure Navigators",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/6-infrastructure-home/1-infrastructure-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 6. Service Bureau",
    "content": "Discover how you can restrict usage by creating separate Access Tokens and setting limits. 1. Access Tokens If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization.\nIn the UI click on the » bottom left and select the Settings → Access Tokens under General Settings.\nThe Access Tokens Interface provides an overview of your allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first set up, but there will typically be multiple Tokens configured.\nEach Token is unique and can be assigned limits for the number of Hosts, Containers, Custom Metrics and High Resolution Metrics it can consume.\nThe Usage Status Column quickly shows if a token is above or below its assigned limits.\n2. Creating a new token Let create a new token by clicking on the New Token button. This will provide you with the Name Your Access Token dialog.\nEnter the new name of the new Token by using your Initials e.g. RWC-Token and make sure to tick both Ingest Token and API Token checkboxes!\nAfter you press OK you will be taken back to the Access Token UI. Here your new token should be present, among the ones created by others.\nIf you have made an error in your naming, want to disable/enable a token or set a Token limit, click on the ellipsis (…) menu button behind a token limit to open the manage token menu.\nIf you made a typo you can use the Rename Token option to correct the name of your token.\n3. Disabling a token If you need to make sure a token cannot be used to send Metrics in you can disable a token.\nClick on Disable to disable the token, this means the token cannot be used for sending in data to Splunk Observability Cloud.\nThe line with your token should have become greyed out to indicate that it has been disabled as you can see in the screenshot below.\nGo ahead and click on the ellipsis (…) menu button to Disable and Enable your token.\n4. Manage token usage limits Now, let’s start limiting usage by clicking on Manage Token Limit in the 3 … menu.\nThis will show the Manage Token Limit Dialog:\nIn this dialog, you can set the limits per category.\nPlease go ahead and specify the limits as follows for each usage metric:\nLimit Value Host Limit 5 Container Limit 15 Custom Metric Limit 20 High Resolution Metric Limit 0 For our lab use your email address, and double check that you have the correct numbers in your dialog box as shown in the table above.\nToken limits are used to trigger an alert that notifies one or more recipients when the usage has been above 90% of the limit for 5 minutes.\nTo specify the recipients, click Add Recipient, then select the recipient or notification method you want to use (specifying recipients is optional but highly recommended).\nThe severity of token alerts is always Critical.\nClick on Update to save your Access Tokens limits and The Alert Settings.\nNote: Going above token limit When a token is at or above its limit in a usage category, new metrics for that usage category will not be stored and processed by Observability Cloud. This will make sure there will be no unexpected cost due to a team sending in data without restriction.\nNote: Advanced alerting If you wish to get alerts before you hit 90%, you can create additional detectors using whatever values you want. These detectors could target the Teams consuming the specific Access Tokens so they can take action before the admins need to get involved.\nIn your company you would distribute these new Access Tokens to various teams, controlling how much information/data they can send to Observability Cloud.\nThis will allow you to fine-tune the way you consume your Observability Cloud allotment and prevent overages from happening.\nCongratulations! You have now completed the Service Bureau module.",
    "description": "Discover how you can restrict usage by creating separate Access Tokens and setting limits. 1. Access Tokens If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization.\nIn the UI click on the » bottom left and select the Settings → Access Tokens under General Settings.\nThe Access Tokens Interface provides an overview of your allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first set up, but there will typically be multiple Tokens configured.",
    "tags": [],
    "title": "Controlling Usage",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/servicebureau/tokens/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 6. Routing Data",
    "content": "Exercise Update the original traces pipeline to use routing:\nTo enable routing, update the original traces pipeline to use routing as the only exporter. This ensures all span data is sent through the Routing Connector for evaluation and then onwards to connected pipelines. Also, remove all processors and replace it with an empty array ([]) as this will now behandeld in the traces/route1-regular and traces/route2-security pipelines, allowing for custom behaviour for each route. Your traces: configuration should look like this:\ntraces: # Traces pipeline receivers: - otlp # OTLP receiver processors: [] # Processors for traces exporters: - routing Add both the route1-regular and route2-security traces pipelines below the existing traces pipeline:\nConfigure Route1-regular pipeline: This pipeline will handle all spans that have no match in the routing table in the connector. Notice this uses routing as its only receiver and will recieve data thought its connection from the original traces pipeline.\ntraces/route1-regular: # Default pipeline for unmatched spans receivers: - routing # Receive data from the routing connector processors: - memory_limiter # Memory Limiter Processor - resource/add_mode # Adds collector mode metadata - batch exporters: - debug # Debug Exporter - file/traces/route1-regular # File Exporter for unmatched spans Add the route2-security pipeline: This pipeline processes all spans that do match our rule \"[deployment.environment\"] == \"security-applications\" in the the routing rule. This pipeline is also using routing as its receiver. Add this pipline below the traces/route1-regular one.\ntraces/route2-security: # Default pipeline for unmatched spans receivers: - routing # Receive data from the routing connector processors: - memory_limiter # Memory Limiter Processor - resource/add_mode # Adds collector mode metadata - batch exporters: - debug # Debug exporter - file/traces/route2-security # File exporter for unmatched spans",
    "description": "Exercise Update the original traces pipeline to use routing:\nTo enable routing, update the original traces pipeline to use routing as the only exporter. This ensures all span data is sent through the Routing Connector for evaluation and then onwards to connected pipelines. Also, remove all processors and replace it with an empty array ([]) as this will now behandeld in the traces/route1-regular and traces/route2-security pipelines, allowing for custom behaviour for each route. Your traces: configuration should look like this:",
    "tags": [],
    "title": "6.2 Configuring the Pipelines",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/6-routing-data/6-2-pipelines/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 6. Routing Data",
    "content": "Exercise Update the original traces pipeline to use routing:\nTo enable routing, update the original traces pipeline to use routing as the only exporter. This ensures all span data is sent through the Routing Connector for evaluation and then onwards to connected pipelines. Also, remove all processors and replace it with an empty array ([]) as this will now behandeld in the traces/route1-regular and traces/route2-security pipelines, allowing for custom behaviour for each route. Your traces: configuration should look like this:\ntraces: # Traces pipeline receivers: - otlp # OTLP receiver processors: [] # Processors for traces exporters: - routing Add both the route1-regular and route2-security traces pipelines below the existing traces pipeline:\nConfigure Route1-regular pipeline: This pipeline will handle all spans that have no match in the routing table in the connector. Notice this uses routing as its only receiver and will recieve data thought its connection from the original traces pipeline.\ntraces/route1-regular: # Default pipeline for unmatched spans receivers: - routing # Receive data from the routing connector processors: - memory_limiter # Memory Limiter Processor - resource/add_mode # Adds collector mode metadata - batch exporters: - debug # Debug Exporter - file/traces/route1-regular # File Exporter for unmatched spans Add the route2-security pipeline: This pipeline processes all spans that do match our rule \"[deployment.environment\"] == \"security-applications\" in the the routing rule. This pipeline is also using routing as its receiver. Add this pipline below the traces/route1-regular one.\ntraces/route2-security: # Default pipeline for unmatched spans receivers: - routing # Receive data from the routing connector processors: - memory_limiter # Memory Limiter Processor - resource/add_mode # Adds collector mode metadata - batch exporters: - debug # Debug exporter - file/traces/route2-security # File exporter for unmatched spans Validate the agent configuration using otelbin.io. For reference, the traces: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO5(batch\u003cbr\u003efa:fa-microchip):::processor PRO6(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026emsp;\u0026emsp;file\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003etraces):::exporter EXP3(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP4(\u0026emsp;\u0026emsp;file\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003etraces):::exporter ROUTE1(\u0026nbsp;routing\u0026nbsp;\u003cbr\u003efa:fa-route):::con-export ROUTE2(\u0026nbsp;routing\u0026nbsp;\u003cbr\u003efa:fa-route):::con-receive ROUTE3(\u0026nbsp;routing\u0026nbsp;\u003cbr\u003efa:fa-route):::con-receive %% Links subID1:::sub-traces subID2:::sub-traces subID3:::sub-traces subgraph \" \" direction LR subgraph subID1[**Traces**] REC1 --\u003e ROUTE1 end subgraph subID2[**Traces/route2-security**] ROUTE1 --\u003e ROUTE2 ROUTE2 --\u003e PRO1 PRO1 --\u003e PRO3 PRO3 --\u003e PRO5 PRO5 --\u003e EXP1 PRO5 --\u003e EXP2 end subgraph subID3[**Traces/route1-regular**] ROUTE1 --\u003e ROUTE3 ROUTE3 --\u003e PRO2 PRO2 --\u003e PRO4 PRO4 --\u003e PRO6 PRO6 --\u003e EXP3 PRO6 --\u003e EXP4 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3;",
    "description": "Exercise Update the original traces pipeline to use routing:\nTo enable routing, update the original traces pipeline to use routing as the only exporter. This ensures all span data is sent through the Routing Connector for evaluation and then onwards to connected pipelines. Also, remove all processors and replace it with an empty array ([]) as this will now behandeld in the traces/route1-regular and traces/route2-security pipelines, allowing for custom behaviour for each route. Your traces: configuration should look like this:",
    "tags": [],
    "title": "6.2 Configuring the Pipelines",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/6-routing-data/6-2-pipelines/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 6. Service",
    "content": "Prometheus Internal Receiver Earlier in the workshop, we also renamed the prometheus receiver to reflect that is was collecting metrics internal to the collector, renaming it to prometheus/internal.\nWe now need to enable the prometheus/internal receiver under the metrics pipeline. Update the receivers section to include prometheus/internal under the metrics pipeline:\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch] exporters: [debug]",
    "description": "Prometheus Internal Receiver Earlier in the workshop, we also renamed the prometheus receiver to reflect that is was collecting metrics internal to the collector, renaming it to prometheus/internal.\nWe now need to enable the prometheus/internal receiver under the metrics pipeline. Update the receivers section to include prometheus/internal under the metrics pipeline:\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch] exporters: [debug]",
    "tags": [],
    "title": "OpenTelemetry Collector Service",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/6-service/2-prometheus/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 6. Sensitive Data",
    "content": "In this exercise, we will delete the user.account_password, update the user.phone_number attribute and hash the user.email in the span data before it is exported by the agent.\nExercise Start the Gateway: In your Gateway terminal window start the gateway.\n../otelcol --config=gateway.yaml Start the Agent: In your Agent terminal window start the agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Spans terminal window start the loadgen:\n../loadgen -count 1 Check the debug output: For both the agent and gateway debug output, confirm that user.account_password has been removed, and both user.phone_number \u0026 user.email have been updated.\n​ New Debug Output Original Debug Output -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(UNKNOWN NUMBER) -\u003e user.email: Str(62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287) -\u003e payment.amount: Double(51.71) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(95.22) Check file output: Using jq validate that user.account_password has been removed, and user.phone_number \u0026 user.email have been updated in gateway-taces.out:\n​ Validate attribute changes Output jq '.resourceSpans[].scopeSpans[].spans[].attributes[] | select(.key == \"user.password\" or .key == \"user.phone_number\" or .key == \"user.email\") | {key: .key, value: .value.stringValue}' ./gateway-traces.out Notice that the user.account_password has been removed, and the user.phone_number \u0026 user.email have been updated:\n{ \"key\": \"user.phone_number\", \"value\": \"UNKNOWN NUMBER\" } { \"key\": \"user.email\", \"value\": \"62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287\" } Important Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "In this exercise, we will delete the user.account_password, update the user.phone_number attribute and hash the user.email in the span data before it is exported by the agent.\nExercise Start the Gateway: In your Gateway terminal window start the gateway.\n../otelcol --config=gateway.yaml Start the Agent: In your Agent terminal window start the agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Spans terminal window start the loadgen:",
    "tags": [],
    "title": "6.2 Test Attribute Processor",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/6-sensitive-data/6-2-test-delete-tag/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 7. Transform Data",
    "content": "Exercise Start the Gateway: In the Gateway terminal run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Load Generator: Open the Logs terminal window and run the loadgen.\nImportant To ensure the logs are structured in JSON format, include the -json flag when starting the script.\n​ Log Generator ../loadgen -logs -json -count 5 The loadgen will write 5 log lines to ./quotes.log in JSON format.",
    "description": "Exercise Start the Gateway: In the Gateway terminal run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent: In the Agent terminal run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Load Generator: Open the Logs terminal window and run the loadgen.\nImportant To ensure the logs are structured in JSON format, include the -json flag when starting the script.",
    "tags": [],
    "title": "7.2 Setup Environment",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/7-transform-data/7-2-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 7. Count \u0026 Sum Connector",
    "content": "In this section, we’ll explore how the Sum Connector can extract values from spans and convert them into metrics.\nWe’ll specifically use the credit card charges from our base spans and leverage the Sum Connector to retrieve the total charges as a metric.\nThe connector can be used to collect (sum) attribute values from spans, span events, metrics, data points, and log records. It captures each individual value, transforms it into a metric, and passes it along. However, it’s the backend’s job to use these metrics and their attributes for calculations and further processing.\nExercise Switch to your Agent terminal window and open the agent.yaml file in your editor.\nAdd the Sum Connector\nInclude the Sum Connector in the connectors section of your configuration and define the metrics counters: sum: spans: user.card-charge: source_attribute: payment.amount conditions: - attributes[\"payment.amount\"] != \"NULL\" attributes: - key: user.name In the example above, we check for the payment.amount attribute in spans. If it has a valid value, the Sum connector generates a metric called user.card-charge and includes the user.name as an attribute. This enables the backend to track and display a user’s total charges over an extended period, such as a billing cycle.\nIn the pipeline configuration below, the connector exporter is added to the traces section, while the connector receiver is added to the metrics section.\nExercise Configure the Count Connector in the pipelines pipelines: traces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes - redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp - sum # Sum connector which aggregates payment.amount from spans and sends to metrics pipeline metrics: receivers: - sum # Receives metrics from the sum exporter in the traces pipeline - count # Receives count metric from logs count exporter in logs pipeline. - otlp #- hostmetrics # Host Metrics Receiver processors: - memory_limiter - resourcedetection - resource/add_mode - batch exporters: - debug - otlphttp logs: receivers: - otlp - filelog/quotes processors: - memory_limiter - resourcedetection - resource/add_mode - transform/logs # Transform logs processor - batch exporters: - count # Count Connector that exports count as a metric to metrics pipeline. - debug - otlphttp Validate the agent configuration using otelbin.io. For reference, the traces and metrics: sections of your pipelines will look like this: %%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(otlp\u003cbr\u003efa:fa-download\u003cbr\u003e ):::receiver REC3(otlp\u003cbr\u003efa:fa-download\u003cbr\u003e ):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO2(memory_limiter\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO5(batch\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO6(batch\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO7(resourcedetection\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO8(resourcedetection\u003cbr\u003efa:fa-microchip\u003cbr\u003e):::processor PROA(attributes\u003cbr\u003efa:fa-microchip\u003cbr\u003eredact):::processor PROB(redaction\u003cbr\u003efa:fa-microchip\u003cbr\u003eupdate):::processor EXP1(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter EXP2(\u0026emsp;\u0026emsp;file\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter EXP3(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter EXP4(\u0026emsp;\u0026emsp;otlphttp\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter EXP5(\u0026emsp;\u0026emsp;otlphttp\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter ROUTE1(\u0026nbsp;sum\u0026nbsp;\u003cbr\u003efa:fa-route\u003cbr\u003e ):::con-export ROUTE2(\u0026nbsp;count\u0026nbsp;\u003cbr\u003efa:fa-route\u003cbr\u003e ):::con-receive ROUTE3(\u0026nbsp;sum\u0026nbsp;\u003cbr\u003efa:fa-route\u003cbr\u003e ):::con-receive %% Links subID1:::sub-traces subID2:::sub-metrics subgraph \" \" direction LR subgraph subID1[**Traces**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PROA PROA --\u003e PROB PROB --\u003e PRO7 PRO7 --\u003e PRO3 PRO3 --\u003e PRO5 PRO5 --\u003e EXP1 PRO5 --\u003e EXP2 PRO5 --\u003e EXP5 PRO5 --\u003e ROUTE1 end subgraph subID2[**Metrics**] direction LR ROUTE1 --\u003e ROUTE3 ROUTE3 --\u003e PRO2 ROUTE2 --\u003e PRO2 REC3 --\u003e PRO2 PRO2 --\u003e PRO8 PRO8 --\u003e PRO4 PRO4 --\u003e PRO6 PRO6 --\u003e EXP3 PRO6 --\u003e EXP4 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3; classDef sub-metrics stroke:#38bdf8,stroke-width:1px, color:#38bdf8,stroke-dasharray: 3 3;",
    "description": "In this section, we’ll explore how the Sum Connector can extract values from spans and convert them into metrics.\nWe’ll specifically use the credit card charges from our base spans and leverage the Sum Connector to retrieve the total charges as a metric.\nThe connector can be used to collect (sum) attribute values from spans, span events, metrics, data points, and log records. It captures each individual value, transforms it into a metric, and passes it along. However, it’s the backend’s job to use these metrics and their attributes for calculations and further processing.",
    "tags": [],
    "title": "7.2 Create metrics with Sum Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/7-sum-count/7-2-sum/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 8. Routing Data",
    "content": "Exercise Update the traces pipeline to use routing:\nTo enable routing, update the original traces: pipeline by using routing as the only exporter. This ensures all span data is sent through the routing connector for evaluation.\nRemove all processors and replace it with an empty array ([]). These are now defined in the traces/standard and traces/security pipelines.\npipelines: traces: # Original traces pipeline receivers: - otlp # OTLP Receiver processors: [] exporters: - routing # Routing Connector Add both the standard and security traces pipelines:\nConfigure the Security pipeline: This pipeline will handle all spans that match the routing rule for security. This uses routing as its receiver. Place it below the existing traces: pipeline:\ntraces/security: # New Security Traces/Spans Pipeline receivers: - routing # Receive data from the routing connector processors: - memory_limiter # Memory Limiter Processor - resource/add_mode # Adds collector mode metadata - batch exporters: - debug # Debug Exporter - file/traces/security # File Exporter for spans matching rule Add the Standard pipeline: This pipeline processes all spans that do not match the routing rule. This pipeline is also using routing as its receiver. Add this below the traces/security one:\ntraces/standard: # Default pipeline for unmatched spans receivers: - routing # Receive data from the routing connector processors: - memory_limiter # Memory Limiter Processor - resource/add_mode # Adds collector mode metadata - batch exporters: - debug # Debug exporter - file/traces/standard # File exporter for unmatched spans Validate the agent configuration using otelbin.io. For reference, the traces: section of your pipelines will look similar to this:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO5(batch\u003cbr\u003efa:fa-microchip):::processor PRO6(batch\u003cbr\u003efa:fa-microchip):::processor EXP1(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026emsp;\u0026emsp;file\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003etraces):::exporter EXP3(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP4(\u0026emsp;\u0026emsp;file\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003etraces):::exporter ROUTE1(\u0026nbsp;routing\u0026nbsp;\u003cbr\u003efa:fa-route):::con-export ROUTE2(\u0026nbsp;routing\u0026nbsp;\u003cbr\u003efa:fa-route):::con-receive ROUTE3(\u0026nbsp;routing\u0026nbsp;\u003cbr\u003efa:fa-route):::con-receive %% Links subID1:::sub-traces subID2:::sub-traces subID3:::sub-traces subgraph \" \" direction LR subgraph subID1[**Traces**] REC1 --\u003e ROUTE1 end subgraph subID2[**Traces/standard**] ROUTE1 --\u003e ROUTE2 ROUTE2 --\u003e PRO1 PRO1 --\u003e PRO3 PRO3 --\u003e PRO5 PRO5 --\u003e EXP1 PRO5 --\u003e EXP2 end subgraph subID3[**Traces/security**] ROUTE1 --\u003e ROUTE3 ROUTE3 --\u003e PRO2 PRO2 --\u003e PRO4 PRO4 --\u003e PRO6 PRO6 --\u003e EXP3 PRO6 --\u003e EXP4 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3;",
    "description": "Exercise Update the traces pipeline to use routing:\nTo enable routing, update the original traces: pipeline by using routing as the only exporter. This ensures all span data is sent through the routing connector for evaluation.\nRemove all processors and replace it with an empty array ([]). These are now defined in the traces/standard and traces/security pipelines.\npipelines: traces: # Original traces pipeline receivers: - otlp # OTLP Receiver processors: [] exporters: - routing # Routing Connector Add both the standard and security traces pipelines:",
    "tags": [],
    "title": "8.2 Configuring the Pipelines",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/8-routing-data/8-2-pipelines/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 9. Count \u0026 Sum Connector",
    "content": "In this section, we’ll explore how the Sum Connector can extract values from spans and convert them into metrics.\nWe’ll specifically use the credit card charges from our base spans and leverage the Sum Connector to retrieve the total charges as a metric.\nThe connector can be used to collect (sum) attribute values from spans, span events, metrics, data points, and log records. It captures each individual value, transforms it into a metric, and passes it along. However, it’s the backend’s job to use these metrics and their attributes for calculations and further processing.\nExercise Switch to your Agent terminal window and open the agent.yaml file in your editor.\nAdd the Sum Connector\nInclude the Sum Connector in the connectors section of your configuration and define the metrics counters: sum: spans: user.card-charge: source_attribute: payment.amount conditions: - attributes[\"payment.amount\"] != \"NULL\" attributes: - key: user.name In the example above, we check for the payment.amount attribute in spans. If it has a valid value, the Sum connector generates a metric called user.card-charge and includes the user.name as an attribute. This enables the backend to track and display a user’s total charges over an extended period, such as a billing cycle.\nIn the pipeline configuration below, the connector exporter is added to the traces section, while the connector receiver is added to the metrics section.\nExercise Configure the Count Connector in the pipelines pipelines: traces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes - redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp - sum # Sum connector which aggregates payment.amount from spans and sends to metrics pipeline metrics: receivers: - sum # Receives metrics from the sum exporter in the traces pipeline - count # Receives count metric from logs count exporter in logs pipeline. - otlp #- hostmetrics # Host Metrics Receiver processors: - memory_limiter - resourcedetection - resource/add_mode - batch exporters: - debug - otlphttp logs: receivers: - otlp - filelog/quotes processors: - memory_limiter - resourcedetection - resource/add_mode - transform/logs # Transform logs processor - batch exporters: - count # Count Connector that exports count as a metric to metrics pipeline. - debug - otlphttp Validate the agent configuration using otelbin.io. For reference, the traces and metrics: sections of your pipelines will look like this: %%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(otlp\u003cbr\u003efa:fa-download\u003cbr\u003e ):::receiver REC3(otlp\u003cbr\u003efa:fa-download\u003cbr\u003e ):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO2(memory_limiter\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO5(batch\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO6(batch\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO7(resourcedetection\u003cbr\u003efa:fa-microchip\u003cbr\u003e ):::processor PRO8(resourcedetection\u003cbr\u003efa:fa-microchip\u003cbr\u003e):::processor PROA(attributes\u003cbr\u003efa:fa-microchip\u003cbr\u003eredact):::processor PROB(redaction\u003cbr\u003efa:fa-microchip\u003cbr\u003eupdate):::processor EXP1(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter EXP2(\u0026emsp;\u0026emsp;file\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter EXP3(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter EXP4(\u0026emsp;\u0026emsp;otlphttp\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter EXP5(\u0026emsp;\u0026emsp;otlphttp\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload\u003cbr\u003e ):::exporter ROUTE1(\u0026nbsp;sum\u0026nbsp;\u003cbr\u003efa:fa-route\u003cbr\u003e ):::con-export ROUTE2(\u0026nbsp;count\u0026nbsp;\u003cbr\u003efa:fa-route\u003cbr\u003e ):::con-receive ROUTE3(\u0026nbsp;sum\u0026nbsp;\u003cbr\u003efa:fa-route\u003cbr\u003e ):::con-receive %% Links subID1:::sub-traces subID2:::sub-metrics subgraph \" \" direction LR subgraph subID1[**Traces**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PROA PROA --\u003e PROB PROB --\u003e PRO7 PRO7 --\u003e PRO3 PRO3 --\u003e PRO5 PRO5 --\u003e EXP1 PRO5 --\u003e EXP2 PRO5 --\u003e EXP5 PRO5 --\u003e ROUTE1 end subgraph subID2[**Metrics**] direction LR ROUTE1 --\u003e ROUTE3 ROUTE3 --\u003e PRO2 ROUTE2 --\u003e PRO2 REC3 --\u003e PRO2 PRO2 --\u003e PRO8 PRO8 --\u003e PRO4 PRO4 --\u003e PRO6 PRO6 --\u003e EXP3 PRO6 --\u003e EXP4 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3; classDef sub-traces stroke:#fbbf24,stroke-width:1px, color:#fbbf24,stroke-dasharray: 3 3; classDef sub-metrics stroke:#38bdf8,stroke-width:1px, color:#38bdf8,stroke-dasharray: 3 3;",
    "description": "In this section, we’ll explore how the Sum Connector can extract values from spans and convert them into metrics.\nWe’ll specifically use the credit card charges from our base spans and leverage the Sum Connector to retrieve the total charges as a metric.\nThe connector can be used to collect (sum) attribute values from spans, span events, metrics, data points, and log records. It captures each individual value, transforms it into a metric, and passes it along. However, it’s the backend’s job to use these metrics and their attributes for calculations and further processing.",
    "tags": [],
    "title": "Create metrics with Sum Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/9-sum-count/9-2-sum/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops",
    "content": "The goal of this workshop is to help you gain confidence in creating and modifying OpenTelemetry Collector configuration files. You’ll start with a minimal agent.yaml file and progressively build it out to handle several advanced, real-world scenarios.\nA key focus of this workshop is learning how to configure the OpenTelemetry Collector to store telemetry data locally, rather than sending it to a third-party vendor backend. This approach not only simplifies debugging and troubleshooting but is also ideal for testing and development environments where you want to avoid sending data to production systems.\nTo make the most of this workshop, you should have:\nA basic understanding of the OpenTelemetry Collector and its configuration file structure. Proficiency in editing YAML files. Everything in this workshop is designed to run locally, ensuring a hands-on and accessible learning experience. Let’s dive in and start building!\nWorkshop Overview During this workshop, we will cover the following topics:\nSetting up the agent locally: Add metadata, and introduce the debug and file exporters. Configuring a gateway: Route traffic from the agent to the gateway. Configuring the FileLog receiver: Collect log data from various log files. Enhancing agent resilience: Basic configurations for fault tolerance. Configuring processors: Filter out noise by dropping specific spans (e.g., health checks). Remove unnecessary tags, and handle sensitive data. Transform data using OTTL (OpenTelemetry Transformation Language) in the pipeline before exporting. Configuring Connectors: Route data to different endpoints based on the values received. Convert log and span data to metrics. By the end of this workshop, you’ll be familiar with configuring the OpenTelemetry Collector for a variety of real-world use cases.",
    "description": "Practice setting up the OpenTelemetry Collector configuration from scratch and go though several advanced configuration scenarios's.",
    "tags": [],
    "title": "Advanced Collector Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops",
    "content": "The goal of this workshop is to help you gain confidence in creating and modifying OpenTelemetry Collector configuration files. You’ll start with a minimal agent.yaml and gateway.yaml file and progressively build them out to handle several advanced, real-world scenarios.\nA key focus of this workshop is learning how to configure the OpenTelemetry Collector to store telemetry data locally, rather than sending it to a third-party vendor backend. This approach not only simplifies debugging and troubleshooting but is also ideal for testing and development environments where you want to avoid sending data to production systems.\nTo make the most of this workshop, you should have:\nA basic understanding of the OpenTelemetry Collector and its configuration file structure. Proficiency in editing YAML files. Everything in this workshop is designed to run locally, ensuring a hands-on and accessible learning experience. Let’s dive in and start building!\nWorkshop Overview During this workshop, we will cover the following topics:\nSetting up the agent and gateway locally: Test that metrics, traces, and logs go via the agent to the gateway. Enhancing agent resilience: Basic configurations for fault tolerance. Configuring processors: Filter out noise by dropping specific spans (e.g., health checks). Remove unnecessary tags, and handle sensitive data. Transform data using OTTL (OpenTelemetry Transformation Language) in the pipeline before exporting. Configuring Connectors: Route data to different endpoints based on the values received. By the end of this workshop, you’ll be familiar with configuring the OpenTelemetry Collector for a variety of real-world use cases.",
    "description": "Practice setting up the OpenTelemetry Collector configuration from scratch and go though several advanced configuration scenarios's.",
    "tags": [],
    "title": "Advanced OpenTelemetry Collector",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops",
    "content": "The goal of this workshop is to help you gain confidence in creating and modifying OpenTelemetry Collector configuration files. You’ll start with a minimal agent.yaml and gateway.yaml file and progressively build them out to handle several advanced, real-world scenarios.\nA key focus of this workshop is learning how to configure the OpenTelemetry Collector to store telemetry data locally, rather than sending it to a third-party vendor backend. This approach not only simplifies debugging and troubleshooting but is also ideal for testing and development environments where you want to avoid sending data to production systems.\nTo make the most of this workshop, you should have:\nA basic understanding of the OpenTelemetry Collector and its configuration file structure. Proficiency in editing YAML files. Everything in this workshop is designed to run locally, ensuring a hands-on and accessible learning experience. Let’s dive in and start building!\nWorkshop Overview During this workshop, we will cover the following topics:\nSetting up the agent and gateway locally: Test that metrics, traces, and logs go via the agent to the gateway. Enhancing agent resilience: Basic configurations for fault tolerance. Configuring processors: Filter out noise by dropping specific spans (e.g., health checks). Remove unnecessary tags, and handle sensitive data. Transform data using OTTL (OpenTelemetry Transformation Language) in the pipeline before exporting. Configuring Connectors: Route data to different endpoints based on the values received. By the end of this workshop, you’ll be familiar with configuring the OpenTelemetry Collector for a variety of real-world use cases.",
    "description": "Practice setting up the OpenTelemetry Collector configuration from scratch and go though several advanced configuration scenarios's.",
    "tags": [],
    "title": "Advanced OpenTelemetry Collector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources",
    "content": "A collection of the common questions and their answers associated with Observability, DevOps, Incident Response and Splunk On-Call.\nQ: Alerts v. Incident Response v. Incident Management A: Alerts, Incident Response and Incident Management are related functions. Together they comprise the incident response and resolution process.\nMonitoring and Observability tools send alerts to incident response platforms. Those platforms take a collection of alerts and correlate them into incidents.\nThose incidents are recorded into incident management (ITSM) platforms for record. Alerts are the trigger that something has happened, and provide context to an incident.\nIncidents consist of the alert payload, all activity associated with the incident from the time it was created, and the on-call policies to be followed. ITSM is the system of record for incidents that are active and after they have been resolved.\nAll these components are necessary for successful incident response and management practices.\nOn-Call Q: Is Observability Monitoring A: The key difference between Monitoring and Observability is the difference between “known knowns” and “unknown knowns” respectively.\nIn monitoring the operator generally has prior knowledge of the architecture and elements in their system. They can reliably predict the relationship between elements, and their associated metadata. Monitoring is good for stateful infrastructure that is not frequently changed.\nObservability is for systems where the operators ability to predict and trace all elements in the system and their relationships is limited.\nObservability is a set of practices and technology, which include traditional monitoring metrics.\nThese practices and technologies combined give the operator the ability to understand ephemeral and highly complex environments without prior knowledge of all elements of a system. Observability technology can also account for fluctuations in the environment, and variation in metadata (cardinality) better than traditional monitoring which is more static.\nObservability Q: What are Traces and Spans A: Traces and spans, combined with metrics and logs, make up the core types of data that feed modern Observability tools. They all have specific elements and functions, but work well together.\nBecause microservices based architectures are distributed, transactions in the system touch multiple services before completing. This makes accurately pinpointing the location of an issue difficult. Traces are a method for tracking the full path of a request through all the services in a distributed system. Spans are the timed operations in each service. Traces are the connective tissue for the spans and together they give more detail on individual service processes. While metrics give a good snapshot of the health of a system, and logs give depth when investigating issues, traces and spans help navigate operators to the source of issues with greater context. This saves time when investigating incidents, and supports the increasing complexity of modern architectures.\nAPM Q: What is the Sidecar Pattern? A: The sidecar pattern is a design pattern for having related services connected directly by infrastructure. Related services can be adding functionality or supporting the application logic they are connected to. It is used heavily as a method for deploying agents associated with the management plan along with the application service they support.\nIn Observability the sidecar services are the application logic, and the agent collecting data from that service. The setup requires two containers one with the application service, and one running the agent. The containers share a pod, and resources such as disk, network, and namespace. They are also deployed together and share the same lifecycle.\nObservability",
    "description": "A collection of the common questions and their answers associated with Observability, DevOps, Incident Response and Splunk On-Call.",
    "tags": [],
    "title": "Frequently Asked Questions",
    "uri": "/observability-workshop/v6.5/en/resources/faq/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops",
    "content": "In this workshop, we’ll demonstrate how Splunk Observability Cloud delivers value to our financial services customers due to its ability to provide real-time, full-fidelity, AI-powered monitoring across your entire digital ecosystem from infrastructure to applications to user experiences. It’s purpose-built for modern, cloud-native, microservices-based environments. You’ll have the opportunity to explore some of the platform’s most powerful features, which set it apart from other observability solutions:\nInfrastructure Monitoring Complete end-to-end trace visibility with NoSample Full-fidelity Application Performance Monitoring (APM) No-code log querying Root cause analysis with tag analytics and error stacks Related Content for seamless navigation between components One of the core strengths of Splunk Observability Cloud is its ability to unify telemetry data, creating a comprehensive picture of both the end-user experience and your entire application stack.\nThe workshop will focus on a microservices-based Wire transfer application deployed on Kubernetes. Users can initiate a wire transfer and all of the appropriate checks for user, balance, and compliance will be handled. This application is fully instrumented with OpenTelemetry to capture detailed performance data.\nWhat is OpenTelemetry?\nOpenTelemetry is an open-source collection of tools, APIs, and software development kits (SDKs) designed to help you instrument, generate, collect, and export telemetry data—such as metrics, traces, and logs. This data enables in-depth analysis of your software’s performance and behavior.\nThe OpenTelemetry community is growing rapidly, supported by leading companies like Splunk, Google, Microsoft, and Amazon. It currently has the second-largest number of contributors within the Cloud Native Computing Foundation, following only Kubernetes.",
    "description": "This workshop, tailored for the Financial Services sector, will demonstrate how Splunk Observability Cloud delivers real-time insights into user experience, spanning from front-end applications to back-end services. You'll explore key product features and unique advantages that set Splunk Observability Cloud apart.",
    "tags": [],
    "title": "Financial Services Observability Cloud",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "This workshop will equip you with a basic understanding of monitoring Kubernetes using the Splunk OpenTelemetry Collector. During the workshop, you will deploy PHP/Apache and a load generator.\nYou will learn about OpenTelemetry Receivers, Kubernetes Namespaces, ReplicaSets, Kubernetes Horizontal Pod AutoScaling and how to monitor all this using the Splunk Observability Cloud. The main learnings from the workshop will be a better understanding of the Kubernetes Navigator (and Dashboards) in Splunk Observability Cloud as well as seeing Kubernetes metrics, events and Detectors.\nFor this workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2 all pre-configured for you.\nTo get access to the instance that you will be using in the workshop, please visit the URL provided by the workshop leader.",
    "description": "This workshop will equip you with the basic understanding of monitoring Kubernetes using the Splunk OpenTelemetry Collector",
    "tags": [],
    "title": "Monitoring Horizontal Pod Autoscaling in Kubernetes",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/2-hpa/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources \u003e Local Hosting",
    "content": "Install Orbstack and jq:\nbrew install orbstack jq Clone workshop repository:\ngit clone https://github.com/splunk/observability-workshop Change into Orbstack directory:\ncd observability-workshop/local-hosting/orbstack Run the script and provide and instance name and SWiPE ID e.g.:\n./start.sh my-instance 12345678 Once the instance has been successfully created (this can take several minutes), you will automatically be logged into the instance. If you exit you can SSH back in using the following command (replace \u003cmy_instance\u003e with the name of your instance):\nssh splunk@\u003cmy_instance\u003e@orb Once in the shell, you can validate that the instance is ready by running the following command:\nkubectl version --output=yaml To get the IP address of the instance, run the following command:\nifconfig eth0 To delete the instance, run the following command:\norb delete my-instance",
    "description": "Learn how to create a local hosting environment with OrbStack - Mac (Apple Silicon)",
    "tags": [],
    "title": "Local Hosting with OrbStack",
    "uri": "/observability-workshop/v6.5/en/resources/local-hosting/orbstack/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 6.2 Optional Exercise",
    "content": "This is Part 2, of the Infrastructure Monitoring exercise, you should now have a single cluster visible.\nIn the Kubernetes Navigator, the cluster is represented by the square with the black line around it. It will contain one or more blue squares representing the node(s) or compute engines. Each of them containing one or more colored boxes that represent Pods. (this is where your services run in). And as you can guess, green means healthy and red means that there is a problem. Given there are two red boxes or tiles, let’s see what is going on and if this will affect our Online Boutique site.\nExercise First, set the time window we are working with to the last 15 minutes. You do this by changing the the Time picker in the filter pane from -4h to Last 15 minutes. Hover with your mouse over the Cluster, Node and pods, both green and red ones. The resulting information pane that appears will tell you the state of the object. Note, That the red Pods show that they are in Pod Phase: Failed. This means they have crashed and are not working. Examine the Cluster Metric charts that provide information on your cluster. (The charts below the cluster image). They provide general information about the health of your cluster like Memory consumption and the number of pods per node. Nothing flags for the red pods, as crashed pods do not affect the performance of Kubernetes. Let’s check if the Spunk Kubernetes Analyzer can tell us something more useful, so click on K8s Analyzer. Spunk Kubernetes Analyzer The Splunk Kubernetes Analyzer is a smart process that runs in the background in Splunk Observability Cloud and is designed to detect relations between anomalies.\nThe K8s Analyzer should have detected that the two red pods are similar, indicated by the 2 after each line, and running in the same Namespace. In the K8s analyzer view can you find what namespace? (hint, look for k8s.namespace.name). Next, we want to check this on the node level as well, so drill down to the node, first by hovering your mouse over the cluster until you see a blue line appear around the node with a in the left top, inside the black Cluster Line. Click on the triangle . Your view should now show little boxes in each pod, these represent the containers that run the actual code. The K8s Analyzer should confirm that this issue is also occurring on the node level. Click on K8s node. This will show the node metrics, and if you examine the charts, you can see that there are only two pods in the development namespace. It is easier to see if you filter on the k8s.namespace.name=development in the Filter Pane. The # Total Pods chart shows only two pods and in the Node Workload chart there is only the test-job and it has failed. Spunk Kubernetes Analyzer The above scenario is common in a shared Kubernetes environment, where teams deploy applications in different stages. Kubernetes is designed to keep these environments completely separate.\nNone of the Pods that make up our Online Boutique site run in the development namespace and all the other pods are green, we can safely assume these pods do not affect us, so let’s move on to look at a few more things.",
    "description": "This is Part 2, of the Infrastructure Monitoring exercise, you should now have a single cluster visible.\nIn the Kubernetes Navigator, the cluster is represented by the square with the black line around it. It will contain one or more blue squares representing the node(s) or compute engines. Each of them containing one or more colored boxes that represent Pods. (this is where your services run in). And as you can guess, green means healthy and red means that there is a problem. Given there are two red boxes or tiles, let’s see what is going on and if this will affect our Online Boutique site.",
    "tags": [],
    "title": "Infrastructure Exercise - Part 2",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/30-im-exercise/2-im-exercise/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6.2 Optional Exercise",
    "content": "This is Part 2, of the Infrastructure Monitoring exercise, you should now have a single cluster visible.\nIn the Kubernetes Navigator, the cluster is represented by the square with the black line around it. It will contain one or more blue squares representing the node(s) or compute engines. Each of them containing one or more colored boxes that represent Pods. (this is where your services run in). And as you can guess, green means healthy and red means that there is a problem. Given there are two red boxes or tiles, let’s see what is going on and if this will affect our Online Boutique site.\nExercise First, set the time window we are working with to the last 15 minutes. You do this by changing the the Time picker in the filter pane from -4h to Last 15 minutes. Hover with your mouse over the Cluster, Node and pods, both green and red ones. The resulting information pane that appears will tell you the state of the object. Note, That the red Pods show that they are in Pod Phase: Failed. This means they have crashed and are not working. Examine the Cluster Metric charts that provide information on your cluster. (The charts below the cluster image). They provide general information about the health of your cluster like Memory consumption and the number of pods per node. Nothing flags for the red pods, as crashed pods do not affect the performance of Kubernetes. Let’s check if the Spunk Kubernetes Analyzer can tell us something more useful, so click on K8s Analyzer. Spunk Kubernetes Analyzer The Splunk Kubernetes Analyzer is a smart process that runs in the background in Splunk Observability Cloud and is designed to detect relations between anomalies.\nThe K8s Analyzer should have detected that the two red pods are similar, indicated by the 2 after each line, and running in the same Namespace. In the K8s analyzer view can you find what namespace? (hint, look for k8s.namespace.name). Next, we want to check this on the node level as well, so drill down to the node, first by hovering your mouse over the cluster until you see a blue line appear around the node with a in the left top, inside the black Cluster Line. Click on the triangle . Your view should now show little boxes in each pod, these represent the containers that run the actual code. The K8s Analyzer should confirm that this issue is also occurring on the node level. Click on K8s node. This will show the node metrics, and if you examine the charts, you can see that there are only two pods in the development namespace. It is easier to see if you filter on the k8s.namespace.name=development in the Filter Pane. The # Total Pods chart shows only two pods and in the Node Workload chart there is only the test-job and it has failed. Spunk Kubernetes Analyzer The above scenario is common in a shared Kubernetes environment, where teams deploy applications in different stages. Kubernetes is designed to keep these environments completely separate.\nNone of the Pods that make up our Online Boutique site run in the development namespace and all the other pods are green, we can safely assume these pods do not affect us, so let’s move on to look at a few more things.",
    "description": "This is Part 2, of the Infrastructure Monitoring exercise, you should now have a single cluster visible.\nIn the Kubernetes Navigator, the cluster is represented by the square with the black line around it. It will contain one or more blue squares representing the node(s) or compute engines. Each of them containing one or more colored boxes that represent Pods. (this is where your services run in). And as you can guess, green means healthy and red means that there is a problem. Given there are two red boxes or tiles, let’s see what is going on and if this will affect our Online Boutique site.",
    "tags": [],
    "title": "Infrastructure Exercise - Part 2",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/30-im-exercise/2-im-exercise/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops",
    "content": "The goal of this workshop is to introduce the features of Splunk’s automatic discovery and configuration for Java.\nThe workshop scenario will be created by installing a simple (un-instrumented) Java microservices application in Kubernetes.\nFollowing simple steps to install the Splunk OpenTelemetry Collector with automatic discovery for existing Java based deployments, we will see how easy it is to send metrics, traces and logs to Splunk Observability Cloud.\nPrerequisites Outbound SSH access to port 2222. Outbound HTTP access to port 81. Familiarity with the Linux command line. During this workshop we will cover the following components:\nSplunk Infrastructure Monitoring (IM) Splunk automatic discovery and configuration for Java (APM) Database Query Performance AlwaysOn Profiling Splunk Log Observer (LO) Splunk Real User Monitoring (RUM) Splunk Synthetics is feeling a little left out here, but we cover that in other workshops",
    "description": "Learn how to enable automatic discovery and configuration for your Java-based application running in Kubernetes. Experience real-time monitoring to help you maximize application behavior with end-to-end visibility.",
    "tags": [],
    "title": "Spring PetClinic SpringBoot Based Microservices On Kubernetes",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "Prerequisites Proficiency in editing YAML files using vi, vim, nano, or your preferred text editor. Supported Environments: A provided Splunk Workshop Instance (preferred). Outbound access to port 2222 is required for ssh access. Apple Mac (Apple Silicon). Installation of jq is required - https://jqlang.org/download/ Exercise Create a directory: In your environment create a new directory and change into it:\nmkdir advanced-otel-workshop \u0026\u0026 \\ cd advanced-otel-workshop We will refer to this directory as [WORKSHOP] for the remainder of the workshop.\nRemove any existing OpenTelemetry Collectors If you have completed the Splunk IM workshop, please ensure you have deleted the collector running in Kubernetes before continuing. This can be done by running the following command:\nhelm delete splunk-otel-collector The EC2 instance in that case may also run some services that can interfere with this workshop , so run the following command to make sure they are stopped if present:\nkubectl delete ~/workshop/apm/deployment.yaml Download workshop binaries: Change into your [WORKSHOP] directory and download the OpenTelemetry Collector, Load Generator binaries and setup script:\n​ Splunk Workshop Instance Apple Silicon curl -L https://github.com/signalfx/splunk-otel-collector/releases/download/v0.132.0/otelcol_linux_amd64 -o otelcol \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/loadgen/build/loadgen-linux-amd64 -o loadgen \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/setup-workshop.sh -o setup-workshop.sh \u0026\u0026 \\ chmod +x setup-workshop.sh curl -L https://github.com/signalfx/splunk-otel-collector/releases/download/v0.132.0/otelcol_darwin_arm64 -o otelcol \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/loadgen/build/loadgen-darwin-arm64 -o loadgen \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/setup-workshop.sh -o setup-workshop.sh \u0026\u0026 \\ chmod +x setup-workshop.sh Run the setup-workshop.sh script which will configure the correct permissions and also create the initial configurations for the Agent and the Gateway:\n​ Setup Workshop Verify Setup ./setup-workshop.sh ███████╗██████╗ ██╗ ██╗ ██╗███╗ ██╗██╗ ██╗ ██╗ ██╔════╝██╔══██╗██║ ██║ ██║████╗ ██║██║ ██╔╝ ╚██╗ ███████╗██████╔╝██║ ██║ ██║██╔██╗ ██║█████╔╝ ╚██╗ ╚════██║██╔═══╝ ██║ ██║ ██║██║╚██╗██║██╔═██╗ ██╔╝ ███████║██║ ███████╗╚██████╔╝██║ ╚████║██║ ██╗ ██╔╝ ╚══════╝╚═╝ ╚══════╝ ╚═════╝ ╚═╝ ╚═══╝╚═╝ ╚═╝ ╚═╝ Welcome to the Splunk Advanced OpenTelemetry Workshop! ====================================================== macOS detected. Removing quarantine attributes... otelcol version v0.126.0 Usage: loadgen [OPTIONS] Options: -base Send base traces (enabled by default) -health Send health traces -security Send security traces -logs Enable logging of random quotes to quotes.log -json Output logs in JSON format (only applicable with -logs) -count Number of traces or logs to send (default: infinite) -h, --help Display this help message Example: loadgen -health -security -count 10 Send 10 health and security traces loadgen -logs -json -count 5 Write 5 random quotes in JSON format to quotes.log Creating workshop directories... ✓ Created subdirectories: ├── 1-agent-gateway ├── 2-building-resilience ├── 3-dropping-spans ├── 4-sensitive-data ├── 5-transform-data └── 6-routing-data Creating configuration files for 1-agent-gateway... Creating OpenTelemetry Collector agent configuration file: 1-agent-gateway/agent.yaml ✓ Configuration file created successfully: 1-agent-gateway/agent.yaml ✓ File size: 4355 bytes Creating OpenTelemetry Collector gateway configuration file: 1-agent-gateway/gateway.yaml ✓ Configuration file created successfully: 1-agent-gateway/gateway.yaml ✓ File size: 3376 bytes ✓ Completed configuration files for 1-agent-gateway Creating configuration files for 2-building-resilience... Creating OpenTelemetry Collector agent configuration file: 2-building-resilience/agent.yaml ✓ Configuration file created successfully: 2-building-resilience/agent.yaml ✓ File size: 4355 bytes Creating OpenTelemetry Collector gateway configuration file: 2-building-resilience/gateway.yaml ✓ Configuration file created successfully: 2-building-resilience/gateway.yaml ✓ File size: 3376 bytes ✓ Completed configuration files for 2-building-resilience Workshop environment setup complete! Configuration files created in the following directories: 1-agent-gateway/ ├── agent.yaml └── gateway.yaml 2-building-resilience/ ├── agent.yaml └── gateway.yaml ​ Initial Directory Structure [WORKSHOP] ├── 1-agent-gateway ├── 2-building-resilience ├── 3-dropping-spans ├── 4-sensitive-data ├── 5-transform-data ├── 6-routing-data ├── loadgen ├── otelcol └── setup-workshop.sh",
    "description": "Prerequisites Proficiency in editing YAML files using vi, vim, nano, or your preferred text editor. Supported Environments: A provided Splunk Workshop Instance (preferred). Outbound access to port 2222 is required for ssh access. Apple Mac (Apple Silicon). Installation of jq is required - https://jqlang.org/download/ Exercise Create a directory: In your environment create a new directory and change into it:\nmkdir advanced-otel-workshop \u0026\u0026 \\ cd advanced-otel-workshop We will refer to this directory as [WORKSHOP] for the remainder of the workshop.",
    "tags": [],
    "title": "Pre-requisites",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/prerequisites/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "Prerequisites Proficiency in editing YAML files using vi, vim, nano, or your preferred text editor. Supported Environments: Splunk Workshop Instance (preferred) Apple Mac (Apple Silicon). Installation of jq is required - https://jqlang.org/download/ Exercise Create a workshop directory: In your environment create a new directory (e.g., advanced-otel-workshop). We will refer to this directory as [WORKSHOP] for the remainder of the workshop.\nDownload workshop binaries: Change into your [WORKSHOP] directory and download the OpenTelemetry Collector and Load Generator binaries:\n​ Splunk Workshop Instance Apple Silicon curl -L https://github.com/signalfx/splunk-otel-collector/releases/download/v0.132.0/otelcol_linux_amd64 -o otelcol \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/loadgen/build/loadgen-linux-amd64 -o loadgen Update file permissions: Once downloaded, update the file permissions to make both executable:\nchmod +x otelcol loadgen \u0026\u0026 \\ ./otelcol -v \u0026\u0026 \\ ./loadgen --help curl -L https://github.com/signalfx/splunk-otel-collector/releases/download/v0.132.0/otelcol_darwin_arm64 -o otelcol \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/loadgen/build/loadgen-darwin-arm64 -o loadgen macOS Users Before running the binaries on macOS, you need to remove the quarantine attribute that macOS applies to downloaded files. This step ensures they can execute without restrictions.\nRun the following command in your terminal:\n​ Remove Quarantine Attribute xattr -dr com.apple.quarantine otelcol \u0026\u0026 \\ xattr -dr com.apple.quarantine loadgen Update file permissions: Once downloaded, update the file permissions to make both executable:\nchmod +x otelcol loadgen \u0026\u0026 \\ ./otelcol -v \u0026\u0026 \\ ./loadgen --help ​ Initial Directory Structure [WORKSHOP] ├── otelcol # OpenTelemetry Collector binary └── loadgen # Load Generator binary",
    "description": "Prerequisites Proficiency in editing YAML files using vi, vim, nano, or your preferred text editor. Supported Environments: Splunk Workshop Instance (preferred) Apple Mac (Apple Silicon). Installation of jq is required - https://jqlang.org/download/ Exercise Create a workshop directory: In your environment create a new directory (e.g., advanced-otel-workshop). We will refer to this directory as [WORKSHOP] for the remainder of the workshop.\nDownload workshop binaries: Change into your [WORKSHOP] directory and download the OpenTelemetry Collector and Load Generator binaries:",
    "tags": [],
    "title": "Pre-requisites",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/prerequisites/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "Prerequisites Proficiency in editing YAML files using vi, vim, nano, or your preferred text editor. Supported Environments: A provided Splunk Workshop Instance (preferred). Outbound access to port 2222 is required for ssh access. Apple Mac (Apple Silicon). Installation of jq is required - https://jqlang.org/download/ Exercise Create a directory: In your environment create a new directory and change into it:\nmkdir advanced-otel-workshop \u0026\u0026 \\ cd advanced-otel-workshop We will refer to this directory as [WORKSHOP] for the remainder of the workshop.\nRemove any existing OpenTelemetry Collectors If you have completed the Splunk IM workshop, please ensure you have deleted the collector running in Kubernetes before continuing. This can be done by running the following command:\nhelm delete splunk-otel-collector The EC2 instance in that case may also run some services that can interfere with this workshop , so run the following command to make sure they are stopped if present:\nkubectl delete ~/workshop/apm/deployment.yaml Download workshop binaries: Change into your [WORKSHOP] directory and download the OpenTelemetry Collector, Load Generator binaries and setup script:\n​ Splunk Workshop Instance Apple Silicon curl -L https://github.com/signalfx/splunk-otel-collector/releases/download/v0.132.0/otelcol_linux_amd64 -o otelcol \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/loadgen/build/loadgen-linux-amd64 -o loadgen \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/setup-workshop.sh -o setup-workshop.sh \u0026\u0026 \\ chmod +x setup-workshop.sh curl -L https://github.com/signalfx/splunk-otel-collector/releases/download/v0.132.0/otelcol_darwin_arm64 -o otelcol \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/loadgen/build/loadgen-darwin-arm64 -o loadgen \u0026\u0026 \\ curl -L https://github.com/splunk/observability-workshop/raw/refs/heads/main/workshop/ninja/advanced-otel/setup-workshop.sh -o setup-workshop.sh \u0026\u0026 \\ chmod +x setup-workshop.sh Run the setup-workshop.sh script which will configure the correct permissions and also create the initial configurations for the Agent and the Gateway:\n​ Setup Workshop Verify Setup ./setup-workshop.sh ███████╗██████╗ ██╗ ██╗ ██╗███╗ ██╗██╗ ██╗ ██╗ ██╔════╝██╔══██╗██║ ██║ ██║████╗ ██║██║ ██╔╝ ╚██╗ ███████╗██████╔╝██║ ██║ ██║██╔██╗ ██║█████╔╝ ╚██╗ ╚════██║██╔═══╝ ██║ ██║ ██║██║╚██╗██║██╔═██╗ ██╔╝ ███████║██║ ███████╗╚██████╔╝██║ ╚████║██║ ██╗ ██╔╝ ╚══════╝╚═╝ ╚══════╝ ╚═════╝ ╚═╝ ╚═══╝╚═╝ ╚═╝ ╚═╝ Welcome to the Splunk Advanced OpenTelemetry Workshop! ====================================================== macOS detected. Removing quarantine attributes... otelcol version v0.126.0 Usage: loadgen [OPTIONS] Options: -base Send base traces (enabled by default) -health Send health traces -security Send security traces -logs Enable logging of random quotes to quotes.log -json Output logs in JSON format (only applicable with -logs) -count Number of traces or logs to send (default: infinite) -h, --help Display this help message Example: loadgen -health -security -count 10 Send 10 health and security traces loadgen -logs -json -count 5 Write 5 random quotes in JSON format to quotes.log Creating workshop directories... ✓ Created subdirectories: ├── 1-agent-gateway ├── 2-building-resilience ├── 3-dropping-spans ├── 4-sensitive-data ├── 5-transform-data ├── 6-routing-data └── 7-sum-count Creating configuration files for 1-agent-gateway... Creating OpenTelemetry Collector agent configuration file: 1-agent-gateway/agent.yaml ✓ Configuration file created successfully: 1-agent-gateway/agent.yaml ✓ File size: 4355 bytes Creating OpenTelemetry Collector gateway configuration file: 1-agent-gateway/gateway.yaml ✓ Configuration file created successfully: 1-agent-gateway/gateway.yaml ✓ File size: 3376 bytes ✓ Completed configuration files for 1-agent-gateway Creating configuration files for 2-building-resilience... Creating OpenTelemetry Collector agent configuration file: 2-building-resilience/agent.yaml ✓ Configuration file created successfully: 2-building-resilience/agent.yaml ✓ File size: 4355 bytes Creating OpenTelemetry Collector gateway configuration file: 2-building-resilience/gateway.yaml ✓ Configuration file created successfully: 2-building-resilience/gateway.yaml ✓ File size: 3376 bytes ✓ Completed configuration files for 2-building-resilience Workshop environment setup complete! Configuration files created in the following directories: 1-agent-gateway/ ├── agent.yaml └── gateway.yaml 2-building-resilience/ ├── agent.yaml └── gateway.yaml ​ Initial Directory Structure [WORKSHOP] ├── 1-agent-gateway ├── 2-building-resilience ├── 3-dropping-spans ├── 4-sensitive-data ├── 5-transform-data ├── 6-routing-data ├── 7-sum-count ├── loadgen ├── otelcol └── setup-workshop.sh",
    "description": "Prerequisites Proficiency in editing YAML files using vi, vim, nano, or your preferred text editor. Supported Environments: A provided Splunk Workshop Instance (preferred). Outbound access to port 2222 is required for ssh access. Apple Mac (Apple Silicon). Installation of jq is required - https://jqlang.org/download/ Exercise Create a directory: In your environment create a new directory and change into it:\nmkdir advanced-otel-workshop \u0026\u0026 \\ cd advanced-otel-workshop We will refer to this directory as [WORKSHOP] for the remainder of the workshop.",
    "tags": [],
    "title": "Pre-requisites",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/prerequisites/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences",
    "content": "With RUM instrumented, we will be able to better understand our end users, what they are doing, and what issues they are encountering.\nThis workshop walks through how our demo site is instrumented and how to interpret the data. If you already have a RUM license, this will help you understand how RUM works and how you can use it to optimize your end user experience.\nTip Our Docs also contain guidance such as scenarios using Splunk RUM and demo applications to test out RUM for mobile apps.",
    "description": "End-to-end visibility helps you pinpoint customer-impacting issues from web browsers and native mobile apps to your backend services.",
    "tags": [],
    "title": "RUM",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/2-rum/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops",
    "content": "Splunk APM is a NoSample Full-fidelity application performance monitoring and troubleshooting solution for cloud-native, microservices-based applications.\nBy collecting all traces, instead of a sampled subset, no anomaly goes undetected. Whether a user experiences an error or longer-than-usual latency, you’ll be able to know and act on it within seconds. Furthermore, not all bad behavior results in errors — as your developers create new applications they need to know whether their canary releases provide the expected results. Only by collecting all trace data will you ensure that your cloud-native applications behave the way they are supposed to.\nInfrastructure and application performance are interdependent. To see the full picture, Splunk APM provides a seamless correlation between cloud infrastructure and the microservices running on top of it. If your application acts out because of memory leakage, a noisy neighbor container or any other infrastructure-related issue, Splunk will let you know. To complete the picture, in-context access to Splunk logs and events enables deeper troubleshooting and root-cause analysis.",
    "description": "Splunk APM is a NoSample Full-fidelity application performance monitoring and troubleshooting solution for cloud-native, microservices-based applications.",
    "tags": [],
    "title": "Splunk APM",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/2-apm/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "Automatic Discovery WorkshopsAutomatic Discovery Workshops\nHorizontal Pod AutoscalingThis workshop will equip you with the basic understanding of monitoring Kubernetes using the Splunk OpenTelemetry Collector\nOpenTelemetry Collector Workshops OpenTelemetry Collector ConceptsLearn the concepts of the OpenTelemetry Collector and how to use it to send data to Splunk Observability Cloud.\nAdvanced OpenTelemetry CollectorPractice setting up the OpenTelemetry Collector configuration from scratch and go though several advanced configuration scenarios's.\nSplunk Synthetic ScriptingProactively find and fix performance issues across user flows, business transactions and APIs to deliver better digital experiences.\nLambda TracingThis workshop will enable you to build a distributed trace for a small serverless application that runs on AWS Lambda, producing and consuming a message via AWS Kinesis\nDashboard WorkshopDashboard\nHands-On OpenTelemetry, Docker, and K8sBy the end of this workshop you'll have gotten hands-on experience instrumenting a .NET application with OpenTelemetry, then Dockerizing the application and deploying it to Kubernetes. You’ll also gain experience deploying the OpenTelemetry collector using Helm, customizing the collector configuration, and troubleshooting collector configuration issues.\nSolving Problems with O11y CloudBy the end of this workshop you'll have gotten hands-on experience deploying the OpenTelemetry Collector, instrumenting an application with OpenTelemetry, capturing tags from the application, and using Troubleshooting MetricSets and Tag Spotlight to determine the root cause of an issue.\nIngest Processor for Observability CloudScenario Description",
    "description": "The following workshops require Ninja skills, wax on, wax off.",
    "tags": [],
    "title": "Splunk4Ninjas Workshops",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices",
    "content": "Splunk Observability Cloud includes powerful features that dramatically reduce the time required for SREs to isolate issues across services, so they know which team to engage to troubleshoot the issue further, and can provide context to help engineering get a head start on debugging.\nUnlocking these features requires tags to be included with your application traces. But how do you know which tags are the most valuable and how do you capture them?\nIn this workshop, we’ll explore:\nWhat are tags and why are they such a critical part of making an application observable. How to use OpenTelemetry to capture tags of interest from your application. How to index tags in Splunk Observability Cloud and the differences between Troubleshooting MetricSets and Monitoring MetricSets. How to utilize tags in Splunk Observability Cloud to find “unknown unknowns” using the Tag Spotlight and Dynamic Service Map features. How to utilize tags for alerting and dashboards. The workshop uses a simple microservices-based application to illustrate these concepts. Let’s get started!\nTip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "This workshop shows how tags can be used to reduce the time required for SREs to isolate issues across services, so they know which team to engage to troubleshoot the issue further, and can provide context to help engineering get a head start on debugging.",
    "tags": [],
    "title": "Tagging Workshop",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "Welcome! In this section, we’ll begin with a fully functional OpenTelemetry setup that includes both an Agent and a Gateway.\nWe’ll start by quickly reviewing their configuration files to get familiar with the overall structure and to highlight key sections that control the telemetry pipeline.\nTip Throughout the workshop, you’ll work with multiple terminal windows. To keep things organized, give each terminal a unique name or color. This will help you easily recognize and switch between them during the exercises.\nWe will refer to these terminals as: Agent, Gateway, Loadgen, and Test.\nExercise Create your first terminal window and name it Agent. Navigate to the directory for the first exercise [WORKSHOP]/1-agent-gateway and verify that the required files have been generated:\ncd 1-agent-gateway ls -l You should see the following files in the directory. If not, re-run the setup-workshop.sh script as described in the Pre-requisites section:\n​ Directory Structure . ├── agent.yaml └── gateway.yaml Understanding the Agent configuration Let’s review the key components of the agent.yaml file used in this workshop. We’ve made some important additions to support metrics, traces, and logs.\nReceivers The receivers section defines how the Agent ingests telemetry data. In this setup, three types of receivers have been configured:\nHost Metrics Receiver\nhostmetrics: # Host Metrics Receiver collection_interval: 3600s # Collection Interval (1hr) scrapers: cpu: # CPU Scraper Collects CPU usage from the local system every hour. We’ll use this to generate example metric data.\nOTLP Receiver (HTTP protocol)\notlp: # OTLP Receiver protocols: http: # Configure HTTP protocol endpoint: \"0.0.0.0:4318\" # Endpoint to bind to Enables the agent to receive metrics, traces, and logs over HTTP on port 4318. This is used to send data to the collector in future exercises.\nFileLog Receiver\nfilelog/quotes: # Receiver Type/Name include: ./quotes.log # The file to read log data from include_file_path: true # Include file path in the log data include_file_name: false # Exclude file name from the log data resource: # Add custom resource attributes com.splunk.source: ./quotes.log # Source of the log data com.splunk.sourcetype: quotes # Source type of the log data Enables the agent to tail a local log file (quotes.log) and convert it to structured log events, enriched with metadata such as source and sourceType.\nExporters Debug Exporter\ndebug: # Exporter Type verbosity: detailed # Enabled detailed debug output OTLPHTTP Exporter\notlphttp: # Exporter Type endpoint: \"http://localhost:5318\" # Gateway OTLP endpoint The debug exporter sends data to the console for visibility and debugging during the workshop while the otlphttp exporter forwards all telemetry to the local Gateway instance.\nThis dual-export strategy ensures you can see the raw data locally while also sending it downstream for further processing and export.",
    "description": "Welcome! In this section, we’ll begin with a fully functional OpenTelemetry setup that includes both an Agent and a Gateway.\nWe’ll start by quickly reviewing their configuration files to get familiar with the overall structure and to highlight key sections that control the telemetry pipeline.\nTip Throughout the workshop, you’ll work with multiple terminal windows. To keep things organized, give each terminal a unique name or color. This will help you easily recognize and switch between them during the exercises.",
    "tags": [],
    "title": "1. Verify Agent Configuration",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/1-agent-gateway/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "Welcome! In this section, we’ll begin with a fully functional OpenTelemetry setup that includes both an Agent and a Gateway.\nWe’ll start by quickly reviewing their configuration files to get familiar with the overall structure and to highlight key sections that control the telemetry pipeline.\nTip Throughout the workshop, you’ll work with multiple terminal windows. To keep things organized, give each terminal a unique name or color. This will help you easily recognize and switch between them during the exercises.\nWe will refer to these terminals as: Agent, Gateway, Loadgen, and Test.\nExercise Create your first terminal window and name it Agent. Navigate to the directory for the first exercise [WORKSHOP]/1-agent-gateway and verify that the required files have been generated:\ncd 1-agent-gateway ls -l You should see the following files in the directory. If not, re-run the setup-workshop.sh script as described in the Pre-requisites section:\n​ Directory Structure . ├── agent.yaml └── gateway.yaml Understanding the Agent configuration Let’s review the key components of the agent.yaml file used in this workshop. We’ve made some important additions to support metrics, traces, and logs.\nReceivers The receivers section defines how the Agent ingests telemetry data. In this setup, three types of receivers have been configured:\nHost Metrics Receiver\nhostmetrics: # Host Metrics Receiver collection_interval: 3600s # Collection Interval (1hr) scrapers: cpu: # CPU Scraper Collects CPU usage from the local system every hour. We’ll use this to generate example metric data.\nOTLP Receiver (HTTP protocol)\notlp: # OTLP Receiver protocols: http: # Configure HTTP protocol endpoint: \"0.0.0.0:4318\" # Endpoint to bind to Enables the agent to receive metrics, traces, and logs over HTTP on port 4318. This is used to send data to the collector in future exercises.\nFileLog Receiver\nfilelog/quotes: # Receiver Type/Name include: ./quotes.log # The file to read log data from include_file_path: true # Include file path in the log data include_file_name: false # Exclude file name from the log data resource: # Add custom resource attributes com.splunk.source: ./quotes.log # Source of the log data com.splunk.sourcetype: quotes # Source type of the log data Enables the agent to tail a local log file (quotes.log) and convert it to structured log events, enriched with metadata such as source and sourceType.\nExporters Debug Exporter\ndebug: # Exporter Type verbosity: detailed # Enabled detailed debug output OTLPHTTP Exporter\notlphttp: # Exporter Type endpoint: \"http://localhost:5318\" # Gateway OTLP endpoint The debug exporter sends data to the console for visibility and debugging during the workshop while the otlphttp exporter forwards all telemetry to the local Gateway instance.\nThis dual-export strategy ensures you can see the raw data locally while also sending it downstream for further processing and export.",
    "description": "Welcome! In this section, we’ll begin with a fully functional OpenTelemetry setup that includes both an Agent and a Gateway.\nWe’ll start by quickly reviewing their configuration files to get familiar with the overall structure and to highlight key sections that control the telemetry pipeline.\nTip Throughout the workshop, you’ll work with multiple terminal windows. To keep things organized, give each terminal a unique name or color. This will help you easily recognize and switch between them during the exercises.",
    "tags": [],
    "title": "1. Verify Agent Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/1-agent-gateway/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "Tip During this workshop, you will be using up to five terminal windows simultaneously. To stay organized, consider customizing each terminal or shell with unique names and colors. This will help you quickly identify and switch between them as needed.\nWe will refer to these terminals as: Agent, Gateway, Spans, Logs and Tests.\nExercise In the Agent terminal window, change into the [WORKSHOP] directory and create a new subdirectory named 1-agent.\nmkdir 1-agent \u0026\u0026 \\ cd 1-agent Create a file named agent.yaml. This file will define the basic structure of an OpenTelemetry Collector configuration.\nCopy and paste the following initial configuration into agent.yaml:\n# Extensions extensions: health_check: # Health Check Extension endpoint: 0.0.0.0:13133 # Health Check Endpoint # Receivers receivers: hostmetrics: # Host Metrics Receiver collection_interval: 3600s # Collection Interval (1hr) scrapers: cpu: # CPU Scraper otlp: # OTLP Receiver protocols: http: # Configure HTTP protocol endpoint: \"0.0.0.0:4318\" # Endpoint to bind to # Exporters exporters: debug: # Debug Exporter verbosity: detailed # Detailed verbosity level # Processors processors: memory_limiter: # Limits memory usage check_interval: 2s # Check interval limit_mib: 512 # Memory limit in MiB resourcedetection: # Resource Detection Processor detectors: [system] # Detect system resources override: true # Overwrites existing attributes resource/add_mode: # Resource Processor attributes: - action: insert # Action to perform key: otelcol.service.mode # Key name value: \"agent\" # Key value # Connectors #connectors: # leave this commented out; we will uncomment in an upcoming exercise # Service Section - Enabled Pipelines service: extensions: - health_check # Health Check Extension pipelines: traces: receivers: - otlp # OTLP Receiver processors: - memory_limiter # Memory Limiter processor - resourcedetection # Add system attributes to the data - resource/add_mode # Add collector mode metadata exporters: - debug # Debug Exporter metrics: receivers: - otlp processors: - memory_limiter - resourcedetection - resource/add_mode exporters: - debug logs: receivers: - otlp processors: - memory_limiter - resourcedetection - resource/add_mode exporters: - debug Your directory structure should now look like this:\n. └── agent.yaml # OpenTelemetry Collector configuration file",
    "description": "Tip During this workshop, you will be using up to five terminal windows simultaneously. To stay organized, consider customizing each terminal or shell with unique names and colors. This will help you quickly identify and switch between them as needed.\nWe will refer to these terminals as: Agent, Gateway, Spans, Logs and Tests.\nExercise In the Agent terminal window, change into the [WORKSHOP] directory and create a new subdirectory named 1-agent.",
    "tags": [],
    "title": "1. Agent Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/1-agent/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 1. Getting Started",
    "content": "Introduction For this workshop, we’ll be using the OpenTelemetry Demo Application running in Kubernetes. This application is for an online retailer and includes more than a dozen services written in many different languages. While metrics, traces, and logs are being collected from this application, this workshop is primarily focused on how Splunk Observability Cloud can be used to more efficiently monitor infrastructure.\nPre-requisites You should have access to the EC2 instance provided in section 1.1 Access AWS/EC2 Instance Initial Steps The initial setup can be completed by executing the following steps on the command line of your EC2 instance.\ncd ~/workshop/optimize-cloud-monitoring \u0026\u0026 \\ ./deploy-application.sh You’ll be asked to enter your favorite city. This will be used in the OpenTelemetry Collector configuration as a custom tag to show how easy it is to add additional context to your observability data.\nNote Custom tagging will be covered in more detail in the Standardize Data Collection section of this workshop.\nYour application should now be running and sending data to Splunk Observability Cloud. You’ll dig into the data in the next section.",
    "description": "Introduction For this workshop, we’ll be using the OpenTelemetry Demo Application running in Kubernetes. This application is for an online retailer and includes more than a dozen services written in many different languages. While metrics, traces, and logs are being collected from this application, this workshop is primarily focused on how Splunk Observability Cloud can be used to more efficiently monitor infrastructure.\nPre-requisites You should have access to the EC2 instance provided in section 1.1 Access AWS/EC2 Instance Initial Steps The initial setup can be completed by executing the following steps on the command line of your EC2 instance.",
    "tags": [],
    "title": "Deploy OpenTelemetry Demo Applciation",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/1-getting-started/2-deploy-application/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 1. Agent Configuration",
    "content": "Now, we can start the Gateway and the Agent, which is configured to automatically send Host Metrics at startup. We do this to verify that data is properly routed from the Agent to the Gateway.\nExercise Gateway: In the Gateway terminal window, run the following command to start the Gateway:\n​ Start the Gateway ../otelcol --config=gateway.yaml If everything is configured correctly, the collector will start and state Everything is ready. Begin running and processing data. in the output, similar to the following:\n2025-06-09T09:22:11.944+0100 info service@v0.126.0/service.go:289 Everything is ready. Begin running and processing data. {\"resource\": {}} Once the Gateway is running, it will listen for incoming data on port 5318 and export the received data to the following files:\ngateway-traces.out gateway-metrics.out gateway-logs.out Start the Agent: In the Agent terminal window start the agent with the agent configuration:\n​ Start the Agent ../otelcol --config=agent.yaml Verify CPU Metrics:\nCheck that when the Agent starts, it immediately starts sending CPU metrics. Both the Agent and the Gateway will display this activity in their debug output. The output should resemble the following snippet: \u003csnip\u003e NumberDataPoints #31 Data point attributes: -\u003e cpu: Str(cpu3) -\u003e state: Str(wait) StartTimestamp: 2025-07-07 16:49:42 +0000 UTC Timestamp: 2025-07-09 09:36:21.190226459 +0000 UTC Value: 77.380000 {\"resource\": {}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\"} At this stage, the Agent continues to collect CPU metrics once per hour or upon each restart and sends them to the gateway. The Gateway processes these metrics and exports them to a file named gateway-metrics.out. This file stores the exported metrics as part of the pipeline service.\nVerify Data arrived at Gateway: To confirm that CPU metrics, specifically for cpu0, have successfully reached the gateway, we’ll inspect the gateway-metrics.out file using the jq command.\nThe following command filters and extracts the system.cpu.time metric, focusing on cpu0. It displays the metric’s state (e.g., user, system, idle, interrupt) along with the corresponding values.\nOpen or create your third terminal window and name it Tests. Run the command below in the Tests terminal to check the system.cpu.time metric:\n​ Check CPU Metrics Example Output jq '.resourceMetrics[].scopeMetrics[].metrics[] | select(.name == \"system.cpu.time\") | .sum.dataPoints[] | select(.attributes[0].value.stringValue == \"cpu0\") | {cpu: .attributes[0].value.stringValue, state: .attributes[1].value.stringValue, value: .asDouble}' gateway-metrics.out { \"cpu\": \"cpu0\", \"state\": \"user\", \"value\": 123407.02 } { \"cpu\": \"cpu0\", \"state\": \"system\", \"value\": 64866.6 } { \"cpu\": \"cpu0\", \"state\": \"idle\", \"value\": 216427.87 } { \"cpu\": \"cpu0\", \"state\": \"interrupt\", \"value\": 0 } Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "Now, we can start the Gateway and the Agent, which is configured to automatically send Host Metrics at startup. We do this to verify that data is properly routed from the Agent to the Gateway.\nExercise Gateway: In the Gateway terminal window, run the following command to start the Gateway:\n​ Start the Gateway ../otelcol --config=gateway.yaml If everything is configured correctly, the collector will start and state Everything is ready. Begin running and processing data. in the output, similar to the following:",
    "tags": [],
    "title": "1.2 Validate \u0026 Test Configuration",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/1-agent-gateway/1-2-send-metrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 1. Agent Configuration",
    "content": "Now, we can start the Gateway and the Agent, which is configured to automatically send Host Metrics at startup. We do this to verify that data is properly routed from the Agent to the Gateway.\nExercise Gateway: In the Gateway terminal window, run the following command to start the Gateway:\n​ Start the Gateway ../otelcol --config=gateway.yaml If everything is configured correctly, the collector will start and state Everything is ready. Begin running and processing data. in the output, similar to the following:\n2025-06-09T09:22:11.944+0100 info service@v0.126.0/service.go:289 Everything is ready. Begin running and processing data. {\"resource\": {}} Once the Gateway is running, it will listen for incoming data on port 5318 and export the received data to the following files:\ngateway-traces.out gateway-metrics.out gateway-logs.out Start the Agent: In the Agent terminal window start the agent with the agent configuration:\n​ Start the Agent ../otelcol --config=agent.yaml Verify CPU Metrics:\nCheck that when the Agent starts, it immediately starts sending CPU metrics. Both the Agent and the Gateway will display this activity in their debug output. The output should resemble the following snippet: \u003csnip\u003e NumberDataPoints #31 Data point attributes: -\u003e cpu: Str(cpu3) -\u003e state: Str(wait) StartTimestamp: 2025-07-07 16:49:42 +0000 UTC Timestamp: 2025-07-09 09:36:21.190226459 +0000 UTC Value: 77.380000 {\"resource\": {}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\"} At this stage, the Agent continues to collect CPU metrics once per hour or upon each restart and sends them to the gateway. The Gateway processes these metrics and exports them to a file named gateway-metrics.out. This file stores the exported metrics as part of the pipeline service.\nVerify Data arrived at Gateway: To confirm that CPU metrics, specifically for cpu0, have successfully reached the gateway, we’ll inspect the gateway-metrics.out file using the jq command.\nThe following command filters and extracts the system.cpu.time metric, focusing on cpu0. It displays the metric’s state (e.g., user, system, idle, interrupt) along with the corresponding values.\nOpen or create your third terminal window and name it Tests. Run the command below in the Tests terminal to check the system.cpu.time metric:\n​ Check CPU Metrics Example Output jq '.resourceMetrics[].scopeMetrics[].metrics[] | select(.name == \"system.cpu.time\") | .sum.dataPoints[] | select(.attributes[0].value.stringValue == \"cpu0\") | {cpu: .attributes[0].value.stringValue, state: .attributes[1].value.stringValue, value: .asDouble}' gateway-metrics.out { \"cpu\": \"cpu0\", \"state\": \"user\", \"value\": 123407.02 } { \"cpu\": \"cpu0\", \"state\": \"system\", \"value\": 64866.6 } { \"cpu\": \"cpu0\", \"state\": \"idle\", \"value\": 216427.87 } { \"cpu\": \"cpu0\", \"state\": \"interrupt\", \"value\": 0 } Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "Now, we can start the Gateway and the Agent, which is configured to automatically send Host Metrics at startup. We do this to verify that data is properly routed from the Agent to the Gateway.\nExercise Gateway: In the Gateway terminal window, run the following command to start the Gateway:\n​ Start the Gateway ../otelcol --config=gateway.yaml If everything is configured correctly, the collector will start and state Everything is ready. Begin running and processing data. in the output, similar to the following:",
    "tags": [],
    "title": "1.2 Validate \u0026 Test Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/1-agent-gateway/1-2-send-metrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 2. API Test",
    "content": "Add Authentication Request Click on + Add requests and enter the request step name e.g. Authenticate with Spotify API.\nExpand the Request section, from the drop-down change the request method to POST and enter the following URL:\nhttps://accounts.spotify.com/api/token In the Payload body section enter the following:\ngrant_type=client_credentials Next, add two request headers with the following key/value pairings:\nCONTENT-TYPE: application/x-www-form-urlencoded AUTHORIZATION: Basic {{env.encoded_auth}} Expand the Validation section and add the following extraction:\nExtract from Response body JSON $.access_token as access_token. This will parse the JSON payload that is received from the Spotify API, extract the access token and store it as a custom variable.",
    "description": "Add Authentication Request Click on + Add requests and enter the request step name e.g. Authenticate with Spotify API.\nExpand the Request section, from the drop-down change the request method to POST and enter the following URL:\nhttps://accounts.spotify.com/api/token In the Payload body section enter the following:\ngrant_type=client_credentials Next, add two request headers with the following key/value pairings:\nCONTENT-TYPE: application/x-www-form-urlencoded AUTHORIZATION: Basic {{env.encoded_auth}} Expand the Validation section and add the following extraction:",
    "tags": [],
    "title": "Authentication Request",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/2-api-test/3-authentication-request/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk OnCall \u003e 1. Getting Started",
    "content": "Aim Escalation policies determine who is actually on-call for a given team and are the link to utilizing any rotations that have been created.\nThe aim of this module is for you to create three different Escalation Policies to demonstrate a number of different features and operating models.\nThe instructor will start by explaining the concepts before you proceed with the configuration.\nNavigate to the Escalation Polices tab on the Teams sub menu, you should have no existing Polices so we need to create some.\nWe are going to create the following Polices to cover off three typical use cases.\n1. 24/7 Policy Click Add Escalation Policy\nPolicy Name: 24/7 Step 1 Immediately Notify the on-duty user(s) in rotation → Senior SRE Escalation Click Save 2. Primary Policy Click Add Escalation Policy\nPolicy Name: Primary Step 1 Immediately Notify the on-duty user(s) in rotation → Follow the Sun Support - Business Hours Click Add Step Step 2 If still un-acknowledged after 15 minutes Notify the next user(s) in the current on-duty shift → Follow the Sun Support - Business Hours Click Add Step Step 3 If still un-acknowledged after 15 more minutes Execute Policy → [Your Team Name] : 24/7 Click Save 3. Waiting Room Policy Click Add Escalation Policy\nPolicy Name: Waiting Room Step 1 If still un-acknowledged after 10 more minutes Execute Policy → [Your Team Name] : Primary Click Save You should now have the following three escalation polices:\nYou may have noticed that when we created each policy there was the following warning message:\nWarning There are no routing keys for this policy - it will only receive incidents via manual reroute or when on another escalation policy\nThis is because there are no Routing Keys linked to these Escalation Polices, so now that we have these polices configured we can create the Routing Keys and link them to our Polices..\nContinue and also complete the Creating Routing Keys module.",
    "description": "Aim Escalation policies determine who is actually on-call for a given team and are the link to utilizing any rotations that have been created.\nThe aim of this module is for you to create three different Escalation Policies to demonstrate a number of different features and operating models.\nThe instructor will start by explaining the concepts before you proceed with the configuration.\nNavigate to the Escalation Polices tab on the Teams sub menu, you should have no existing Polices so we need to create some.",
    "tags": [],
    "title": "Configure Escalation Policies",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/getting_started/escalation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 1. Real Browser Test",
    "content": "To begin configuring our test, we need to import the JSON that we exported from the Chrome DevTools Recorder. To enable the Import button, we must first give our test a name e.g. \u003cyour initials\u003e - Online Boutique.\nOnce the Import button is enabled, click on it and either drop the JSON file that you exported from the Chrome DevTools Recorder or upload the file.\nOnce the JSON file has been uploaded, click on Continue to edit steps\nBefore we make any edits to the test, let’s first configure the settings, click on \u003c Return to test",
    "description": "To begin configuring our test, we need to import the JSON that we exported from the Chrome DevTools Recorder. To enable the Import button, we must first give our test a name e.g. \u003cyour initials\u003e - Online Boutique.\nOnce the Import button is enabled, click on it and either drop the JSON file that you exported from the Chrome DevTools Recorder or upload the file.",
    "tags": [],
    "title": "1.3 Import JSON",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/1-real-browser-test/3-import-json/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 1. Agent Setup",
    "content": "So far, we’ve simply exported an exact copy of the span sent through the OpenTelemetry Collector.\nNow, let’s improve the base span by adding metadata with processors. This extra information can be helpful for troubleshooting and correlation.\nExercise Stop the collector: In your Agent terminal window, and stop the running collector by pressing Ctrl-C. Once the agent has stopped, open the agent.yaml.\nUpdate All Pipelines: Add both processors (resourcedetection and resource/add_mode) to the processors array in all pipelines. Ensure memory_limiter remains the first processor.\nThe Resource Detection Processor is used to detect resource information from the host and append or override the resource value in telemetry data with this information. The Resource Processor is used to apply changes on resource attributes. In this case, the default configuration adds a new attribute otelcol.service.mode with the value agent. service: # Services configured for this Collector extensions: # Enabled extensions - health_check pipelines: # Array of configured pipelines traces: receivers: - otlp # OTLP Receiver processors: - memory_limiter # Memory Limiter Processor - resourcedetection # Adds system attributes to the data - resource/add_mode # Adds collector mode metadata exporters: - debug # Debug Exporter - file # File Exporter metrics: receivers: - otlp # OTLP Receiver processors: - memory_limiter # Memory Limiter Processor - resourcedetection # Adds system attributes to the data - resource/add_mode # Adds collector mode metadata exporters: - debug # Debug Exporter - file # File Exporter logs: receivers: - otlp # OTLP Receiver processors: - memory_limiter # Memory Limiter Processor - resourcedetection # Adds system attributes to the data - resource/add_mode # Adds collector mode metadata exporters: - debug # Debug Exporter - file # File Exporter By adding these processors, we enrich the data with system metadata and the agent’s operational mode, which aids in troubleshooting and provides useful context for related content.\nValidate the agent configuration using otelbin.io:\n%%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(\u0026nbsp;\u0026nbsp;otlp\u0026nbsp;\u0026nbsp;\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor EXP1(\u0026ensp;debug\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026ensp;file\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter %% Links subID1:::sub-traces subgraph \" \" subgraph subID1[**Traces/Metrics/Logs**] direction LR REC1 --\u003e PRO1 PRO1 --\u003e PRO2 PRO2 --\u003e PRO3 PRO3 --\u003e EXP1 PRO3 --\u003e EXP2 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-traces stroke:#fff,stroke-width:1px, color:#fff,stroke-dasharray: 3 3;",
    "description": "So far, we’ve simply exported an exact copy of the span sent through the OpenTelemetry Collector.\nNow, let’s improve the base span by adding metadata with processors. This extra information can be helpful for troubleshooting and correlation.\nExercise Stop the collector: In your Agent terminal window, and stop the running collector by pressing Ctrl-C. Once the agent has stopped, open the agent.yaml.\nUpdate All Pipelines: Add both processors (resourcedetection and resource/add_mode) to the processors array in all pipelines. Ensure memory_limiter remains the first processor.",
    "tags": [],
    "title": "1.4 Resource Metadata",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/1-agent/1-4-metadata/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 6. Service Health Dashboard",
    "content": "In this part of the workshop we are going to create a chart that we will add to our dashboard, we will also link it to the detector we previously built. This will allow us to see the behavior of our test and get alerted if one or more of our test runs breach its SLA.\nExercise At the top of the dashboard click on the + and select Chart. First, use the Untitled chart input field and name the chart Requests by version \u0026 error. For this exercise we want a bar or column chart, so click on the 3rd icon in the chart option box. In the Plot editor enter spans.count (this is runtime in duration for our test) in the Signal box and hit enter. Click Add filter and choose sf_service:wire-transfer-service Right now we see different colored bars, a different color for each region the test runs from. As this is not needed we can change that behavior by adding some analytics. Click the Add analytics button. From the drop-down choose the Sum option, then pick sum:aggregation and click version and then click sf_error to group by both of these dimensions. Notice how the chart changes as the metrics are now aggregated. Click the Save and close button. In the dashboard, move the charts so they look like the screenshot below: For the final task, click three dots … at the top of the page (next to Event Overlay) and click on View fullscreen. This will be the view you would use on the TV monitor on the wall (press Esc to go back). Tip In your spare time have a try at adding another custom chart to the dashboard using APM or Infrastructure metrics. You could copy a chart from the out-of-the-box Kubernetes dashboard group. Or you could use the APM metric traces.count to create a chart that shows the number of errors on a specific endpoint.\nFinally, we will run through a workshop wrap-up.",
    "description": "In this part of the workshop we are going to create a chart that we will add to our dashboard, we will also link it to the detector we previously built. This will allow us to see the behavior of our test and get alerted if one or more of our test runs breach its SLA.\nExercise At the top of the dashboard click on the + and select Chart. First, use the Untitled chart input field and name the chart Requests by version \u0026 error. For this exercise we want a bar or column chart, so click on the 3rd icon in the chart option box. In the Plot editor enter spans.count (this is runtime in duration for our test) in the Signal box and hit enter. Click Add filter and choose sf_service:wire-transfer-service Right now we see different colored bars, a different color for each region the test runs from. As this is not needed we can change that behavior by adding some analytics. Click the Add analytics button. From the drop-down choose the Sum option, then pick sum:aggregation and click version and then click sf_error to group by both of these dimensions. Notice how the chart changes as the metrics are now aggregated. Click the Save and close button. In the dashboard, move the charts so they look like the screenshot below: For the final task, click three dots … at the top of the page (next to Event Overlay) and click on View fullscreen. This will be the view you would use on the TV monitor on the wall (press Esc to go back). Tip In your spare time have a try at adding another custom chart to the dashboard using APM or Infrastructure metrics. You could copy a chart from the out-of-the-box Kubernetes dashboard group. Or you could use the APM metric traces.count to create a chart that shows the number of errors on a specific endpoint.",
    "tags": [],
    "title": "Adding a Custom Chart",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/6-custom-dashboard/3-custom-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 9. Service Health Dashboard",
    "content": "In this part of the workshop we are going to create a chart that we will add to our dashboard, we will also link it to the detector we previously built. This will allow us to see the behavior of our test and get alerted if one or more of our test runs breach its SLA.\nExercise At the top of the dashboard click on the + and select Chart. First, use the Untitled chart input field and name the chart Overall Test Duration. For this exercise we want a bar or column chart, so click on the 3rd icon in the chart option box. In the Plot editor enter synthetics.run.duration.time.ms (this is runtime in duration for our test) in the Signal box and hit enter. Right now we see different colored bars, a different color for each region the test runs from. As this is not needed we can change that behavior by adding some analytics. Click the Add analytics button. From the drop-down choose the Mean option, then pick mean:aggregation and click outside the dialog box. Notice how the chart changes to a single color as the metrics are now aggregated. The x-axis does not currently represent time to change this click on the settings icon at the end of the plot line. The following following dialog will open: Change the Display units (2) in the drop-down box from None to Time (autoscaling)/Milliseconds(ms). The drop-down changes to Millisecond and the x-axis of the chart now represents the test duration time. Close the dialog, either by clicking on the settings icon or the close button. Add our detector by clicking the Link Detector button and start typing the name of the detector you created earlier. Click on the detector name to select it. Notice that a colored border appears around the chart, indicating the status of the alert, along with a bell icon at the top of the dashboard as shown below: Click the Save and close button. In the dashboard, move the charts so they look like the screenshot below: For the final task, click three dots … at the top of the page (next to Event Overlay) and click on View fullscreen. This will be the view you would use on the TV monitor on the wall (press Esc to go back). Tip In your spare time have a try at adding another custom chart to the dashboard using RUM metrics. You could copy a chart from the out-of-the-box RUM applications dashboard group. Or you could use the RUM metric rum.client_error.count to create a chart that shows the number of client errors in the application.\nFinally, we will run through a workshop wrap-up.",
    "description": "In this part of the workshop we are going to create a chart that we will add to our dashboard, we will also link it to the detector we previously built. This will allow us to see the behavior of our test and get alerted if one or more of our test runs breach its SLA.\nExercise At the top of the dashboard click on the + and select Chart. First, use the Untitled chart input field and name the chart Overall Test Duration. For this exercise we want a bar or column chart, so click on the 3rd icon in the chart option box. In the Plot editor enter synthetics.run.duration.time.ms (this is runtime in duration for our test) in the Signal box and hit enter. Right now we see different colored bars, a different color for each region the test runs from. As this is not needed we can change that behavior by adding some analytics. Click the Add analytics button. From the drop-down choose the Mean option, then pick mean:aggregation and click outside the dialog box. Notice how the chart changes to a single color as the metrics are now aggregated. The x-axis does not currently represent time to change this click on the settings icon at the end of the plot line. The following following dialog will open: Change the Display units (2) in the drop-down box from None to Time (autoscaling)/Milliseconds(ms). The drop-down changes to Millisecond and the x-axis of the chart now represents the test duration time. Close the dialog, either by clicking on the settings icon or the close button. Add our detector by clicking the Link Detector button and start typing the name of the detector you created earlier. Click on the detector name to select it. Notice that a colored border appears around the chart, indicating the status of the alert, along with a bell icon at the top of the dashboard as shown below: Click the Save and close button. In the dashboard, move the charts so they look like the screenshot below: For the final task, click three dots … at the top of the page (next to Event Overlay) and click on View fullscreen. This will be the view you would use on the TV monitor on the wall (press Esc to go back). Tip In your spare time have a try at adding another custom chart to the dashboard using RUM metrics. You could copy a chart from the out-of-the-box RUM applications dashboard group. Or you could use the RUM metric rum.client_error.count to create a chart that shows the number of client errors in the application.",
    "tags": [],
    "title": "Adding a Custom Chart",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/9-custom-dashboard/3-custom-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "Splunk APM provides a NoSample end-to-end visibility of every service and its dependency to solve problems quicker across monoliths and microservices. Teams can immediately detect problems from new deployments, confidently troubleshoot by scoping and isolating the source of an issue, and optimize service performance by understanding how back-end services impact end users and business workflows.\nReal-time monitoring and alerting: Splunk provides out-of-the-box service dashboards and automatically detects and alerts on RED metrics (rate, error and duration) when there is a sudden change.\nDynamic telemetry maps: Easily visualize service performance in modern production environments in real-time. End-to-end visibility of service performance from infrastructure, applications, end users, and all dependencies helps quickly scope new issues and troubleshoot more effectively.\nIntelligent tagging and analysis: View all tags from your business, infrastructure and applications in one place to easily compare new trends in latency or errors to their specific tag values.\nAI-directed troubleshooting identifies the most impactful issues: Instead of manually digging through individual dashboards, isolate problems more efficiently. Automatically identify anomalies and the sources of errors that impact services and customers the most.\nComplete distributed tracing analyses every transaction: Identify problems in your cloud-native environment more effectively. Splunk distributed tracing visualizes and correlates every transaction from the back-end and front-end in context with your infrastructure, business workflows and applications.\nFull stack correlation: Within Splunk Observability, APM links traces, metrics, logs and profiling together to easily understand the performance of every component and its dependency across your stack.\nMonitor database query performance: Easily identify how slow and high execution queries from SQL and NoSQL databases impact your services, endpoints and business workflows — no instrumentation required.",
    "description": "Splunk APM provides a NoSample end-to-end visibility of every service and its dependency to solve problems quicker across monoliths and microservices. Teams can immediately detect problems from new deployments, confidently troubleshoot by scoping and isolating the source of an issue, and optimize service performance by understanding how back-end services impact end users and business workflows.\nReal-time monitoring and alerting: Splunk provides out-of-the-box service dashboards and automatically detects and alerts on RED metrics (rate, error and duration) when there is a sudden change.",
    "tags": [],
    "title": "Application Performance Monitoring Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/2-apm-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 2. Preparation",
    "content": "The first deployment of our application will be using prebuilt containers to give this base scenario: a regular Java microservices-based application running in Kubernetes that we want to start observing. So let’s deploy the application:\n​ kubectl apply Output kubectl apply -f ~/workshop/petclinic/deployment.yaml deployment.apps/config-server created service/config-server created deployment.apps/discovery-server created service/discovery-server created deployment.apps/api-gateway created service/api-gateway created service/api-gateway-external created deployment.apps/customers-service created service/customers-service created deployment.apps/vets-service created service/vets-service created deployment.apps/visits-service created service/visits-service created deployment.apps/admin-server created service/admin-server created service/petclinic-db created deployment.apps/petclinic-db created configmap/petclinic-db-initdb-config created deployment.apps/petclinic-loadgen-deployment created configmap/scriptfile created At this point, we can verify the deployment by checking that the Pods are running. The containers need to be downloaded and started, so this may take a couple of minutes.\n​ kubectl get pods Output kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-655dcd9b6b-dcvkb 1/1 Running 0 114s splunk-otel-collector-agent-dg2vj 1/1 Running 0 114s splunk-otel-collector-operator-57cbb8d7b4-dk5wf 2/2 Running 0 114s petclinic-db-64d998bb66-2vzpn 1/1 Running 0 58s api-gateway-d88bc765-jd5lg 1/1 Running 0 58s visits-service-7f97b6c579-bh9zj 1/1 Running 0 58s admin-server-76d8b956c5-mb2zv 1/1 Running 0 58s customers-service-847db99f79-mzlg2 1/1 Running 0 58s vets-service-7bdcd7dd6d-2tcfd 1/1 Running 0 58s petclinic-loadgen-deployment-5d69d7f4dd-xxkn4 1/1 Running 0 58s config-server-67f7876d48-qrsr5 1/1 Running 0 58s discovery-server-554b45cfb-bqhgt 1/1 Running 0 58s Make sure the output of kubectl get pods matches the output as shown in the above Output tab. Ensure all the services are shown as Running (or use k9s to continuously monitor the status).\nTo test the application, you need to obtain the public IP address of your instance. You can do this by running the following command:\ncurl http://ifconfig.me Validate if the application is running by visiting http://\u003cIP_ADDRESS\u003e:81 (replace \u003cIP_ADDRESS\u003e with the IP address you obtained above). You should see the PetClinic application running. The application is also running on ports 80 \u0026 443 if you prefer to use those or port 81 is unreachable.\nMake sure the application is working correctly by visiting the All Owners (1) and Veterinarians (2) tabs, confirming that you see a list of names on each page.",
    "description": "The first deployment of our application will be using prebuilt containers to give this base scenario: a regular Java microservices-based application running in Kubernetes that we want to start observing. So let’s deploy the application:\n​ kubectl apply Output kubectl apply -f ~/workshop/petclinic/deployment.yaml deployment.apps/config-server created service/config-server created deployment.apps/discovery-server created service/discovery-server created deployment.apps/api-gateway created service/api-gateway created service/api-gateway-external created deployment.apps/customers-service created service/customers-service created deployment.apps/vets-service created service/vets-service created deployment.apps/visits-service created service/visits-service created deployment.apps/admin-server created service/admin-server created service/petclinic-db created deployment.apps/petclinic-db created configmap/petclinic-db-initdb-config created deployment.apps/petclinic-loadgen-deployment created configmap/scriptfile created At this point, we can verify the deployment by checking that the Pods are running. The containers need to be downloaded and started, so this may take a couple of minutes.",
    "tags": [],
    "title": "Deploy the PetClinic Application",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/2-preparation/2-petclinic/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "The instructor will provide you with the login information for the instance that we will be using during the workshop.\nWhen you first log into your instance, you will be greeted by the Splunk Logo as shown below. If you have any issues connecting to your workshop instance, please reach out to your Instructor.\n$ ssh -p 2222 splunk@\u003cIP-ADDRESS\u003e ███████╗██████╗ ██╗ ██╗ ██╗███╗ ██╗██╗ ██╗ ██╗ ██╔════╝██╔══██╗██║ ██║ ██║████╗ ██║██║ ██╔╝ ╚██╗ ███████╗██████╔╝██║ ██║ ██║██╔██╗ ██║█████╔╝ ╚██╗ ╚════██║██╔═══╝ ██║ ██║ ██║██║╚██╗██║██╔═██╗ ██╔╝ ███████║██║ ███████╗╚██████╔╝██║ ╚████║██║ ██╗ ██╔╝ ╚══════╝╚═╝ ╚══════╝ ╚═════╝ ╚═╝ ╚═══╝╚═╝ ╚═╝ ╚═╝ Last login: Mon Feb 5 11:04:54 2024 from [Redacted] splunk@show-no-config-i-0d1b29d967cb2e6ff ~ $ To ensure your instance is configured correctly, we need to confirm that the required environment variables for this workshop are set correctly. In your terminal, run the following script and check that the environment variables are present and set with actual valid values:\n​ Script Example Output . ~/workshop/petclinic/scripts/check_env.sh ACCESS_TOKEN = \u003credacted\u003e REALM = \u003ce.g. eu0, us1, us2, jp0, au0 etc.\u003e RUM_TOKEN = \u003credacted\u003e HEC_TOKEN = \u003credacted\u003e HEC_URL = https://\u003c...\u003e/services/collector/event INSTANCE = \u003cinstance_name\u003e Please make a note of the INSTANCE environment variable value as this will be used later to filter data in Splunk Observability Cloud.\nFor this workshop, all the above environment variables are required. If any have values missing, please contact your Instructor.\nDelete any existing OpenTelemetry Collectors If you have previously completed a Splunk Observability workshop using this EC2 instance, you need to ensure that any existing installation of the Splunk OpenTelemetry Collector is deleted. This can be achieved by running the following command:\nhelm delete splunk-otel-collector",
    "description": "The instructor will provide you with the login information for the instance that we will be using during the workshop.\nWhen you first log into your instance, you will be greeted by the Splunk Logo as shown below. If you have any issues connecting to your workshop instance, please reach out to your Instructor.\n$ ssh -p 2222 splunk@\u003cIP-ADDRESS\u003e ███████╗██████╗ ██╗ ██╗ ██╗███╗ ██╗██╗ ██╗ ██╗ ██╔════╝██╔══██╗██║ ██║ ██║████╗ ██║██║ ██╔╝ ╚██╗ ███████╗██████╔╝██║ ██║ ██║██╔██╗ ██║█████╔╝ ╚██╗ ╚════██║██╔═══╝ ██║ ██║ ██║██║╚██╗██║██╔═██╗ ██╔╝ ███████║██║ ███████╗╚██████╔╝██║ ╚████║██║ ██╗ ██╔╝ ╚══════╝╚═╝ ╚══════╝ ╚═════╝ ╚═╝ ╚═══╝╚═╝ ╚═╝ ╚═╝ Last login: Mon Feb 5 11:04:54 2024 from [Redacted] splunk@show-no-config-i-0d1b29d967cb2e6ff ~ $ To ensure your instance is configured correctly, we need to confirm that the required environment variables for this workshop are set correctly. In your terminal, run the following script and check that the environment variables are present and set with actual valid values:",
    "tags": [],
    "title": "Preparation of the Workshop instance",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/2-preparation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 7. Log Observer",
    "content": "The bottom pane is where any related content will be reported. In the screenshot below you can see that APM has found a trace that is related to this log line (1):\nBy clicking (2) on Trace for 960432ac9f16b98be84618778905af50 we will be taken to the waterfall in APM for this specific trace, where this log line was generated:\nNote that you now have a Related Content pane for Logs appearing (1). Clicking on this will take you back to Log Observer and will display all the log lines that are part of this trace.",
    "description": "The bottom pane is where any related content will be reported. In the screenshot below you can see that APM has found a trace that is related to this log line (1):\nBy clicking (2) on Trace for 960432ac9f16b98be84618778905af50 we will be taken to the waterfall in APM for this specific trace, where this log line was generated:",
    "tags": [],
    "title": "Related Content",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/7-log-observer-connect/2-related-content/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk APM",
    "content": "APM Overview - RED metrics Using the Service Map Introduction to Tag Spotlight Example Traces Contextual Links to Infra 1. Traces and Spans explained A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services.\nEach span has a name, representing the operation captured by this span, and a service name, representing within which service the operation took place.\nAdditionally, spans may reference another span as their parent, defining the relationships between the operations captured in the trace that were performed to process that transaction.\nEach span contains a lot of information about the method, operation, or block of code that it captures, including:\nthe operation name the start time of the operation with microsecond precision how long the operation took to execute, also with microsecond precision the logical name of the service on which the operation took place the IP address of the service instance on which the operation took place",
    "description": "An overview of how to use Splunk APM",
    "tags": [],
    "title": "2. Using Splunk APM",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/2-apm/using-splunk-apm/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 2. Get Data In",
    "content": "Deploy a NGINX ReplicaSet into your K3s cluster and confirm the discovery of your NGINX deployment. Run a load test to create metrics and confirm them streaming into Splunk Observability Cloud! 1. Start your NGINX Verify the number of pods running in the Splunk UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster.\nNote the single agent container running per node among the default Kubernetes pods. This single container will monitor all the pods and services being deployed on this node!\nNow switch back to the default cluster node view by selecting the MAP tab and selecting your cluster again.\nIn your AWS/EC2 or Multipass shell session change into the nginx directory:\n​ Change Directory cd ~/workshop/k3s/nginx 2. Create NGINX deployment Create the NGINX ConfigMap1 using the nginx.conf file:\n​ Kubectl Configmap Create Kubectl Create Configmap Output kubectl create configmap nginxconfig --from-file=nginx.conf configmap/nginxconfig created\nThen create the deployment:\n​ Kubectl Create Deployment Kubectl Create Deployment Output kubectl create -f nginx-deployment.yaml deployment.apps/nginx created service/nginx created\nNext, we will deploy Locust2 which is an open-source tool used for creating a load test against NGINX:\n​ Kubectl Create Deployment Kubectl Create Deployment Output kubectl create -f locust-deployment.yaml deployment.apps/nginx-loadgenerator created service/nginx-loadgenerator created\nValidate the deployment has been successful and that the Locust and NGINX pods are running.\nIf you have the Splunk UI open you should see new Pods being started and containers being deployed.\nIt should only take around 20 seconds for the pods to transition into a Running state. In the Splunk UI you will have a cluster that looks like the screenshot below:\nIf you select the WORKLOADS tab again you will now see that there is a new ReplicaSet and a deployment added for NGINX:\nLet’s validate this in your shell as well:\n​ Kubectl Get Pods Kubectl Get Pods Output kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-77784c659c-ttmpk 1/1 Running 0 9m19s splunk-otel-collector-agent-249rd 1/1 Running 0 9m19s svclb-nginx-vtnzg 1/1 Running 0 5m57s nginx-7b95fb6b6b-7sb9x 1/1 Running 0 5m57s nginx-7b95fb6b6b-lnzsq 1/1 Running 0 5m57s nginx-7b95fb6b6b-hlx27 1/1 Running 0 5m57s nginx-7b95fb6b6b-zwns9 1/1 Running 0 5m57s svclb-nginx-loadgenerator-nscx4 1/1 Running 0 2m20s nginx-loadgenerator-755c8f7ff6-x957q 1/1 Running 0 2m20s\n3. Run Locust load test Locust, an open-source load generator, is available on port 8083 of the EC2 instance’s IP address. Open a new tab in your web browser and go to http://{==EC2-IP==}:8083/, you will then be able to see the Locust running.\nSet the Spawn rate to 2 and click Start Swarming.\nThis will start a gentle continuous load on the application.\nAs you can see from the above screenshot, most of the calls will report a fail, this is expected, as we have not yet deployed the application behind it, however, NGINX is reporting on your attempts and you should be able to see those metrics.\nValidate you are seeing those metrics in the UI by selecting Dashboards → Built-in Dashboard Groups → NGINX → NGINX Servers. Using the Overrides filter on k8s.cluster.name:, find the name of your cluster as returned by echo $INSTANCE-k3s-cluster in the terminal.\nA ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images so that your applications are easily portable. ↩︎\nWhat is Locust? ↩︎",
    "description": "Deploy a NGINX ReplicaSet into your K3s cluster and confirm the discovery of your NGINX deployment. Run a load test to create metrics and confirm them streaming into Splunk Observability Cloud! 1. Start your NGINX Verify the number of pods running in the Splunk UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster.",
    "tags": [],
    "title": "Deploying NGINX in K3s",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/gdi/nginx/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 2. Standardize Data Collection",
    "content": "When you deployed the OpenTelemetry Demo Application in the Getting Started section of this workshop, you were asked to enter your favorite city. For this workshop, we’ll be using that to show the value of custom tags.\nFor this workshop, the OpenTelemetry collector is pre-configured to use the city you provided as a custom tag called store.location which will be used to emulate Kubernetes Clusters running in different geographic locations. We’ll use this tag as a filter to show how you can use Out-of-the-Box integration dashboards to quickly create views for specific teams, applications, or other attributes about your environment. Efficiently enabling content to be reused across teams without increasing technical debt.\nHere is the OpenTelemetry Collector configuration used to add the store.location tag to all of the data sent to this collector. This means any metrics, traces, or logs will contain the store.location tag which can then be used to search, filter, or correlate this value.\nTip If you’re interested in a deeper dive on the OpenTelemetry Collector, head over to the Self Service Observability workshop where you can get hands-on with configuring the collector or the OpenTelemetry Collector Ninja Workshop where you’ll dissect the inner workings of each collector component.\nWhile this example uses a hard-coded value for the tag, parameterized values can also be used, allowing you to customize the tags dynamically based on the context of each host, application, or operation. This flexibility enables you to capture relevant metadata, user-specific details, or system parameters, providing a rich context for metrics, tracing, and log data while enhancing the observability of your distributed systems.\nNow that you have the appropriate context, which as we’ve established is critical to Observability, let’s head over to Splunk Observability Cloud and see how we can use the data we’ve just configured.",
    "description": "When you deployed the OpenTelemetry Demo Application in the Getting Started section of this workshop, you were asked to enter your favorite city. For this workshop, we’ll be using that to show the value of custom tags.\nFor this workshop, the OpenTelemetry collector is pre-configured to use the city you provided as a custom tag called store.location which will be used to emulate Kubernetes Clusters running in different geographic locations. We’ll use this tag as a filter to show how you can use Out-of-the-Box integration dashboards to quickly create views for specific teams, applications, or other attributes about your environment. Efficiently enabling content to be reused across teams without increasing technical debt.",
    "tags": [],
    "title": "Adding Context With Tags",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/2-standardize-data-collection/2-adding-context-with-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk APM \u003e 2. Using Splunk APM",
    "content": "Tag Spotlight On the right hand side of the screen scroll down on Tag Spotlight, ensure Top Across All Indexed Tags is selected in the dropdown click the full screen button as indicated in the screenshot below.\nThe Tag Spotlight Page will be displayed. From this page you can view the top tags in your application and their corresponding error rates and request rates.\nNote that for the version span tag it appears that version 350.10 has a 100% error rate and for our tenant.level span tag it shows that all three tenants (Gold, Silver \u0026 Bronze) have errors present.\nThe Tag Spotlight page is interactive and allows you to add a tag as a filter by simply clicking on your desired tag. Click on gold under tenant.level to add it as a filter. Once this is done the page will now only display data with gold as it’s tenant.level.\nTag Spotlight is very useful for analysing your data and spotting trends. We can see that for the Gold Tenant that out of the total number of requests, 55 of them are in error (this number will vary in your workshop).\nIf we correlate this to the version tag, we can see that version 350.10 served 55 requests and version 350.9 served 17 requests. This means that all of the requests that went through version 350.10 ended up in an error state.\nIn order to test this theory further that all of the requests from paymentservice version 350.10 result in an error, we can change our filter to another tenant by using the tag selector. Change your filter from gold tenant to silver tenant.\nNow we can perform a similar analysis by looking at the number of requests in error for the silver tenant and correlating that with the version number. Note the amount of errors for the silver tenant match the amount of requests for version 350.10.\nTag Spotlight not only allows you to look at request and error rates but also at the latency per service. In order to do this just select the latency button and remove your Silver Tenant Tag so that you can see the latency for all of the Payment Service.\nGo back to your service map by pressing the X button on the far right underneath Clear All.\nClick anywhere on the pink line in the Services by Error Rate graph in the top right hand corner. Once selected you should see a list of example traces. Click on one of the example traces with an initiating operation of frontend: POST /cart/checkout.",
    "description": "Tag Spotlight On the right hand side of the screen scroll down on Tag Spotlight, ensure Top Across All Indexed Tags is selected in the dropdown click the full screen button as indicated in the screenshot below.\nThe Tag Spotlight Page will be displayed. From this page you can view the top tags in your application and their corresponding error rates and request rates.",
    "tags": [],
    "title": "2.2 Tag Spotlight",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/2-apm/using-splunk-apm/tag_spotlight/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics \u003e 2. API Test",
    "content": "Click on + Add requests and enter the request step name e.g. Authenticate with Spotify API.\nExpand the Request section, from the drop-down change the request method to POST and enter the following URL:\nhttps://accounts.spotify.com/api/token In the Payload body section enter the following:\ngrant_type=client_credentials Next, add two + Request headers with the following key/value pairings:\nCONTENT-TYPE: application/x-www-form-urlencoded AUTHORIZATION: Basic {{env.encoded_auth}} Expand the Validation section and add the following extraction:\nExtract from Response body JSON $.access_token as access_token This will parse the JSON payload that is received from the Spotify API, extract the access token and store it as a custom variable.",
    "description": "Click on + Add requests and enter the request step name e.g. Authenticate with Spotify API.\nExpand the Request section, from the drop-down change the request method to POST and enter the following URL:\nhttps://accounts.spotify.com/api/token In the Payload body section enter the following:\ngrant_type=client_credentials Next, add two + Request headers with the following key/value pairings:\nCONTENT-TYPE: application/x-www-form-urlencoded AUTHORIZATION: Basic {{env.encoded_auth}} Expand the Validation section and add the following extraction:",
    "tags": [],
    "title": "Authentication Request",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/2-api-test/3-authentication-request/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Self-Service Observability \u003e 2 Collect Data with Standards",
    "content": "Reconfigure Collector To reconfigure the collector we need to make these changes:\nIn agent_config.yaml We need to adjust the signalfx exporter to use the gateway The otlp exporter is already there, so we leave it alone We need to change the pipelines to use otlp In splunk-otel-collector.conf We need to set the SPLUNK_GATEWAY_URL to the url provided by the instructor See this documentation page for more details.\nThe exporters will be the following:\nexporters: # Metrics + Events signalfx: access_token: \"${SPLUNK_ACCESS_TOKEN}\" #api_url: \"${SPLUNK_API_URL}\" #ingest_url: \"${SPLUNK_INGEST_URL}\" # Use instead when sending to gateway api_url: \"http://${SPLUNK_GATEWAY_URL}:6060\" ingest_url: \"http://${SPLUNK_GATEWAY_URL}:9943\" sync_host_metadata: true correlation: # Send to gateway otlp: endpoint: \"${SPLUNK_GATEWAY_URL}:4317\" tls: insecure: true The others you can leave as they are, but they won’t be used, as you will see in the pipelines.\nThe pipeline changes (you can see the items commented out and uncommented out):\nservice: pipelines: traces: receivers: [jaeger, otlp, smartagent/signalfx-forwarder, zipkin] processors: - memory_limiter - batch - resourcedetection #- resource/add_environment #exporters: [sapm, signalfx] # Use instead when sending to gateway exporters: [otlp, signalfx] metrics: receivers: [hostmetrics, otlp, signalfx, smartagent/signalfx-forwarder] processors: [memory_limiter, batch, resourcedetection] #exporters: [signalfx] # Use instead when sending to gateway exporters: [otlp] metrics/internal: receivers: [prometheus/internal] processors: [memory_limiter, batch, resourcedetection] # When sending to gateway, at least one metrics pipeline needs # to use signalfx exporter so host metadata gets emitted exporters: [signalfx] logs/signalfx: receivers: [signalfx, smartagent/processlist] processors: [memory_limiter, batch, resourcedetection] exporters: [signalfx] logs: receivers: [fluentforward, otlp] processors: - memory_limiter - batch - resourcedetection #- resource/add_environment #exporters: [splunk_hec, splunk_hec/profiling] # Use instead when sending to gateway exporters: [otlp] And finally we can add the SPLUNK_GATEWAY_URL in splunk-otel-collector.conf, for example:\nSPLUNK_GATEWAY_URL=gateway.splunko11y.com Then we can restart the collector:\nsudo systemctl restart splunk-otel-collector.service And check the status:\nsudo systemctl status splunk-otel-collector.service And finally see the new dimension on the metrics:",
    "description": "Reconfigure Collector To reconfigure the collector we need to make these changes:\nIn agent_config.yaml We need to adjust the signalfx exporter to use the gateway The otlp exporter is already there, so we leave it alone We need to change the pipelines to use otlp In splunk-otel-collector.conf We need to set the SPLUNK_GATEWAY_URL to the url provided by the instructor See this documentation page for more details.\nThe exporters will be the following:",
    "tags": [],
    "title": "Reconfigure Collector",
    "uri": "/observability-workshop/v6.5/en/scenarios/self-service-observability/2-collect-with-standards/3-reconfigure-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 2. Gateway Setup",
    "content": "Exercise Start the Agent: In the Agent terminal window start the agent with the updated configuration:\n​ Start the Agent ../otelcol --config=agent.yaml Verify CPU Metrics:\nCheck that when the agent starts, it immediately starts sending CPU metrics. Both the agent and the gateway will display this activity in their debug output. The output should resemble the following snippet: \u003csnip\u003e NumberDataPoints #37 Data point attributes: -\u003e cpu: Str(cpu0) -\u003e state: Str(system) StartTimestamp: 2024-12-09 14:18:28 +0000 UTC Timestamp: 2025-01-15 15:27:51.319526 +0000 UTC Value: 9637.660000 At this stage, the agent continues to collect CPU metrics once per hour or upon each restart and sends them to the gateway.\nThe gateway processes these metrics and exports them to a file named ./gateway-metrics.out. This file stores the exported metrics as part of the pipeline service.\nVerify Data arrived at Gateway: To confirm that CPU metrics, specifically for cpu0, have successfully reached the gateway, we’ll inspect the gateway-metrics.out file using the jq command.\nThe following command filters and extracts the system.cpu.time metric, focusing on cpu0. It displays the metric’s state (e.g., user, system, idle, interrupt) along with the corresponding values.\nRun the command below in the Tests terminal to check the system.cpu.time metric:\n​ Check CPU Metrics Example Output jq '.resourceMetrics[].scopeMetrics[].metrics[] | select(.name == \"system.cpu.time\") | .sum.dataPoints[] | select(.attributes[0].value.stringValue == \"cpu0\") | {cpu: .attributes[0].value.stringValue, state: .attributes[1].value.stringValue, value: .asDouble}' gateway-metrics.out { \"cpu\": \"cpu0\", \"state\": \"user\", \"value\": 123407.02 } { \"cpu\": \"cpu0\", \"state\": \"system\", \"value\": 64866.6 } { \"cpu\": \"cpu0\", \"state\": \"idle\", \"value\": 216427.87 } { \"cpu\": \"cpu0\", \"state\": \"interrupt\", \"value\": 0 }",
    "description": "Exercise Start the Agent: In the Agent terminal window start the agent with the updated configuration:\n​ Start the Agent ../otelcol --config=agent.yaml Verify CPU Metrics:\nCheck that when the agent starts, it immediately starts sending CPU metrics. Both the agent and the gateway will display this activity in their debug output. The output should resemble the following snippet: \u003csnip\u003e NumberDataPoints #37 Data point attributes: -\u003e cpu: Str(cpu0) -\u003e state: Str(system) StartTimestamp: 2024-12-09 14:18:28 +0000 UTC Timestamp: 2025-01-15 15:27:51.319526 +0000 UTC Value: 9637.660000 At this stage, the agent continues to collect CPU metrics once per hour or upon each restart and sends them to the gateway.",
    "tags": [],
    "title": "2.3 Send metrics from the Agent to the Gateway",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/2-gateway/2-3-send-metrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 2. Building Resilience",
    "content": "To assess the Agent’s resilience, we’ll simulate a temporary Gateway outage and observe how the Agent handles it:\nExercise Simulate a network failure: In the Gateway terminal stop the Gateway with Ctrl-C and wait until the gateway console shows that it has stopped. The Agent will continue running, but it will not be able to send data to the gateway. The output in the Gateway terminal should look similar to this:\n2025-07-09T10:22:37.941Z info service@v0.126.0/service.go:345 Shutdown complete. {\"resource\": {}} Send traces: In the Loadgen terminal window send five more traces using the loadgen.\n​ Start Load Generator ../loadgen -count 5 Notice that the agent’s retry mechanism is activated as it continuously attempts to resend the data. In the agent’s console output, you will see repeated messages similar to the following:\n2025-01-28T14:22:47.020+0100 info internal/retry_sender.go:126 Exporting failed. Will retry the request after interval. {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"otlphttp\", \"error\": \"failed to make an HTTP request: Post \\\"http://localhost:5318/v1/traces\\\": dial tcp 127.0.0.1:5318: connect: connection refused\", \"interval\": \"9.471474933s\"} Stop the Agent: In the Agent terminal window, use Ctrl-C to stop the agent. Wait until the agent’s console confirms it has stopped:\n2025-07-09T10:25:59.344Z info service@v0.126.0/service.go:345 Shutdown complete. {\"resource\": {}} When you stop the agent, any metrics, traces, or logs held in memory for retry will be lost. However, because we have configured the FileStorage Extension, all telemetry that has not yet been accepted by the target endpoint are safely checkpointed on disk.\nStopping the agent is a crucial step to clearly demonstrate how the system recovers when the agent is restarted.",
    "description": "To assess the Agent’s resilience, we’ll simulate a temporary Gateway outage and observe how the Agent handles it:\nExercise Simulate a network failure: In the Gateway terminal stop the Gateway with Ctrl-C and wait until the gateway console shows that it has stopped. The Agent will continue running, but it will not be able to send data to the gateway. The output in the Gateway terminal should look similar to this:",
    "tags": [],
    "title": "2.3 Simulate Failure",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/2-building-resilience/2-3-failure/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 2. Building Resilience",
    "content": "To assess the Agent’s resilience, we’ll simulate a temporary Gateway outage and observe how the Agent handles it:\nExercise Simulate a network failure: In the Gateway terminal stop the Gateway with Ctrl-C and wait until the gateway console shows that it has stopped. The Agent will continue running, but it will not be able to send data to the gateway. The output in the Gateway terminal should look similar to this:\n2025-07-09T10:22:37.941Z info service@v0.126.0/service.go:345 Shutdown complete. {\"resource\": {}} Send traces: In the Loadgen terminal window send five more traces using the loadgen.\n​ Start Load Generator ../loadgen -count 5 Notice that the agent’s retry mechanism is activated as it continuously attempts to resend the data. In the agent’s console output, you will see repeated messages similar to the following:\n2025-01-28T14:22:47.020+0100 info internal/retry_sender.go:126 Exporting failed. Will retry the request after interval. {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"otlphttp\", \"error\": \"failed to make an HTTP request: Post \\\"http://localhost:5318/v1/traces\\\": dial tcp 127.0.0.1:5318: connect: connection refused\", \"interval\": \"9.471474933s\"} Stop the Agent: In the Agent terminal window, use Ctrl-C to stop the agent. Wait until the agent’s console confirms it has stopped:\n2025-07-09T10:25:59.344Z info service@v0.126.0/service.go:345 Shutdown complete. {\"resource\": {}} Important When you stop the agent, any metrics, traces, or logs held in memory for retry will be lost. However, because we have configured the FileStorage Extension, all telemetry that has not yet been accepted by the target endpoint are safely checkpointed on disk.\nStopping the agent is a crucial step to clearly demonstrate how the system recovers when the agent is restarted.",
    "description": "To assess the Agent’s resilience, we’ll simulate a temporary Gateway outage and observe how the Agent handles it:\nExercise Simulate a network failure: In the Gateway terminal stop the Gateway with Ctrl-C and wait until the gateway console shows that it has stopped. The Agent will continue running, but it will not be able to send data to the gateway. The output in the Gateway terminal should look similar to this:",
    "tags": [],
    "title": "2.3 Simulate Failure",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/2-building-resilience/2-3-failure/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 2. Extensions",
    "content": "zPages zPages are an in-process alternative to external exporters. When included, they collect and aggregate tracing and metrics information in the background; this data is served on web pages when requested. zPages are an extremely useful diagnostic feature to ensure the collector is running as expected.\n​ ServiceZ PipelineZ ExtensionZ ServiceZ gives an overview of the collector services and quick access to the pipelinez, extensionz, and featurez zPages. The page also provides build and runtime information.\nExample URL: http://localhost:55679/debug/servicez (change localhost to reflect your own environment).\nPipelineZ provides insights on the running pipelines running in the collector. You can find information on type, if data is mutated, and you can also see information on the receivers, processors and exporters that are used for each pipeline.\nExample URL: http://localhost:55679/debug/pipelinez (change localhost to reflect your own environment).\nExtensionZ shows the extensions that are active in the collector.\nExample URL: http://localhost:55679/debug/extensionz (change localhost to reflect your own environment).\nNinja: Improve data durability with storage extension For this, we will need to validate that our distribution has the file_storage extension installed. This can be done by running the command otelcol-contrib components which should show results like:\n​ Truncated Output Full Output # ... truncated for clarity extensions: - file_storage buildinfo: command: otelcol-contrib description: OpenTelemetry Collector Contrib version: 0.80.0 receivers: - prometheus_simple - apache - influxdb - purefa - purefb - receiver_creator - mongodbatlas - vcenter - snmp - expvar - jmx - kafka - skywalking - udplog - carbon - kafkametrics - memcached - prometheus - windowseventlog - zookeeper - otlp - awsecscontainermetrics - iis - mysql - nsxt - aerospike - elasticsearch - httpcheck - k8sobjects - mongodb - hostmetrics - signalfx - statsd - awsxray - cloudfoundry - collectd - couchdb - kubeletstats - jaeger - journald - riak - splunk_hec - active_directory_ds - awscloudwatch - sqlquery - windowsperfcounters - flinkmetrics - googlecloudpubsub - podman_stats - wavefront - k8s_events - postgresql - rabbitmq - sapm - sqlserver - redis - solace - tcplog - awscontainerinsightreceiver - awsfirehose - bigip - filelog - googlecloudspanner - cloudflare - docker_stats - k8s_cluster - pulsar - zipkin - nginx - opencensus - azureeventhub - datadog - fluentforward - otlpjsonfile - syslog processors: - resource - batch - cumulativetodelta - groupbyattrs - groupbytrace - k8sattributes - experimental_metricsgeneration - metricstransform - routing - attributes - datadog - deltatorate - spanmetrics - span - memory_limiter - redaction - resourcedetection - servicegraph - transform - filter - probabilistic_sampler - tail_sampling exporters: - otlp - carbon - datadog - f5cloud - kafka - mezmo - skywalking - awsxray - dynatrace - loki - prometheus - logging - azuredataexplorer - azuremonitor - instana - jaeger - loadbalancing - sentry - splunk_hec - tanzuobservability - zipkin - alibabacloud_logservice - clickhouse - file - googlecloud - prometheusremotewrite - awscloudwatchlogs - googlecloudpubsub - jaeger_thrift - logzio - sapm - sumologic - otlphttp - googlemanagedprometheus - opencensus - awskinesis - coralogix - influxdb - logicmonitor - signalfx - tencentcloud_logservice - awsemf - elasticsearch - pulsar extensions: - zpages - bearertokenauth - oidc - host_observer - sigv4auth - file_storage - memory_ballast - health_check - oauth2client - awsproxy - http_forwarder - jaegerremotesampling - k8s_observer - pprof - asapclient - basicauth - headers_setter This extension provides exporters the ability to queue data to disk in the event that exporter is unable to send data to the configured endpoint.\nIn order to configure the extension, you will need to update your config to include the information below. First, be sure to create a /tmp/otel-data directory and give it read/write permissions:\nextensions: ... file_storage: directory: /tmp/otel-data timeout: 10s compaction: directory: /tmp/otel-data on_start: true on_rebound: true rebound_needed_threshold_mib: 5 rebound_trigger_threshold_mib: 3 # ... truncated for clarity service: extensions: [health_check, pprof, zpages, file_storage] Why queue data to disk? This allows the collector to weather network interruptions (and even collector restarts) to ensure data is sent to the upstream provider.\nConsiderations for queuing data to disk? There is a potential that this could impact data throughput performance due to disk performance.\nReferences https://community.splunk.com/t5/Community-Blog/Data-Persistence-in-the-OpenTelemetry-Collector/ba-p/624583 https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/extension/storage/filestorage Configuration Check-in Now that we’ve covered extensions, let’s check our configuration changes.\nCheck-inReview your configuration ​ config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 # See https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/security-best-practices.md#safeguards-against-denial-of-service-attacks extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 opencensus: endpoint: 0.0.0.0:55678 # Collect own metrics prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 10s static_configs: - targets: ['0.0.0.0:8888'] jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_binary: endpoint: 0.0.0.0:6832 thrift_compact: endpoint: 0.0.0.0:6831 thrift_http: endpoint: 0.0.0.0:14268 zipkin: endpoint: 0.0.0.0:9411 processors: batch: exporters: debug: verbosity: detailed service: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [otlp, opencensus, prometheus] processors: [batch] exporters: [debug] logs: receivers: [otlp] processors: [batch] exporters: [debug] extensions: [health_check, pprof, zpages] Now that we have reviewed extensions, let’s dive into the data pipeline portion of the workshop. A pipeline defines a path the data follows in the Collector starting from reception, moving to further processing or modification, and finally exiting the Collector via exporters.\nThe data pipeline in the OpenTelemetry Collector is made up of receivers, processors, and exporters. We will first start with receivers.",
    "description": "zPages zPages are an in-process alternative to external exporters. When included, they collect and aggregate tracing and metrics information in the background; this data is served on web pages when requested. zPages are an extremely useful diagnostic feature to ensure the collector is running as expected.\n​ ServiceZ PipelineZ ExtensionZ ServiceZ gives an overview of the collector services and quick access to the pipelinez, extensionz, and featurez zPages. The page also provides build and runtime information.",
    "tags": [],
    "title": "OpenTelemetry Collector Extensions",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/2-extensions/3-zpages/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Profiling Workshop",
    "content": "Let’s learn how to enable the memory and CPU profilers, verify their operation, and use the results in Splunk Observability Cloud to find out why our application startup is slow.\nUpdate the application configuration We will need to pass additional configuration arguments to the Splunk OpenTelemetry Java agent in order to enable both profilers. The configuration is documented here in detail, but for now we just need the following settings:\nSPLUNK_PROFILER_ENABLED=\"true\" SPLUNK_PROFILER_MEMORY_ENABLED=\"true\" Since our application is deployed in Kubernetes, we can update the Kubernetes manifest file to set these environment variables. Open the doorgame/doorgame.yaml file for editing, and ensure the values of the following environment variables are set to “true”:\n- name: SPLUNK_PROFILER_ENABLED value: \"true\" - name: SPLUNK_PROFILER_MEMORY_ENABLED value: \"true\" Next, let’s redeploy the Door Game application by running the following command:\ncd workshop/profiling kubectl apply -f doorgame/doorgame.yaml After a few seconds, a new pod will be deployed with the updated application settings.\nConfirm operation To ensure the profiler is enabled, let’s review the application logs with the following commands:\nkubectl logs -l app=doorgame --tail=100 | grep JfrActivator You should see a line in the application log output that shows the profiler is active:\n[otel.javaagent 2024-02-05 19:01:12:416 +0000] [main] INFO com.splunk.opentelemetry.profiler.JfrActivator - Profiler is active.``` This confirms that the profiler is enabled and sending data to the OpenTelemetry collector deployed in our Kubernetes cluster, which in turn sends profiling data to Splunk Observability Cloud.\nProfiling in APM Visit http://\u003cyour IP address\u003e:81 and play a few more rounds of The Door Game.\nThen head back to Splunk Observability Cloud, click on APM, and click on the doorgame service at the bottom of the screen.\nClick on “Traces” on the right-hand side to load traces for this service. Filter on traces involving the doorgame service and the GET new-game operation (since we’re troubleshooting the game startup sequence):\nSelecting one of these traces brings up the following screen:\nYou can see that the spans now include “Call Stacks”, which is a result of us enabling CPU and memory profiling earlier.\nClick on the span named doorgame: SELECT doorgamedb, then click on CPU stack traces on the right-hand side:\nThis brings up the CPU call stacks captured by the profiler.\nLet’s open the AlwaysOn Profiler to review the CPU stack trace in more detail. We can do this by clicking on the Span link beside View in AlwaysOn Profiler:\nThe AlwaysOn Profiler includes both a table and a flamegraph. Take some time to explore this view by doing some of the following:\nclick a table item and notice the change in flamegraph navigate the flamegraph by clicking on a stack frame to zoom in, and a parent frame to zoom out add a search term like splunk or jetty to highlight some matching stack frames Let’s have a closer look at the stack trace, starting with the DoorGame.startNew method (since we already know that it’s the slowest part of the request)\ncom.splunk.profiling.workshop.DoorGame.startNew(DoorGame.java:24) com.splunk.profiling.workshop.UserData.loadUserData(UserData.java:33) com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1168) com.mysql.cj.NativeSession.execSQL(NativeSession.java:655) com.mysql.cj.protocol.a.NativeProtocol.sendQueryString(NativeProtocol.java:998) com.mysql.cj.protocol.a.NativeProtocol.sendQueryPacket(NativeProtocol.java:1065) com.mysql.cj.protocol.a.NativeProtocol.readAllResults(NativeProtocol.java:1715) com.mysql.cj.protocol.a.NativeProtocol.read(NativeProtocol.java:1661) com.mysql.cj.protocol.a.TextResultsetReader.read(TextResultsetReader.java:48) com.mysql.cj.protocol.a.TextResultsetReader.read(TextResultsetReader.java:87) com.mysql.cj.protocol.a.NativeProtocol.read(NativeProtocol.java:1648) com.mysql.cj.protocol.a.ResultsetRowReader.read(ResultsetRowReader.java:42) com.mysql.cj.protocol.a.ResultsetRowReader.read(ResultsetRowReader.java:75) com.mysql.cj.protocol.a.MultiPacketReader.readMessage(MultiPacketReader.java:44) com.mysql.cj.protocol.a.MultiPacketReader.readMessage(MultiPacketReader.java:66) com.mysql.cj.protocol.a.TimeTrackingPacketReader.readMessage(TimeTrackingPacketReader.java:41) com.mysql.cj.protocol.a.TimeTrackingPacketReader.readMessage(TimeTrackingPacketReader.java:62) com.mysql.cj.protocol.a.SimplePacketReader.readMessage(SimplePacketReader.java:45) com.mysql.cj.protocol.a.SimplePacketReader.readMessage(SimplePacketReader.java:102) com.mysql.cj.protocol.a.SimplePacketReader.readMessageLocal(SimplePacketReader.java:137) com.mysql.cj.protocol.FullReadInputStream.readFully(FullReadInputStream.java:64) java.io.FilterInputStream.read(Unknown Source:0) sun.security.ssl.SSLSocketImpl$AppInputStream.read(Unknown Source:0) sun.security.ssl.SSLSocketImpl.readApplicationRecord(Unknown Source:0) sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(Unknown Source:0) sun.security.ssl.SSLSocketInputRecord.readHeader(Unknown Source:0) sun.security.ssl.SSLSocketInputRecord.read(Unknown Source:0) java.net.SocketInputStream.read(Unknown Source:0) java.net.SocketInputStream.read(Unknown Source:0) java.lang.ThreadLocal.get(Unknown Source:0) We can interpret the stack trace as follows:\nWhen starting a new Door Game, a call is made to load user data. This results in executing a SQL query to load the user data (which is related to the slow SQL query we saw earlier). We then see calls to read data in from the database. So, what does this all mean? It means that our application startup is slow since it’s spending time loading user data. In fact, the profiler has told us the exact line of code where this happens:\ncom.splunk.profiling.workshop.UserData.loadUserData(UserData.java:33) Let’s open the corresponding source file (./doorgame/src/main/java/com/splunk/profiling/workshop/UserData.java) and look at this code in more detail:\npublic class UserData { static final String DB_URL = \"jdbc:mysql://mysql/DoorGameDB\"; static final String USER = \"root\"; static final String PASS = System.getenv(\"MYSQL_ROOT_PASSWORD\"); static final String SELECT_QUERY = \"select * FROM DoorGameDB.Users, DoorGameDB.Organizations\"; HashMap\u003cString, User\u003e users; public UserData() { users = new HashMap\u003cString, User\u003e(); } public void loadUserData() { // Load user data from the database and store it in a map Connection conn = null; Statement stmt = null; ResultSet rs = null; try{ conn = DriverManager.getConnection(DB_URL, USER, PASS); stmt = conn.createStatement(); rs = stmt.executeQuery(SELECT_QUERY); while (rs.next()) { User user = new User(rs.getString(\"UserId\"), rs.getString(\"FirstName\"), rs.getString(\"LastName\")); users.put(rs.getString(\"UserId\"), user); } Here we can see the application logic in action. It establishes a connection to the database, then executes the SQL query we saw earlier:\nselect * FROM DoorGameDB.Users, DoorGameDB.Organizations It then loops through each of the results, and loads each user into a HashMap object, which is a collection of User objects.\nWe have a good understanding of why the game startup sequence is so slow, but how do we fix it?\nFor more clues, let’s have a look at the other part of AlwaysOn Profiling: memory profiling. To do this, click on the Memory tab in AlwaysOn profiling:\nAt the top of this view, we can see how much heap memory our application is using, the heap memory allocation rate, and garbage collection activity.\nWe can see that our application is using about 400 MB out of the max 1 GB heap size, which seems excessive for such a simple application. We can also see that some garbage collection occurred, which caused our application to pause (and probably annoyed those wanting to play the Game Door).\nAt the bottom of the screen, which can see which methods in our Java application code are associated with the most heap memory usage. Click on the first item in the list to show the Memory Allocation Stack Traces associated with the java.util.Arrays.copyOf method specifically:\nWith help from the profiler, we can see that the loadUserData method not only consumes excessive CPU time, but it also consumes excessive memory when storing the user data in the HashMap collection object.\nWhat did we accomplish? We’ve come a long way already!\nWe learned how to enable the profiler in the Splunk OpenTelemetry Java instrumentation agent. We learned how to verify in the agent output that the profiler is enabled. We have explored several profiling related workflows in APM: How to navigate to AlwaysOn Profiling from the troubleshooting view How to explore the flamegraph and method call duration table through navigation and filtering How to identify when a span has sampled call stacks associated with it How to explore heap utilization and garbage collection activity How to view memory allocation stack traces for a particular method In the next section, we’ll apply a fix to our application to resolve the slow startup performance.",
    "description": "Let’s learn how to enable the memory and CPU profilers, verify their operation, and use the results in Splunk Observability Cloud to find out why our application startup is slow.\nUpdate the application configuration We will need to pass additional configuration arguments to the Splunk OpenTelemetry Java agent in order to enable both profilers. The configuration is documented here in detail, but for now we just need the following settings:\nSPLUNK_PROFILER_ENABLED=\"true\" SPLUNK_PROFILER_MEMORY_ENABLED=\"true\" Since our application is deployed in Kubernetes, we can update the Kubernetes manifest file to set these environment variables. Open the doorgame/doorgame.yaml file for editing, and ensure the values of the following environment variables are set to “true”:",
    "tags": [],
    "title": "Enable AlwaysOn Profiling",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/profiling/3-enable-profiling/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Self-Service Observability",
    "content": "Introduction Managing Observability costs is a huge challenge. Users can send in data at will, change data, turn on new integrations, and incur costs that are hard to detect before the surprise overage at the end of the month. Exactly the kind of thing that’s likely to happen to this new team as they set up their new application.\nLet’s walk through some ways we can mitigate those challenges, while still making the platform easy for teams to send the data they need to.\nTokens In the last section we selected our workshop token to use, which is very similar to the token we would be provided as a team.\nLet’s navigate to Settings \u003e Access Tokens. As admins we will be able to see all of the tokens in the environment. However we can also limit who can see tokens – either a specific set of users or teams, or everyone.\nStill making the product\nA way to measure and monitor utilization by team A way to mitigate unexpected utilization A license model that is forgiving Let’s walk through how Splunk delivers each of these.",
    "description": "Introduction Managing Observability costs is a huge challenge. Users can send in data at will, change data, turn on new integrations, and incur costs that are hard to detect before the surprise overage at the end of the month. Exactly the kind of thing that’s likely to happen to this new team as they set up their new application.\nLet’s walk through some ways we can mitigate those challenges, while still making the platform easy for teams to send the data they need to.",
    "tags": [],
    "title": "Manage Costs",
    "uri": "/observability-workshop/v6.5/en/scenarios/self-service-observability/3-manage-costs/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop \u003e 7. Use Tags for Monitoring",
    "content": "We can now use the created Monitoring MetricSet together with Service Level Objectives a similar way we used them with dashboards and detectors/alerts before. For that we want to be clear about some key concepts:\nKey Concepts of Service Level Monitoring (skip if you know this)\nConcept Definition Examples Service level indicator (SLI) An SLI is a quantitative measurement showing some health of a service, expressed as a metric or combination of metrics. Availability SLI: Proportion of requests that resulted in a successful response\nPerformance SLI: Proportion of requests that loaded in \u003c 100 ms Service level objective (SLO) An SLO defines a target for an SLI and a compliance period over which that target must be met. An SLO contains 3 elements: an SLI, a target, and a compliance period. Compliance periods can be calendar, such as monthly, or rolling, such as past 30 days. Availability SLI over a calendar period: Our service must respond successfully to 95% of requests in a month\nPerformance SLI over a rolling period: Our service must respond to 99% of requests in \u003c 100 ms over a 7-day period Service level agreement (SLA) An SLA is a contractual agreement that indicates service levels your users can expect from your organization. If an SLA is not met, there can be financial consequences. A customer service SLA indicates that 90% of support requests received on a normal support day must have a response within 6 hours. Error budget A measurement of how your SLI performs relative to your SLO over a period of time. Error budget measures the difference between actual and desired performance. It determines how unreliable your service might be during this period and serves as a signal when you need to take corrective action. Our service can respond to 1% of requests in \u003e100 ms over a 7 day period. Burn rate A unitless measurement of how quickly a service consumes the error budget during the compliance window of the SLO. Burn rate makes the SLO and error budget actionable, showing service owners when a current incident is serious enough to page an on-call responder. For an SLO with a 30-day compliance window, a constant burn rate of 1 means your error budget is used up in exactly 30 days. Creating a new Service Level Objective There is an easy to follow wizard to create a new Service Level Objective (SLO). In the left navigation just follow the link “Detectors \u0026 SLOs”. From there select the third tab “SLOs” and click the blue button to the right that says “Create SLO”.\nThe wizard guides you through some easy steps. And if everything during the previous steps worked out, you will have no problems here. ;)\nIn our case we want to use Service \u0026 endpoint as our Metric type instead of Custom metric. We filter the Environment down to the environment that we are using during this workshop (i.e. tagging-workshop-yourname) and select the creditcheckservice from the Service and endpoint list. Our Indicator type for this workshop will be Request latency and not Request success.\nNow we can select our Filters. Since we are using the Request latency as the Indicator type and that is a metric of the APM Service, we can filter on credit.score.category. Feel free to try out what happens, when you set the Indicator type to Request success.\nToday we are only interested in our exceptional credit scores. So please select that as the filter.\nIn the next step we define the objective we want to reach. For the Request latency type, we define the Target (%), the Latency (ms) and the Compliance Window. Please set these to 99, 100 and Last 7 days. This will give us a good idea what we are achieving already.\nHere we will already be in shock or play around with the numbers to make it not so scary. Feel free to play around with the numbers to see how well we achieve the objective and how much we have left to burn.\nThe third step gives us the chance to alert (aka annoy) people who should be aware about these SLOs to initiate countermeasures. These “people” can also be mechanism like ITSM systems or webhooks to initiate automatic remediation steps.\nActivate all categories you want to alert on and add recipients to the different alerts.\nThe next step is only the naming for this SLO. Have your own naming convention ready for this. In our case we would just name it creditchceckservice:score:exceptional:YOURNAME and click the Create-button BUT you can also just cancel the wizard by clicking anything in the left navigation and confirming to Discard changes.\nAnd with that we have (nearly) successfully created an SLO including the alerting in case we might miss or goals.",
    "description": "We can now use the created Monitoring MetricSet together with Service Level Objectives a similar way we used them with dashboards and detectors/alerts before. For that we want to be clear about some key concepts:\nKey Concepts of Service Level Monitoring (skip if you know this)\nConcept Definition Examples Service level indicator (SLI) An SLI is a quantitative measurement showing some health of a service, expressed as a metric or combination of metrics. Availability SLI: Proportion of requests that resulted in a successful response\nPerformance SLI: Proportion of requests that loaded in \u003c 100 ms Service level objective (SLO) An SLO defines a target for an SLI and a compliance period over which that target must be met. An SLO contains 3 elements: an SLI, a target, and a compliance period. Compliance periods can be calendar, such as monthly, or rolling, such as past 30 days. Availability SLI over a calendar period: Our service must respond successfully to 95% of requests in a month\nPerformance SLI over a rolling period: Our service must respond to 99% of requests in \u003c 100 ms over a 7-day period Service level agreement (SLA) An SLA is a contractual agreement that indicates service levels your users can expect from your organization. If an SLA is not met, there can be financial consequences. A customer service SLA indicates that 90% of support requests received on a normal support day must have a response within 6 hours. Error budget A measurement of how your SLI performs relative to your SLO over a period of time. Error budget measures the difference between actual and desired performance. It determines how unreliable your service might be during this period and serves as a signal when you need to take corrective action. Our service can respond to 1% of requests in \u003e100 ms over a 7 day period. Burn rate A unitless measurement of how quickly a service consumes the error budget during the compliance window of the SLO. Burn rate makes the SLO and error budget actionable, showing service owners when a current incident is serious enough to page an on-call responder. For an SLO with a 30-day compliance window, a constant burn rate of 1 means your error budget is used up in exactly 30 days. Creating a new Service Level Objective There is an easy to follow wizard to create a new Service Level Objective (SLO). In the left navigation just follow the link “Detectors \u0026 SLOs”. From there select the third tab “SLOs” and click the blue button to the right that says “Create SLO”.",
    "tags": [],
    "title": "Use Tags with Service Level Objectives",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/7-alerting-dashboards-slos/3-slos/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "Splunk APM provides a NoSample end-to-end visibility of every service and its dependency to solve problems quicker across monoliths and microservices. Teams can immediately detect problems from new deployments, confidently troubleshoot by scoping and isolating the source of an issue, and optimize service performance by understanding how back-end services impact end users and business workflows.\nReal-time monitoring and alerting: Splunk provides out-of-the-box service dashboards and automatically detects and alerts on RED metrics (rate, error and duration) when there is a sudden change. Dynamic telemetry maps: Easily visualize service performance in modern production environments in real-time. End-to-end visibility of service performance from infrastructure, applications, end users, and all dependencies helps quickly scope new issues and troubleshoot more effectively.\nIntelligent tagging and analysis: View all tags from your business, infrastructure and applications in one place to easily compare new trends in latency or errors to their specific tag values.\nAI-directed troubleshooting identifies the most impactful issues: Instead of manually digging through individual dashboards, isolate problems more efficiently. Automatically identify anomalies and the sources of errors that impact services and customers the most.\nComplete distributed tracing analyses every transaction: Identify problems in your cloud-native environment more effectively. Splunk distributed tracing visualizes and correlates every transaction from the back-end and front-end in context with your infrastructure, business workflows and applications.\nFull stack correlation: Within Splunk Observability, APM links traces, metrics, logs and profiling together to easily understand the performance of every component and its dependency across your stack.\nMonitor database query performance: Easily identify how slow and high execution queries from SQL and NoSQL databases impact your services, endpoints and business workflows — no instrumentation required.",
    "description": "Splunk APM provides a NoSample end-to-end visibility of every service and its dependency to solve problems quicker across monoliths and microservices. Teams can immediately detect problems from new deployments, confidently troubleshoot by scoping and isolating the source of an issue, and optimize service performance by understanding how back-end services impact end users and business workflows.\nReal-time monitoring and alerting: Splunk provides out-of-the-box service dashboards and automatically detects and alerts on RED metrics (rate, error and duration) when there is a sudden change. Dynamic telemetry maps: Easily visualize service performance in modern production environments in real-time. End-to-end visibility of service performance from infrastructure, applications, end users, and all dependencies helps quickly scope new issues and troubleshoot more effectively.\nIntelligent tagging and analysis: View all tags from your business, infrastructure and applications in one place to easily compare new trends in latency or errors to their specific tag values.\nAI-directed troubleshooting identifies the most impactful issues: Instead of manually digging through individual dashboards, isolate problems more efficiently. Automatically identify anomalies and the sources of errors that impact services and customers the most.\nComplete distributed tracing analyses every transaction: Identify problems in your cloud-native environment more effectively. Splunk distributed tracing visualizes and correlates every transaction from the back-end and front-end in context with your infrastructure, business workflows and applications.\nFull stack correlation: Within Splunk Observability, APM links traces, metrics, logs and profiling together to easily understand the performance of every component and its dependency across your stack.\nMonitor database query performance: Easily identify how slow and high execution queries from SQL and NoSQL databases impact your services, endpoints and business workflows — no instrumentation required.",
    "tags": [],
    "title": "Application Performance Monitoring Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/3-apm-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 5. APM Features",
    "content": "While we examine our spans, let’s look at several out-of-the-box features that you get without code modifications when using automatic discovery and configuration on top of tracing:\nFirst, in the Waterfall Pane, make sure the customers-service:SELECT petclinic or similar span is selected as shown in the screenshot below:\nThe basic latency information is shown as a bar for the instrumented function or call. In our example above, it took 17.8 Milliseconds. Several similar Spans (1) are only visible if the span is repeated multiple times. In this case, there are 10 repeats in our example. You can show/hide them all by clicking on the 10x and all spans will show in order. Inferred Services: Calls made to external systems that are not instrumented show up as a grey ‘inferred’ span. The Inferred Service or span in our example is a call to the Mysql Database mysql:petclinic SELECT petclinic (2) as shown above. Span Tags: In the Tag Pane, we see standard tags produced by the automatic discovery and configuration. In this case, the span is calling a Database, so it includes the db.statement tag (3). This tag will hold the DB query statement and is used by the Database call performed during this span. This will be used by the DB-Query Performance feature. We look at DB-Query Performance in the next section. Always-on Profiling: IF the system is configured to capture Profiling data during a Span life cycle. It will show the number of Call Stacks captured in the Spans timeline. In our example above, we see 18 Call Stacks for the customer-service:GET /owners Span. (4) We will look at Profiling in the next section.",
    "description": "While we examine our spans, let’s look at several out-of-the-box features that you get without code modifications when using automatic discovery and configuration on top of tracing:\nFirst, in the Waterfall Pane, make sure the customers-service:SELECT petclinic or similar span is selected as shown in the screenshot below:\nThe basic latency information is shown as a bar for the instrumented function or call. In our example above, it took 17.8 Milliseconds. Several similar Spans (1) are only visible if the span is repeated multiple times. In this case, there are 10 repeats in our example. You can show/hide them all by clicking on the 10x and all spans will show in order. Inferred Services: Calls made to external systems that are not instrumented show up as a grey ‘inferred’ span. The Inferred Service or span in our example is a call to the Mysql Database mysql:petclinic SELECT petclinic (2) as shown above. Span Tags: In the Tag Pane, we see standard tags produced by the automatic discovery and configuration. In this case, the span is calling a Database, so it includes the db.statement tag (3). This tag will hold the DB query statement and is used by the Database call performed during this span. This will be used by the DB-Query Performance feature. We look at DB-Query Performance in the next section. Always-on Profiling: IF the system is configured to capture Profiling data during a Span life cycle. It will show the number of Call Stacks captured in the Spans timeline. In our example above, we see 18 Call Stacks for the customer-service:GET /owners Span. (4) We will look at Profiling in the next section.",
    "tags": [],
    "title": "APM Span",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/5-traces/3-spans/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 4. Splunk APM",
    "content": "Exercise To view the tags for the wire-transfer-service click on the wire-transfer-service and then click on Tag Spotlight in the right-hand side functions pane (you may need to scroll down depending upon your screen resolution).* Once in Tag Spotlight ensure the toggle Show tags with no values is off. The views in Tag Spotlight are configurable for both the chart and cards. The view defaults to Requests \u0026 Errors.\nIt is also possible to configure which tag metrics are displayed in the cards. It is possible to select any combinations of:\nRequests Errors Root cause errors P50 Latency P90 Latency P99 Latency Also ensure that the Show tags with no values toggle is unchecked.\nScroll through the cards and get familiar with the tags provided by the wire-transfer-service’s telemetry.\nExercise ​ Question Answer Which card exposes the tag that identifies what the problem is?\nThe version card. The number of requests against v350.10 matches the number of errors i.e. 100%\nNow that we have identified the version of the wire-transfer-service that is causing the issue, let’s see if we can find out more information about the error. Press the back button on your browser to get back to the Service Map.",
    "description": "Exercise To view the tags for the wire-transfer-service click on the wire-transfer-service and then click on Tag Spotlight in the right-hand side functions pane (you may need to scroll down depending upon your screen resolution).* Once in Tag Spotlight ensure the toggle Show tags with no values is off. The views in Tag Spotlight are configurable for both the chart and cards. The view defaults to Requests \u0026 Errors.",
    "tags": [],
    "title": "3. APM Tag Spotlight",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/4-apm/3-apm-tag-spotlight/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6. Splunk APM",
    "content": "Exercise To view the tags for the paymentservice click on the paymentservice and then click on Tag Spotlight in the right-hand side functions pane (you may need to scroll down depending upon your screen resolution).* Once in Tag Spotlight ensure the toggle Show tags with no values is off. The views in Tag Spotlight are configurable for both the chart and cards. The view defaults to Requests \u0026 Errors.\nIt is also possible to configure which tag metrics are displayed in the cards. It is possible to select any combinations of:\nRequests Errors Root cause errors P50 Latency P90 Latency P99 Latency Also ensure that the Show tags with no values toggle is unchecked.\nExercise ​ Question Answer Which card exposes the tag that identifies what the problem is?\nThe version card. The number of requests against v350.10 matches the number of errors i.e. 100%\nNow that we have identified the version of the paymentservice that is causing the issue, let’s see if we can find out more information about the error. Click on ← Tag Spotlight at the top of the page to get back to the Service Map.",
    "description": "Exercise To view the tags for the paymentservice click on the paymentservice and then click on Tag Spotlight in the right-hand side functions pane (you may need to scroll down depending upon your screen resolution).* Once in Tag Spotlight ensure the toggle Show tags with no values is off. The views in Tag Spotlight are configurable for both the chart and cards. The view defaults to Requests \u0026 Errors.",
    "tags": [],
    "title": "3. APM Tag Spotlight",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/6-apm/3-apm-tag-spotlight/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Monolith Workshop",
    "content": "You can now start the application with the following command. Notice that we are passing the mysql profile to the application, this will tell the application to use the MySQL database we started earlier. We are also setting the otel.service.name and otel.resource.attributes to a logical names using the instance name. These will also be used in the UI for filtering:\njava \\ -Dserver.port=8083 \\ -Dotel.service.name=$INSTANCE-petclinic-service \\ -Dotel.resource.attributes=deployment.environment=$INSTANCE-petclinic-env \\ -jar target/spring-petclinic-*.jar --spring.profiles.active=mysql You can validate the application is running by visiting http://\u003cIP_ADDRESS\u003e:8083 (replace \u003cIP_ADDRESS\u003e with the IP address you obtained earlier).\nWhen we installed the collector we configured it to enable AlwaysOn Profiling and Metrics. This means that the collector will automatically generate CPU and Memory profiles for the application and send them to Splunk Observability Cloud.\nWhen you start the PetClinic application you will see the collector automatically detect the application and instrument it for traces and profiling.\n​ Example output Picked up JAVA_TOOL_OPTIONS: -javaagent:/usr/lib/splunk-instrumentation/splunk-otel-javaagent.jar OpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended [otel.javaagent 2024-08-20 11:35:58:970 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: splunk-2.6.0-otel-2.6.0 [otel.javaagent 2024-08-20 11:35:59:730 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - ----------------------- [otel.javaagent 2024-08-20 11:35:59:730 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - Profiler configuration: [otel.javaagent 2024-08-20 11:35:59:730 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.enabled : true [otel.javaagent 2024-08-20 11:35:59:731 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.directory : /tmp [otel.javaagent 2024-08-20 11:35:59:731 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.recording.duration : 20s [otel.javaagent 2024-08-20 11:35:59:731 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.keep-files : false [otel.javaagent 2024-08-20 11:35:59:732 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.logs-endpoint : null [otel.javaagent 2024-08-20 11:35:59:732 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - otel.exporter.otlp.endpoint : null [otel.javaagent 2024-08-20 11:35:59:732 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.memory.enabled : true [otel.javaagent 2024-08-20 11:35:59:732 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.memory.event.rate : 150/s [otel.javaagent 2024-08-20 11:35:59:732 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.call.stack.interval : PT10S [otel.javaagent 2024-08-20 11:35:59:733 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.include.internal.stacks : false [otel.javaagent 2024-08-20 11:35:59:733 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - splunk.profiler.tracing.stacks.only : false [otel.javaagent 2024-08-20 11:35:59:733 +0000] [main] INFO com.splunk.opentelemetry.profiler.ConfigurationLogger - ----------------------- [otel.javaagent 2024-08-20 11:35:59:733 +0000] [main] INFO com.splunk.opentelemetry.profiler.JfrActivator - Profiler is active. You can now visit the Splunk APM UI and examine the application components, traces, profiling, DB Query performance and metrics. From the left-hand menu click APM and then click the Environment dropdown and select your environment e.g. \u003cINSTANCE\u003e-petclinic (where\u003cINSTANCE\u003e is replaced with the value you noted down earlier).\nOnce your validation is complete you can stop the application by pressing Ctrl-c.\nResource attributes can be added to every reported span. For example version=0.314. A comma-separated list of resource attributes can also be defined e.g. key1=val1,key2=val2.\nLet’s launch the PetClinic again using new resource attributes. Note, that adding resource attributes to the run command will override what was defined when we installed the collector. Let’s add a new resource attribute version=0.314:\njava \\ -Dserver.port=8083 \\ -Dotel.service.name=$INSTANCE-petclinic-service \\ -Dotel.resource.attributes=deployment.environment=$INSTANCE-petclinic-env,version=0.314 \\ -jar target/spring-petclinic-*.jar --spring.profiles.active=mysql Back in the Splunk APM UI we can drill down on a recent trace and see the new version attribute in a span.",
    "description": "You can now start the application with the following command. Notice that we are passing the mysql profile to the application, this will tell the application to use the MySQL database we started earlier. We are also setting the otel.service.name and otel.resource.attributes to a logical names using the instance name. These will also be used in the UI for filtering:\njava \\ -Dserver.port=8083 \\ -Dotel.service.name=$INSTANCE-petclinic-service \\ -Dotel.resource.attributes=deployment.environment=$INSTANCE-petclinic-env \\ -jar target/spring-petclinic-*.jar --spring.profiles.active=mysql You can validate the application is running by visiting http://\u003cIP_ADDRESS\u003e:8083 (replace \u003cIP_ADDRESS\u003e with the IP address you obtained earlier).",
    "tags": [],
    "title": "Automatic discovery and configuration for Java",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/1-petclinic-monolith/3-auto-discovery/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics",
    "content": "We have started testing our endpoints, now let’s test the front end browser experience.\nStarting with a single page browser test will let us capture how first- and third-party resources impact how our end users experience our browser-based site. It also allows us to start to understand our user experience metrics before introducing the complexity of multiple steps in one test.\nA page where your users commonly “land” is a good choice to start with a single page test. This could be your site homepage, a section main page, or any other high-traffic URL that is important to you and your end users.\nClick Create new test and select Browser test Include your team name and initials in the test name. Add to the Name and Custom properties to describe the scope of the test (like Desktop for device type). Then click + Edit steps Change the transaction label (top left) and step name (on the right) to something readable that describes the step. Add the URL you’d like to test. Your workshop instructor can provide you with a URL as well. In the below example, the transaction is “Home” and the step name is “Go to homepage”. To validate the test, change the location as needed and click Try now. See the docs for more information on the try now feature. Wait for the test validation to complete. If the test validation failed, double check your URL and test location and try again. With Try now you can see what the result of the test will be if it were saved and run as-is. Click \u003c Return to test to continue the configuration. Edit the locations you want to use, keeping in mind any regional rules you have for your site.\nYou can edit the Device and Frequency or leave them at their default values for now. Click Submit at the bottom of the form to save the test and start running it.\nBonus Exercise Have a few spare seconds? Copy this test and change just the title and device type, and save. Now you have visibility into the end user experience on another device and connection speed!\nWhile our Synthetic tests are running, let’s see how RUM is instrumented to start getting data from our real users.",
    "description": "We have started testing our endpoints, now let’s test the front end browser experience.\nStarting with a single page browser test will let us capture how first- and third-party resources impact how our end users experience our browser-based site. It also allows us to start to understand our user experience metrics before introducing the complexity of multiple steps in one test.\nA page where your users commonly “land” is a good choice to start with a single page test. This could be your site homepage, a section main page, or any other high-traffic URL that is important to you and your end users.",
    "tags": [],
    "title": "Single Page Browser Test",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/3-browser/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop",
    "content": "Please proceed to one of the subsections for Java or Python. Ask your instructor for the one used during the workshop!",
    "description": "Please proceed to one of the subsections for Java or Python. Ask your instructor for the one used during the workshop!",
    "tags": [],
    "title": "Capture Tags with OpenTelemetry",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/3-capture-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Detectors",
    "content": "With our custom dashboard charts, we can create detectors focussed directly on the data and conditions we care about. In building our charts, we also built signals that can trigger alerts.\nStatic detectors For many KPIs, we have a static value in mind as a threshold.\nIn your custom End User Experience dashboard, go to the “LCP - all tests” chart.\nClick the bell icon on the top right of the chart, and select “New detector from chart” Change the detector name to include your team name and initials, and adjust the alert details. Change the threshold to 2500 or 4000 and see how the alert noise preview changes. Change the severity, and add yourself as a recipient before you save this detector. Click Activate. Advanced: Dynamic detectors Sometimes we have metrics that vary naturally, so we want to create a more dynamic detector that isn’t limited by the static threshold we decide in the moment.\nTo create dynamic detectors on your chart, click the link to the “old” detector wizard. Change the detector name to include your team name and initials, and Click Create alert rule Confirm the signal looks correct and proceed to Alert condition. Select the “Sudden Change” condition and proceed to Alert settings Play with the settings and see how the estimated alert noise is previewed in the chart above. Tune the settings, and change the advanced settings if you’d like, before proceeding to the Alert message. Customize the severity, runbook URL, any tips, and message payload before proceeding to add recipients. For the sake of this workshop, only add your own email address as recipient. This is where you would add other options like webhooks, ticketing systems, and Slack channels if it’s in your real environment. Finally, confirm the detector name before clicking Activate Alert Rule",
    "description": "With our custom dashboard charts, we can create detectors focussed directly on the data and conditions we care about. In building our charts, we also built signals that can trigger alerts.\nStatic detectors For many KPIs, we have a static value in mind as a threshold.\nIn your custom End User Experience dashboard, go to the “LCP - all tests” chart.\nClick the bell icon on the top right of the chart, and select “New detector from chart”",
    "tags": [],
    "title": "Chart Detectors",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/5-detectors/3-chart-detector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e GDI (OTel \u0026 UF)",
    "content": "Code to Kubernetes - Python Objective: Understand activities to instrument a python application and run it on Kubernetes.\nVerify the code Containerize the app Deploy the container in Kubernetes Note: these steps do not involve Splunk\nDuration: 15 Minutes\n1. Verify the code - Review service Navigate to the review directory\ncd /home/splunk/realtime_enrichment/flask_apps/review/ Inspect review.py (realtime_enrichment/flask_apps/review)\ncat review.py from flask import Flask, jsonify import random import subprocess review = Flask(__name__) num_reviews = 8635403 num_reviews = 100000 reviews_file = '/var/appdata/yelp_academic_dataset_review.json' @review.route('/') def hello_world(): return jsonify(message='Hello, you want to hit /get_review. We have ' + str(num_reviews) + ' reviews!') @review.route('/get_review') def get_review(): random_review_int = str(random.randint(1,num_reviews)) line_num = random_review_int + 'q;d' command = [\"sed\", line_num, reviews_file] # sed \"7997242q;d\" \u003cfile\u003e random_review = subprocess.run(command, stdout=subprocess.PIPE, text=True) return random_review.stdout if __name__ == \"__main__\": review.run(host ='0.0.0.0', port = 5000, debug = True) Inspect requirements.txt\nFlask==2.0.2 Create a virtual environment and Install the necessary python packages\ncd /home/splunk/realtime_enrichment/workshop/flask_apps_start/review/ pip freeze #note output pip install -r requirements.txt pip freeze #note output Start the REVIEW service. Note: You can stop the app with control+C\npython3 review.py * Serving Flask app 'review' (lazy loading) * Environment: production ...snip... * Running on http://10.160.145.246:5000/ (Press CTRL+C to quit) * Restarting with stat 127.0.0.1 - - [17/May/2022 22:46:38] \"GET / HTTP/1.1\" 200 - 127.0.0.1 - - [17/May/2022 22:47:02] \"GET /get_review HTTP/1.1\" 200 - 127.0.0.1 - - [17/May/2022 22:47:58] \"GET /get_review HTTP/1.1\" 200 - Verify that the service is working\nOpen a new terminal and ssh into your ec2 instance. Then use the curl command in your terminal. curl http://localhost:5000 Or hit the URL http://{Your_EC2_IP_address}:5000 and http://{Your_EC2_IP_address}:5000/get_review with a browser curl localhost:5000 { \"message\": \"Hello, you want to hit /get_review. We have 100000 reviews!\" } curl localhost:5000/get_review {\"review_id\":\"NjbiESXotcEdsyTc4EM3fg\",\"user_id\":\"PR9LAM19rCM_HQiEm5OP5w\",\"business_id\":\"UAtX7xmIfdd1W2Pebf6NWg\",\"stars\":3.0,\"useful\":0,\"funny\":0,\"cool\":0,\"text\":\"-If you're into cheap beer (pitcher of bud-light for $7) decent wings and a good time, this is the place for you. Its generally very packed after work hours and weekends. Don't expect cocktails. \\n\\n-You run into a lot of sketchy characters here sometimes but for the most part if you're chilling with friends its not that bad. \\n\\n-Friendly bouncer and bartenders.\",\"date\":\"2016-04-12 20:23:24\"} Workshop Question What does this application do? Do you see the yelp dataset being used? Why did the output of pip freeze differ each time you ran it? Which port is the REVIEW app listening on? Can other python apps use this same port? 2. Create a REVIEW container To create a container image, you need to create a Dockerfile, run docker build to build the image referencing the Docker file and push it up to a remote repository so it can be pulled by other sources.\nCreate a Dockerfile Creating a Dockerfile typically requires you to consider the following: Identify an appropriate container image ubuntu vs. python vs. alpine/slim ubuntu - overkill, large image size, wasted resources when running in K8 this is a python app, so pick an image that is optimized for it avoid alpine for python Order matters you’re building layers. re-use the layers as much as possible have items that change often towards the end Other Best practices for writing Dockerfiles Dockerfile for review\nFROM python:3.10-slim WORKDIR /app COPY requirements.txt /app RUN pip install -r requirements.txt COPY ./review.py /app EXPOSE 5000 CMD [ \"python\", \"review.py\" ] Create a container image (locally) Run ‘docker build’ to build a local container image referencing the Dockerfile\n​ docker build docker build Output docker build -f Dockerfile -t localhost:8000/review:0.01 . [+] Building 35.5s (11/11) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s ...snip... =\u003e [3/5] COPY requirements.txt /app 0.0s =\u003e [4/5] RUN pip install -r requirements.txt 4.6s =\u003e [5/5] COPY ./review.py /app 0.0s =\u003e exporting to image 0.2s =\u003e =\u003e exporting layers 0.2s =\u003e =\u003e writing image sha256:61da27081372723363d0425e0ceb34bbad6e483e698c6fe439c5 0.0s =\u003e =\u003e naming to docker.io/localhost:8000/review:0.1 0.0 Push the container image into a container repository Run ‘docker push’ to place a copy of the REVIEW container to a remote location\n​ docker push docker push Output docker push localhost:8000/review:0.01 The push refers to repository [docker.io/localhost:8000/review] 02c36dfb4867: Pushed ...snip... fd95118eade9: Pushed 0.1: digest: sha256:3651f740abe5635af95d07acd6bcf814e4d025fcc1d9e4af9dee023a9b286f38 size: 2202 Verify that the image is in Docker Hub. The same info can be found in Docker Desktop\n​ get catalog get catalog Output curl -s http://localhost:8000/v2/_catalog {\"repositories\":[\"review\"]} 3. Run REVIEW in Kubernetes Create K8 deployment yaml file for the REVIEW app\nReference: Creating a Deployment\nreview.deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: review labels: app: review spec: replicas: 1 selector: matchLabels: app: review template: metadata: labels: app: review spec: imagePullSecrets: - name: regcred containers: - image: localhost:8000/review:0.01 name: review volumeMounts: - mountPath: /var/appdata name: appdata volumes: - name: appdata hostPath: path: /var/appdata Notes regarding review.deployment.yaml:\nlabels - K8 uses labels and selectors to tag and identify resources In the next step, we’ll create a service and associate it to this deployment using the label replicas = 1 K8 allows you to scale your deployments horizontally We’ll leverage this later to add load and increase our ingestion rate regcred provides this deployment with the ability to access your dockerhub credentials which is necessary to pull the container image. The volume definition and volumemount make the yelp dataset visible to the container Create a K8 service yaml file for the review app.\nReference: Creating a service:\nreview.service.yaml\napiVersion: v1 kind: Service metadata: name: review spec: type: NodePort selector: app: review ports: - port: 5000 targetPort: 5000 nodePort: 30000 Notes about review.service.yaml:\nthe selector associates this service to pods with the label app with the value being review the review service exposes the review pods as a network service other pods can now ping ‘review’ and they will hit a review pod. a pod would get a review if it ran curl http://review:5000 NodePort service the service is accessible to the K8 host by the nodePort, 30000 Another machine that has this can get a review if it ran curl http://\u003ck8 host ip\u003e:30000 Apply the review deployment and service\nkubectl apply -f review.service.yaml -f review.deployment.yaml Verify that the deployment and services are running:\n​ kubectl get deployments kubectl get deployments output kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE review 1/1 1 1 19h ​ kubectl get services kubectl get services output kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE review NodePort 10.43.175.21 \u003cnone\u003e 5000:30000/TCP 154d ​ curl localhost curl localhost Output curl localhost:30000 { \"message\": \"Hello, you want to hit /get_review. We have 100000 reviews!\" } ​ get review get review Output curl localhost:30000/get_review {\"review_id\":\"Vv9rHtfBrFc-1M1DHRKN9Q\",\"user_id\":\"EaNqIwKkM7p1bkraKotqrg\",\"business_id\":\"TA1KUSCu8GkWP9w0rmElxw\",\"stars\":3.0,\"useful\":1,\"funny\":0,\"cool\":0,\"text\":\"This is the first time I've actually written a review for Flip, but I've probably been here about 10 times. \\n\\nThis used to be where I would take out of town guests who wanted a good, casual, and relatively inexpensive meal. \\n\\nI hadn't been for a while, so after a long day in midtown, we decided to head to Flip. \\n\\nWe had the fried pickles, onion rings, the gyro burger, their special burger, and split a nutella milkshake. I have tasted all of the items we ordered previously (with the exception of the special) and have been blown away with how good they were. My guy had the special which was definitely good, so no complaints there. The onion rings and the fried pickles were greasier than expected. Though I've thought they were delicious in the past, I probably wouldn't order either again. The gyro burger was good, but I could have used a little more sauce. It almost tasted like all of the ingredients didn't entirely fit together. Something was definitely off. It was a friday night and they weren't insanely busy, so I'm not sure I would attribute it to the staff not being on their A game...\\n\\nDon't get me wrong. Flip is still good. The wait staff is still amazingly good looking. They still make delicious milk shakes. It's just not as amazing as it once was, which really is a little sad.\",\"date\":\"2010-10-11 18:18:35\"} Workshop Question What changes are required if you need to make an update to your Dockerfile now?",
    "description": "Code to Kubernetes - Python Objective: Understand activities to instrument a python application and run it on Kubernetes.\nVerify the code Containerize the app Deploy the container in Kubernetes Note: these steps do not involve Splunk\nDuration: 15 Minutes\n1. Verify the code - Review service Navigate to the review directory\ncd /home/splunk/realtime_enrichment/flask_apps/review/ Inspect review.py (realtime_enrichment/flask_apps/review)\ncat review.py from flask import Flask, jsonify import random import subprocess review = Flask(__name__) num_reviews = 8635403 num_reviews = 100000 reviews_file = '/var/appdata/yelp_academic_dataset_review.json' @review.route('/') def hello_world(): return jsonify(message='Hello, you want to hit /get_review. We have ' + str(num_reviews) + ' reviews!') @review.route('/get_review') def get_review(): random_review_int = str(random.randint(1,num_reviews)) line_num = random_review_int + 'q;d' command = [\"sed\", line_num, reviews_file] # sed \"7997242q;d\" \u003cfile\u003e random_review = subprocess.run(command, stdout=subprocess.PIPE, text=True) return random_review.stdout if __name__ == \"__main__\": review.run(host ='0.0.0.0', port = 5000, debug = True) Inspect requirements.txt",
    "tags": [],
    "title": "Code to Kubernetes - Python",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/8-gdi/3-code-to-kubernetes/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM",
    "content": "Introduction to the Dashboards and Charts Editing and creating charts Filtering and analytical functions Using formulas Saving charts in a dashboard Introduction to SignalFlow 1. Dashboards Dashboards are groupings of charts and visualizations of metrics. Well-designed dashboards can provide useful and actionable insight into your system at a glance. Dashboards can be complex or contain just a few charts that drill down only into the data you want to see.\nDuring this module, we are going to create the following charts and dashboard and connect them to your Team page.\n2. Your Teams’ Page Click on the from the navbar. As you have already been assigned to a team, you will land on the team dashboard. We use the Example Team as an example here. The one in your workshop will be different!\nThis page shows the total number of team members, how many active alerts for your team and all dashboards that are assigned to your team. Right now there are no dashboards assigned but as stated before, we will add the new dashboard that you will create to your Teams page later.\n3. Sample Charts To continue, click on All Dashboards in the top right corner of the screen. This brings you to the view that shows all the available dashboards, including the pre-built ones.\nIf you are already receiving metrics from a Cloud API integration or another service through the Splunk Agent you will see relevant dashboards for these services.\n4. Inspecting the Sample Data Among the dashboards, you will see a Dashboard group called Sample Data. Expand the Sample Data dashboard group by clicking on it, and then click on the Sample Charts dashboard.\nIn the Sample Charts dashboard, you can see a selection of charts that show a sample of the various styles, colors and formats you can apply to your charts in the dashboards.\nHave a look through all the dashboards in this dashboard group (PART 1, PART 2, PART 3 and INTRO TO SPLUNK OBSERVABILITY CLOUD)",
    "description": "Introduction to the Dashboards and Charts Editing and creating charts Filtering and analytical functions Using formulas Saving charts in a dashboard Introduction to SignalFlow 1. Dashboards Dashboards are groupings of charts and visualizations of metrics. Well-designed dashboards can provide useful and actionable insight into your system at a glance. Dashboards can be complex or contain just a few charts that drill down only into the data you want to see.",
    "tags": [],
    "title": "Working with Dashboards",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/dashboards/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 6. Advanced Features",
    "content": "With Database Query Performance, you can monitor the impact of your database queries on service availability directly in Splunk APM. This way, you can quickly identify long-running, unoptimized, or heavy queries and mitigate issues they might be causing, without having to instrument your databases.\nTo look at the performance of your database queries, make sure you are on the APM Service Map page either by going back in the browser or navigating to the APM section in the Menu bar, then click on the Service Map tile.\nSelect the inferred database service mysql:petclinic Inferred Database server in the Dependency map (1), then scroll the right-hand pane to find the Database Query Performance Pane (2).\nIf the service you have selected in the map is indeed an (inferred) database server, this pane will populate with the top 90% (P90) database calls based on duration. To dive deeper into the db-query performance function, click somewhere on the word Database Query Performance at the top of the pane.\nThis will bring us to the DB-query Performance overview screen:\nDatabase Query Normalization By default, Splunk APM instrumentation sanitizes database queries to remove or mask sensible data, such as secrets or personally identifiable information (PII) from the db.statements. You can find how to turn off database query normalization here.\nThis screen will show us all the Database queries (1) done to our database from your application, based on the Traces \u0026 Spans sent to the Splunk Observability Cloud. Note that you can compare them across a time block or sort them on Total Time, P90 Latency \u0026 Requests (2).\nFor each Database query in the list, we see the highest latency, the total number of calls during the time window and the number of requests per second (3). This allows you to identify places where you might optimize your queries.\nYou can select traces containing Database Calls via the two charts in the right-hand pane (5). Use the Tag Spotlight pane (6) to drill down to see which tags are related to the database calls, based on endpoints or tags.\nIf you need to see a detailed view of a query:\nClick on the specific Query (1). This will open the Query Details pane (2), which you can use for more detailed investigations.",
    "description": "With Database Query Performance, you can monitor the impact of your database queries on service availability directly in Splunk APM. This way, you can quickly identify long-running, unoptimized, or heavy queries and mitigate issues they might be causing, without having to instrument your databases.\nTo look at the performance of your database queries, make sure you are on the APM Service Map page either by going back in the browser or navigating to the APM section in the Menu bar, then click on the Service Map tile.",
    "tags": [],
    "title": "Database Query Performance",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/6-profiling-db-query/3-dbquery/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "Prerequisites Before deploying the application, we’ll need to install the .NET 8 SDK on our instance.\n​ Script Example Output sudo apt-get update \u0026\u0026 \\ sudo apt-get install -y dotnet-sdk-8.0 Hit:1 http://us-west-1.ec2.archive.ubuntu.com/ubuntu jammy InRelease Hit:2 http://us-west-1.ec2.archive.ubuntu.com/ubuntu jammy-updates InRelease Hit:3 http://us-west-1.ec2.archive.ubuntu.com/ubuntu jammy-backports InRelease Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease Ign:5 https://splunk.jfrog.io/splunk/otel-collector-deb release InRelease Hit:6 https://splunk.jfrog.io/splunk/otel-collector-deb release Release Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done The following additional packages will be installed: aspnetcore-runtime-8.0 aspnetcore-targeting-pack-8.0 dotnet-apphost-pack-8.0 dotnet-host-8.0 dotnet-hostfxr-8.0 dotnet-runtime-8.0 dotnet-targeting-pack-8.0 dotnet-templates-8.0 liblttng-ust-common1 liblttng-ust-ctl5 liblttng-ust1 netstandard-targeting-pack-2.1-8.0 The following NEW packages will be installed: aspnetcore-runtime-8.0 aspnetcore-targeting-pack-8.0 dotnet-apphost-pack-8.0 dotnet-host-8.0 dotnet-hostfxr-8.0 dotnet-runtime-8.0 dotnet-sdk-8.0 dotnet-targeting-pack-8.0 dotnet-templates-8.0 liblttng-ust-common1 liblttng-ust-ctl5 liblttng-ust1 netstandard-targeting-pack-2.1-8.0 0 upgraded, 13 newly installed, 0 to remove and 0 not upgraded. Need to get 138 MB of archives. After this operation, 495 MB of additional disk space will be used. etc. Refer to Install .NET SDK or .NET Runtime on Ubuntu for further details.\nReview the .NET Application In the terminal, navigate to the application directory:\ncd ~/workshop/docker-k8s-otel/helloworld We’ll use a simple “Hello World” .NET application for this workshop. The main logic is found in the HelloWorldController.cs file:\npublic class HelloWorldController : ControllerBase { private ILogger\u003cHelloWorldController\u003e logger; public HelloWorldController(ILogger\u003cHelloWorldController\u003e logger) { this.logger = logger; } [HttpGet(\"/hello/{name?}\")] public string Hello(string name) { if (string.IsNullOrEmpty(name)) { logger.LogInformation(\"/hello endpoint invoked anonymously\"); return \"Hello, World!\"; } else { logger.LogInformation(\"/hello endpoint invoked by {name}\", name); return String.Format(\"Hello, {0}!\", name); } } } Build and Run the .NET Application We can build the application using the following command:\n​ Script Example Output dotnet build MSBuild version 17.8.5+b5265ef37 for .NET Determining projects to restore... All projects are up-to-date for restore. helloworld -\u003e /home/splunk/workshop/docker-k8s-otel/helloworld/bin/Debug/net8.0/helloworld.dll Build succeeded. 0 Warning(s) 0 Error(s) Time Elapsed 00:00:02.04 If that’s successful, we can run it as follows:\n​ Script Example Output dotnet run Building... info: Microsoft.Hosting.Lifetime[14] Now listening on: http://localhost:8080 info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down. info: Microsoft.Hosting.Lifetime[0] Hosting environment: Development info: Microsoft.Hosting.Lifetime[0] Content root path: /home/splunk/workshop/docker-k8s-otel/helloworld Once it’s running, open a second SSH terminal to your Ubuntu instance and access the application using curl:\n​ Script Example Output curl http://localhost:8080/hello Hello, World! You can also pass in your name:\n​ Script Example Output curl http://localhost:8080/hello/Tom Hello, Tom! Press Ctrl + C to quit your Helloworld app before moving to the next step.\nNext Steps What are the three methods that we can use to instrument our application with OpenTelemetry?\nSee: Instrument your .NET application for Splunk Observability Cloud for a discussion of the options.",
    "description": "Prerequisites Before deploying the application, we’ll need to install the .NET 8 SDK on our instance.\n​ Script Example Output sudo apt-get update \u0026\u0026 \\ sudo apt-get install -y dotnet-sdk-8.0 Hit:1 http://us-west-1.ec2.archive.ubuntu.com/ubuntu jammy InRelease Hit:2 http://us-west-1.ec2.archive.ubuntu.com/ubuntu jammy-updates InRelease Hit:3 http://us-west-1.ec2.archive.ubuntu.com/ubuntu jammy-backports InRelease Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease Ign:5 https://splunk.jfrog.io/splunk/otel-collector-deb release InRelease Hit:6 https://splunk.jfrog.io/splunk/otel-collector-deb release Release Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done The following additional packages will be installed: aspnetcore-runtime-8.0 aspnetcore-targeting-pack-8.0 dotnet-apphost-pack-8.0 dotnet-host-8.0 dotnet-hostfxr-8.0 dotnet-runtime-8.0 dotnet-targeting-pack-8.0 dotnet-templates-8.0 liblttng-ust-common1 liblttng-ust-ctl5 liblttng-ust1 netstandard-targeting-pack-2.1-8.0 The following NEW packages will be installed: aspnetcore-runtime-8.0 aspnetcore-targeting-pack-8.0 dotnet-apphost-pack-8.0 dotnet-host-8.0 dotnet-hostfxr-8.0 dotnet-runtime-8.0 dotnet-sdk-8.0 dotnet-targeting-pack-8.0 dotnet-templates-8.0 liblttng-ust-common1 liblttng-ust-ctl5 liblttng-ust1 netstandard-targeting-pack-2.1-8.0 0 upgraded, 13 newly installed, 0 to remove and 0 not upgraded. Need to get 138 MB of archives. After this operation, 495 MB of additional disk space will be used. etc. Refer to Install .NET SDK or .NET Runtime on Ubuntu for further details.",
    "tags": [],
    "title": "Deploy a .NET Application",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/3-deploy-dotnet-app/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Solving Problems with O11y Cloud",
    "content": "At this point, we’ve deployed an OpenTelemetry collector in our K8s cluster, and it’s successfully collecting infrastructure metrics.\nThe next step is to deploy a sample application and instrument with OpenTelemetry to capture traces.\nWe’ll use a microservices-based application written in Python. To keep the workshop simple, we’ll focus on two services: a credit check service and a credit processor service.\nDeploy the Application To save time, we’ve built Docker images for both of these services already which are available in Docker Hub. We can deploy the credit check service in our K8s cluster with the following command:\n​ Script Example Output kubectl apply -f /home/splunk/workshop/tagging/creditcheckservice-py-with-tags/creditcheckservice-dockerhub.yaml deployment.apps/creditcheckservice created service/creditcheckservice created Next, let’s deploy the credit processor service:\n​ Script Example Output kubectl apply -f /home/splunk/workshop/tagging/creditprocessorservice/creditprocessorservice-dockerhub.yaml deployment.apps/creditprocessorservice created service/creditprocessorservice created Finally, let’s deploy a load generator to generate traffic:\n​ Script Example Output kubectl apply -f /home/splunk/workshop/tagging/loadgenerator/loadgenerator-dockerhub.yaml deployment.apps/loadgenerator created Explore the Application We’ll provide an overview of the application in this section. If you’d like to see the complete source code for the application, refer to the Observability Workshop repository in GitHub\nOpenTelemetry Instrumentation If we look at the Dockerfile’s used to build the credit check and credit processor services, we can see that they’ve already been instrumented with OpenTelemetry. For example, let’s look at /home/splunk/workshop/tagging/creditcheckservice-py-with-tags/Dockerfile:\nFROM python:3.11-slim # Set working directory WORKDIR /app # Copy requirements over COPY requirements.txt . RUN apt-get update \u0026\u0026 apt-get install --yes gcc python3-dev ENV PIP_ROOT_USER_ACTION=ignore # Install Python dependencies RUN pip install --no-cache-dir -r requirements.txt # Copy main app COPY main.py . # Bootstrap OTel RUN splunk-py-trace-bootstrap # Set the entrypoint command to run the application CMD [\"splunk-py-trace\", \"python3\", \"main.py\"] We can see that splunk-py-trace-bootstrap was included, which installs OpenTelemetry instrumentation for supported packages used by our applications. We can also see that splunk-py-trace is used as part of the command to start the application.\nAnd if we review the /home/splunk/workshop/tagging/creditcheckservice-py-with-tags/requirements.txt file, we can see that splunk-opentelemetry[all] was included in the list of packages.\nFinally, if we review the Kubernetes manifest that we used to deploy this service (/home/splunk/workshop/tagging/creditcheckservice-py-with-tags/creditcheckservice-dockerhub.yaml), we can see that environment variables were set in the container to tell OpenTelemetry where to export OTLP data to:\nenv: - name: PORT value: \"8888\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP - name: OTEL_EXPORTER_OTLP_ENDPOINT value: \"http://$(NODE_IP):4317\" - name: OTEL_SERVICE_NAME value: \"creditcheckservice\" - name: OTEL_PROPAGATORS value: \"tracecontext,baggage\" This is all that’s needed to instrument the service with OpenTelemetry!\nExplore the Application We’ve captured several custom tags with our application, which we’ll explore shortly. Before we do that, let’s introduce the concept of tags and why they’re important.\nWhat are tags? Tags are key-value pairs that provide additional metadata about spans in a trace, allowing you to enrich the context of the spans you send to Splunk APM.\nFor example, a payment processing application would find it helpful to track:\nThe payment type used (i.e. credit card, gift card, etc.) The ID of the customer that requested the payment This way, if errors or performance issues occur while processing the payment, we have the context we need for troubleshooting.\nWhile some tags can be added with the OpenTelemetry collector, the ones we’ll be working with in this workshop are more granular, and are added by application developers using the OpenTelemetry SDK.\nWhy are tags so important? Tags are essential for an application to be truly observable. They add the context to the traces to help us understand why some users get a great experience and others don’t. And powerful features in Splunk Observability Cloud utilize tags to help you jump quickly to root cause.\nA note about terminology before we proceed. While we discuss tags in this workshop, and this is the terminology we use in Splunk Observability Cloud, OpenTelemetry uses the term attributes instead. So when you see tags mentioned throughout this workshop, you can treat them as synonymous with attributes.\nHow are tags captured? To capture tags in a Python application, we start by importing the trace module by adding an import statement to the top of the /home/splunk/workshop/tagging/creditcheckservice-py-with-tags/main.py file:\nimport requests from flask import Flask, request from waitress import serve from opentelemetry import trace # \u003c--- ADDED BY WORKSHOP ... Next, we need to get a reference to the current span so we can add an attribute (aka tag) to it:\ndef credit_check(): current_span = trace.get_current_span() # \u003c--- ADDED BY WORKSHOP customerNum = request.args.get('customernum') current_span.set_attribute(\"customer.num\", customerNum) # \u003c--- ADDED BY WORKSHOP ... That was pretty easy, right? We’ve captured a total of four tags in the credit check service, with the final result looking like this:\ndef credit_check(): current_span = trace.get_current_span() # \u003c--- ADDED BY WORKSHOP customerNum = request.args.get('customernum') current_span.set_attribute(\"customer.num\", customerNum) # \u003c--- ADDED BY WORKSHOP # Get Credit Score creditScoreReq = requests.get(\"http://creditprocessorservice:8899/getScore?customernum=\" + customerNum) creditScoreReq.raise_for_status() creditScore = int(creditScoreReq.text) current_span.set_attribute(\"credit.score\", creditScore) # \u003c--- ADDED BY WORKSHOP creditScoreCategory = getCreditCategoryFromScore(creditScore) current_span.set_attribute(\"credit.score.category\", creditScoreCategory) # \u003c--- ADDED BY WORKSHOP # Run Credit Check creditCheckReq = requests.get(\"http://creditprocessorservice:8899/runCreditCheck?customernum=\" + str(customerNum) + \"\u0026score=\" + str(creditScore)) creditCheckReq.raise_for_status() checkResult = str(creditCheckReq.text) current_span.set_attribute(\"credit.check.result\", checkResult) # \u003c--- ADDED BY WORKSHOP return checkResult Review Trace Data Before looking at the trace data in Splunk Observability Cloud, let’s review what the debug exporter has captured by tailing the agent collector logs with the following command:\nkubectl logs -l component=otel-collector-agent -f Hint: use CTRL+C to stop tailing the logs.\nYou should see traces written to the agent collector logs such as the following:\nInstrumentationScope opentelemetry.instrumentation.flask 0.44b0 Span #0 Trace ID : 9f9fc109903f25ba57bea9b075aa4833 Parent ID : ID : 6d71519f454f6059 Name : /check Kind : Server Start time : 2024-12-23 19:55:25.815891965 +0000 UTC End time : 2024-12-23 19:55:27.824664949 +0000 UTC Status code : Unset Status message : Attributes: -\u003e http.method: Str(GET) -\u003e http.server_name: Str(waitress.invalid) -\u003e http.scheme: Str(http) -\u003e net.host.port: Int(8888) -\u003e http.host: Str(creditcheckservice:8888) -\u003e http.target: Str(/check?customernum=30134241) -\u003e net.peer.ip: Str(10.42.0.19) -\u003e http.user_agent: Str(python-requests/2.31.0) -\u003e net.peer.port: Str(47248) -\u003e http.flavor: Str(1.1) -\u003e http.route: Str(/check) -\u003e customer.num: Str(30134241) -\u003e credit.score: Int(443) -\u003e credit.score.category: Str(poor) -\u003e credit.check.result: Str(OK) -\u003e http.status_code: Int(200) Notice how the trace includes the tags (aka attributes) that we captured in the code, such as credit.score and credit.score.category. We’ll use these in the next section, when we analyze the traces in Splunk Observability Cloud to find the root cause of a performance issue.",
    "description": "At this point, we’ve deployed an OpenTelemetry collector in our K8s cluster, and it’s successfully collecting infrastructure metrics.\nThe next step is to deploy a sample application and instrument with OpenTelemetry to capture traces.\nWe’ll use a microservices-based application written in Python. To keep the workshop simple, we’ll focus on two services: a credit check service and a credit processor service.\nDeploy the Application To save time, we’ve built Docker images for both of these services already which are available in Docker Hub. We can deploy the credit check service in our K8s cluster with the following command:",
    "tags": [],
    "title": "Deploy the Sample Application and Instrument with OpenTelemetry",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/9-solving-problems-with-o11y-cloud/3-deploy-sample-app/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Horizontal Pod Autoscaling",
    "content": "1. Namespaces in Kubernetes Most of our customers will make use of some kind of private or public cloud service to run Kubernetes. They often choose to have only a few large Kubernetes clusters as it is easier to manage centrally.\nNamespaces are a way to organize these large Kubernetes clusters into virtual sub-clusters. This can be helpful when different teams or projects share a Kubernetes cluster as this will give them the easy ability to just see and work with their resources.\nAny number of namespaces are supported within a cluster, each logically separated from others but with the ability to communicate with each other. Components are only visible when selecting a namespace or when adding the --all-namespaces flag to kubectl instead of allowing you to view just the components relevant to your project by selecting your namespace.\nMost customers will want to install the applications into a separate namespace. This workshop will follow that best practice.\n2. DNS and Services in Kubernetes The Domain Name System (DNS) is a mechanism for linking various sorts of information with easy-to-remember names, such as IP addresses. Using a DNS system to translate request names into IP addresses makes it easy for end-users to reach their target domain name effortlessly.\nMost Kubernetes clusters include an internal DNS service configured by default to offer a lightweight approach for service discovery. Even when Pods and Services are created, deleted, or shifted between nodes, built-in service discovery simplifies applications to identify and communicate with services on the Kubernetes clusters.\nIn short, the DNS system for Kubernetes will create a DNS entry for each Pod and Service. In general, a Pod has the following DNS resolution:\npod-name.my-namespace.pod.cluster-domain.example For example, if a Pod in the default namespace has the Pod name my_pod, and the domain name for your cluster is cluster.local, then the Pod has a DNS name:\nmy_pod.default.pod.cluster.local Any Pods exposed by a Service have the following DNS resolution available:\nmy_pod.service-name.my-namespace.svc.cluster-domain.example More information can be found here: DNS for Service and Pods\n3. Review OTel receiver for PHP/Apache Inspect the YAML file ~/workshop/k3s/otel-apache.yaml and validate the contents using the following command:\ncat ~/workshop/k3s/otel-apache.yaml This file contains the configuration for the OpenTelemetry agent to monitor the PHP/Apache deployment.\nagent: config: receivers: receiver_creator: receivers: smartagent/apache: rule: type == \"port\" \u0026\u0026 pod.name matches \"apache\" \u0026\u0026 port == 80 config: type: collectd/apache url: http://php-apache-svc.apache.svc.cluster.local/server-status?auto extraDimensions: service.name: php-apache 4. Observation Rules in the OpenTelemetry config The above file contains an observation rule for Apache using the OTel receiver_creator. This receiver can instantiate other receivers at runtime based on whether observed endpoints match a configured rule.\nThe configured rules will be evaluated for each endpoint discovered. If the rule evaluates to true, then the receiver for that rule will be started as configured against the matched endpoint.\nIn the file above we tell the OpenTelemetry agent to look for Pods that match the name apache and have port 80 open. Once found, the agent will configure an Apache receiver to read Apache metrics from the configured URL. Note, the K8s DNS-based URL in the above YAML for the service.\nTo use the Apache configuration, you can upgrade the existing Splunk OpenTelemetry Collector Helm chart to use the otel-apache.yaml file with the following command:\n​ Helm Upgrade helm upgrade splunk-otel-collector \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-k3s-cluster\" \\ --set=\"logsEngine=otel\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml \\ -f ~/workshop/k3s/otel-apache.yaml NOTE The REVISION number of the deployment has changed, which is a helpful way to keep track of your changes.\nRelease \"splunk-otel-collector\" has been upgraded. Happy Helming! NAME: splunk-otel-collector LAST DEPLOYED: Mon Nov 4 14:56:25 2024 NAMESPACE: default STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Platform endpoint \"https://http-inputs-workshop.splunkcloud.com:443/services/collector/event\". Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Observability realm eu0. 5. Kubernetes ConfigMaps A ConfigMap is an object in Kubernetes consisting of key-value pairs that can be injected into your application. With a ConfigMap, you can separate configuration from your Pods.\nUsing ConfigMap, you can prevent hardcoding configuration data. ConfigMaps are useful for storing and sharing non-sensitive, unencrypted configuration information.\nThe OpenTelemetry collector/agent uses ConfigMaps to store the configuration of the agent and the K8s Cluster receiver. You can/will always verify the current configuration of an agent after a change by running the following commands:\nkubectl get cm Workshop Question How many ConfigMaps are used by the collector?\nWhen you have a list of ConfigMaps from the namespace, select the one for the otel-agent and view it with the following command:\nkubectl get cm splunk-otel-collector-otel-agent -o yaml NOTE The option -o yaml will output the content of the ConfigMap in a readable YAML format.\nWorkshop Question Is the configuration from otel-apache.yaml visible in the ConfigMap for the collector agent?\n6. Review PHP/Apache deployment YAML Inspect the YAML file ~/workshop/k3s/php-apache.yaml and validate the contents using the following command:\ncat ~/workshop/k3s/php-apache.yaml This file contains the configuration for the PHP/Apache deployment and will create a new StatefulSet with a single replica of the PHP/Apache image.\nA stateless application does not care which network it is using, and it does not need permanent storage. Examples of stateless apps may include web servers such as Apache, Nginx, or Tomcat.\napiVersion: apps/v1 kind: StatefulSet metadata: name: php-apache spec: updateStrategy: type: RollingUpdate selector: matchLabels: run: php-apache serviceName: \"php-apache-svc\" replicas: 1 template: metadata: labels: run: php-apache spec: containers: - name: php-apache image: ghcr.io/splunk/php-apache:latest ports: - containerPort: 80 resources: limits: cpu: \"8\" memory: \"8Mi\" requests: cpu: \"6\" memory: \"4Mi\" --- apiVersion: v1 kind: Service metadata: name: php-apache-svc labels: run: php-apache spec: ports: - port: 80 selector: run: php-apache 7. Deploy PHP/Apache Create an apache namespace then deploy the PHP/Apache application to the cluster.\nCreate the apache namespace:\nkubectl create namespace apache Deploy the PHP/Apache application:\nkubectl apply -f ~/workshop/k3s/php-apache.yaml -n apache Ensure the deployment has been created:\nkubectl get statefulset -n apache Workshop Question What metrics for your Apache instance are being reported in the Apache Navigator?\nTip: Use the Navigator Sidebar and click on the service name.\nWorkshop Question Using Log Observer what is the issue with the PHP/Apache deployment?\nTip: Adjust your filters to use: object = php-apache-svc and k8s.cluster.name = \u003cyour_cluster\u003e.",
    "description": "1. Namespaces in Kubernetes Most of our customers will make use of some kind of private or public cloud service to run Kubernetes. They often choose to have only a few large Kubernetes clusters as it is easier to manage centrally.\nNamespaces are a way to organize these large Kubernetes clusters into virtual sub-clusters. This can be helpful when different teams or projects share a Kubernetes cluster as this will give them the easy ability to just see and work with their resources.",
    "tags": [],
    "title": "Deploying PHP/Apache",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/2-hpa/3-deploy-apache/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e NodeJS Zero-Config Workshop",
    "content": "1. Patching the Frontend service First, confirm that you can see your environment in APM. There should be a service called loadgenerator displayed in the Service map.\nNext, we will patch the frontend deployment with an annotation to inject the NodeJS auto instrumentation. This will allow us to see the frontend service in APM. Note, that at this point we have not edited any code.\nkubectl patch deployment opentelemetry-demo-frontend -n otel-demo -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"instrumentation.opentelemetry.io/inject-nodejs\":\"default/splunk-otel-collector\"}}}} }' This will cause the opentelemetry-demo-frontend pod to restart. The annotation value default/splunk-otel-collector refers to the instrumentation configuration named splunk-otel-collector in the default namespace. If the chart is not installed in the default namespace, modify the annotation value to be {chart_namespace}/splunk-otel-collector. After a few minutes, you should see the frontend service in APM.\nWith the frontend service highlighted, click on the Traces tab to see the traces for the service. Select one of the traces and confirm that the trace contains metadata confirming that the Splunk Zero-Configuration Auto-Instrumentation for NodeJS is being used.",
    "description": "1. Patching the Frontend service First, confirm that you can see your environment in APM. There should be a service called loadgenerator displayed in the Service map.\nNext, we will patch the frontend deployment with an annotation to inject the NodeJS auto instrumentation. This will allow us to see the frontend service in APM. Note, that at this point we have not edited any code.",
    "tags": [],
    "title": "Zero Configuration - Frontend Service",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/3-nodejs-kubernetes/3-frontend-service/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Advanced Synthetics",
    "content": "To begin configuring our test, we need to import the JSON that we exported from the Chrome DevTools Recorder. To enable the Import button, we must first give our test a name e.g. [\u003cyour team name\u003e] \u003cyour initials\u003e - Online Boutique.\nOnce the Import button is enabled, click on it and either drop the JSON file that you exported from the Chrome DevTools Recorder or upload the file.\nOnce the JSON file has been uploaded, click on Continue to edit steps\nBefore we make any edits to the test, let’s first configure the settings, click on \u003c Return to test",
    "description": "To begin configuring our test, we need to import the JSON that we exported from the Chrome DevTools Recorder. To enable the Import button, we must first give our test a name e.g. [\u003cyour team name\u003e] \u003cyour initials\u003e - Online Boutique.\nOnce the Import button is enabled, click on it and either drop the JSON file that you exported from the Chrome DevTools Recorder or upload the file.",
    "tags": [],
    "title": "Import JSON",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/3-advanced-synthetics/3-import-json/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Lambda Tracing",
    "content": "The Lambda functions should be generating a sizeable amount of trace data, which we would need to take a look at. Through the combination of environment variables and the OpenTelemetry Lambda layer configured in the resource definition for our Lambda functions, we should now be ready to view our functions and traces in Splunk APM.\nView your Environment name in the Splunk APM Overview Let’s start by making sure that Splunk APM is aware of our Environment from the trace data it is receiving. This is the deployment.name we set as part of the OTEL_RESOURCE_ATTRIBUTES variable we set on our Lambda function definitions in main.tf. It was also one of the outputs from the terraform apply command we ran earlier.\nIn Splunk Observability Cloud:\nClick on the APM Button from the Main Menu on the left. This will take you to the Splunk APM Overview.\nSelect your APM Environment from the Environment: dropdown.\nYour APM environment should be in the PREFIX-lambda-shop format, where the PREFIX is obtained from the environment variable you set in the Prerequisites section Note It may take a few minutes for your traces to appear in Splunk APM. Try hitting refresh on your browser until you find your environment name in the list of environments.\nView your Environment’s Service Map Once you’ve selected your Environment name from the Environment drop down, you can take a look at the Service Map for your Lambda functions.\nClick the Service Map Button on the right side of the APM Overview page. This will take you to your Service Map view. You should be able to see the producer-lambda function and the call it is making to the Kinesis Stream to put your record.\nWorkshop Question What about your consumer-lambda function?\nExplore the Traces from your Lambda Functions Click the Traces button to view the Trace Analyzer. On this page, we can see the traces that have been ingested from the OpenTelemetry Lambda layer of your producer-lambda function.\nSelect a trace from the list to examine by clicking on its hyperlinked Trace ID. We can see that the producer-lambda function is putting a record into the Kinesis Stream. But the action of the consumer-lambda function is missing!\nThis is because the trace context is not being propagated. Trace context propagation is not supported out-of-the-box by Kinesis service at the time of this workshop. Our distributed trace stops at the Kinesis service, and because its context isn’t automatically propagated through the stream, we can’t see any further.\nNot yet, at least…\nLet’s see how we work around this in the next section of this workshop. But before that, let’s clean up after ourselves!\nClean Up The resources we deployed as part of this auto-instrumenation exercise need to be cleaned. Likewise, the script that was generating traffice against our producer-lambda endpoint needs to be stopped, if it’s still running. Follow the below steps to clean up.\nKill the send_message If the send_message.py script is still running, stop it with the follwing commands:\nfg This brings your background process to the foreground. Next you can hit [CONTROL-C] to kill the process. Destroy all AWS resources Terraform is great at managing the state of our resources individually, and as a deployment. It can even update deployed resources with any changes to their definitions. But to start afresh, we will destroy the resources and redeploy them as part of the manual instrumentation portion of this workshop.\nPlease follow these steps to destroy your resources:\nEnsure you are in the auto directory:\npwd The expected output would be ~/workshop/lambda/auto If you are not in the auto directory, run the following command:\ncd ~/workshop/lambda/auto Destroy the Lambda functions and other AWS resources you deployed earlier:\nterraform destroy respond yes when you see the Enter a value: prompt This will result in the resources being destroyed, leaving you with a clean environment This process will leave you with the files and directories created as a result of our activity. Do not worry about those.",
    "description": "The Lambda functions should be generating a sizeable amount of trace data, which we would need to take a look at. Through the combination of environment variables and the OpenTelemetry Lambda layer configured in the resource definition for our Lambda functions, we should now be ready to view our functions and traces in Splunk APM.\nView your Environment name in the Splunk APM Overview Let’s start by making sure that Splunk APM is aware of our Environment from the trace data it is receiving. This is the deployment.name we set as part of the OTEL_RESOURCE_ATTRIBUTES variable we set on our Lambda function definitions in main.tf. It was also one of the outputs from the terraform apply command we ran earlier.",
    "tags": [],
    "title": "Splunk APM, Lambda Functions \u0026 Traces",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/6-lambda-kinesis/3-lambdas-in-splunk/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Lambda Tracing and Kinesis",
    "content": "Lambdas in Splunk APM Now it’s time to check how your Lambda traffic has been captured in Splunk APM.\nNavigate to your Splunk Observability Cloud Select APM from the Main Menu and then select your APM Environment. Your APM environment should be in the format $INSTANCE-apm-lambda where the hostname value is a four letter name of your lab host. (Check it by looking at your command prompt, or by running echo $INSTANCE).\nNote It may take a few minutes for you traces to appear in Splunk APM. Try hitting refresh on your browser until you find your environement name in the list of Envrionments Go to Explore the Service Map to see the Dependencies between your Lambda Functions.\nYou should be able to see the producer-lambda and the call it is making to Kinesis service.\nWorkshop Question What about your consumer-lambda?\nClick into Traces and examine some traces that container procuder function calls and traces with consumer function calls.\nWe can see the producer-lambda putting a Record on the Kinesis stream. But the action of consumer-function is disconnected!\nThis is because the Trace Context is not being propagated.\nThis is not something that is supported automatically Out-of-the-Box by Kinesis service at the time of this lab. Our Distributed Trace stops at Kinesis inferred service, and we can’t see the propagation any further.\nNot yet…\nLet’s see how we work around this in the next section of this lab.",
    "description": "Lambdas in Splunk APM Now it’s time to check how your Lambda traffic has been captured in Splunk APM.\nNavigate to your Splunk Observability Cloud Select APM from the Main Menu and then select your APM Environment. Your APM environment should be in the format $INSTANCE-apm-lambda where the hostname value is a four letter name of your lab host. (Check it by looking at your command prompt, or by running echo $INSTANCE).",
    "tags": [],
    "title": "Lambdas in Splunk APM",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/6-lambda-kinesis/3-lambdas-in-splunk/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 5. Splunk Log Observer",
    "content": "Once you have a specific view in Log Observer, it is very useful to be able to use that view in a dashboard, to help in the future with reducing the time to detect or resolve issues. As part of the workshop, we will create an example custom dashboard that will use these charts.\nLet’s look at creating a Log Timeline chart. The Log Timeline chart is used for visualizing log messages over time. It is a great way to see the frequency of log messages and to identify patterns. It is also a great way to see the distribution of log messages across your environment. These charts can be saved to a custom dashboard.\nExercise First, we will reduce the amount of information to only the columns we are interested in:\nClick on the Configure Table icon above the Logs table to open the Table Settings, untick _raw and ensure the following fields are selected k8s.pod.name, message and version. Remove the fixed time from the time picker, and set it to the Last 15 minutes. To make this work for all traces, remove the trace_id from the filter and add the fields sf_service=wire-transfer-service and sf_environment=[WORKSHOPNAME]. Click Save and select Save to Dashboard. In the chart creation dialog box that appears, for the Chart name use Log Timeline. Click Select Dashboard and then click New dashboard in the Dashboard Selection dialog box. In the New dashboard dialog box, enter a name for the new dashboard (no need to enter a description). Use the following format: Initials - Service Health Dashboard and click Save Ensure the new dashboard is highlighted in the list (1) and click OK (2). Ensure that Log Timeline is selected as the Chart Type. Click the Save button (do not click Save and goto dashboard at this time). Next, we will create a Log View chart.",
    "description": "Once you have a specific view in Log Observer, it is very useful to be able to use that view in a dashboard, to help in the future with reducing the time to detect or resolve issues. As part of the workshop, we will create an example custom dashboard that will use these charts.\nLet’s look at creating a Log Timeline chart. The Log Timeline chart is used for visualizing log messages over time. It is a great way to see the frequency of log messages and to identify patterns. It is also a great way to see the distribution of log messages across your environment. These charts can be saved to a custom dashboard.",
    "tags": [],
    "title": "3. Log Timeline Chart",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/5-log-observer/3-log-timeline-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 7. Splunk Log Observer",
    "content": "Once you have a specific view in Log Observer, it is very useful to be able to use that view in a dashboard, to help in the future with reducing the time to detect or resolve issues. As part of the workshop, we will create an example custom dashboard that will use these charts.\nLet’s look at creating a Log Timeline chart. The Log Timeline chart is used for visualizing log messages over time. It is a great way to see the frequency of log messages and to identify patterns. It is also a great way to see the distribution of log messages across your environment. These charts can be saved to a custom dashboard.\nExercise First, we will reduce the amount of information to only the columns we are interested in:\nClick on the Configure Table icon above the Logs table to open the Table Settings, untick _raw and ensure the following fields are selected k8s.pod.name, message and version. Remove the fixed time from the time picker, and set it to the Last 15 minutes. To make this work for all traces, remove the trace_id from the filter and add the fields sf_service=paymentservice and sf_environment=[WORKSHOPNAME]. Click Save and select Save to Dashboard. In the chart creation dialog box that appears, for the Chart name use Log Timeline. Click Select Dashboard and then click New dashboard in the Dashboard Selection dialog box. In the New dashboard dialog box, enter a name for the new dashboard (no need to enter a description). Use the following format: Initials - Service Health Dashboard and click Save Ensure the new dashboard is highlighted in the list (1) and click OK (2). Ensure that Log Timeline is selected as the Chart Type. Click the Save button (do not click Save and goto dashboard at this time). Next, we will create a Log View chart.",
    "description": "Once you have a specific view in Log Observer, it is very useful to be able to use that view in a dashboard, to help in the future with reducing the time to detect or resolve issues. As part of the workshop, we will create an example custom dashboard that will use these charts.\nLet’s look at creating a Log Timeline chart. The Log Timeline chart is used for visualizing log messages over time. It is a great way to see the frequency of log messages and to identify patterns. It is also a great way to see the distribution of log messages across your environment. These charts can be saved to a custom dashboard.",
    "tags": [],
    "title": "3. Log Timeline Chart",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/7-log-observer/3-log-timeline-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 3. Receivers",
    "content": "Other Receivers You will notice in the default configuration there are other receivers: otlp, opencensus, jaeger and zipkin. These are used to receive telemetry data from other sources. We will not be covering these receivers in this workshop and they can be left as they are.\nNinja: Create receivers dynamically To help observe short lived tasks like docker containers, kubernetes pods, or ssh sessions, we can use the receiver creator with observer extensions to create a new receiver as these services start up.\nWhat do we need? In order to start using the receiver creator and its associated observer extensions, they will need to be part of your collector build manifest.\nSee installation for the details.\nThings to consider? Some short lived tasks may require additional configuration such as username, and password. These values can be referenced via environment variables, or use a scheme expand syntax such as ${file:./path/to/database/password}. Please adhere to your organisation’s secret practices when taking this route.\nThe Ninja Zone There are only two things needed for this ninja zone:\nMake sure you have added receiver creater and observer extensions to the builder manifest. Create the config that can be used to match against discovered endpoints. To create the templated configurations, you can do the following:\nreceiver_creator: watch_observers: [host_observer] receivers: redis: rule: type == \"port\" \u0026\u0026 port == 6379 config: password: ${env:HOST_REDIS_PASSWORD} For more examples, refer to these receiver creator’s examples.\nConfiguration Check-in We’ve now covered receivers, so let’s now check our configuration changes.\nCheck-inReview your configuration ​ config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 # To limit exposure to denial of service attacks, change the host in endpoints below from 0.0.0.0 to a specific network interface. # See https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/security-best-practices.md#safeguards-against-denial-of-service-attacks extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 receivers: hostmetrics: collection_interval: 10s scrapers: # CPU utilization metrics cpu: # Disk I/O metrics disk: # File System utilization metrics filesystem: # Memory utilization metrics memory: # Network interface I/O metrics \u0026 TCP connection metrics network: # CPU load metrics load: # Paging/Swap space utilization and I/O metrics paging: # Process count metrics processes: # Per process CPU, Memory and Disk I/O metrics. Disabled by default. # process: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 opencensus: endpoint: 0.0.0.0:55678 # Collect own metrics prometheus/internal: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 10s static_configs: - targets: ['0.0.0.0:8888'] jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_binary: endpoint: 0.0.0.0:6832 thrift_compact: endpoint: 0.0.0.0:6831 thrift_http: endpoint: 0.0.0.0:14268 zipkin: endpoint: 0.0.0.0:9411 processors: batch: exporters: debug: verbosity: detailed service: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [otlp, opencensus, prometheus] processors: [batch] exporters: [debug] logs: receivers: [otlp] processors: [batch] exporters: [debug] extensions: [health_check, pprof, zpages] Now that we have reviewed how data gets into the OpenTelemetry Collector through receivers, let’s now take a look at how the Collector processes the received data.\nWarning As the /etc/otelcol-contrib/config.yaml is not complete, please do not attempt to restart the collector at this point.",
    "description": "Other Receivers You will notice in the default configuration there are other receivers: otlp, opencensus, jaeger and zipkin. These are used to receive telemetry data from other sources. We will not be covering these receivers in this workshop and they can be left as they are.\nNinja: Create receivers dynamically To help observe short lived tasks like docker containers, kubernetes pods, or ssh sessions, we can use the receiver creator with observer extensions to create a new receiver as these services start up.",
    "tags": [],
    "title": "OpenTelemetry Collector Receivers",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/3-receivers/3-other-receivers/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Users and Workflows As we go through this workshop we will be switching roles from SRE to Developer. First we will start with alert responders or SREs who will identify an issue in Splunk Observability UI. Next, we will jump to a Developer Role to see how a Developer will debug and repair/fix a software problem using trace data provided by our SRE.\nOf course, we are not requiring 2 people for this workshop as each participant will play both roles.\nToday we will learn Today we will learn how Splunk APM with Full Fidelity tracing can accelerate the time to repair for Development teams. We will focus on the full fidelity data ( In the form of traces ) that an SRE or alert responder would send to a developer to then repair/fix software. We will do this with both Auto-Instrumentation data and Custom attributes or Custom Tags, via Manual Instrumentation data.\nWhile we will be spending time Debugging code, …. Don’t worry, …there is no programming experience necessary, as our goal here is for every participant to understand how using Custom Attributes/Tags in Splunk APM @ Full Fidelity accelerates Mean Time to Repair Software problems for Development Teams.\nImportant Definitions Let’s define a few terms for those new to APM / Software Development or Java\nWhat Are Custom Attributes / Custom Tags in Splunk APM ? First, you will hear poeple refer to Custom Attributes in the context of Splunk Enterprise, however in Splunk APM Custom Attributes are called Custom Tags as defined in Opentelemetry and shown in Splunk APM Tag Splotlight.\nWhat is a Function or a method in Java? A Function in most languages, including Java, is a logical chunk of code that – when executed – solves a repeatable task. This is basically what development teams spend thier time building and where software issues will most commonly be.\nWhat is an Exception in Java? An exception is an exceptional error condition that indicates abonormal behavior, or an unhandled condition, that interrupts program execution abnormally.",
    "description": "Users and Workflows As we go through this workshop we will be switching roles from SRE to Developer. First we will start with alert responders or SREs who will identify an issue in Splunk Observability UI. Next, we will jump to a Developer Role to see how a Developer will debug and repair/fix a software problem using trace data provided by our SRE.\nOf course, we are not requiring 2 people for this workshop as each participant will play both roles.",
    "tags": [],
    "title": "Overview of the Workshop",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/3-overview/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Monolith Workshop",
    "content": "For the Real User Monitoring (RUM) instrumentation, we will add the Open Telemetry Javascript https://github.com/signalfx/splunk-otel-js-web snippet in the pages, we will use the wizard again Data Management → Add Integration → RUM Instrumentation → Browser Instrumentation.\nYour instructor will inform you which token to use from the dropdown, click Next. Enter App name and Environment using the following syntax:\n\u003cINSTANCE\u003e-petclinic-service - replacing \u003cINSTANCE\u003e with the value you noted down earlier. \u003cINSTANCE\u003e-petclinic-env - replacing \u003cINSTANCE\u003e with the value you noted down earlier. The wizard will then show a snippet of HTML code that needs to be placed at the top of the pages in the \u003chead\u003e section. The following is an example of the (do not use this snippet, use the one generated by the wizard):\n/* IMPORTANT: Replace the \u003cversion\u003e placeholder in the src URL with a version from https://github.com/signalfx/splunk-otel-js-web/releases */ \u003cscript src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e \u003cscript\u003e SplunkRum.init({ realm: \"eu0\", rumAccessToken: \"\u003credacted\u003e\", applicationName: \"petclinic-1be0-petclinic-service\", deploymentEnvironment: \"petclinic-1be0-petclinic-env\" }); \u003c/script\u003e The Spring PetClinic application uses a single HTML page as the “layout” page, that is reused across all pages of the application. This is the perfect location to insert the Splunk RUM Instrumentation Library as it will be loaded in all pages automatically.\nLet’s then edit the layout page:\nvi src/main/resources/templates/fragments/layout.html Next, insert the snippet we generated above in the \u003chead\u003e section of the page. Make sure you don’t include the comment and replace \u003cversion\u003e in the source URL to latest e.g.\n\u003c!doctype html\u003e \u003chtml th:fragment=\"layout (template, menu)\"\u003e \u003chead\u003e \u003cscript src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e \u003cscript\u003e SplunkRum.init({ realm: \"eu0\", rumAccessToken: \"\u003credacted\u003e\", applicationName: \"petclinic-1be0-petclinic-service\", deploymentEnvironment: \"petclinic-1be0-petclinic-env\" }); \u003c/script\u003e ... With the code changes complete, we need to rebuild the application and run it again. Run the maven command to compile/build/package PetClinic:\n./mvnw package -Dmaven.test.skip=true java \\ -Dserver.port=8083 \\ -Dotel.service.name=$INSTANCE-petclinic-service \\ -Dotel.resource.attributes=deployment.environment=$INSTANCE-petclinic-env,version=0.314 \\ -jar target/spring-petclinic-*.jar --spring.profiles.active=mysql Then let’s visit the application using a browser to generate real-user traffic http://\u003cIP_ADDRESS\u003e:8083.\nIn RUM, filter down into the environment as defined in the RUM snippet above and click through to the dashboard.\nWhen you drill down into a RUM trace you will see a link to APM in the spans. Clicking on the trace ID will take you to the corresponding APM trace for the current RUM trace.",
    "description": "For the Real User Monitoring (RUM) instrumentation, we will add the Open Telemetry Javascript https://github.com/signalfx/splunk-otel-js-web snippet in the pages, we will use the wizard again Data Management → Add Integration → RUM Instrumentation → Browser Instrumentation.\nYour instructor will inform you which token to use from the dropdown, click Next. Enter App name and Environment using the following syntax:\n\u003cINSTANCE\u003e-petclinic-service - replacing \u003cINSTANCE\u003e with the value you noted down earlier. \u003cINSTANCE\u003e-petclinic-env - replacing \u003cINSTANCE\u003e with the value you noted down earlier. The wizard will then show a snippet of HTML code that needs to be placed at the top of the pages in the \u003chead\u003e section. The following is an example of the (do not use this snippet, use the one generated by the wizard):",
    "tags": [],
    "title": "3. Real User Monitoring",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/1-petclinic-monolith/4-rum/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts",
    "content": "Welcome to the receiver portion of the workshop! This is the starting point of the data pipeline of the OpenTelemetry Collector. Let’s dive in.\nA receiver, which can be push or pull based, is how data gets into the Collector. Receivers may support one or more data sources. Generally, a receiver accepts data in a specified format, translates it into the internal format and passes it to processors and exporters defined in the applicable pipelines.\n%%{ init:{ \"theme\":\"base\", \"themeVariables\": { \"primaryColor\": \"#ffffff\", \"clusterBkg\": \"#eff2fb\", \"defaultLinkColor\": \"#333333\" } } }%% flowchart LR; style M fill:#e20082,stroke:#333,stroke-width:4px,color:#fff subgraph Collector A[OTLP] --\u003e M(Receivers) B[JAEGER] --\u003e M(Receivers) C[Prometheus] --\u003e M(Receivers) end subgraph Processors M(Receivers) --\u003e H(Filters, Attributes, etc) E(Extensions) end subgraph Exporters H(Filters, Attributes, etc) --\u003e S(OTLP) H(Filters, Attributes, etc) --\u003e T(JAEGER) H(Filters, Attributes, etc) --\u003e U(Prometheus) end",
    "description": "Welcome to the receiver portion of the workshop! This is the starting point of the data pipeline of the OpenTelemetry Collector. Let’s dive in.\nA receiver, which can be push or pull based, is how data gets into the Collector. Receivers may support one or more data sources. Generally, a receiver accepts data in a specified format, translates it into the internal format and passes it to processors and exporters defined in the applicable pipelines.",
    "tags": [],
    "title": "OpenTelemetry Collector Receivers",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/3-receivers/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e RUM",
    "content": "Visit the RUM landing page and and check the overview of the performance of all your RUM enabled applications with the Application Summary Dashboard (Both Mobile and Web based) 1. Visit the RUM Landing Page Login into Splunk Observability. From the left side menu bar select RUM . This will bring you to your the RUM Landing Page.\nThe goal of this page is to give you in a single page, a clear indication of the health, performance and potential errors found in your application(s) and allow you to dive deeper into the information about your User Sessions collected from your web page/App. You will have a pane for each of your active RUM applications. (The view below is the default expanded view)\nIf you have multiple applications, (which will be the case when every attendee is using their own ec2 instance for the RUM workshop), the pane view may be automatically reduced by collapsing the panes as shown below:\nYou can expanded a condensed RUM Application Summary View to the full dashboard by clicking on the small browser or Mobile icon. (Depending on the type of application: Mobile or Browser based) on the left in front of the applications name, highlighted by the red arrow.\nFirst find the right application to use for the workshop:\nIf you are participating in a stand alone RUM workshop, the workshop leader will tell you the name of the application to use, in the case of a combined workshop, it will follow the naming convention we used for IM and APM and use the ec2 node name as a unique id like jmcj-store as shown as the last app in the screenshot above.\n2. Configure the RUM Application Summary Dashboard Header Section RUM Application Summary Dashboard consists of 6 major sections. The first is the selection header, where you can set/filter a number of options:\nA drop down for the Time Window you’re reviewing (You are looking at the past 15 minutes by default) A drop down to select the Environment1 you want to look at. This allows you to focus on just the subset of applications belonging to that environment, or Select all to view all available. A drop down list with the various Apps being monitored. You can use the one provided by the workshop host or select your own. This will focus you on just one application. A drop down to select the Source, Browser or Mobile applications to view. For the Workshop leave All selected. A hamburger menu located at the right of the header allowing you to configure some settings of your Splunk RUM application. (We will visit this in a later section). For the workshop lets do a deeper dive into the Application Summary screen in the next section: Check Health Browser Application\nA common application deployment pattern is to have multiple, distinct application environments that don’t interact directly with each other but that are all being monitored by Splunk APM or RUM: for instance, quality assurance (QA) and production environments, or multiple distinct deployments in different datacenters, regions or cloud providers. A deployment environment is a distinct deployment of your system or application that allows you to set up configurations that don’t overlap with configurations in other deployments of the same application. Separate deployment environments are often used for different stages of the development process, such as development, staging, and production. ↩︎",
    "description": "Visit the RUM landing page and and check the overview of the performance of all your RUM enabled applications with the Application Summary Dashboard (Both Mobile and Web based) 1. Visit the RUM Landing Page Login into Splunk Observability. From the left side menu bar select RUM . This will bring you to your the RUM Landing Page.\nThe goal of this page is to give you in a single page, a clear indication of the health, performance and potential errors found in your application(s) and allow you to dive deeper into the information about your User Sessions collected from your web page/App. You will have a pane for each of your active RUM applications. (The view below is the default expanded view)",
    "tags": [],
    "title": "RUM Landing Page",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/2-rum/3-rum-landing/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "Visit the RUM landing page and and check the overview of the performance of all your RUM enabled applications with the Application Summary Dashboard (Both Mobile and Web based) 1. Visit the RUM Landing Page Login into your Splunk IM/APM/RUM Website. From the left side menu bar select RUM . This will bring you to your the RUM Landing Page.\nThe goal of this page is to give you in a single page, a clear indication of the health, performance and potential errors found in your application(s) and allow you to dive deeper into the information about your User Sessions collected from your web page/App. You will have a pane for each of your active RUM applications. (The view below is the default expanded view)\nIf you have multiple applications, (which will be the case when every attendee is using their own ec2 instance for the RUM workshop), the pane view may be automatically reduced by collapsing the panes as shown below:\nYou can expanded a condensed RUM Application Summary View to the full dashboard by clicking on the small browser or Mobile icon. (Depending on the type of application: Mobile or Browser based) on the left in front of the applications name, highlighted by the red arrow.\nFirst find the right application to use for the workshop:\nIf you are participating in a stand alone RUM workshop, the workshop leader will tell you the name of the application to use, in the case of a combined workshop, it will follow the naming convention we used for IM and APM and use the ec2 node name as a unique id like jmcj-store as shown as the last app in the screenshot above.\n2. Configure the RUM Application Summary Dashboard Header Section RUM Application Summary Dashboard consists of 6 major sections. The first is the selection header, where you can set/filter a number of options:\nA drop down for the Time Window you’re reviewing (You are looking at the past 15 minutes by default) A drop down to select the Environment1 you want to look at. This allows you to focus on just the subset of applications belonging to that environment, or Select all to view all available. A drop down list with the various Apps being monitored. You can use the one provided by the workshop host or select your own. This will focus you on just one application. A drop down to select the Source, Browser or Mobile applications to view. For the Workshop leave All selected. A hamburger menu located at the right of the header allowing you to configure some settings of your Splunk RUM application. (We will visit this in a later section). For the workshop lets do a deeper dive into the Application Summary screen in the next section: Check Health Browser Application\nA common application deployment pattern is to have multiple, distinct application environments that don’t interact directly with each other but that are all being monitored by Splunk APM or RUM: for instance, quality assurance (QA) and production environments, or multiple distinct deployments in different datacenters, regions or cloud providers. A deployment environment is a distinct deployment of your system or application that allows you to set up configurations that don’t overlap with configurations in other deployments of the same application. Separate deployment environments are often used for different stages of the development process, such as development, staging, and production. ↩︎",
    "description": "Visit the RUM landing page and and check the overview of the performance of all your RUM enabled applications with the Application Summary Dashboard (Both Mobile and Web based) 1. Visit the RUM Landing Page Login into your Splunk IM/APM/RUM Website. From the left side menu bar select RUM . This will bring you to your the RUM Landing Page.\nThe goal of this page is to give you in a single page, a clear indication of the health, performance and potential errors found in your application(s) and allow you to dive deeper into the information about your User Sessions collected from your web page/App. You will have a pane for each of your active RUM applications. (The view below is the default expanded view)",
    "tags": [],
    "title": "3. RUM Landing Page",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/3-rum-landing/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 8. Real User Monitoring",
    "content": "We are now looking at the RUM Trace waterfall, this will tell you what happened during the session on the user device as they visited the page of our petclinic application.\nIf you scroll down the waterfall find click on the #!/owners/details segment on the right (1), you see a list of actions that occurred during the handling of the Vets request. Note, that the HTTP request have a blue APM link before the return code. Pick one, and click on the APM link. This will show you the APM info for this backend service call hosted in Kubernetes.\nNote, if you want to drill down to verify what happened with the request, click on the Trace ID url.\nThis will show you the trace related to your request from RUM:\nYou can see that the entry point into your service now has a RUM (1) related content link added, allowing you to return back to your RUM session after you validated what happened in your backend service.",
    "description": "We are now looking at the RUM Trace waterfall, this will tell you what happened during the session on the user device as they visited the page of our petclinic application.\nIf you scroll down the waterfall find click on the #!/owners/details segment on the right (1), you see a list of actions that occurred during the handling of the Vets request. Note, that the HTTP request have a blue APM link before the return code. Pick one, and click on the APM link. This will show you the APM info for this backend service call hosted in Kubernetes.",
    "tags": [],
    "title": "RUM trace Waterfall view \u0026 linking to APM",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/8-rum/3-rum-tour/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 5. Splunk RUM",
    "content": "Sessions A session is a collection of traces that correspond to the actions a single user takes when interacting with an application. By default, a session lasts until 15 minutes have passed from the last event captured in the session. The maximum session duration is 4 hours.\nExercise In the User Sessions table, click on the top Session ID with the longest Duration (over 20 seconds or longer), this will take you to the RUM Session view. Exercise Click the RUM Session Replay Replay button. RUM Session Replay allows you to replay and see the user session. This is a great way to see exactly what the user experienced. Click the button to start the replay. RUM Session Replay can redact information, by default text is redacted. You can also redact images (which has been done for this workshop example). This is useful if you are replaying a session that contains sensitive information. You can also change the playback speed and pause the replay.\nTip When playing back the session, notice how the mouse movements are captured. This is useful to see where the user is focusing their attention.",
    "description": "Sessions A session is a collection of traces that correspond to the actions a single user takes when interacting with an application. By default, a session lasts until 15 minutes have passed from the last event captured in the session. The maximum session duration is 4 hours.\nExercise In the User Sessions table, click on the top Session ID with the longest Duration (over 20 seconds or longer), this will take you to the RUM Session view.",
    "tags": [],
    "title": "3. Session Replay",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/5-rum/3-session-replay/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 8. Splunk Synthetics",
    "content": "We now should have a view similar to the one below.\nExercise In the waterfall find an entry that starts with POST checkout. Click on the \u003e button in front of it to drop open the metadata section. Observe the metadata that is collected, and note the Server-Timing header. This header is what allows us to correlate the test run to a back-end trace. Click on the blue APM link on the POST checkout line in the waterfall. Exercise Validate you see one or more errors for the paymentservice (1). To validate that it’s the same error, click on the related content for Logs (2). Repeat the earlier exercise to filter down to the errors only. View the error log to validate the failed payment due to an invalid token.",
    "description": "We now should have a view similar to the one below.\nExercise In the waterfall find an entry that starts with POST checkout. Click on the \u003e button in front of it to drop open the metadata section. Observe the metadata that is collected, and note the Server-Timing header. This header is what allows us to correlate the test run to a back-end trace. Click on the blue APM link on the POST checkout line in the waterfall.",
    "tags": [],
    "title": "3. Synthetics to APM",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/8-synthetics/3-synthetics-to-apm/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud",
    "content": "We are going to start with a short walkthrough of the various components of Splunk Observability Cloud. The aim of this is to get you familiar with the UI.\nSigning in to Splunk Observability Cloud Application Performance Monitoring (APM) Infrastructure Monitoring Log Observer Tip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "A quick tour of the Splunk Observability Cloud UI.",
    "tags": [],
    "title": "UI - Quick Tour 🚌",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "We are going to start with a short walkthrough of the various components of Splunk Observability Cloud. The aim of this is to get you familiar with the UI.\nSigning in to Splunk Observability Cloud Real User Monitoring (RUM) Application Performance Monitoring (APM) Log Observer Synthetics Infrastructure Monitoring Tip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "A quick tour of the Splunk Observability Cloud UI.",
    "tags": [],
    "title": "UI - Quick Tour 🚌",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Workshop Setup",
    "content": "Workshop Cleanup After completing your workshop session, it’s important to clean up the environment to save costs and ensure resources are properly managed. Use SWiPE to handle the cleanup process efficiently.\nWhat SWiPE Deletes SWiPE automates the following cleanup tasks:\nDelete all users associated with the workshop. Delete the team created for the workshop. Delete tokens, including INGEST, API, and RUM tokens. Delete user dashboards created during the workshop. Advanced Cleanup Features SWiPE also offers advanced cleanup options, such as deleting tokens and detectors. Use these features with caution, as they permanently remove resources.\nDon’t Forget to Delete Workshop Instances In addition to using SWiPE, ensure you delete your workshop instances in Splunk Show to avoid unnecessary costs.\nHappy Workshopping! 🚀",
    "description": "Workshop Cleanup After completing your workshop session, it’s important to clean up the environment to save costs and ensure resources are properly managed. Use SWiPE to handle the cleanup process efficiently.\nWhat SWiPE Deletes SWiPE automates the following cleanup tasks:\nDelete all users associated with the workshop. Delete the team created for the workshop. Delete tokens, including INGEST, API, and RUM tokens. Delete user dashboards created during the workshop.",
    "tags": [],
    "title": "3. Workshop Clean-up",
    "uri": "/observability-workshop/v6.5/en/workshop-setup/3-clean-up/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Alerting and Monitoring with Splunk IT Service Intelligence \u003e 3. Creating Services in ITSI",
    "content": "Starting with an AppDynamics Based Service Access Services: In ITSI click “Configuration”, click on “Services”.\nCreate Service: AD-Ecommerce2: Click “Create Service -\u003e Create Service”.\nService Details (AD-Ecommerce2):\nTitle: “AD-Ecommerce2” Description (Optional): e.g., “Ecommerce Service - version 2” Select Template: Choose “Link service to a service template” and search for “AppDynamics App Performance Monitoring” from the template dropdown. Click Create to save the new service.\nEntity Assignment:\nThe page will load and display the new Service and you will be on the Entities page. This demo defaults to selecting the AD-Ecommerce:18112:demo1.saas.appdynamics.com entity. In a real world situation you would need to match the entity_name to the entity name manually. Direct Entity Selection (If Available): Search for the entity using entity_name=\"AD-Ecommerce:18112:demo1.saas.appdynamics.com\" and select it. Settings: Click the “Settings” tab, enable Backfill and keep that standard 7 days. Enable the Service, and click “Save”\nSetting AD-Ecommerce2’s Service Health as a Dependency for AD.Ecommerce Navigate back to Services page: Click “Configuration -\u003e Services”\nLocate AD.Ecommerce: Find the “AD.Ecommerce” service in the service list.\nEdit AD.Ecommerce: Click “Edit”.\nService Dependencies: Look for the “Service Dependencies” section.\nAdd Dependency: There should be an option to add a dependent service. Search for “AD-Ecommerce2”.\nSelect KPI: Check the box next to ServiceHealthScore for AD-Ecommerce2.\nSave Changes: Save the changes to the “AD.Ecommerce” service.\nVerification Click on “Service Analyzer” and select the “Default Analyzer” Filter the service to just “Buttercup Business Health” Verify that AD-Ecommerce2 is now present below AD.Ecommerce and should be in a grey status.",
    "description": "Starting with an AppDynamics Based Service Access Services: In ITSI click “Configuration”, click on “Services”.\nCreate Service: AD-Ecommerce2: Click “Create Service -\u003e Create Service”.\nService Details (AD-Ecommerce2):\nTitle: “AD-Ecommerce2” Description (Optional): e.g., “Ecommerce Service - version 2” Select Template: Choose “Link service to a service template” and search for “AppDynamics App Performance Monitoring” from the template dropdown. Click Create to save the new service.\nEntity Assignment:\nThe page will load and display the new Service and you will be on the Entities page. This demo defaults to selecting the AD-Ecommerce:18112:demo1.saas.appdynamics.com entity. In a real world situation you would need to match the entity_name to the entity name manually. Direct Entity Selection (If Available): Search for the entity using entity_name=\"AD-Ecommerce:18112:demo1.saas.appdynamics.com\" and select it. Settings: Click the “Settings” tab, enable Backfill and keep that standard 7 days. Enable the Service, and click “Save”",
    "tags": [],
    "title": "Creating an AppD Based Service",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/3-creating-services-in-itsi/2-creating-appd-service/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 3. Reuse Content Across Teams",
    "content": "ITOps teams responsible for monitoring fleets of infrastructure frequently find themselves manually creating dashboards to visualize and analyze metrics, traces, and log data emanating from rapidly changing cloud-native workloads hosted in Kubernetes and serverless architectures, alongside existing on-premises systems. Moreover, due to the absence of a standardized troubleshooting workflow, teams often resort to creating numerous custom dashboards, each resembling the other in structure and content. As a result, administrative overhead skyrockets and MTTR slows.\nTo address this, you can use the out-of-the-box dashboards available in Splunk Observability Cloud for each and every integration. These dashboards are filterable and can be used for ad hoc troubleshooting or as a templated approach to getting users the information they need without having to start from scratch. Not only do the out-of-the-box dashboards provide rich visibility into the infrastructure that is being monitored they can also be cloned.\nExercise: Create a Mirrored Dashboard In Splunk Observability Cloud, click the Global Search button. (Global Search can be used to quickly find content) Search for Pods and select K8s pods (Kubernetes) This will take you to the out-of-the-box Kubernetes Pods dashboard which we will use as a template for mirroring dashboards. In the upper right corner of the dashboard click the Dashboard actions button (3 horizontal dots) -\u003e Click Save As… Enter a dashboard name (i.e. Kubernetes Pods Dashboard) Under Dashboard group search for your e-mail address and select it. Click Save Note: Every Observability Cloud user who has set a password and logged in at least once, gets a user dashboard group and user dashboard. Your user dashboard group is your individual workspace within Observability Cloud.\nAfter saving, you will be taken to the newly created dashboard in the Dashboard Group for your user. This is an example of cloning an out-of-the-box dashboard which can be further customized and enables users to quickly build role, application, or environment relevant views.\nCustom dashboards are meant to be used by multiple people and usually represent a curated set of charts that you want to make accessible to a broad cross-section of your organization. They are typically organized by service, team, or environment.",
    "description": "ITOps teams responsible for monitoring fleets of infrastructure frequently find themselves manually creating dashboards to visualize and analyze metrics, traces, and log data emanating from rapidly changing cloud-native workloads hosted in Kubernetes and serverless architectures, alongside existing on-premises systems. Moreover, due to the absence of a standardized troubleshooting workflow, teams often resort to creating numerous custom dashboards, each resembling the other in structure and content. As a result, administrative overhead skyrockets and MTTR slows.",
    "tags": [],
    "title": "Dashboard Cloning",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/3-reuse-content-across-teams/2-clone-dashboards/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud \u003e 3. Create an Ingest Pipeline",
    "content": "In this section you will review the Kubernetes Audit Logs that are being collected. You can see that the events are quite robust, which can make charting them inefficient. To address this, you will create an Ingest Pipeline in Ingest Processor that will convert these events to metrics that will be sent to Splunk Observability Cloud. This will allow you to chart the events much more efficiently and take advantage of the real-time streaming metrics in Splunk Observability Cloud.\nExercise: Create Ingest Pipeline 1. Open your Ingest Processor Cloud Stack instance using the URL provided in the Splunk Show workshop details.\n2. Navigate to Apps → Search and Reporting\n3. In the search bar, enter the following SPL search string.\nNote Make sure to replace USER_ID with the User ID provided in your Splunk Show instance information.\n### Replace USER_ID with the User ID provided in your Splunk Show instance information index=main sourcetype=\"kube:apiserver:audit:USER_ID\" 4. Press Enter or click the green magnifying glass to run the search.\nNote You should now see the Kubernetes Audit Logs for your environment. Notice that the events are fairly robust. Explore the available fields and start to think about what information would be good candidates for metrics and dimensions. Ask yourself: What fields would I like to chart, and how would I like to be able to filter, group, or split those fields?",
    "description": "In this section you will review the Kubernetes Audit Logs that are being collected. You can see that the events are quite robust, which can make charting them inefficient. To address this, you will create an Ingest Pipeline in Ingest Processor that will convert these events to metrics that will be sent to Splunk Observability Cloud. This will allow you to chart the events much more efficiently and take advantage of the real-time streaming metrics in Splunk Observability Cloud.",
    "tags": [],
    "title": "Review Kubernetes Audit Logs",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/3-create-an-ingest-pipeline/2-review-k8s-events/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 3. Reuse Content Across Teams",
    "content": "Not only do the out-of-the-box dashboards provide rich visibility into the infrastructure that is being monitored they can also be mirrored. This is important because it enables you to create standard dashboards for use by teams throughout your organization. This allows all teams to see any changes to the charts in the dashboard, and members of each team can set dashboard variables and filter customizations relevant to their requirements.\nExercise: Create a Mirrored Dashboard While on the Kubernetes Pods dashboard, you created in the previous step, In the upper right corner of the dashboard click the Dashboard actions button (3 horizontal dots) -\u003e Click Add a mirror…. A configuration modal for the Dashboard Mirror will open.\nUnder My dashboard group search for your e-mail address and select it.\n(Optional) Modify the dashboard in Dashboard name override name.\n(Optional) Add a dashboard description in Dashboard description override.\nUnder Default filter overrides search for k8s.cluster.name and select the name of your Kubernetes cluster.\nUnder Default filter overrides search for store.location and select the city you entered during the workshop setup. Click Save\nYou will now be taken to the newly created dashboard which is a mirror of the Kubernetes Pods dashboard you created in the previous section. Any changes to the original dashboard will be reflected in this dashboard as well. This allows teams to have a consistent yet specific view of the systems they care about and any modifications or updates can be applied in a single location, significantly minimizing the effort needed when compared to updating each individual dashboard.\nIn the next section, you’ll add a new logs-based chart to the original dashboard and see how the dashboard mirror is automatically updated as well.",
    "description": "Not only do the out-of-the-box dashboards provide rich visibility into the infrastructure that is being monitored they can also be mirrored. This is important because it enables you to create standard dashboards for use by teams throughout your organization. This allows all teams to see any changes to the charts in the dashboard, and members of each team can set dashboard variables and filter customizations relevant to their requirements.\nExercise: Create a Mirrored Dashboard While on the Kubernetes Pods dashboard, you created in the previous step, In the upper right corner of the dashboard click the Dashboard actions button (3 horizontal dots) -\u003e Click Add a mirror…. A configuration modal for the Dashboard Mirror will open.",
    "tags": [],
    "title": "Dashboard Mirroring",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/3-reuse-content-across-teams/3-mirror-dashboards/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 3. Dashboards",
    "content": "1 Creating a new chart Let’s now create a new chart and save it in our dashboard!\nSelect the plus icon (top right of the UI) and from the drop down, choose the option Chart. Or click on the + New Chart Button to create a new chart.\nYou will now see a chart template like the following.\nLet’s enter a metric to plot. We are still going to use the metric demo.trans.latency.\nIn the Plot Editor tab under Signal enter demo.trans.latency.\nYou should now have a familiar line chart. Please switch the time to 15 mins.\n2. Filtering and Analytics Let’s now select the Paris datacenter to do some analytics - for that we will use a filter.\nLet’s go back to the Plot Editor tab and click on Add Filter , wait until it automatically populates, choose demo_datacenter, and then Paris.\nIn the F(x) column, add the analytic function Percentile:Aggregation, and leave the value to 95 (click outside to confirm).\nFor info on the Percentile function and the other functions see Chart Analytics.\n3. Using Timeshift analytical function Let’s now compare with older metrics. Click on ... and then on Clone in the dropdown to clone Signal A.\nYou will see a new row identical to A, called B, both visible and plotted.\nFor Signal B, in the F(x) column add the analytic function Timeshift and enter 1w (or 7d for 7 days), and click outside to confirm.\nClick on the cog on the far right, and choose a Plot Color e.g. pink, to change color for the plot of B.\nClick on Close.\nWe now see plots for Signal A (the past 15 minutes) as a blue plot, and the plots from a week ago in pink.\nIn order to make this clearer we can click on the Area chart icon to change the visualization.\nWe now can see when last weeks latency was higher!\nNext, click into the field next to Time on the Override bar and choose Past Hour from the dropdown.\n4. Using Formulas Let’s now plot the difference of all metric values for a day with 7 days in between.\nClick on Enter Formula then enter A-B (A minus B) and hide (deselect) all Signals using the eye, except C.\nWe now see only the difference of all metric values of A and B being plotted. We see that we have some negative values on the plot because a metric value of B has some times larger value than the metric value of A at that time.\nLets look at the Signalflow that drives our Charts and Detectors!",
    "description": "1 Creating a new chart Let’s now create a new chart and save it in our dashboard!\nSelect the plus icon (top right of the UI) and from the drop down, choose the option Chart. Or click on the + New Chart Button to create a new chart.\nYou will now see a chart template like the following.",
    "tags": [],
    "title": "3.3 Using Filters \u0026 Formulas",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/dashboards/filtering/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 4. Correlate Metrics and Logs",
    "content": "In this section, we’ll dive into the seamless correlation of metrics and logs facilitated by the robust naming standards offered by OpenTelemetry. By harnessing the power of OpenTelemetry within Splunk Observability Cloud, we’ll demonstrate how troubleshooting issues becomes significantly more efficient for Site Reliability Engineers (SREs) and operators. With this integration, contextualizing data across various telemetry sources no longer demands manual effort to correlate information. Instead, SREs and operators gain immediate access to the pertinent context they need, allowing them to swiftly pinpoint and resolve issues, improving system reliability and performance.\nExercise: View pod logs The Kubernetes Pods Dashboard you created in the previous section already includes a chart that contains all of the pod logs for your Kubernetes Cluster. The log entries are split by container in this stacked bar chart. To view specific log entries perform the following steps:\nOn the Kubernetes Pods Dashboard click on one of the bar charts. A modal will open with the most recent log entries for the container you’ve selected.\nClick one of the log entries.\nHere we can see the entire log event with all of the fields and values. You can search for specific field names or values within the event itself using the Search for fields bar in the event.\nEnter the city you configured during the application deployment\nThe event will now be filtered to the store.location field. This feature is great for exploring large log entries for specific fields and values unique to your environment or to search for keywords like Error or Failure.\nClose the event using the X in the upper right corner.\nClick the Chart actions (three horizontal dots) on the Pod log event rate chart\nClick View in Log Observer\nThis will take us to Log Observer. In the next section, you’ll create a chart based on log events and add it to the K8s Pod Dashboard you cloned in section 3.2 Dashboard Cloning. You’ll also see how this new chart is automatically added to the mirrored dashboard you created in section 3.3 Dashboard Mirroring.",
    "description": "In this section, we’ll dive into the seamless correlation of metrics and logs facilitated by the robust naming standards offered by OpenTelemetry. By harnessing the power of OpenTelemetry within Splunk Observability Cloud, we’ll demonstrate how troubleshooting issues becomes significantly more efficient for Site Reliability Engineers (SREs) and operators. With this integration, contextualizing data across various telemetry sources no longer demands manual effort to correlate information. Instead, SREs and operators gain immediate access to the pertinent context they need, allowing them to swiftly pinpoint and resolve issues, improving system reliability and performance.",
    "tags": [],
    "title": "Correlate Metrics and Logs",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/4-correlate-metrics-logs/1-correlate-metrics-and-logs/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 4. Correlate Metrics and Logs",
    "content": "In Log Observer, you can perform codeless queries on logs to detect the source of problems in your systems. You can also extract fields from logs to set up log processing rules and transform your data as it arrives or send data to Infinite Logging S3 buckets for future use. See What can I do with Log Observer? to learn more about Log Observer capabilities.\nIn this section, you’ll create a chart filtered to logs that include errors which will be added to the K8s Pod Dashboard you cloned in section 3.2 Dashboard Cloning.\nExercise: Create Log-based Chart Because you drilled into Log Observer from the K8s Pod Dashboard in the previous section, the dashboard will already be filtered to your cluster and store location using the k8s.cluster.name and store.location fields and the bar chart is split by k8s.pod.name. To filter the dashboard to only logs that contain errors complete the following steps:\nLog Observer can be filtered using Keywords or specific key-value pairs.\nIn Log Observer click Add Filter along the top.\nMake sure you’ve selected Fields as the filter type and enter severity in the Find a field… search bar.\nSelect severity from the fields list.\nYou should now see a list of severities and the number of log entries for each.\nUnder Top values, hover over Error and click the = button to apply the filter.\nThe dashboard will now be filtered to only log entries with a severity of Error and the bar chart will be split by the Kubernetes Pod that contains the errors. Next, you’ll save the chart on your Kubernetes Pods Dashboard.\nIn the upper right corner of the Log Observer dashboard click Save.\nSelect Save to Dashboard.\nIn the Chart name field enter a name for your chart.\n(Optional) In the Chart description field enter a description for your chart.\nClick Select Dashboard and search for the name of the Dashboard you cloned in section 3.2 Dashboard Cloning.\nSelect the dashboard in the Dashboard Group for your email address.\nClick OK\nFor the Chart type select Log timeline\nClick Save and go to the dashboard\nYou will now be taken to your Kubernetes Pods Dashboard where you should see the chart you just created for pod errors.\nBecause you updated the original Kubernetes Pods Dashboard, your mirrored dashboard will also include this chart as well! You can see this by clicking the mirrored version of your dashboard along the top of the Dashboard Group for your user.\nNow that you’ve seen how data can be reused across teams by cloning the dashboard, creating dashboard mirrors and how metrics can easily be correlated with logs, let’s take a look at how to create alerts so your teams can be notified when there is an issue with their infrastructure, services, or applications.",
    "description": "In Log Observer, you can perform codeless queries on logs to detect the source of problems in your systems. You can also extract fields from logs to set up log processing rules and transform your data as it arrives or send data to Infinite Logging S3 buckets for future use. See What can I do with Log Observer? to learn more about Log Observer capabilities.\nIn this section, you’ll create a chart filtered to logs that include errors which will be added to the K8s Pod Dashboard you cloned in section 3.2 Dashboard Cloning.",
    "tags": [],
    "title": "Create Log-based Chart",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/4-correlate-metrics-logs/2-create-log-based-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud \u003e 4. Update Pipeline and Visualize Metrics",
    "content": "Now that your metric has dimensions you will create a chart showing the health of different Kubernetes actions using the verb dimension from the events.\nExercise: Visualize Kubernetes Audit Event Metrics 1. If you closed the chart you created in the previous section, in the upper-right corner, click the + Icon → Chart to create a new chart.\n2. In the Plot Editor of the newly created chart enter k8s_audit* in the metric name field. You will use a wildcard here so that you can see all the metrics that are being ingested.\n3. Notice the change from one to many metrics, which is when you updated the pipeline to include the dimensions. Now that we have this metric available, let’s adjust the chart to show us if any of our actions have errors associated with them.\nFirst you’ll filter the Kubernetes events to only those that were not successful using the HTTP response code which is available in the response_status field. We only want events that have a response code of 409, which indicates that there was a conflict (for example a trying to create a resource that already exists) or 503, which indicates that the API was unresponsive for the request.\n4. In the plot editor of your chart click the Add filter, use response_status for the field and select 409.0 and 503.0 for the values.\nNext, you’ll add a function to the chart which will calculate the total number of events grouped by the resource, action, and response status. This will allow us to see exactly which actions and the associated resources had errors. Now we are only looking at Kubernetes events that were not successful.\n5. Click Add analytics → Sum → Sum:Aggregation and add resource, action, and response_status in the Group by field.\n6. Using the chart type along the top buttons, change the chart to a heatmap. Next to the Plot editor, click Chart options. In the Group by section select response_status then action. Change the Color threshold from Auto to Fixed. Click the blue + button to add another threshold. Change the Down arrow to Yellow, the Middle to orange. Leave the Up arrow as red. Enter 5 for the middle threshold and 20 for the upper threshold.\n7. In the upper right corner of the chart click the blue Save as… button. Enter a name for your chart (For Example: Kubernetes Audit Logs - Conflicts and Failures).\n8. On the Choose a dashboard select New dashboard.\n9. Enter a name for your dashboard that includes your initials, so you can easily find it later. Click Save.\n10. Make sure the new dashboard you just created is selected and click Ok.\nYou should now be taken to your new Kubernetes Audit Events dashboard with the chart you created. You can add new charts from other metrics in your environment, such as application errors and response times from the applications running in the Kubernetes cluster, or other Kubernetes metrics such as pod phase, pod memory utilization, etc. giving you a correlated view of your Kubernetes environment from cluster events to application health.\nMake a copy of this chart using the three dots ... in the top right of your chart’s visualization box\nPaste into the same dashboard you’ve been working in using the + icon in the top right of the UI\nClick into your pasted chart and change the visualization to a Column chart.\nChange SUM to just resource, namespace (our filters filter down to just problem codes)\nIn Chart options change title to Kubernetes Audit Logs - Conflicts by Namespace\nClick Save and close\nExercise: Create a detector based on Kubernetes Audit Logs On your Conflicts by Namespace chart click the little bell icon and New detector from chart\nChoose a name and click Create alert rule\nFor Alert condition click Static Threshold and click Proceed to Alert Settings\nEnter a Threshold of 20\nWe wont choose any recipients for this alert so click into Activate and choose Activate Alert Rule and Save\nClick Save one final time in the top right to save your detector\nNavigate back to your dashboard and you will see a detector associated with the chart denoted by a lit up bell icon on the chart\nExercise: Visualize your time series data in Splunk Cloud - Dashboard Studio Now that we have our time series metrics ingested to the Splunk Observability Cloud data store we can easily visualize these time series metrics in Splunk Cloud!\nIn your Splunk Cloud instance browse to Dashboards and select Create New Dashboard\nChoose a Dashboard title, permissions and Dashboard Studio along with any Layout Mode. Click Create.\nIn Dashboard Studio click the chart icon and choose Column\nIn Select data source choose Create splunk observability cloud metric search\nChoose a name for your new datasource and click the Content Import link under Search for metric or metadata\nCopy and paste the URL for your chart into the Content URL field\nClick Import\nSize your chart to your dashboard\nExpand Interactions in the right side of your charts Configuration and click Add Interaction\nCopy the URL from your dashboard in Splunk Observability\nIn On click choose Link to custom URL and add the URL for your dashboard in Splunk Observability Cloud for easy navigation back to the source data. Also choose Open in new tab for friendly navigation.\nClick Save in the top right to save your Dashboard.\nHighlight and click a Column or name in your chart\nYou will be told you are navigating back to Splunk Observability. Click Continue\nYou’ve now navigated back to your corresponding Splunk Observability dashboard from Splunk Cloud.",
    "description": "Now that your metric has dimensions you will create a chart showing the health of different Kubernetes actions using the verb dimension from the events.\nExercise: Visualize Kubernetes Audit Event Metrics 1. If you closed the chart you created in the previous section, in the upper-right corner, click the + Icon → Chart to create a new chart.",
    "tags": [],
    "title": "Visualize Kubernetes Audit Event Metrics",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/4-update-pipeline-and-visualize/2-create-dashboard/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 4. Processors",
    "content": "Attributes Processor The attributes processor modifies attributes of a span, log, or metric. This processor also supports the ability to filter and match input data to determine if they should be included or excluded for specified actions.\nIt takes a list of actions that are performed in the order specified in the config. The supported actions are:\ninsert: Inserts a new attribute in input data where the key does not already exist. update: Updates an attribute in input data where the key does exist. upsert: Performs insert or update. Inserts a new attribute in input data where the key does not already exist and updates an attribute in input data where the key does exist. delete: Deletes an attribute from the input data. hash: Hashes (SHA1) an existing attribute value. extract: Extracts values using a regular expression rule from the input key to target keys specified in the rule. If a target key already exists, it will be overridden. We are going to create an attributes processor to insert a new attribute to all our host metrics called participant.name with a value of your name e.g. marge_simpson.\nWarning Ensure you replace INSERT_YOUR_NAME_HERE with your name and also ensure you do not use spaces in your name.\nLater on in the workshop, we will use this attribute to filter our metrics in Splunk Observability Cloud.\n​ Attributes Processor Configuration processors: batch: resourcedetection/system: detectors: [system] system: hostname_sources: [os] resourcedetection/ec2: detectors: [ec2] attributes/conf: actions: - key: participant.name action: insert value: \"INSERT_YOUR_NAME_HERE\" Ninja: Using connectors to gain internal insights One of the most recent additions to the collector was the notion of a connector, which allows you to join the output of one pipeline to the input of another pipeline.\nAn example of how this is beneficial is that some services emit metrics based on the amount of datapoints being exported, the number of logs containing an error status, or the amount of data being sent from one deployment environment. The count connector helps address this for you out of the box.\nWhy a connector instead of a processor? A processor is limited in what additional data it can produce considering it has to pass on the data it has processed making it hard to expose additional information. Connectors do not have to emit the data they receive which means they provide an opportunity to create the insights we are after.\nFor example, a connector could be made to count the number of logs, metrics, and traces that do not have the deployment environment attribute.\nA very simple example with the output of being able to break down data usage by deployment environment.\nConsiderations with connectors A connector only accepts data exported from one pipeline and receiver by another pipeline, this means you may have to consider how you construct your collector config to take advantage of it.\nReferences https://opentelemetry.io/docs/collector/configuration/#connectors https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/connector/countconnector Configuration Check-in That’s processors covered, let’s check our configuration changes.\nCheck-inReview your configuration ​ config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 # To limit exposure to denial of service attacks, change the host in endpoints below from 0.0.0.0 to a specific network interface. # See https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/security-best-practices.md#safeguards-against-denial-of-service-attacks extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 receivers: hostmetrics: collection_interval: 10s scrapers: # CPU utilization metrics cpu: # Disk I/O metrics disk: # File System utilization metrics filesystem: # Memory utilization metrics memory: # Network interface I/O metrics \u0026 TCP connection metrics network: # CPU load metrics load: # Paging/Swap space utilization and I/O metrics paging: # Process count metrics processes: # Per process CPU, Memory and Disk I/O metrics. Disabled by default. # process: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 opencensus: endpoint: 0.0.0.0:55678 # Collect own metrics prometheus/internal: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 10s static_configs: - targets: ['0.0.0.0:8888'] jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_binary: endpoint: 0.0.0.0:6832 thrift_compact: endpoint: 0.0.0.0:6831 thrift_http: endpoint: 0.0.0.0:14268 zipkin: endpoint: 0.0.0.0:9411 processors: batch: resourcedetection/system: detectors: [system] system: hostname_sources: [os] resourcedetection/ec2: detectors: [ec2] attributes/conf: actions: - key: participant.name action: insert value: \"INSERT_YOUR_NAME_HERE\" exporters: debug: verbosity: detailed service: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [otlp, opencensus, prometheus] processors: [batch] exporters: [debug] logs: receivers: [otlp] processors: [batch] exporters: [debug] extensions: [health_check, pprof, zpages]",
    "description": "Attributes Processor The attributes processor modifies attributes of a span, log, or metric. This processor also supports the ability to filter and match input data to determine if they should be included or excluded for specified actions.\nIt takes a list of actions that are performed in the order specified in the config. The supported actions are:\ninsert: Inserts a new attribute in input data where the key does not already exist. update: Updates an attribute in input data where the key does exist. upsert: Performs insert or update. Inserts a new attribute in input data where the key does not already exist and updates an attribute in input data where the key does exist. delete: Deletes an attribute from the input data. hash: Hashes (SHA1) an existing attribute value. extract: Extracts values using a regular expression rule from the input key to target keys specified in the rule. If a target key already exists, it will be overridden. We are going to create an attributes processor to insert a new attribute to all our host metrics called participant.name with a value of your name e.g. marge_simpson.",
    "tags": [],
    "title": "OpenTelemetry Collector Processors",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/4-processors/3-attributes/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 4. Building Resilience",
    "content": "To assess the Agent’s resilience, we’ll simulate a temporary gateway outage and observe how the agent handles it:\nSummary:\nSend Traces to the Agent – Generate traffic by sending traces to the agent. Stop the Gateway – This will trigger the agent to enter retry mode. Restart the Gateway – The agent will recover traces from its persistent queue and forward them successfully. Without the persistent queue, these traces would have been lost permanently. Exercise Simulate a network failure: In the Gateway terminal stop the gateway with Ctrl-C and wait until the gateway console shows that it has stopped:\n2025-01-28T13:24:32.785+0100 info service@v0.120.0/service.go:309 Shutdown complete. Send traces: In the Spans terminal window send five more traces using the loadgen.\nNotice that the agent’s retry mechanism is activated as it continuously attempts to resend the data. In the agent’s console output, you will see repeated messages similar to the following:\n2025-01-28T14:22:47.020+0100 info internal/retry_sender.go:126 Exporting failed. Will retry the request after interval. {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"otlphttp\", \"error\": \"failed to make an HTTP request: Post \\\"http://localhost:5318/v1/traces\\\": dial tcp 127.0.0.1:5318: connect: connection refused\", \"interval\": \"9.471474933s\"} Stop the Agent: In the Agent terminal window, use Ctrl-C to stop the agent. Wait until the agent’s console confirms it has stopped:\n2025-01-28T14:40:28.702+0100 info extensions/extensions.go:66 Stopping extensions... 2025-01-28T14:40:28.702+0100 info service@v0.120.0/service.go:309 Shutdown complete. Tip Stopping the agent will halt its retry attempts and prevent any future retry activity.\nIf the agent runs for too long without successfully delivering data, it may begin dropping traces, depending on the retry configuration, to conserve memory. By stopping the agent, any metrics, traces, or logs currently stored in memory are lost before being dropped, ensuring they remain available for recovery.\nThis step is essential for clearly observing the recovery process when the agent is restarted.",
    "description": "To assess the Agent’s resilience, we’ll simulate a temporary gateway outage and observe how the agent handles it:\nSummary:\nSend Traces to the Agent – Generate traffic by sending traces to the agent. Stop the Gateway – This will trigger the agent to enter retry mode. Restart the Gateway – The agent will recover traces from its persistent queue and forward them successfully. Without the persistent queue, these traces would have been lost permanently. Exercise Simulate a network failure: In the Gateway terminal stop the gateway with Ctrl-C and wait until the gateway console shows that it has stopped:",
    "tags": [],
    "title": "4.3 Simulate Failure",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/4-building-resilience/4-3-failure/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 4. Sensitive Data",
    "content": "The redaction processor gives precise control over which attributes and values are permitted or removed from telemetry data.\nIn this exercise, we will redact the user.visa \u0026 user.mastercard values in the span data before it is exported by the Agent. Exercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config=gateway.yaml Enable the redaction/redact processor: In the Agent terminal window, edit agent.yaml and remove the # we inserted in the previous exercise.\ntraces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes - redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window start the loadgen:\n../loadgen -count 1 Check the debug output: For both the Agent and Gateway confirm the values for user.visa \u0026 user.mastercard have been updated. Notice user.amex attribute value was NOT redacted because a matching regex pattern was not added to blocked_values\n​ New Debug Output Original Debug Output -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(UNKNOWN NUMBER) -\u003e user.email: Str(62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287) -\u003e payment.amount: Double(69.71) -\u003e user.visa: Str(****) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(****) -\u003e redaction.masked.keys: Str(user.mastercard,user.visa) -\u003e redaction.masked.count: Int(2) -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(65.54) Note By including summary:debug in the redaction processor, the debug output will include summary information about which matching key values were redacted, along with the count of values that were masked.\n-\u003e redaction.masked.keys: Str(user.mastercard,user.visa) -\u003e redaction.masked.count: Int(2) Check file output: Using jq verify that user.visa \u0026 user.mastercard have been updated in the gateway-traces.out.\n​ Validate attribute changes Output jq '.resourceSpans[].scopeSpans[].spans[].attributes[] | select(.key == \"user.visa\" or .key == \"user.mastercard\" or .key == \"user.amex\") | {key: .key, value: .value.stringValue}' ./gateway-traces.out Notice that user.amex has not been redacted because a matching regex pattern was not added to blocked_values:\n{ \"key\": \"user.visa\", \"value\": \"****\" } { \"key\": \"user.amex\", \"value\": \"3782 822463 10005\" } { \"key\": \"user.mastercard\", \"value\": \"****\" } These are just a couple of examples of how attributes and redaction processors can be configured to protect sensitive data.\nImportant Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "The redaction processor gives precise control over which attributes and values are permitted or removed from telemetry data.\nIn this exercise, we will redact the user.visa \u0026 user.mastercard values in the span data before it is exported by the Agent. Exercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config=gateway.yaml Enable the redaction/redact processor: In the Agent terminal window, edit agent.yaml and remove the # we inserted in the previous exercise.",
    "tags": [],
    "title": "4.3 Test Redaction Processor",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/4-sensitive-data/4-3-test-redaction/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 4. Sensitive Data",
    "content": "The redaction processor gives precise control over which attributes and values are permitted or removed from telemetry data.\nIn this exercise, we will redact the user.visa \u0026 user.mastercard values in the span data before it is exported by the Agent. Exercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config=gateway.yaml Enable the redaction/redact processor: In the Agent terminal window, edit agent.yaml and remove the # we inserted in the previous exercise.\ntraces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes - redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Loadgen terminal window start the loadgen:\n../loadgen -count 1 Check the debug output: For both the Agent and Gateway confirm the values for user.visa \u0026 user.mastercard have been updated. Notice user.amex attribute value was NOT redacted because a matching regex pattern was not added to blocked_values\n​ New Debug Output Original Debug Output -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(UNKNOWN NUMBER) -\u003e user.email: Str(62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287) -\u003e payment.amount: Double(69.71) -\u003e user.visa: Str(****) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(****) -\u003e redaction.masked.keys: Str(user.mastercard,user.visa) -\u003e redaction.masked.count: Int(2) -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(65.54) Note By including summary:debug in the redaction processor, the debug output will include summary information about which matching key values were redacted, along with the count of values that were masked.\n-\u003e redaction.masked.keys: Str(user.mastercard,user.visa) -\u003e redaction.masked.count: Int(2) Check file output: Using jq verify that user.visa \u0026 user.mastercard have been updated in the gateway-traces.out.\n​ Validate attribute changes Output jq '.resourceSpans[].scopeSpans[].spans[].attributes[] | select(.key == \"user.visa\" or .key == \"user.mastercard\" or .key == \"user.amex\") | {key: .key, value: .value.stringValue}' ./gateway-traces.out Notice that user.amex has not been redacted because a matching regex pattern was not added to blocked_values:\n{ \"key\": \"user.visa\", \"value\": \"****\" } { \"key\": \"user.amex\", \"value\": \"3782 822463 10005\" } { \"key\": \"user.mastercard\", \"value\": \"****\" } These are just a couple of examples of how attributes and redaction processors can be configured to protect sensitive data.\nImportant Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "The redaction processor gives precise control over which attributes and values are permitted or removed from telemetry data.\nIn this exercise, we will redact the user.visa \u0026 user.mastercard values in the span data before it is exported by the Agent. Exercise Start the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config=gateway.yaml Enable the redaction/redact processor: In the Agent terminal window, edit agent.yaml and remove the # we inserted in the previous exercise.",
    "tags": [],
    "title": "4.3 Test Redaction Processor",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/4-sensitive-data/4-3-test-redaction/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 5. Transform Data",
    "content": "This test verifies that the com.splunk/source and os.type metadata have been removed from the log resource attributes before being exported by the Agent. Additionally, the test ensures that:\nThe log body is parsed to extract severity information. SeverityText and SeverityNumber are set on the LogRecord. JSON fields from the log body are promoted to log attributes. This ensures proper metadata filtering, severity mapping, and structured log enrichment before exporting.\nExercise Check the debug output: For both the Agent and Gateway confirm that com.splunk/source and os.type have been removed:\n​ Gateway Debug Output Agent Debug Output Resource attributes: -\u003e com.splunk.sourcetype: Str(quotes) -\u003e host.name: Str(workshop-instance) -\u003e otelcol.service.mode: Str(agent) Resource attributes: -\u003e com.splunk.source: Str(./quotes.log) -\u003e com.splunk.sourcetype: Str(quotes) -\u003e host.name: Str(workshop-instance) -\u003e os.type: Str(linux) -\u003e otelcol.service.mode: Str(agent) For both the Agent and Gateway confirm that SeverityText and SeverityNumber in the LogRecord is now defined with the severity level from the log body. Confirm that the JSON fields from the body can be accessed as top-level log Attributes:\n​ Gateway Debug Output Agemt Debug Output \u003csnip\u003e SeverityText: WARN SeverityNumber: Warn(13) Body: Str({\"level\":\"WARN\",\"message\":\"Your focus determines your reality.\",\"movie\":\"SW\",\"timestamp\":\"2025-03-07 11:17:26\"}) Attributes: -\u003e log.file.path: Str(quotes.log) -\u003e level: Str(WARN) -\u003e message: Str(Your focus determines your reality.) -\u003e movie: Str(SW) -\u003e timestamp: Str(2025-03-07 11:17:26) \u003c/snip\u003e \u003csnip\u003e SeverityText: SeverityNumber: Unspecified(0) Body: Str({\"level\":\"WARN\",\"message\":\"Your focus determines your reality.\",\"movie\":\"SW\",\"timestamp\":\"2025-03-07 11:17:26\"}) Attributes: -\u003e log.file.path: Str(quotes.log) \u003c/snip\u003e Check file output: In the new gateway-logs.out file verify the data has been transformed:\n​ jq Query Example Output jq '[.resourceLogs[].scopeLogs[].logRecords[] | {severityText, severityNumber, body: .body.stringValue}]' gateway-logs.out [ { \"severityText\": \"DEBUG\", \"severityNumber\": 5, \"body\": \"{\\\"level\\\":\\\"DEBUG\\\",\\\"message\\\":\\\"All we have to decide is what to do with the time that is given us.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"WARN\", \"severityNumber\": 13, \"body\": \"{\\\"level\\\":\\\"WARN\\\",\\\"message\\\":\\\"The Force will be with you. Always.\\\",\\\"movie\\\":\\\"SW\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"ERROR\", \"severityNumber\": 17, \"body\": \"{\\\"level\\\":\\\"ERROR\\\",\\\"message\\\":\\\"One does not simply walk into Mordor.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"DEBUG\", \"severityNumber\": 5, \"body\": \"{\\\"level\\\":\\\"DEBUG\\\",\\\"message\\\":\\\"Do or do not, there is no try.\\\",\\\"movie\\\":\\\"SW\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" } ] [ { \"severityText\": \"ERROR\", \"severityNumber\": 17, \"body\": \"{\\\"level\\\":\\\"ERROR\\\",\\\"message\\\":\\\"There is some good in this world, and it's worth fighting for.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" } ] Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "This test verifies that the com.splunk/source and os.type metadata have been removed from the log resource attributes before being exported by the Agent. Additionally, the test ensures that:\nThe log body is parsed to extract severity information. SeverityText and SeverityNumber are set on the LogRecord. JSON fields from the log body are promoted to log attributes. This ensures proper metadata filtering, severity mapping, and structured log enrichment before exporting.\nExercise Check the debug output: For both the Agent and Gateway confirm that com.splunk/source and os.type have been removed:",
    "tags": [],
    "title": "5.3 Test Transform Processor",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/5-transform-data/5-3-test-transform/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 5. Transform Data",
    "content": "This test verifies that the com.splunk/source and os.type metadata have been removed from the log resource attributes before being exported by the Agent. Additionally, the test ensures that:\nThe log body is parsed to extract severity information. SeverityText and SeverityNumber are set on the LogRecord. JSON fields from the log body are promoted to log attributes. This ensures proper metadata filtering, severity mapping, and structured log enrichment before exporting.\nExercise Check the debug output: For both the Agent and Gateway confirm that com.splunk/source and os.type have been removed:\n​ Gateway Debug Output Agent Debug Output Resource attributes: -\u003e com.splunk.sourcetype: Str(quotes) -\u003e host.name: Str(workshop-instance) -\u003e otelcol.service.mode: Str(agent) Resource attributes: -\u003e com.splunk.source: Str(./quotes.log) -\u003e com.splunk.sourcetype: Str(quotes) -\u003e host.name: Str(workshop-instance) -\u003e os.type: Str(linux) -\u003e otelcol.service.mode: Str(agent) For both the Agent and Gateway confirm that SeverityText and SeverityNumber in the LogRecord is now defined with the severity level from the log body. Confirm that the JSON fields from the body can be accessed as top-level log Attributes:\n​ Gateway Debug Output Agemt Debug Output \u003csnip\u003e SeverityText: WARN SeverityNumber: Warn(13) Body: Str({\"level\":\"WARN\",\"message\":\"Your focus determines your reality.\",\"movie\":\"SW\",\"timestamp\":\"2025-03-07 11:17:26\"}) Attributes: -\u003e log.file.path: Str(quotes.log) -\u003e level: Str(WARN) -\u003e message: Str(Your focus determines your reality.) -\u003e movie: Str(SW) -\u003e timestamp: Str(2025-03-07 11:17:26) \u003c/snip\u003e \u003csnip\u003e SeverityText: SeverityNumber: Unspecified(0) Body: Str({\"level\":\"WARN\",\"message\":\"Your focus determines your reality.\",\"movie\":\"SW\",\"timestamp\":\"2025-03-07 11:17:26\"}) Attributes: -\u003e log.file.path: Str(quotes.log) \u003c/snip\u003e Check file output: In the new gateway-logs.out file verify the data has been transformed:\n​ jq Query Example Output jq '[.resourceLogs[].scopeLogs[].logRecords[] | {severityText, severityNumber, body: .body.stringValue}]' gateway-logs.out [ { \"severityText\": \"DEBUG\", \"severityNumber\": 5, \"body\": \"{\\\"level\\\":\\\"DEBUG\\\",\\\"message\\\":\\\"All we have to decide is what to do with the time that is given us.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"WARN\", \"severityNumber\": 13, \"body\": \"{\\\"level\\\":\\\"WARN\\\",\\\"message\\\":\\\"The Force will be with you. Always.\\\",\\\"movie\\\":\\\"SW\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"ERROR\", \"severityNumber\": 17, \"body\": \"{\\\"level\\\":\\\"ERROR\\\",\\\"message\\\":\\\"One does not simply walk into Mordor.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"DEBUG\", \"severityNumber\": 5, \"body\": \"{\\\"level\\\":\\\"DEBUG\\\",\\\"message\\\":\\\"Do or do not, there is no try.\\\",\\\"movie\\\":\\\"SW\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" } ] [ { \"severityText\": \"ERROR\", \"severityNumber\": 17, \"body\": \"{\\\"level\\\":\\\"ERROR\\\",\\\"message\\\":\\\"There is some good in this world, and it's worth fighting for.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" } ] Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "This test verifies that the com.splunk/source and os.type metadata have been removed from the log resource attributes before being exported by the Agent. Additionally, the test ensures that:\nThe log body is parsed to extract severity information. SeverityText and SeverityNumber are set on the LogRecord. JSON fields from the log body are promoted to log attributes. This ensures proper metadata filtering, severity mapping, and structured log enrichment before exporting.\nExercise Check the debug output: For both the Agent and Gateway confirm that com.splunk/source and os.type have been removed:",
    "tags": [],
    "title": "5.3 Test Transform Processor",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/5-transform-data/5-3-test-transform/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 6. Service",
    "content": "Resource Detection Processor We also added resourcedetection/system and resourcedetection/ec2 processors so that the collector can capture the instance hostname and AWS/EC2 metadata. We now need to enable these two processors under the metrics pipeline.\nUpdate the processors section to include resourcedetection/system and resourcedetection/ec2 under the metrics pipeline:\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch, resourcedetection/system, resourcedetection/ec2] exporters: [debug]",
    "description": "Resource Detection Processor We also added resourcedetection/system and resourcedetection/ec2 processors so that the collector can capture the instance hostname and AWS/EC2 metadata. We now need to enable these two processors under the metrics pipeline.\nUpdate the processors section to include resourcedetection/system and resourcedetection/ec2 under the metrics pipeline:\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch, resourcedetection/system, resourcedetection/ec2] exporters: [debug]",
    "tags": [],
    "title": "OpenTelemetry Collector Service",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/6-service/3-resourcedetection/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 6. Sensitive Data",
    "content": "The redaction processor gives precise control over which attributes and values are permitted or removed from telemetry data.\nIn this exercise, we will redact the user.visa \u0026 user.mastercard values in the span data before it is exported by the agent. Exercise Prepare the terminals: Delete the *.out files and clear the screen.\nStart the Gateway: In your Gateway terminal window start the gateway.\n../otelcol --config=gateway.yaml Enable the redaction/redact processor: In the Agent terminal window, edit agent.yaml and remove the # we inserted in the previous exercise.\ntraces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes - redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp Start the Agent: In your Agent terminal window start the agent.\n../otelcol --config=agent.yaml Start the Load Generator: In the Spans terminal window start the loadgen:\n../loadgen -count 1 Check the debug output: For both the agent and gateway confirm the values for user.visa \u0026 user.mastercard have been updated. Notice user.amex attribute value was NOT redacted because a matching regex pattern was not added to blocked_values\n​ New Debug Output Original Debug Output -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(UNKNOWN NUMBER) -\u003e user.email: Str(62d5e03d8fd5808e77aee5ebbd90cf7627a470ae0be9ffd10e8025a4ad0e1287) -\u003e payment.amount: Double(69.71) -\u003e user.visa: Str(****) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(****) -\u003e redaction.masked.keys: Str(user.mastercard,user.visa) -\u003e redaction.masked.count: Int(2) -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(65.54) Note By including summary:debug in the redaction processor, the debug output will include summary information about which matching key values were redacted, along with the count of values that were masked.\n-\u003e redaction.masked.keys: Str(user.mastercard,user.visa) -\u003e redaction.masked.count: Int(2) Check file output: Using jq verify that user.visa \u0026 user.mastercard have been updated in the gateway-traces.out.\n​ Validate attribute changes Output jq '.resourceSpans[].scopeSpans[].spans[].attributes[] | select(.key == \"user.visa\" or .key == \"user.mastercard\" or .key == \"user.amex\") | {key: .key, value: .value.stringValue}' ./gateway-traces.out Notice that user.amex has not been redacted because a matching regex pattern was not added to blocked_values:\n{ \"key\": \"user.visa\", \"value\": \"****\" } { \"key\": \"user.amex\", \"value\": \"3782 822463 10005\" } { \"key\": \"user.mastercard\", \"value\": \"****\" } These are just a few examples of how attributes and redaction processors can be configured to protect sensitive data.\nImportant Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "The redaction processor gives precise control over which attributes and values are permitted or removed from telemetry data.\nIn this exercise, we will redact the user.visa \u0026 user.mastercard values in the span data before it is exported by the agent. Exercise Prepare the terminals: Delete the *.out files and clear the screen.\nStart the Gateway: In your Gateway terminal window start the gateway.",
    "tags": [],
    "title": "6.3 Test Redaction Processor",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/6-sensitive-data/6-3-test-redaction/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 6. Routing Data",
    "content": "Exercise In this section, we will test the routing rule configured for the Gateway. The expected result is that a span generated by the loadgen that match the \"[deployment.environment\"] == \"security-applications\" rule will be sent to the gateway-traces-route2-security.out file.\nStart the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config agent.yaml Send a Regular Span: In the Loadgen terminal window send a regular span using the loadgen:\n../loadgen -count 1 Both the Agent and Gateway will display debug information. The gateway will also generate a new gateway-traces-route1-regular.out file, as this is now the designated destination for regular spans.\nTip If you check gateway-traces-route1-regular.out, it will contain the span sent by loadgen. You will also see an empty gateway-traces-route2-security..out file, as the routing configuration creates output files immediately, even if no matching spans have been processed yet.\nSend a Security Span: In the Loadgen terminal window send a security span using the security flag:\n../loadgen -security -count 1 Again, both the Agent and Gateway should display debug information, including the span you just sent. This time, the Gateway will write a line to the gateway-traces-route2-security.out file, which is designated for spans where the deployment.environment resource attribute matches \"security-applications\".\n​ Validate resource attribute matches Output jq -c '.resourceSpans[] as $resource | $resource.scopeSpans[].spans[] | {spanId: .spanId, deploymentEnvironment: ($resource.resource.attributes[] | select(.key == \"deployment.environment\") | .value.stringValue)}' gateway-traces-route2-security.out {\"spanId\":\"cb799e92e26d5782\",\"deploymentEnvironment\":\"security-applications\"} You can repeat this scenario multiple times, and each trace will be written to its corresponding output file.\nImportant Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.\nConclusion In this section, we successfully tested the routing connector in the gateway by sending different spans and verifying their destinations.\nRegular spans were correctly routed to gateway-traces-route1-regular.out, confirming that spans without a matching deployment.environment attribute follow the default pipeline.\nSecurity-related spans were routed to gateway-traces-route2-security.out, demonstrating that the routing rule based on \"deployment.environment\": \"security-applications\" works as expected.\nBy inspecting the output files, we confirmed that the OpenTelemetry Collector correctly evaluates span attributes and routes them to the appropriate destinations. This validates that routing rules can effectively separate and direct telemetry data for different use cases.\nYou can now extend this approach by defining additional routing rules to further categorize spans, metrics, and logs based on different attributes.",
    "description": "Exercise In this section, we will test the routing rule configured for the Gateway. The expected result is that a span generated by the loadgen that match the \"[deployment.environment\"] == \"security-applications\" rule will be sent to the gateway-traces-route2-security.out file.\nStart the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config gateway.yaml Start the Agent: In your Agent terminal window start the Agent.",
    "tags": [],
    "title": "6.3 Test Routing Connector",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/6-routing-data/6-3-test-routing/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 6. Routing Data",
    "content": "Exercise In this section, we will test the routing rule configured for the Gateway. The expected result is that a span generated by the loadgen that match the \"[deployment.environment\"] == \"security-applications\" rule will be sent to the gateway-traces-route2-security.out file.\nStart the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config gateway.yaml Start the Agent: In your Agent terminal window start the Agent.\n../otelcol --config agent.yaml Send a Regular Span: In the Loadgen terminal window send a regular span using the loadgen:\n../loadgen -count 1 Both the Agent and Gateway will display debug information. The gateway will also generate a new gateway-traces-route1-regular.out file, as this is now the designated destination for regular spans.\nTip If you check gateway-traces-route1-regular.out, it will contain the span sent by loadgen. You will also see an empty gateway-traces-route2-security..out file, as the routing configuration creates output files immediately, even if no matching spans have been processed yet.\nSend a Security Span: In the Loadgen terminal window send a security span using the security flag:\n../loadgen -security -count 1 Again, both the Agent and Gateway should display debug information, including the span you just sent. This time, the Gateway will write a line to the gateway-traces-route2-security.out file, which is designated for spans where the deployment.environment resource attribute matches \"security-applications\".\n​ Validate resource attribute matches Output jq -c '.resourceSpans[] as $resource | $resource.scopeSpans[].spans[] | {spanId: .spanId, deploymentEnvironment: ($resource.resource.attributes[] | select(.key == \"deployment.environment\") | .value.stringValue)}' gateway-traces-route2-security.out {\"spanId\":\"cb799e92e26d5782\",\"deploymentEnvironment\":\"security-applications\"} You can repeat this scenario multiple times, and each trace will be written to its corresponding output file.\nImportant Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.\nConclusion In this section, we successfully tested the routing connector in the gateway by sending different spans and verifying their destinations.\nRegular spans were correctly routed to gateway-traces-route1-regular.out, confirming that spans without a matching deployment.environment attribute follow the default pipeline.\nSecurity-related spans were routed to gateway-traces-route2-security.out, demonstrating that the routing rule based on \"deployment.environment\": \"security-applications\" works as expected.\nBy inspecting the output files, we confirmed that the OpenTelemetry Collector correctly evaluates span attributes and routes them to the appropriate destinations. This validates that routing rules can effectively separate and direct telemetry data for different use cases.\nYou can now extend this approach by defining additional routing rules to further categorize spans, metrics, and logs based on different attributes.",
    "description": "Exercise In this section, we will test the routing rule configured for the Gateway. The expected result is that a span generated by the loadgen that match the \"[deployment.environment\"] == \"security-applications\" rule will be sent to the gateway-traces-route2-security.out file.\nStart the Gateway: In your Gateway terminal window start the Gateway.\n../otelcol --config gateway.yaml Start the Agent: In your Agent terminal window start the Agent.",
    "tags": [],
    "title": "6.3 Test Routing Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/6-routing-data/6-3-test-routing/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 7. Count \u0026 Sum Connector",
    "content": "Exercise Start the Gateway:\nIn the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent:\nIn the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Loadgen:\nIn the Spans terminal window send 8 spans with the following loadgen command:\n​ Loadgen ../loadgen -count 8 Both the Agent and Gateway will display debug information, showing they are processing data. Wait until the loadgen completes.\nVerify the metrics:\nWhile processing the span, the Agent, has generated metrics and passed them on to the Gateway. The Gateway has written them into gateway-metrics.out.\nTo confirm the presence of user.card-chargeand verify each one has an attribute user.name in the metrics output, run the following jq query:\n​ jq query command jq example output jq -r '.resourceMetrics[].scopeMetrics[].metrics[] | select(.name == \"user.card-charge\") | .sum.dataPoints[] | \"\\(.attributes[] | select(.key == \"user.name\").value.stringValue)\\t\\(.asDouble)\"' gateway-metrics.out | while IFS=$'\\t' read -r name charge; do printf \"%-20s %s\\n\" \"$name\" \"$charge\" done George Lucas 67.49 Frodo Baggins 87.14 Thorin Oakenshield 90.98 Luke Skywalker 51.37 Luke Skywalker 65.56 Thorin Oakenshield 67.5 Thorin Oakenshield 66.66 Peter Jackson 94.39 Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "Exercise Start the Gateway:\nIn the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent:\nIn the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Loadgen:\nIn the Spans terminal window send 8 spans with the following loadgen command:",
    "tags": [],
    "title": "7.3 Testing the Count Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/7-sum-count/7-3-sum-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 7. Transform Data",
    "content": "This test verifies that the com.splunk/source and os.type metadata have been removed from the log resource attributes before being exported by the agent. Additionally, the test ensures that:\nThe log body is parsed to extract severity information. SeverityText and SeverityNumber are set on the LogRecord. JSON fields from the log body are promoted to log attributes. This ensures proper metadata filtering, severity mapping, and structured log enrichment before export.\nExercise Check the debug output: For both the agent and gateway confirm that com.splunk/source and os.type have been removed:\n​ New Debug Output Original Debug Output Resource attributes: -\u003e com.splunk.sourcetype: Str(quotes) -\u003e host.name: Str(workshop-instance) -\u003e otelcol.service.mode: Str(agent) Resource attributes: -\u003e com.splunk.source: Str(./quotes.log) -\u003e com.splunk.sourcetype: Str(quotes) -\u003e host.name: Str(workshop-instance) -\u003e os.type: Str(linux) -\u003e otelcol.service.mode: Str(agent) For both the agent and gateway confirm that SeverityText and SeverityNumber in the LogRecord is now defined with the severity level from the log body. Confirm that the JSON fields from the body can be accessed as top-level log Attributes:\n​ New Debug Output Original Debug Output \u003csnip\u003e SeverityText: WARN SeverityNumber: Warn(13) Body: Str({\"level\":\"WARN\",\"message\":\"Your focus determines your reality.\",\"movie\":\"SW\",\"timestamp\":\"2025-03-07 11:17:26\"}) Attributes: -\u003e log.file.path: Str(quotes.log) -\u003e level: Str(WARN) -\u003e message: Str(Your focus determines your reality.) -\u003e movie: Str(SW) -\u003e timestamp: Str(2025-03-07 11:17:26) \u003c/snip\u003e \u003csnip\u003e SeverityText: SeverityNumber: Unspecified(0) Body: Str({\"level\":\"WARN\",\"message\":\"Your focus determines your reality.\",\"movie\":\"SW\",\"timestamp\":\"2025-03-07 11:17:26\"}) Attributes: -\u003e log.file.path: Str(quotes.log) \u003c/snip\u003e Check file output: In the new gateway-logs.out file verify the data has been transformed:\n​ jq Query Example Output jq '[.resourceLogs[].scopeLogs[].logRecords[] | {severityText, severityNumber, body: .body.stringValue}]' gateway-logs.out [ { \"severityText\": \"DEBUG\", \"severityNumber\": 5, \"body\": \"{\\\"level\\\":\\\"DEBUG\\\",\\\"message\\\":\\\"All we have to decide is what to do with the time that is given us.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"WARN\", \"severityNumber\": 13, \"body\": \"{\\\"level\\\":\\\"WARN\\\",\\\"message\\\":\\\"The Force will be with you. Always.\\\",\\\"movie\\\":\\\"SW\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"ERROR\", \"severityNumber\": 17, \"body\": \"{\\\"level\\\":\\\"ERROR\\\",\\\"message\\\":\\\"One does not simply walk into Mordor.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" }, { \"severityText\": \"DEBUG\", \"severityNumber\": 5, \"body\": \"{\\\"level\\\":\\\"DEBUG\\\",\\\"message\\\":\\\"Do or do not, there is no try.\\\",\\\"movie\\\":\\\"SW\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" } ] [ { \"severityText\": \"ERROR\", \"severityNumber\": 17, \"body\": \"{\\\"level\\\":\\\"ERROR\\\",\\\"message\\\":\\\"There is some good in this world, and it's worth fighting for.\\\",\\\"movie\\\":\\\"LOTR\\\",\\\"timestamp\\\":\\\"2025-03-07 11:56:29\\\"}\" } ] Important Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "This test verifies that the com.splunk/source and os.type metadata have been removed from the log resource attributes before being exported by the agent. Additionally, the test ensures that:\nThe log body is parsed to extract severity information. SeverityText and SeverityNumber are set on the LogRecord. JSON fields from the log body are promoted to log attributes. This ensures proper metadata filtering, severity mapping, and structured log enrichment before export.\nExercise Check the debug output: For both the agent and gateway confirm that com.splunk/source and os.type have been removed:",
    "tags": [],
    "title": "7.3 Test Transform Processor",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/7-transform-data/7-3-test-transform/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 8. Routing Data",
    "content": "Exercise In this section, we will test the routing rule configured for the Gateway. The expected result is that thespan generated by the loadgen will be sent to the gateway-traces-security.out file.\nStart the Gateway: In your Gateway terminal window start the gateway.\n../otelcol --config gateway.yaml Start the Agent: In your Agent terminal window start the agent.\n../otelcol --config agent.yaml Send a Regular Span: In the Spans terminal window send a regular span using the loadgen:\n../loadgen -count 1 Both the agent and gateway will display debug information. The gateway will also generate a new gateway-traces-standard.out file, as this is now the designated destination for regular spans.\nTip If you check gateway-traces-standard.out, it will contain the span sent by loadgen. You will also see an empty gateway-traces-security.out file, as the routing configuration creates output files immediately, even if no matching spans have been processed yet.\nSend a Security Span: In the Spans terminal window send a security span using the security flag:\n../loadgen -security -count 1 Again, both the agent and gateway should display debug information, including the span you just sent. This time, the gateway will write a line to the gateway-traces-security.out file, which is designated for spans where the deployment.environment resource attribute matches \"security-applications\". The gateway-traces-standard.out should be unchanged.\n​ Validate resource attribute matches Output jq -c '.resourceSpans[] as $resource | $resource.scopeSpans[].spans[] | {spanId: .spanId, deploymentEnvironment: ($resource.resource.attributes[] | select(.key == \"deployment.environment\") | .value.stringValue)}' gateway-traces-security.out {\"spanId\":\"cb799e92e26d5782\",\"deploymentEnvironment\":\"security-applications\"} You can repeat this scenario multiple times, and each trace will be written to its corresponding output file.\nImportant Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.\nConclusion In this section, we successfully tested the routing connector in the gateway by sending different spans and verifying their destinations.\nRegular spans were correctly routed to gateway-traces-standard.out, confirming that spans without a matching deployment.environment attribute follow the default pipeline.\nSecurity-related spans were routed to gateway-traces-security.out, demonstrating that the routing rule based on \"deployment.environment\": \"security-applications\" works as expected.\nBy inspecting the output files, we confirmed that the OpenTelemetry Collector correctly evaluates span attributes and routes them to the appropriate destinations. This validates that routing rules can effectively separate and direct telemetry data for different use cases.\nYou can now extend this approach by defining additional routing rules to further categorize spans, metrics, and logs based on different attributes.",
    "description": "Exercise In this section, we will test the routing rule configured for the Gateway. The expected result is that thespan generated by the loadgen will be sent to the gateway-traces-security.out file.\nStart the Gateway: In your Gateway terminal window start the gateway.\n../otelcol --config gateway.yaml Start the Agent: In your Agent terminal window start the agent.\n../otelcol --config agent.yaml Send a Regular Span: In the Spans terminal window send a regular span using the loadgen:",
    "tags": [],
    "title": "8.3 Test Routing Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/8-routing-data/8-3-test-routing/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 9. Count \u0026 Sum Connector",
    "content": "Exercise Start the Gateway:\nIn the Gateway terminal window navigate to the [WORKSHOP]/9-sum-count directory and run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent:\nIn the Agent terminal window navigate to the [WORKSHOP]/9-sum-count directory and run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Loadgen:\nIn the Spans terminal window navigate to the [WORKSHOP]/9-sum-count directory. Send 8 spans with the following loadgen command:\n​ Loadgen ../loadgen -count 8 Both the agent and gateway will display debug information, showing they are processing data. Wait until the loadgen completes.\nVerify that metrics\nWhile processing the span, the Agent, has generated metrics and passed them on to the Gateway. The Gateway has written them into gateway-metrics.out.\nTo confirm the presence of user.card-chargeand verify each one has an attribute user.name in the metrics output, run the following jq query:\n​ jq query command jq example output jq -r '.resourceMetrics[].scopeMetrics[].metrics[] | select(.name == \"user.card-charge\") | .sum.dataPoints[] | \"\\(.attributes[] | select(.key == \"user.name\").value.stringValue)\\t\\(.asDouble)\"' gateway-metrics.out | while IFS=$'\\t' read -r name charge; do printf \"%-20s %s\\n\" \"$name\" \"$charge\" done George Lucas 67.49 Frodo Baggins 87.14 Thorin Oakenshield 90.98 Luke Skywalker 51.37 Luke Skywalker 65.56 Thorin Oakenshield 67.5 Thorin Oakenshield 66.66 Peter Jackson 94.39 Important Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "Exercise Start the Gateway:\nIn the Gateway terminal window navigate to the [WORKSHOP]/9-sum-count directory and run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Start the Agent:\nIn the Agent terminal window navigate to the [WORKSHOP]/9-sum-count directory and run:\n​ Start the Agent ../otelcol --config=agent.yaml Start the Loadgen:\nIn the Spans terminal window navigate to the [WORKSHOP]/9-sum-count directory. Send 8 spans with the following loadgen command:",
    "tags": [],
    "title": "9.3 Testing the Count Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/9-sum-count/9-3-sum-test/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences",
    "content": "Introduction This workshop walks you through using the Chrome DevTools Recorder to create a synthetic test on a Splunk demonstration environment or on your own public website.\nThe exported JSON from the Chrome DevTools Recorder will then be used to create a Splunk Synthetic Monitoring Real Browser Test.\nPre-requisites Google Chrome Browser installed Publicly browser-accessible URL Access to Splunk Observability Cloud Supporting resources Lantern: advanced Selectors for multistep browser tests Chrome for Developers DevTools Tips web.dev Core Web Vitals reference",
    "description": "Proactively find and fix performance issues across user flows, business transactions and APIs to deliver better digital experiences.",
    "tags": [],
    "title": "Advanced Synthetics",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/3-advanced-synthetics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios",
    "content": "Tagging WorkshopThis workshop shows how tags can be used to reduce the time required for SREs to isolate issues across services, so they know which team to engage to troubleshoot the issue further, and can provide context to help engineering get a head start on debugging.\nProfiling WorkshopThis workshop shows how Database Query Performance and AlwaysOn Profiling can be used to reduce the time required for engineers to debug problems in microservices.",
    "description": "This scenario helps engineering teams identify and fix issues caused by planned and unplanned changes to their microservices-based applications.",
    "tags": [],
    "title": "Debug Problems in Microservices",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources",
    "content": "Applying context to your metrics One conversation that frequently comes up is Dimensions vs Properties and when you should use one verus the other. Instead of starting off with their descriptions it makes sense to understand how we use them and how they are similar, before diving into their differences and examples of why you would use one or the other.\nHow are Dimensions and Properties similar? The simplest answer is that they are both metadata key:value pairs that add context to our metrics. Metrics themselves are what we actually want to measure, whether it’s a standard infrastructure metric like​ ​cpu.utilization or a custom metric like number of API calls received.\nIf we receive a value of 50% for th​e ​cpu.utilization metric without knowing where it came from or any other context it is just a number and not useful to us. We would need at least to know what host it comes from.\nThese days it is likely we care more about the performance or utilization of a cluster or data center as a whole then that of an individual host and therefore more interested in things like the average cpu.utilization across a cluster of hosts, when a​ host’s ​cpu.utilization is a outlier when compared to other hosts running the same service or maybe compare the ​average ​cpu.utilization of one environment to another.\nTo be able to slice, aggregate or g​roup our ​cpu.utilization metrics in this way we will need additional metad​ata for the ​cpu.utilization metrics we receive to include what cluster a host belongs to, what service is running on the host and what environment it is a part of. This metadata can be in the form of either dimension or property key:value pairs.\nFor example, if I go to apply a filter to a dashboard or use the Group by function when running analytics, I can use a property or a dimension.\nSo how are Dimensions and Properties different? Dimensions are sent in with metrics at the time of ingest while properties are applied to metrics or dimensions after ingest. This means that any metadata you need to make a datapoint (​a single reported value of a metric) ​unique, like what host a value of cpu.utilization is coming from needs to be a dimension. Metric names + dimensions uniquely define an MTS (metric time series).\nExample: the ​cpu.utilization metric sent by a particular host (​server1) with a dimension ​host:server1 would be considered a unique time series. If you have 10 servers, each sending that metric, then you would have 10 time series, with each time series sharing the metric name ​cpu.utilization and uniquely identified by the dimension key-value pair (​host:server1, host:server2…host:server10).\nHowever, if your server names are only unique within a datacenter vs your whole environment you would need to add a 2n​d​ dimension dc for the datacenter location. You could now have double the number of possible MTSs. cpu.utilization metrics received would now be uniquely identified by 2 sets of dimension key-value pairs.\ncpu.utilization plus ​dc:east \u0026​ host:server1 would create a different time series then ​cpu.utilization plus dc:west \u0026 ​host:server1.\nDimensions are immutable while properties are mutable As we mentioned above, Metric name + dimensions make a unique MTS. Therefore, if the dimension value changes we will have a new unique combination of metric name + dimension value and create a new MTS.\nProperties on the other hand are applied to metrics (or dimensions) after they are ingested. If you apply a property to a metric, it propagates and applies to all MTS that the metric is a part of. Or if you apply a property to a dimension, say ​host:server1 then all metrics from that host will have those properties attached. If you change the value of a property it will propagate and update the value of the property to all MTSs with that property attached. Why is this important? It means that if you care about the historical value of a property you need to make it a dimension.\nExample: We are collecting custom metrics on our application. One metric is ​latency which counts the latency of requests made to our application. We have a dimension ​customer, so we can sort and compare latency by customer. We decide we want to track the ​application version as well so we can sort and compare our application ​latency by the version customers are using. We create a property ​version that we attach to the customer dimension. Initially all customers are using application version 1, so ​version:1.\nWe now have some customers using version 2 of our application, for those customers we update the property to ​version:2. When we update the value of the ​version property for those customers it will propagate down to all MTS for that customer. We lose the history that those customers at some point used ​version 1, so if we wanted to compare ​latency of ​version 1 and ​version 2 over a historical period we would not get accurate data. In this case even though we don’t need application ​version to make out metric time series unique we need to make ​version a dimension, because we care about the historical value.\nSo when should something be a Property instead of a dimension? The first reason would be if there is any metadata you want attached to metrics, but you don’t know it at the time of ingest. The second reason is best practice is if it doesn’t need to be a dimension, make it a property. Why? One reason is that today there is a limit of 5K MTSs per analytics job or chart rendering and the more dimensions you have the more MTS you will create. Properties are completely free-form and let you add as much information as you want or need to metrics or dimensions without adding to MTS counts.\nAs dimensions are sent in with every datapoint, the more dimensions you have the more data you send to us, which could mean higher costs to you if your cloud provider charges for data transfer.\nA good example of some things that should be properties would be additional host information. You want to be able to see things like machine_type, processor, or os, but instead of making these things dimensions and sending them with every metric from a host you could make them properties and attach the properties to the host dimension.\nExample where ​host:server1 you would set properties ​machine_type:ucs, processor:xeon-5560, os:rhel71. Anytime a metric comes in with the dimension ​host:server1 all the above properties will be applied to it automatically.\nSome other examples of use cases for properties would be if you want to know who is the escalation contact for each service or SLA level for every customer. You do not need these items to make metrics uniquely identifiable and you don’t care about the historical values, so they can be properties. The properties could be added to the service dimension and customer dimensions and would then apply to all metrics and MTSs with those dimensions.\nWhat about Tags? Tags are the 3r​d​ type of metadata that can be used to give context to or help organize your metrics. Unlike dimensions and properties, tags are NOT key:value pairs. ​Tags can be thought of as labels or keywords. Similar to Properties, Tags are applied to your data after ingest via the Catalog in the UI or programmatically via the API.​ Tags can be applied to Metrics, Dimensions or other objects such as Detectors.\nWhere would I use Tags? Tags are used when there is a need for a many-to-one relationship of tags to an object or a one-to-many relationship between the tag and the objects you are applying them to. They are useful for grouping together metrics that may not be intrinsically associated.\nOne example is you have hosts that run multiple applications. You can create tags (labels) for each application and then apply multiple tags to each host to label the applications that are running on it.\nExample: Server1 runs 3 applications. You create tags ​app1, app2 and app3 and apply all 3 tags to the dimension ​host:server1\nTo expand on the example above let us say you also collect metrics from your applications.​ You could apply the tags you created to any metrics coming in from the applications themselves. You can filter based on a tag allowing you to filter based on an application, but get the full picture including both application and the relevant host metrics.\nExample: App1 sends in metrics with the dimension ​service:application1. You would apply tag ​app1 to the dimension ​service:application1. You can then filter on the tag ​app1 in charts and dashboards.\nAnother use case for tags for binary states where there is just one possible value. An example is you do canary testing and when you do a canary deployment you want to be able to mark the hosts that received the new code, so you can easily identify their metrics and compare their performance to those hosts that did not receive the new code. There is no need for a key:value pair as there is just a single value “canary”.\nBe aware that while you can filter on tags you cannot use the groupBy function on them. The groupBy function is run by supplying the key part of a key:value pair and the results are then grouped by values of that key pair.\nAdditional information For information on sending dimensions for custom metrics please review the ​Client Libraries documentation for your library of choice.\nFor information on how to apply properties \u0026 tags to metrics or dimensions via the API please see the API documentation for ​/metric/:name​ ​/dimension/:key/:value\nFor information on how to add or edit properties and tags via the Metadata Catalog in the UI please reference the section ​Add or edit metadata in Search the Metric Finder and Metadata catalog.",
    "description": "One conversation that frequently comes up is Dimensions vs Properties and when you should use one verus the other.",
    "tags": [],
    "title": "Dimension, Properties and Tags",
    "uri": "/observability-workshop/v6.5/en/resources/dimensions-properties-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources \u003e Local Hosting",
    "content": "Proxmox Workshop Instance Setup Overview The ubuntu-cloud.sh script automates the creation of a Splunk Observability Workshop VM on Proxmox VE. It creates a complete Ubuntu 22.04 cloud-init based VM with all necessary tools and configurations pre-installed.\nPrerequisites Proxmox VE cluster with administrative access Internet connectivity for downloading cloud images and packages Available VM ID range and storage space Valid SWiPE ID for workshop access Quick Start Run the script directly on your Proxmox host:\nbash -c \"$(curl -fsSL https://raw.githubusercontent.com/splunk/observability-workshop/refs/heads/main/local-hosting/proxmox/ubuntu-cloud.sh)\" BETA: k3d script:\nbash -c \"$(curl -fsSL https://raw.githubusercontent.com/splunk/observability-workshop/refs/heads/main/local-hosting/proxmox/ubuntu-cloud-k3d.sh)\" Or download and run locally:\nwget https://raw.githubusercontent.com/splunk/observability-workshop/refs/heads/main/local-hosting/proxmox/ubuntu-cloud.sh chmod +x ubuntu-cloud.sh ./ubuntu-cloud.sh What the Script Does 1. Initial Setup Updates package repositories and installs required tools (jq, curl) Displays interactive prompts for user confirmation and SWiPE ID input 2. Authentication \u0026 Configuration Validates SWiPE ID against the Splunk workshop API Retrieves workshop tokens and configuration (REALM, RUM_TOKEN, INGEST_TOKEN, etc.) Generates unique VM ID and hostname 3. VM Creation Downloads Ubuntu 22.04 Jammy cloud image Resizes disk to 20GB Creates Proxmox VM with: Memory: 8GB RAM CPU: 4 cores (host CPU type) Storage: Uses local-lvm storage Network: Bridged to vmbr0 with DHCP Boot: UEFI with cloud-init support 4. Software Installation The cloud-init configuration automatically installs:\nContainer Tools: Docker, Docker Compose Kubernetes: K3s, kubectl, Helm, K9s Development: OpenJDK 17, Maven, Python3, Git Infrastructure: Terraform, Ansible Monitoring: Chaos Mesh 5. Workshop Content Downloads Splunk Observability Workshop materials Sets up Kubernetes secrets with workshop tokens Configures private container registry Prepares demo applications and content VM Specifications OS: Ubuntu 22.04 LTS (Jammy) Memory: 8GB RAM CPU: 4 cores Disk: 20GB (expandable) User: splunk / Splunk123! SSH: Password authentication enabled Environment Variables The script configures these environment variables in the VM:\nRUM_TOKEN: Real User Monitoring token ACCESS_TOKEN: Data ingest token API_TOKEN: Splunk API token HEC_TOKEN: HTTP Event Collector token HEC_URL: HEC endpoint URL REALM: Splunk realm INSTANCE: Unique hostname CLUSTER_NAME: Kubernetes cluster name Accessing the VM Wait for VM creation and boot to complete (5-10 minutes)\nFind the VM’s IP address in Proxmox console or DHCP logs\nSSH to the VM:\nssh splunk@\u003cvm-ip\u003e # Password: Splunk123! Useful Commands in the VM # Check Kubernetes status kubectl get nodes # Access Kubernetes dashboard k9s # View workshop materials ls ~/workshop/ # Check environment variables env | grep -E \"(TOKEN|REALM)\" Troubleshooting Invalid SWiPE ID: Verify your workshop registration and ID VM creation fails: Check Proxmox storage space and permissions Network issues: Ensure vmbr0 bridge is properly configured Slow deployment: Allow extra time for cloud-init to complete all installations Tags Created VMs are tagged with: o11y-workshop, jammy, cloudinit",
    "description": "Learn how to create a local hosting environment in Proxmox VE",
    "tags": [],
    "title": "Local Hosting with Proxmox",
    "uri": "/observability-workshop/v6.5/en/resources/local-hosting/proxmox/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops",
    "content": "The goal is to walk through the basic steps to configure the following components of the Splunk Observability Cloud platform:\nSplunk Infrastructure Monitoring (IM) Splunk Zero Configuration Auto Instrumentation for NodeJS (APM) AlwaysOn Profiling Splunk Log Observer (LO) We will deploy the OpenTelemetry Astronomy Shop application in Kubernetes, which contains two NodeJS services (Frontend \u0026 Payment Service). Once the application and the OpenTelemetry Connector are up and running, we will start seeing metrics, traces and logs via the Zero Configuration Auto Instrumentation for NodeJS that will be used by the Splunk Observability Cloud platform to provide insights into the application.\nPrerequisites Outbound SSH access to port 2222. Outbound HTTP access to port 8083. Familiarity with the bash shell and vi/vim editor. Next, we will deploy the OpenTelemetry Demo.\ngraph TD subgraph Service Diagram accountingservice(Accounting Service):::golang adservice(Ad Service):::java cache[(Cache)] cartservice(Cart Service):::dotnet checkoutservice(Checkout Service):::golang currencyservice(Currency Service):::cpp emailservice(Email Service):::ruby frauddetectionservice(Fraud Detection Service):::kotlin frontend(Frontend):::typescript frontendproxy(Frontend Proxy):::cpp loadgenerator([Load Generator]):::python paymentservice(Payment Service):::javascript productcatalogservice(Product Catalog Service):::golang quoteservice(Quote Service):::php recommendationservice(Recommendation Service):::python shippingservice(Shipping Service):::rust featureflagservice(Feature Flag Service):::erlang featureflagstore[(Feature Flag Store)] queue[(queue)] Internet --\u003e|HTTP| frontendproxy frontendproxy --\u003e|HTTP| frontend frontendproxy --\u003e|HTTP| featureflagservice loadgenerator --\u003e|HTTP| frontendproxy accountingservice --\u003e|TCP| queue cartservice ---\u003e|gRPC| featureflagservice checkoutservice ---\u003e|gRPC| cartservice --\u003e cache checkoutservice ---\u003e|gRPC| productcatalogservice checkoutservice ---\u003e|gRPC| currencyservice checkoutservice ---\u003e|HTTP| emailservice checkoutservice ---\u003e|gRPC| paymentservice checkoutservice --\u003e|gRPC| shippingservice checkoutservice ---\u003e|TCP| queue frontend --\u003e|gRPC| adservice frontend --\u003e|gRPC| cartservice frontend --\u003e|gRPC| productcatalogservice frontend --\u003e|gRPC| checkoutservice frontend --\u003e|gRPC| currencyservice frontend --\u003e|gRPC| recommendationservice --\u003e|gRPC| productcatalogservice frontend --\u003e|gRPC| shippingservice --\u003e|HTTP| quoteservice frauddetectionservice --\u003e|TCP| queue adservice ---\u003e|gRPC| featureflagservice productcatalogservice --\u003e|gRPC| featureflagservice recommendationservice --\u003e|gRPC| featureflagservice shippingservice --\u003e|gRPC| featureflagservice featureflagservice --\u003e featureflagstore end classDef dotnet fill:#178600,color:white; classDef cpp fill:#f34b7d,color:white; classDef erlang fill:#b83998,color:white; classDef golang fill:#00add8,color:black; classDef java fill:#b07219,color:white; classDef javascript fill:#f1e05a,color:black; classDef kotlin fill:#560ba1,color:white; classDef php fill:#4f5d95,color:white; classDef python fill:#3572A5,color:white; classDef ruby fill:#701516,color:white; classDef rust fill:#dea584,color:black; classDef typescript fill:#e98516,color:black; graph TD subgraph Service Legend dotnetsvc(.NET):::dotnet cppsvc(C++):::cpp erlangsvc(Erlang/Elixir):::erlang golangsvc(Go):::golang javasvc(Java):::java javascriptsvc(JavaScript):::javascript kotlinsvc(Kotlin):::kotlin phpsvc(PHP):::php pythonsvc(Python):::python rubysvc(Ruby):::ruby rustsvc(Rust):::rust typescriptsvc(TypeScript):::typescript end classDef dotnet fill:#178600,color:white; classDef cpp fill:#f34b7d,color:white; classDef erlang fill:#b83998,color:white; classDef golang fill:#00add8,color:black; classDef java fill:#b07219,color:white; classDef javascript fill:#f1e05a,color:black; classDef kotlin fill:#560ba1,color:white; classDef php fill:#4f5d95,color:white; classDef python fill:#3572A5,color:white; classDef ruby fill:#701516,color:white; classDef rust fill:#dea584,color:black; classDef typescript fill:#e98516,color:black;",
    "description": "A workshop using Zero Configuration Auto-Instrumentation for NodeJS.",
    "tags": [],
    "title": "NodeJS Zero-Config Workshop",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/3-nodejs-kubernetes/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "OpenTelemetry Collector ConceptsLearn the concepts of the OpenTelemetry Collector and how to use it to send data to Splunk Observability Cloud.\nAdvanced OpenTelemetry CollectorPractice setting up the OpenTelemetry Collector configuration from scratch and go though several advanced configuration scenarios's.",
    "description": "OpenTelemetry Collector ConceptsLearn the concepts of the OpenTelemetry Collector and how to use it to send data to Splunk Observability Cloud.\nAdvanced OpenTelemetry CollectorPractice setting up the OpenTelemetry Collector configuration from scratch and go though several advanced configuration scenarios's.",
    "tags": [],
    "title": "OpenTelemetry Collector Workshops",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 6.2 Optional Exercise",
    "content": "Let’s look at some other parts of the UI like the Information Pane on the right of the navigator or the Related Content Pane at the bottom.\nFirst, let’s look at the Information Pane, this pane provides alert and detected services information and the metadata related to the object you’re looking at.\nMeta Data is sent along with the metrics and is very useful for identifying trends when looking into issues. An example could be a pod failing when deployed on a specific Operating System.\nExercise Can you identify the Operating System and Architecture of the node from the metadata? As we have seen in the previous exercise, these fields are very useful for filtering the view in charts and Navigators down to a specific subset of metrics we are interested in.\nAnother feature in the UI is Related content.\nRelated Content The Splunk Observability User Interface will attempt to show you additional information that is related to what you’re actively looking at. A good example of this is the Kubernetes Navigator showing you related Content tiles in the information Pane for the services found running on this node.\nIn the Information Pane, you should see two tiles for services detected, the two databases used by our e-commerce application. Let’s use this Related Content.\nExercise First, make sure you no longer have a filter for the development namespace active. (Simply click on the x to remove it from the Filter Pane) as there are no databases in the Development Namespace. Hoover on the Redis tile, and click on the Goto all my Redis instances button The Navigator view should change to the overall Redis instances view. Select the the instance running on your cluster. (Click on the blue link, named redis-[the name of your workshop], in the Redis Instances pane). We should now see just the information for your Redis Instance \u0026 there should also be an Information Pane. Again we see Meta Data, but we also see that UI is showing in the Related Content tiles that this Redis Server runs in a Container running on Kubernetes. Let’s verify that by clicking on the Kubernetes Tile. We should be back in the Kubernetes Navigator, at the container level. Confirm that the names of our cluster and node are all visible at the top of the page and we are back looking at our K8s Cluster, where we started. This completes the tour of Splunk Observability Cloud. Let’s go and look at our e-commerce site and do some shopping.",
    "description": "Let’s look at some other parts of the UI like the Information Pane on the right of the navigator or the Related Content Pane at the bottom.\nFirst, let’s look at the Information Pane, this pane provides alert and detected services information and the metadata related to the object you’re looking at.\nMeta Data is sent along with the metrics and is very useful for identifying trends when looking into issues. An example could be a pod failing when deployed on a specific Operating System.",
    "tags": [],
    "title": "Infrastructure Exercise - Part 3",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/30-im-exercise/3-im-exercise/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6.2 Optional Exercise",
    "content": "Let’s look at some other parts of the UI like the Information Pane on the right of the navigator or the Related Content Pane at the bottom.\nFirst, let’s look at the Information Pane, this pane provides alert and detected services information and the metadata related to the object you’re looking at.\nMeta Data is sent along with the metrics and is very useful for identifying trends when looking into issues. An example could be a pod failing when deployed on a specific Operating System.\nExercise Can you identify the Operating System and Architecture of the node from the metadata? As we have seen in the previous exercise, these fields are very useful for filtering the view in charts and Navigators down to a specific subset of metrics we are interested in.\nAnother feature in the UI is Related content.\nRelated Content The Splunk Observability User Interface will attempt to show you additional information that is related to what you’re actively looking at. A good example of this is the Kubernetes Navigator showing you related Content tiles in the information Pane for the services found running on this node.\nIn the Information Pane, you should see two tiles for services detected, the two databases used by our e-commerce application. Let’s use this Related Content.\nExercise First, make sure you no longer have a filter for the development namespace active. (Simply click on the x to remove it from the Filter Pane) as there are no databases in the Development Namespace. Hoover on the Redis tile, and click on the Goto all my Redis instances button The Navigator view should change to the overall Redis instances view. Select the the instance running on your cluster. (Click on the blue link, named redis-[the name of your workshop], in the Redis Instances pane). We should now see just the information for your Redis Instance \u0026 there should also be an Information Pane. Again we see Meta Data, but we also see that UI is showing in the Related Content tiles that this Redis Server runs in a Container running on Kubernetes. Let’s verify that by clicking on the Kubernetes Tile. We should be back in the Kubernetes Navigator, at the container level. Confirm that the names of our cluster and node are all visible at the top of the page and we are back looking at our K8s Cluster, where we started. This completes the tour of Splunk Observability Cloud. Let’s go and look at our e-commerce site and do some shopping.",
    "description": "Let’s look at some other parts of the UI like the Information Pane on the right of the navigator or the Related Content Pane at the bottom.\nFirst, let’s look at the Information Pane, this pane provides alert and detected services information and the metadata related to the object you’re looking at.\nMeta Data is sent along with the metrics and is very useful for identifying trends when looking into issues. An example could be a pod failing when deployed on a specific Operating System.",
    "tags": [],
    "title": "Infrastructure Exercise - Part 3",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/30-im-exercise/3-im-exercise/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices",
    "content": "Service Maps and Traces are extremely valuable in determining what service an issue resides in. And related log data helps provide detail on why issues are occurring in that service.\nBut engineers sometimes need to go even deeper to debug a problem that’s occurring in one of their services.\nThis is where features such as Splunk’s AlwaysOn Profiling and Database Query Performance come in.\nAlwaysOn Profiling continuously collects stack traces so that you can discover which lines in your code are consuming the most CPU and memory.\nAnd Database Query Performance can quickly identify long-running, unoptimized, or heavy queries and mitigate issues they might be causing.\nIn this workshop, we’ll explore:\nHow to debug an application with several performance issues. How to use Database Query Performance to find slow-running queries that impact application performance. How to enable AlwaysOn Profiling and use it to find the code that consumes the most CPU and memory. How to apply fixes based on findings from Splunk Observability Cloud and verify the result. The workshop uses a Java-based application called The Door Game hosted in Kubernetes. Let’s get started!\nTip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "This workshop shows how Database Query Performance and AlwaysOn Profiling can be used to reduce the time required for engineers to debug problems in microservices.",
    "tags": [],
    "title": "Profiling Workshop",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/profiling/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources \u003e Local Hosting",
    "content": "Demo-in-a-box is a method for running demo apps easily using a web interface.\nIt provides:\nA quick way to deploy demo apps and states A way to easily change configuration of your otel collector and see logs Get pod status, pod logs, etc. To leverage this locally using multipass:\nFollow the local hosting for multipass instructions In the terraform.tfvars file, set splunk_diab to true and make sure all other options are set to false Then set the other required and important tokens/url Then run the terraform steps Once the instance is up, navigate in your browser to: http://\u003cIP\u003e:8083 In the terraform.tfvars file the wsversion defaults to the current version of the workshop e.g 4.64: To use the latest developments change wsversion to use main There are only three versions of the workshop maintained, development (main) current (e.g. 4.64 and the previous (e.g. 4.63) After making the change, run terraform apply to make the changes Now you can deploy any of the demos; this will also deploy the collector as part of the deployment",
    "description": "Learn how to use Demo-in-a-Box to manage demos and otel collectors in an easy-to-use web interface.",
    "tags": [],
    "title": "Running Demo-in-a-Box",
    "uri": "/observability-workshop/v6.5/en/resources/local-hosting/diab/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "Optimize Cloud MonitoringThis scenario is for ITOps teams managing a hybrid infrastructure that need to troubleshoot cloud-native performance issues, by correlating real-time metrics with logs to troubleshoot faster, improve MTTD/MTTR, and optimize costs.\nDebug Problems in MicroservicesThis scenario helps engineering teams identify and fix issues caused by planned and unplanned changes to their microservices-based applications.\nOptimize End User ExperiencesUse Splunk Real User Monitoring (RUM) and Synthetics to get insight into end user experience, and proactively test scenarios to improve that experience.",
    "description": "Learn how to build observability solutions with Splunk",
    "tags": [],
    "title": "Scenarios",
    "uri": "/observability-workshop/v6.5/en/scenarios/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk OnCall \u003e 1. Getting Started",
    "content": "Aim Routing Keys map the incoming alert messages from your monitoring system to an Escalation Policy which in turn sends the notifications to the appropriate team.\nNote that routing keys are case insensitive and should only be composed of letters, numbers, hyphens, and underscores.\nThe aim of this module is for you to create some routing keys and then link them to your Escalation Policies you have created in the previous exercise.\n1. Instance ID Each participant requires a unique Routing Key so we use the Hostname of the EC2 Instance you were allocated. We are only doing this to ensure your Routing Key is unique and we know all Hostnames are unique. In a production deployment the Routing Key would typically reflect the name of a System or Service being monitored, or a Team such as 1st Line Support etc.\nYour welcome e-mail informed you of the details of your EC2 Instance that has been provided for you to use during this workshop and you should have logged into this as part of the 1st exercise.\nThe e-mail also contained the Hostname of the Instance, but you can also obtain it from the Instance directly. To get your Hostname from within the shell session connected to your Instance run the following command:\n​ Export Hostname Example Output echo ${HOSTNAME} zevn It is very important that when creating the Routing Keys you use the 4 letter hostname allocated to you as a Detector has been configured within Splunk Infrastructure Monitoring using this hostname, so any deviation will cause future exercises to fail.\n2 Create Routing Keys Navigate to Settings on the main menu bar, you should now be at the Routing Keys page.\nYou are going to create the following two Routing Keys using the naming conventions listed in the following table, but replacing {==HOSTNAME==} with the value from above and replace TEAM_NAME with the team you were allocated or created earlier.\nRouting Key Escalation Policies HOSTNAME_PRI TEAM_NAME : Primary HOSTNAME_WR TEAM_NAME : Waiting Room There will probably already be a number of Routing Keys configured, but to add a new one simply scroll to the bottom of the page and then click Add Key\nIn the left hand box, enter the name for the key as per the table above. In the Routing Key column, select your Teams Primary policy from the drop down in the Escalation Polices column. You can start typing your Team Name to filter the results.\nNote If there are a large number of participants on the workshop, resulting in an unusually large number of Escalation Policies sometimes the search filter does not list all the Policies under your Team Name. If this happens instead of using the search feature, simply scroll down to your team name, all the policies will then be listed.\nRepeat the above steps for both Keys, xxxx_PRI and xxxx_WR, mapping them to your Teams Primary and Waiting Room policies.\nYou should now have two Routing Keys configured, similar to the following:\nTip You can assign a Routing Key to multiple Escalation Policies if required by simply selecting more from the list\nIf you now navigate back to Teams → [Your Team Name] → Escalation Policies and look at the settings for your Primary and Waiting Room polices you will see that these now have Routes assigned to them.\nThe 24/7 policy does not have a Route assigned as this will only be triggered via an Execute Policy escalation from the Primary policy.\nPlease wait for the instructor before proceeding to the Incident Lifecycle/Overview module.",
    "description": "Aim Routing Keys map the incoming alert messages from your monitoring system to an Escalation Policy which in turn sends the notifications to the appropriate team.\nNote that routing keys are case insensitive and should only be composed of letters, numbers, hyphens, and underscores.\nThe aim of this module is for you to create some routing keys and then link them to your Escalation Policies you have created in the previous exercise.",
    "tags": [],
    "title": "Creating Routing Keys",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/getting_started/routing/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 2. API Test",
    "content": "Add Search Request Click on + Add Request to add the next step. Name the step Search for Tracks named “Up around the bend”.\nExpand the Request section and change the request method to GET and enter the following URL:\nhttps://api.spotify.com/v1/search?q=Up%20around%20the%20bend\u0026type=track\u0026offset=0\u0026limit=5 Next, add two request headers with the following key/value pairings:\nCONTENT-TYPE: application/json AUTHORIZATION: Bearer {{custom.access_token}} Expand the Validation section and add the following extraction:\nExtract from Response body JSON $.tracks.items[0].id as track.id. Click on \u003c Return to test to return to the test configuration page. And then click Save to save the API test.",
    "description": "Add Search Request Click on + Add Request to add the next step. Name the step Search for Tracks named “Up around the bend”.\nExpand the Request section and change the request method to GET and enter the following URL:\nhttps://api.spotify.com/v1/search?q=Up%20around%20the%20bend\u0026type=track\u0026offset=0\u0026limit=5 Next, add two request headers with the following key/value pairings:\nCONTENT-TYPE: application/json AUTHORIZATION: Bearer {{custom.access_token}}",
    "tags": [],
    "title": "Search Request",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/2-api-test/4-search-request/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 1. Real Browser Test",
    "content": "The simple settings allow you to configure the basics of the test:\nName: The name of the test (e.g. RWC - Online Boutique). Details: Locations: The locations where the test will run from. Device: Emulate different devices and connection speeds. Also, the viewport will be adjusted to match the chosen device. Frequency: How often the test will run. Round-robin: If multiple locations are selected, the test will run from one location at a time, rather than all locations at once. Active: Set the test to active or inactive. ![Return to Test]For this workshop, we will configure the locations that we wish to monitor from. Click in the Locations field and you will be presented with a list of global locations (over 50 in total).\nSelect the following locations:\nAWS - N. Virginia AWS - London AWS - Melbourne Once complete, scroll down and click on Click on Submit to save the test.\nThe test will now be scheduled to run every 5 minutes from the 3 locations that we have selected. This does take a few minutes for the schedule to be created.\nSo whilst we wait for the test to be scheduled, click on Edit test so we can go through the Advanced settings.",
    "description": "The simple settings allow you to configure the basics of the test:\nName: The name of the test (e.g. RWC - Online Boutique). Details: Locations: The locations where the test will run from. Device: Emulate different devices and connection speeds. Also, the viewport will be adjusted to match the chosen device. Frequency: How often the test will run. Round-robin: If multiple locations are selected, the test will run from one location at a time, rather than all locations at once. Active: Set the test to active or inactive. ![Return to Test]For this workshop, we will configure the locations that we wish to monitor from. Click in the Locations field and you will be presented with a list of global locations (over 50 in total).",
    "tags": [],
    "title": "1.4 Settings",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/1-real-browser-test/4-edit-test-settings/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "The OpenTelemetry Collector’s FileStorage Extension is a critical component for building a more resilient telemetry pipeline. It enables the Collector to reliably checkpoint in-flight data, manage retries efficiently, and gracefully handle temporary failures without losing valuable telemetry.\nWith FileStorage enabled, the Collector can persist intermediate states to disk, ensuring that your traces, metrics, and logs are not lost during network disruptions, backend outages, or Collector restarts. This means that even if your network connection drops or your backend becomes temporarily unavailable, the Collector will continue to receive and buffer telemetry, resuming delivery seamlessly once connectivity is restored.\nBy integrating the FileStorage Extension into your pipeline, you can strengthen the durability of your observability stack and maintain high-quality telemetry ingestion, even in environments where connectivity may be unreliable. Note This solution will work for metrics as long as the connection downtime is brief, up to 15 minutes. If the downtime exceeds this, Splunk Observability Cloud might drop data to make sure no data-point is out of order.\nFor logs, there are plans to implement a full enterprise-ready solution in one of the upcoming Splunk OpenTelemetry Collector releases.",
    "description": "The OpenTelemetry Collector’s FileStorage Extension is a critical component for building a more resilient telemetry pipeline. It enables the Collector to reliably checkpoint in-flight data, manage retries efficiently, and gracefully handle temporary failures without losing valuable telemetry.\nWith FileStorage enabled, the Collector can persist intermediate states to disk, ensuring that your traces, metrics, and logs are not lost during network disruptions, backend outages, or Collector restarts. This means that even if your network connection drops or your backend becomes temporarily unavailable, the Collector will continue to receive and buffer telemetry, resuming delivery seamlessly once connectivity is restored.",
    "tags": [],
    "title": "2. Building In Resilience",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/2-building-resilience/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "The OpenTelemetry Collector’s FileStorage Extension is a critical component for building a more resilient telemetry pipeline. It enables the Collector to reliably checkpoint in-flight data, manage retries efficiently, and gracefully handle temporary failures without losing valuable telemetry.\nWith FileStorage enabled, the Collector can persist intermediate states to disk, ensuring that your traces, metrics, and logs are not lost during network disruptions, backend outages, or Collector restarts. This means that even if your network connection drops or your backend becomes temporarily unavailable, the Collector will continue to receive and buffer telemetry, resuming delivery seamlessly once connectivity is restored.\nBy integrating the FileStorage Extension into your pipeline, you can strengthen the durability of your observability stack and maintain high-quality telemetry ingestion, even in environments where connectivity may be unreliable. Note This solution will work for metrics as long as the connection downtime is brief, up to 15 minutes. If the downtime exceeds this, Splunk Observability Cloud might drop data to make sure no data-point is out of order.\nFor logs, there are plans to implement a full enterprise-ready solution in one of the upcoming Splunk OpenTelemetry Collector releases.",
    "description": "The OpenTelemetry Collector’s FileStorage Extension is a critical component for building a more resilient telemetry pipeline. It enables the Collector to reliably checkpoint in-flight data, manage retries efficiently, and gracefully handle temporary failures without losing valuable telemetry.\nWith FileStorage enabled, the Collector can persist intermediate states to disk, ensuring that your traces, metrics, and logs are not lost during network disruptions, backend outages, or Collector restarts. This means that even if your network connection drops or your backend becomes temporarily unavailable, the Collector will continue to receive and buffer telemetry, resuming delivery seamlessly once connectivity is restored.",
    "tags": [],
    "title": "2. Building In Resilience",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/2-building-resilience/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "The OpenTelemetry Gateway is designed to receive, process, and export telemetry data. It acts as an intermediary between telemetry sources (e.g. applications, services) and backends (e.g., observability platforms like Prometheus, Jaeger, or Splunk Observability Cloud).\nThe gateway is useful because it centralizes telemetry data collection, enabling features like data filtering, transformation, and routing to multiple destinations. It also reduces the load on individual services by offloading telemetry processing and ensures consistent data formats across distributed systems. This makes it easier to manage, scale, and analyze telemetry data in complex environments.\nExercise In the Gateway terminal window, change into the [WORKSHOP] directory and create a new subdirectory named 2-gateway. Important Change ALL terminal windows to the [WORKSHOP]/2-gateway directory.\nBack in the Gateway terminal window, copy agent.yaml from the 1-agent directory into 2-gateway. Create a file called gateway.yaml and add the following initial configuration: ​ gateway.yaml ########################### This section holds all the ## Configuration section ## configurations that can be ########################### used in this OpenTelemetry Collector extensions: # List of extensions health_check: # Health check extension endpoint: 0.0.0.0:14133 # Custom port to avoid conflicts receivers: otlp: # OTLP receiver protocols: http: # HTTP protocol endpoint: \"0.0.0.0:5318\" # Custom port to avoid conflicts include_metadata: true # Required for token pass-through exporters: # List of exporters debug: # Debug exporter verbosity: detailed # Enable detailed debug output file/traces: # Exporter Type/Name path: \"./gateway-traces.out\" # Path for OTLP JSON output append: false # Overwrite the file each time file/metrics: # Exporter Type/Name path: \"./gateway-metrics.out\" # Path for OTLP JSON output append: false # Overwrite the file each time file/logs: # Exporter Type/Name path: \"./gateway-logs.out\" # Path for OTLP JSON output append: false # Overwrite the file each time processors: # List of processors memory_limiter: # Limits memory usage check_interval: 2s # Memory check interval limit_mib: 512 # Memory limit in MiB batch: # Batches data before exporting metadata_keys: # Groups data by token - X-SF-Token resource/add_mode: # Adds metadata attributes: - action: upsert # Inserts or updates a key key: otelcol.service.mode # Key name value: \"gateway\" # Key value # Connectors #connectors: # leave this commented out; we will uncomment in an upcoming exercise ########################### ### Activation Section ### ########################### service: # Service configuration telemetry: metrics: level: none # Disable metrics extensions: [health_check] # Enabled extensions pipelines: # Configured pipelines traces: # Traces pipeline receivers: - otlp # OTLP receiver processors: # Processors for traces - memory_limiter - resource/add_mode - batch exporters: - debug # Debug exporter - file/traces metrics: # Metrics pipeline receivers: - otlp # OTLP receiver processors: # Processors for metrics - memory_limiter - resource/add_mode - batch exporters: - debug # Debug exporter - file/metrics logs: # Logs pipeline receivers: - otlp # OTLP receiver processors: # Processors for logs - memory_limiter - resource/add_mode - batch exporters: - debug # Debug exporter - file/logs Note When the gateway is started it will generate three files: gateway-traces.out, gateway-metrics.out, and gateway-logs.out. These files will eventually contain the telemetry data received by the gateway.\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml",
    "description": "The OpenTelemetry Gateway is designed to receive, process, and export telemetry data. It acts as an intermediary between telemetry sources (e.g. applications, services) and backends (e.g., observability platforms like Prometheus, Jaeger, or Splunk Observability Cloud).\nThe gateway is useful because it centralizes telemetry data collection, enabling features like data filtering, transformation, and routing to multiple destinations. It also reduces the load on individual services by offloading telemetry processing and ensures consistent data formats across distributed systems. This makes it easier to manage, scale, and analyze telemetry data in complex environments.",
    "tags": [],
    "title": "2. Gateway Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/2-gateway/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk APM \u003e 2. Using Splunk APM",
    "content": "Example Trace You should now see the entire trace along with the spans for the example trace that was selected. Spans which have errors are indicated by a red exclamation mark beside it. If you have a number such as x6 in a grey box, click it to expand the compacted paymentservice spans.\nNow click one of the paymentservice spans with the red exclamation mark to expand it and see the associated metadata and some error details. Note that we are able to see that this error is caused by a 401 error and other useful information such as ‘tenant’ and ‘version’ is also displayed.\nSo we now know that the error is caused by an Invalid Request but we don’t know what exact request. At the bottom of the page you should see a contextual link to Logs, clink on this link to view the logs associated with this span.\nYou should now be looking at a Log Observer dashboard simialar to the image below.\nWe can use the filter to display only the error logs. Click on ERROR in the top right hand corner, then Add to filter\nYou should now have a shorter list of log entries which have a severity of ERROR\nSelect any of the entries to view the details. We can now see how the error was caused by the use of an Invalid API Token that our developers have accidentally pushed to production!\nCongratulations, you have now completed the APM Workshop.",
    "description": "Example Trace You should now see the entire trace along with the spans for the example trace that was selected. Spans which have errors are indicated by a red exclamation mark beside it. If you have a number such as x6 in a grey box, click it to expand the compacted paymentservice spans.",
    "tags": [],
    "title": "2.3 Example trace",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/2-apm/using-splunk-apm/example_trace/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector \u003e 2. Building Resilience",
    "content": "In this exercise, we’ll test how the OpenTelemetry Collector recovers from a network outage by restarting the Gateway collector. When the Gateway becomes available again, the Agent will resume sending data from its last check-pointed state, ensuring no data loss.\nExercise Restart the Gateway: In the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Restart the Agent: In the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml After the Agent is up and running, the File_Storage extension will detect buffered data in the checkpoint folder. It will start to dequeue the stored spans from the last checkpoint folder, ensuring no data is lost.\nVerify the Agent Debug output: Note that the Agent debug output does NOT change and still shows the following line indicating no new data is being exported:\n2025-07-11T08:31:58.176Z info service@v0.126.0/service.go:289 Everything is ready. Begin running and processing data. {\"resource\": {}} Watch the Gateway Debug output\nYou should see from the Gateway debug screen, it has started receiving the previously missed traces without requiring any additional action on your part e.g.:\nAttributes: -\u003e user.name: Str(Luke Skywalker) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(75.75) {\"resource\": {}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\"} Check the gateway-traces.out file: Using jq, count the number of traces in the recreated gateway-traces.out. It should match the number you send when the Gateway was down.\n​ Check Gateway Traces Out File Example output jq '.resourceSpans | length | \"\\(.) resourceSpans found\"' gateway-traces.out \"5 resourceSpans found\" Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.\nConclusion This exercise demonstrated how to enhance the resilience of the OpenTelemetry Collector by configuring the file_storage extension, enabling retry mechanisms for the otlp exporter, and using a file-backed queue for temporary data storage.\nBy implementing file-based check-pointing and queue persistence, you ensure the telemetry pipeline can gracefully recover from temporary interruptions, making it a more robust and reliable for production environments.",
    "description": "In this exercise, we’ll test how the OpenTelemetry Collector recovers from a network outage by restarting the Gateway collector. When the Gateway becomes available again, the Agent will resume sending data from its last check-pointed state, ensuring no data loss.\nExercise Restart the Gateway: In the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Restart the Agent: In the Agent terminal window run:",
    "tags": [],
    "title": "2.4 Recovery",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/2-building-resilience/2-4-recovery/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector \u003e 2. Building Resilience",
    "content": "In this exercise, we’ll test how the OpenTelemetry Collector recovers from a network outage by restarting the Gateway collector. When the Gateway becomes available again, the Agent will resume sending data from its last check-pointed state, ensuring no data loss.\nExercise Restart the Gateway: In the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Restart the Agent: In the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml After the Agent is up and running, the File_Storage extension will detect buffered data in the checkpoint folder. It will start to dequeue the stored spans from the last checkpoint folder, ensuring no data is lost.\nVerify the Agent Debug output: Note that the Agent debug output does NOT change and still shows the following line indicating no new data is being exported:\n2025-07-11T08:31:58.176Z info service@v0.126.0/service.go:289 Everything is ready. Begin running and processing data. {\"resource\": {}} Watch the Gateway Debug output\nYou should see from the Gateway debug screen, it has started receiving the previously missed traces without requiring any additional action on your part e.g.:\nAttributes: -\u003e user.name: Str(Luke Skywalker) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(75.75) {\"resource\": {}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\"} Check the gateway-traces.out file: Using jq, count the number of traces in the recreated gateway-traces.out. It should match the number you send when the Gateway was down.\n​ Check Gateway Traces Out File Example output jq '.resourceSpans | length | \"\\(.) resourceSpans found\"' gateway-traces.out \"5 resourceSpans found\" Important Stop the Agent and the Gateway processes by pressing Ctrl-C in their respective terminals.\nConclusion This exercise demonstrated how to enhance the resilience of the OpenTelemetry Collector by configuring the file_storage extension, enabling retry mechanisms for the otlp exporter, and using a file-backed queue for temporary data storage.\nBy implementing file-based check-pointing and queue persistence, you ensure the telemetry pipeline can gracefully recover from temporary interruptions, making it a more robust and reliable for production environments.",
    "description": "In this exercise, we’ll test how the OpenTelemetry Collector recovers from a network outage by restarting the Gateway collector. When the Gateway becomes available again, the Agent will resume sending data from its last check-pointed state, ensuring no data loss.\nExercise Restart the Gateway: In the Gateway terminal window run:\n​ Start the Gateway ../otelcol --config=gateway.yaml Restart the Agent: In the Agent terminal window run:",
    "tags": [],
    "title": "2.4 Recovery",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/2-building-resilience/2-4-recovery/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics \u003e 2. API Test",
    "content": "Click on + Add Request to add the next step. Name the step Search for Tracks named “Up around the bend”.\nExpand the Request section and change the request method to GET and enter the following URL:\nhttps://api.spotify.com/v1/search?q=Up%20around%20the%20bend\u0026type=track\u0026offset=0\u0026limit=5 Next, add two request headers with the following key/value pairings:\nCONTENT-TYPE: application/json AUTHORIZATION: Bearer {{custom.access_token}} This uses the custom variable we created in the previous step! Expand the Validation section and add the following extraction:\nExtract from Response body JSON $.tracks.items[0].id as track.id To validate the test before saving, scroll to the top and change the location as needed. Click Try now. See the docs for more information on the try now feature.\nWhen the validation is successful, click on \u003c Return to test to return to the test configuration page. And then click Save to save the API test.\nExtra credit Have more time to work on this test? Take a look at the Response Body in one of your run results. What additional steps would make this test more thorough? Edit the test, and use the Try now feature to validate any changes you make before you save the test.",
    "description": "Click on + Add Request to add the next step. Name the step Search for Tracks named “Up around the bend”.\nExpand the Request section and change the request method to GET and enter the following URL:\nhttps://api.spotify.com/v1/search?q=Up%20around%20the%20bend\u0026type=track\u0026offset=0\u0026limit=5 Next, add two request headers with the following key/value pairings:\nCONTENT-TYPE: application/json AUTHORIZATION: Bearer {{custom.access_token}} This uses the custom variable we created in the previous step!",
    "tags": [],
    "title": "Search Request",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/2-api-test/4-search-request/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 2. Gateway Setup",
    "content": "Exercise Send a Test Trace:\nValidate agent and gateway are still running. In the Spans terminal window, run the following command to send 5 spans and validate the output of the agent and gateway debug logs: ​ Start the Load Generator Agent/Gateway Debug Output ../loadgen -count 5 2025-03-06T11:49:00.456Z info Traces {\"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"Exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1} 2025-03-06T11:49:00.456Z info ResourceSpans #0 Resource SchemaURL: https://opentelemetry.io/schemas/1.6.1 Resource attributes: -\u003e service.name: Str(cinema-service) -\u003e deployment.environment: Str(production) -\u003e host.name: Str(workshop-instance) -\u003e os.type: Str(linux) -\u003e otelcol.service.mode: Str(agent) ScopeSpans #0 ScopeSpans SchemaURL: InstrumentationScope cinema.library 1.0.0 InstrumentationScope attributes: -\u003e fintest.scope.attribute: Str(Starwars, LOTR) Span #0 Trace ID : 97fb4e5b13400b5689e3306da7cff077 Parent ID : ID : 413358465e5b4f15 Name : /movie-validator Kind : Server Start time : 2025-03-06 11:49:00.431915 +0000 UTC End time : 2025-03-06 11:49:01.431915 +0000 UTC Status code : Ok Status message : Success Attributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(87.01) {\"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"Exporter\", \"otelcol.signal\": \"traces\"} Verify the Gateway has handled the spans: Once the gateway processes incoming spans, it writes the trace data to a file named gateway-traces.out. To confirm that the spans have been successfully handled, you can inspect this file.\nIn your Tests terminal, use the jq command to extract and display key details about each span, such as its spanId and its position in the file. Also, we can extract the attributes that the Hostmetrics Receiver added to the spans.\n​ Inspect the Gateway Trace File Example Output jq -c '.resourceSpans[] as $resource | $resource.scopeSpans[].spans[] | \"Span \\(input_line_number) found with spanId \\(.spanId), hostname \\($resource.resource.attributes[] | select(.key == \"host.name\") | .value.stringValue), os \\($resource.resource.attributes[] | select(.key == \"os.type\") | .value.stringValue)\"' ./gateway-traces.out \"Span 1 found with spanId d71fe6316276f97d, hostname workshop-instance, os linux\" \"Span 2 found with spanId e8d19489232f8c2a, hostname workshop-instance, os linux\" \"Span 3 found with spanId 9dfaf22857a6bd05, hostname workshop-instance, os linux\" \"Span 4 found with spanId c7f544a4b5fef5fc, hostname workshop-instance, os linux\" \"Span 5 found with spanId 30bb49261315969d, hostname workshop-instance, os linux\" Important Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "Exercise Send a Test Trace:\nValidate agent and gateway are still running. In the Spans terminal window, run the following command to send 5 spans and validate the output of the agent and gateway debug logs: ​ Start the Load Generator Agent/Gateway Debug Output ../loadgen -count 5 2025-03-06T11:49:00.456Z info Traces {\"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"Exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1} 2025-03-06T11:49:00.456Z info ResourceSpans #0 Resource SchemaURL: https://opentelemetry.io/schemas/1.6.1 Resource attributes: -\u003e service.name: Str(cinema-service) -\u003e deployment.environment: Str(production) -\u003e host.name: Str(workshop-instance) -\u003e os.type: Str(linux) -\u003e otelcol.service.mode: Str(agent) ScopeSpans #0 ScopeSpans SchemaURL: InstrumentationScope cinema.library 1.0.0 InstrumentationScope attributes: -\u003e fintest.scope.attribute: Str(Starwars, LOTR) Span #0 Trace ID : 97fb4e5b13400b5689e3306da7cff077 Parent ID : ID : 413358465e5b4f15 Name : /movie-validator Kind : Server Start time : 2025-03-06 11:49:00.431915 +0000 UTC End time : 2025-03-06 11:49:01.431915 +0000 UTC Status code : Ok Status message : Success Attributes: -\u003e user.name: Str(George Lucas) -\u003e user.phone_number: Str(+1555-867-5309) -\u003e user.email: Str(george@deathstar.email) -\u003e user.password: Str(LOTR\u003eStarWars1-2-3) -\u003e user.visa: Str(4111 1111 1111 1111) -\u003e user.amex: Str(3782 822463 10005) -\u003e user.mastercard: Str(5555 5555 5555 4444) -\u003e payment.amount: Double(87.01) {\"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"Exporter\", \"otelcol.signal\": \"traces\"} Verify the Gateway has handled the spans: Once the gateway processes incoming spans, it writes the trace data to a file named gateway-traces.out. To confirm that the spans have been successfully handled, you can inspect this file.",
    "tags": [],
    "title": "2.4 Send traces from the Agent to the Gateway",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/2-gateway/2-4-send-traces/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "Log Observer Connect allows you to seamlessly bring in the same log data from your Splunk Platform into an intuitive and no-code interface designed to help you find and fix problems quickly. You can easily perform log-based analysis and seamlessly correlate your logs with Splunk Infrastructure Monitoring’s real-time metrics and Splunk APM traces in one place.\nEnd-to-end visibility: By combining the powerful logging capabilities of Splunk Platform with Splunk Observability Cloud’s traces and real-time metrics for deeper insights and more context of your hybrid environment.\nPerform quick and easy log-based investigations: By reusing logs that are already ingested in Splunk Cloud Platform or Enterprise in a simplified and intuitive interface (no need to know SPL!) with customizable and out-of-the-box dashboards\nAchieve higher economies of scale and operational efficiency: By centralizing log management across teams, breaking down data and team silos, and getting better overall support",
    "description": "Log Observer Connect allows you to seamlessly bring in the same log data from your Splunk Platform into an intuitive and no-code interface designed to help you find and fix problems quickly. You can easily perform log-based analysis and seamlessly correlate your logs with Splunk Infrastructure Monitoring’s real-time metrics and Splunk APM traces in one place.\nEnd-to-end visibility: By combining the powerful logging capabilities of Splunk Platform with Splunk Observability Cloud’s traces and real-time metrics for deeper insights and more context of your hybrid environment.",
    "tags": [],
    "title": "Log Observer Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/3-log-observer-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "Once the installation has completed, you can log in to Splunk Observability Cloud and verify that the metrics are flowing in from your Kubernetes cluster.\nFrom the left-hand menu, click on Infrastructure and select Kubernetes, then select the Kubernetes nodes pane. Once you are in the Kubernetes nodes view, change the Time filter from -4h to the last 15 minutes (-15m) to focus on the latest data.\nNext, from the list of nodes, select the Node name of your workshop instance. Note: You can get the unique part from your cluster name by using the INSTANCE from the output from the shell script you ran earlier. (1)\nOpen the Hierarchy Map by clicking on the Hierarchy Map link in the gray pane to show the graphical representation of your node.\nYou will now only have your cluster visible. Scroll down the page to see the metrics coming in from your cluster. Locate the Node log events rate (1) chart and click on a vertical bar to see the log entries coming in from your cluster.",
    "description": "Once the installation has completed, you can log in to Splunk Observability Cloud and verify that the metrics are flowing in from your Kubernetes cluster.\nFrom the left-hand menu, click on Infrastructure and select Kubernetes, then select the Kubernetes nodes pane. Once you are in the Kubernetes nodes view, change the Time filter from -4h to the last 15 minutes (-15m) to focus on the latest data.\nNext, from the list of nodes, select the Node name of your workshop instance. Note: You can get the unique part from your cluster name by using the INSTANCE from the output from the shell script you ran earlier. (1)",
    "tags": [],
    "title": "Verify Kubernetes Cluster metrics",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/3-verify-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 3. FileLog Setup",
    "content": "Exercise Start the Gateway: In your Gateway terminal window start the gateway.\nStart the Agent: In your Agent terminal window start the agent.\nA continuous stream of log data from the quotes.log will be in the agent and gateway debug logs:\n​ Agent/Gateway Debug Output Timestamp: 1970-01-01 00:00:00 +0000 UTC SeverityText: SeverityNumber: Unspecified(0) Body: Str(2025-03-06 15:18:32 [ERROR] - There is some good in this world, and it's worth fighting for. LOTR) Attributes: -\u003e log.file.path: Str(quotes.log) Trace ID: Span ID: Flags: 0 LogRecord #1 Stop the loadgen: In the Logs terminal window, stop the loadgen using Ctrl-C.\nVerify the gateway: Check if the gateway has written a ./gateway-logs.out file.\nAt this point, your directory structure will appear as follows:\n​ Updated Directory Structure . ├── agent.out ├── agent.yaml ├── gateway-logs.out # Output from the logs pipeline ├── gateway-metrics.out # Output from the metrics pipeline ├── gateway-traces.out # Output from the traces pipeline ├── gateway.yaml └── quotes.log # File containing Random log lines Examine a log line: In gateway-logs.out compare a log line with the snippet below. Verify that the log entry includes the same attributes as we have seen in metrics and traces data previously:\n​ cat /gateway-logs.out cat ./gateway-logs.out | jq {\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"com.splunk.source\",\"value\":{\"stringValue\":\"./quotes.log\"}},{\"key\":\"com.splunk.sourcetype\",\"value\":{\"stringValue\":\"quotes\"}},{\"key\":\"host.name\",\"value\":{\"stringValue\":\"workshop-instance\"}},{\"key\":\"os.type\",\"value\":{\"stringValue\":\"linux\"}},{\"key\":\"otelcol.service.mode\",\"value\":{\"stringValue\":\"gateway\"}}]},\"scopeLogs\":[{\"scope\":{},\"logRecords\":[{\"observedTimeUnixNano\":\"1741274312475540000\",\"body\":{\"stringValue\":\"2025-03-06 15:18:32 [DEBUG] - All we have to decide is what to do with the time that is given us. LOTR\"},\"attributes\":[{\"key\":\"log.file.path\",\"value\":{\"stringValue\":\"quotes.log\"}}],\"traceId\":\"\",\"spanId\":\"\"},{\"observedTimeUnixNano\":\"1741274312475560000\",\"body\":{\"stringValue\":\"2025-03-06 15:18:32 [DEBUG] - Your focus determines your reality. SW\"},\"attributes\":[{\"key\":\"log.file.path\",\"value\":{\"stringValue\":\"quotes.log\"}}],\"traceId\":\"\",\"spanId\":\"\"}]}],\"schemaUrl\":\"https://opentelemetry.io/schemas/1.6.1\"}]} { \"resourceLogs\": [ { \"resource\": { \"attributes\": [ { \"key\": \"com.splunk.source\", \"value\": { \"stringValue\": \"./quotes.log\" } }, { \"key\": \"com.splunk.sourcetype\", \"value\": { \"stringValue\": \"quotes\" } }, { \"key\": \"host.name\", \"value\": { \"stringValue\": \"workshop-instance\" } }, { \"key\": \"os.type\", \"value\": { \"stringValue\": \"linux\" } }, { \"key\": \"otelcol.service.mode\", \"value\": { \"stringValue\": \"gateway\" } } ] }, \"scopeLogs\": [ { \"scope\": {}, \"logRecords\": [ { \"observedTimeUnixNano\": \"1741274312475540000\", \"body\": { \"stringValue\": \"2025-03-06 15:18:32 [DEBUG] - All we have to decide is what to do with the time that is given us. LOTR\" }, \"attributes\": [ { \"key\": \"log.file.path\", \"value\": { \"stringValue\": \"quotes.log\" } } ], \"traceId\": \"\", \"spanId\": \"\" }, { \"observedTimeUnixNano\": \"1741274312475560000\", \"body\": { \"stringValue\": \"2025-03-06 15:18:32 [DEBUG] - Your focus determines your reality. SW\" }, \"attributes\": [ { \"key\": \"log.file.path\", \"value\": { \"stringValue\": \"quotes.log\" } } ], \"traceId\": \"\", \"spanId\": \"\" } ] } ], \"schemaUrl\": \"https://opentelemetry.io/schemas/1.6.1\" } ] } You may also have noticed that every log line contains empty placeholders for \"traceId\":\"\" and \"spanId\":\"\". The FileLog receiver will populate these fields only if they are not already present in the log line. For example, if the log line is generated by an application instrumented with an OpenTelemetry instrumentation library, these fields will already be included and will not be overwritten.\nImportant Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.",
    "description": "Exercise Start the Gateway: In your Gateway terminal window start the gateway.\nStart the Agent: In your Agent terminal window start the agent.\nA continuous stream of log data from the quotes.log will be in the agent and gateway debug logs:\n​ Agent/Gateway Debug Output Timestamp: 1970-01-01 00:00:00 +0000 UTC SeverityText: SeverityNumber: Unspecified(0) Body: Str(2025-03-06 15:18:32 [ERROR] - There is some good in this world, and it's worth fighting for. LOTR) Attributes: -\u003e log.file.path: Str(quotes.log) Trace ID: Span ID: Flags: 0 LogRecord #1 Stop the loadgen: In the Logs terminal window, stop the loadgen using Ctrl-C.",
    "tags": [],
    "title": "3.2 Test FileLog Receiver",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/3-filelog/3-2-test-filelog/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud \u003e 3. Create an Ingest Pipeline",
    "content": "In this section you will create an Ingest Pipeline which will convert Kubernetes Audit Logs to metrics which are sent to the Splunk Observability Cloud workshop organization.\nExercise: Create Ingest Pipeline 1. Open the Ingest Processor SCS Tenant using the connection details provided in the Splunk Show event.\nNote When you open the Ingest Processor SCS Tenant, if you are taken to a welcome page, click on Launch under Splunk Cloud Platform to be taken to the Data Management page where you will configure the Ingest Pipeline.\n2. From the Splunk Data Management console select Pipelines → New pipeline → Ingest Processor pipeline.\n3. In the Get started step of the Ingest Processor configuration page select Blank Pipeline and click Next.\n4. In the Define your pipeline’s partition step of the Ingest Processor configuration page select Partition by sourcetype. Select the = equals Operator and enter kube:apiserver:audit:USER_ID (Be sure to replace USER_ID with the User ID you were assigned) for the value. Click Apply.\n5. Click Next\n6. In the Add sample data step of the Ingest Processor configuration page select Capture new snapshot. Enter k8s_audit_USER_ID (Be sure to replace USER_ID with the User ID you were assigned) for the Snapshot name and click Capture.\n7. Make sure your newly created snapshot (k8s_audit_USER_ID) is selected and then click Next.\n8. In the Select a metrics destination step of the Ingest Processor configuration page select show_o11y_org. Click Next.\n9. In the Select a data destination step of the Ingest Processor configuration page select splunk_indexer. Under Specify how you want your events to be routed to an index select Default. Click Done.\n10. In the Pipeline search field replace the default search with the following.\nNote Replace UNIQUE_FIELD in the metric name with a unique value (such as your initials) which will be used to identify your metric in Observability Cloud.\n/*A valid SPL2 statement for a pipeline must start with \"$pipeline\", and include \"from $source\" and \"into $destination\".*/ /* Import logs_to_metrics */ import logs_to_metrics from /splunk/ingest/commands $pipeline = | from $source | thru [ //define the metric name, type, and value for the Kubernetes Events // // REPLACE UNIQUE_FIELD WITH YOUR INITIALS // | logs_to_metrics name=\"k8s_audit_UNIQUE_FIELD\" metrictype=\"counter\" value=1 time=_time | into $metrics_destination ] | eval index = \"kube_logs\" | into $destination; New to SPL2? Here is a breakdown of what the SPL2 query is doing:\nFirst, you are importing the built-in logs_to_metrics command which will be used to convert the kubernetes events to metrics. You’re using the source data, which you can see on the right is any event from the kube:apiserver:audit sourcetype. Now, you use the thru command which writes the source dataset to the following command, in this case logs_to_metrics. You can see that the metric name (k8s_audit), metric type (counter), value, and timestamp are all provided for the metric. You’re using a value of 1 for this metric because we want to count the number of times the event occurs. Next, you choose the destination for the metric using the into $metrics_destintation command, which is our Splunk Observability Cloud organization Finally, you can send the raw log events to another destination, in this case another index, so they are retained if we ever need to access them. 11. In the upper-right corner click the Preview button or press CTRL+Enter (CMD+Enter on Mac). From the Previewing $pipeline dropdown select $metrics_destination. Confirm you are seeing a preview of the metrics that will be sent to Splunk Observability Cloud.\n12. In the upper-right corner click the Save pipeline button . Enter Kubernetes Audit Logs2Metrics USER_ID for your pipeline name and click Save.\n13. After clicking save you will be asked if you would like to apply the newly created pipeline. Click Yes, apply.\nNote The Ingest Pipeline should now be sending metrics to Splunk Observability Cloud. Keep this tab open as it will be used it again in the next section.\nIn the next step you’ll confirm the pipeline is working by viewing the metrics you just created in Splunk Observability Cloud.",
    "description": "In this section you will create an Ingest Pipeline which will convert Kubernetes Audit Logs to metrics which are sent to the Splunk Observability Cloud workshop organization.\nExercise: Create Ingest Pipeline 1. Open the Ingest Processor SCS Tenant using the connection details provided in the Splunk Show event.",
    "tags": [],
    "title": "Create an Ingest Pipeline",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/3-create-an-ingest-pipeline/3-create-ingest-pipeline/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 3. Dashboards",
    "content": "1. Introduction Let’s take a look at SignalFlow - the analytics language of Observability Cloud that can be used to setup monitoring as code.\nThe heart of Splunk Infrastructure Monitoring is the SignalFlow analytics engine that runs computations written in a Python-like language. SignalFlow programs accept streaming input and produce output in real time. SignalFlow provides built-in analytical functions that take metric time series (MTS) as input, perform computations, and output a resulting MTS.\nComparisons with historical norms, e.g. on a week-over-week basis Population overviews using a distributed percentile chart Detecting if the rate of change (or other metric expressed as a ratio, such as a service level objective) has exceeded a critical threshold Finding correlated dimensions, e.g. to determine which service is most correlated with alerts for low disk space Infrastructure Monitoring creates these computations in the Chart Builder user interface, which lets you specify the input MTS to use and the analytical functions you want to apply to them. You can also run SignalFlow programs directly by using the SignalFlow API.\nSignalFlow includes a large library of built-in analytical functions that take a metric time series as an input, performs computations on its datapoints, and outputs time series that are the result of the computation.\nInfo For more information on SignalFlow see Analyze incoming data using SignalFlow.\n2. View SignalFlow In the chart builder, click on View SignalFlow.\nYou will see the SignalFlow code that composes the chart we were working on. You can now edit the SignalFlow directly within the UI. Our documentation has the full list of SignalFlow functions and methods.\nAlso, you can copy the SignalFlow and use it when interacting with the API or with Terraform to enable Monitoring as Code.\n​ SignalFlow A = data('demo.trans.latency', filter=filter('demo_datacenter', 'Paris')).percentile(pct=95).publish(label='A', enable=False) B = data('demo.trans.latency', filter=filter('demo_datacenter', 'Paris')).percentile(pct=95).timeshift('1w').publish(label='B', enable=False) C = (A-B).publish(label='C') Click on View Builder to go back to the Chart Builder UI.\nLet’s save this new chart to our Dashboard!",
    "description": "1. Introduction Let’s take a look at SignalFlow - the analytics language of Observability Cloud that can be used to setup monitoring as code.\nThe heart of Splunk Infrastructure Monitoring is the SignalFlow analytics engine that runs computations written in a Python-like language. SignalFlow programs accept streaming input and produce output in real time. SignalFlow provides built-in analytical functions that take metric time series (MTS) as input, perform computations, and output a resulting MTS.",
    "tags": [],
    "title": "3.4 SignalFlow",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/dashboards/signalflow/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Profiling Workshop",
    "content": "In this section, we’ll use what we learned from the profiling data in Splunk Observability Cloud to resolve the slowness we saw when starting our application.\nExamining the Source Code Open the corresponding source file once again (./doorgame/src/main/java/com/splunk/profiling/workshop/UserData.java) and focus on the following code:\npublic class UserData { static final String DB_URL = \"jdbc:mysql://mysql/DoorGameDB\"; static final String USER = \"root\"; static final String PASS = System.getenv(\"MYSQL_ROOT_PASSWORD\"); static final String SELECT_QUERY = \"select * FROM DoorGameDB.Users, DoorGameDB.Organizations\"; HashMap\u003cString, User\u003e users; public UserData() { users = new HashMap\u003cString, User\u003e(); } public void loadUserData() { // Load user data from the database and store it in a map Connection conn = null; Statement stmt = null; ResultSet rs = null; try{ conn = DriverManager.getConnection(DB_URL, USER, PASS); stmt = conn.createStatement(); rs = stmt.executeQuery(SELECT_QUERY); while (rs.next()) { User user = new User(rs.getString(\"UserId\"), rs.getString(\"FirstName\"), rs.getString(\"LastName\")); users.put(rs.getString(\"UserId\"), user); } After speaking with a database engineer, you discover that the SQL query being executed includes a cartesian join:\nselect * FROM DoorGameDB.Users, DoorGameDB.Organizations Cartesian joins are notoriously slow, and shouldn’t be used, in general.\nUpon further investigation, you discover that there are 10,000 rows in the user table, and 1,000 rows in the organization table. When we execute a cartesian join using both of these tables, we end up with 10,000 x 1,000 rows being returned, which is 10,000,000 rows!\nFurthermore, the query ends up returning duplicate user data, since each record in the user table is repeated for each organization.\nSo when our code executes this query, it tries to load 10,000,000 user objects into the HashMap, which explains why it takes so long to execute, and why it consumes so much heap memory.\nLet’s Fix That Bug After consulting the engineer that originally wrote this code, we determined that the join with the Organizations table was inadvertent.\nSo when loading the users into the HashMap, we simply need to remove this table from the query.\nOpen the corresponding source file once again (./doorgame/src/main/java/com/splunk/profiling/workshop/UserData.java) and change the following line of code:\nstatic final String SELECT_QUERY = \"select * FROM DoorGameDB.Users, DoorGameDB.Organizations\"; to:\nstatic final String SELECT_QUERY = \"select * FROM DoorGameDB.Users\"; Now the method should perform much more quickly, and less memory should be used, as it’s loading the correct number of users into the HashMap (10,000 instead of 10,000,000).\nRebuild and Redeploy Application Let’s test our changes by using the following commands to re-build and re-deploy the Door Game application:\ncd workshop/profiling ./5-redeploy-doorgame.sh Once the application has been redeployed successfully, visit The Door Game again to confirm that your fix is in place: http://\u003cyour IP address\u003e:81\nClicking Let's Play should take us to the game more quickly now (though performance could still be improved):\nStart the game a few more times, then return to Splunk Observability Cloud to confirm that the latency of the GET new-game operation has decreased.\nWhat did we accomplish? We discovered why our SQL query was so slow. We applied a fix, then rebuilt and redeployed our application. We confirmed that the application starts a new game more quickly. In the next section, we’ll explore continue playing the game and fix any remaining performance issues that we find.",
    "description": "In this section, we’ll use what we learned from the profiling data in Splunk Observability Cloud to resolve the slowness we saw when starting our application.\nExamining the Source Code Open the corresponding source file once again (./doorgame/src/main/java/com/splunk/profiling/workshop/UserData.java) and focus on the following code:\npublic class UserData { static final String DB_URL = \"jdbc:mysql://mysql/DoorGameDB\"; static final String USER = \"root\"; static final String PASS = System.getenv(\"MYSQL_ROOT_PASSWORD\"); static final String SELECT_QUERY = \"select * FROM DoorGameDB.Users, DoorGameDB.Organizations\"; HashMap\u003cString, User\u003e users; public UserData() { users = new HashMap\u003cString, User\u003e(); } public void loadUserData() { // Load user data from the database and store it in a map Connection conn = null; Statement stmt = null; ResultSet rs = null; try{ conn = DriverManager.getConnection(DB_URL, USER, PASS); stmt = conn.createStatement(); rs = stmt.executeQuery(SELECT_QUERY); while (rs.next()) { User user = new User(rs.getString(\"UserId\"), rs.getString(\"FirstName\"), rs.getString(\"LastName\")); users.put(rs.getString(\"UserId\"), user); } After speaking with a database engineer, you discover that the SQL query being executed includes a cartesian join:",
    "tags": [],
    "title": "Fix Application Startup Slowness",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/profiling/4-fix-app-startup-slowness/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Self-Service Observability",
    "content": "Introduction xx",
    "description": "Introduction xx",
    "tags": [],
    "title": "Observability as Code",
    "uri": "/observability-workshop/v6.5/en/scenarios/self-service-observability/4-configure-o11yascode/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 4. Splunk APM",
    "content": "Exercise Select the wire-transfer-service in the Service Map. In the right-hand pane click on the Breakdown. Select tenant.level in the list. Back in the Service Map click on gold (our most valuable user tier). Click on Breakdown and select version, this is the tag that exposes the service version. Repeat this for silver and bronze. ​ Question Answer What can you conclude from what you are seeing?\nEvery tenant.level is being impacted by v350.10\nYou will now see the wire-transfer-service broken down into three services, gold, silver and bronze. Each tenant is broken down into two services, one for each version (v350.10 and v350.9).\nSpan Tags Using span tags to break down services is a very powerful feature. It allows you to see how your services are performing for different customers, different versions, different regions, etc. In this exercise, we have determined that v350.10 of the wire-transfer-service is causing problems for all our customers.\nNext, we need to drill down into a trace to see what is going on.",
    "description": "Exercise Select the wire-transfer-service in the Service Map. In the right-hand pane click on the Breakdown. Select tenant.level in the list. Back in the Service Map click on gold (our most valuable user tier). Click on Breakdown and select version, this is the tag that exposes the service version. Repeat this for silver and bronze. ​ Question Answer What can you conclude from what you are seeing?",
    "tags": [],
    "title": "4. APM Service Breakdown",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/4-apm/4-apm-service-breakdown/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6. Splunk APM",
    "content": "Exercise Select the paymentservice in the Service Map. In the right-hand pane click on the Breakdown. Select tenant.level in the list. Back in the Service Map click on gold. Click on Breakdown and select version, this is the tag that exposes the service version. Repeat this for silver and bronze. ​ Question Answer What can you conclude from what you are seeing?\nEvery tenant.level is being impacted by v350.10\nYou will now see the paymentservice broken down into three services, gold, silver and bronze. Each tenant is broken down into two services, one for each version (v350.10 and v350.9).\nSpan Tags Using span tags to break down services is a very powerful feature. It allows you to see how your services are performing for different customers, different versions, different regions, etc. In this exercise, we have determined that v350.10 of the paymentservice is causing problems for all our customers.\nNext, we need to drill down into a trace to see what is going on.",
    "description": "Exercise Select the paymentservice in the Service Map. In the right-hand pane click on the Breakdown. Select tenant.level in the list. Back in the Service Map click on gold. Click on Breakdown and select version, this is the tag that exposes the service version. Repeat this for silver and bronze. ​ Question Answer What can you conclude from what you are seeing?",
    "tags": [],
    "title": "4. APM Service Breakdown",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/6-apm/4-apm-service-breakdown/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e RUM",
    "content": "Get familiar with the UI and options available from this landing page Identify Page Views/JavaScript Errors and Request/Errors in a single view Check the Web Vitals metrics and any Detector that has fired for in relation to your Browser Application Application Summary Dashboard 1.Header Bar As seen in the previous section the RUM Application Summary Dashboard consists of 5 major sections. The first section is the selection header, where you can collapse the Pane via the Browser icon or the \u003e in front of the application name, which is jmcj-store in the example below. It also provides access to the Application Overview page if you click the link with your application name which is jmcj-store in the example below.\nFurther, you can also open the Application Overview or App Health Dashboard via the triple dot menu on the right.\nFor now, let’s look at the high level information we get on the application summary dashboard.\nThe RUM Application Summary Dashboard is focused on providing you with at a glance highlights of the status of your application.\n2. Page Views / JavaScript Errors \u0026 Network Requests / Errors The first section shows Page Views / JavaScript Errors, \u0026 Network Requests and Errors charts show the quantity and trend of these issues in your application. This could be Javascript errors, or failed network calls to back end services.\nIn the example above you can see that there are no failed network calls in the Network chart, but in the Page View chart you can see that a number of pages do experience some errors. These are often not visible for regular users, but can seriously impact the performance of your web site.\nYou can see the count of the Page Views / Network Requests / Errors by hovering over the charts.\n3. JavaScript Errors With the second section of the RUM Application Summary Dashboard we are showing you an overview of the JavaScript errors occurring in your application, along with a count of each error.\nIn the example above you can see there are three JavaScript errors, one that appears 29 times in the selected time slot, and the other two each appear 12 times.\nIf you click on one of the errors a pop-out opens that will show a summary (below) of the errors over time, along with a Stack Trace of the JavaScript error, giving you an indication of where the problems occurred. (We will see this in more detail in one of the following sections)\n4. Web Vitals The next section of the RUM Application Summary Dashboard is showing you Google’s Core Web Vitals, three metrics that are not only used by Google in its search ranking system, but also quantify end user experience in terms of loading, interactivity, and visual stability.\nAs you can see our site is well behaved and scores Good for all three Metrics. These metrics can be used to identify the effect changes to your application have, and help you improve the performance of your site.\nIf you click on any of the Metrics shown in the Web Vitals pane you will be taken to the corresponding Tag Spotlight Dashboard. e.g. clicking on the Largest Contentful Paint (LCP) chartlet, you will be taken to a dashboard similar to the screen shot below, that gives you timeline and table views for how this metric has performed. This should allow you to spot trends and identify where the problem may be more common, such as an operating system, geolocation, or browser version.\n5. Most Recent Detectors The final section of the RUM Application Summary Dashboard is focused on providing you an overview of recent detectors that have triggered for your application. We have created a detector for this screen shot but your pane will be empty for now. We will add some detectors to your site and make sure they are triggered in one of the next sections.\nIn the screen shot you can see we have a critical alert for the RUM Aggregated View Detector, and a Count, how often this alert has triggered in the selected time window. If you happen to have an alert listed, you can click on the name of the Alert (that is shown as a blue link) and you will be taken to the Alert Overview page showing the details of the alert (Note: this will move you away from the current page, Please use the Back option of your browser to return to the overview page).\nExercise Please take a few minutes to experiment with the RUM Application Summary Dashboard and the underlying chart and dashboards before going on to the next section.",
    "description": "Get familiar with the UI and options available from this landing page Identify Page Views/JavaScript Errors and Request/Errors in a single view Check the Web Vitals metrics and any Detector that has fired for in relation to your Browser Application Application Summary Dashboard 1.Header Bar As seen in the previous section the RUM Application Summary Dashboard consists of 5 major sections. The first section is the selection header, where you can collapse the Pane via the Browser icon or the \u003e in front of the application name, which is jmcj-store in the example below. It also provides access to the Application Overview page if you click the link with your application name which is jmcj-store in the example below.",
    "tags": [],
    "title": "Check Browser Applications health at a glance",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/2-rum/4-browser-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "Get familiar with the UI and options available from this landing page Identify Page Views/Errors and Request/Errors and Java Script Errors in a single view Check the Web Vitals metrics and any Detector that has fired for in relation to your Browser Application 1. Application Summary Dashboard Overview 1.1. Header Bar As seen in the previous section the RUM Application Summary Dashboard consists of 5 major sections. The first section is the selection header, where you can collapse the Pane via the Browser icon or the \u003e in front of the application name, which is jmcj-store in the example below. It also provides access to the Application Overview page if you click the link with your application name which is jmcj-store in the example below.\nFurther, you can also open the Application Overview or App Health Dashboard via the triple dot menu on the right.\nFirst use the View Dashboard link to open the Browser App Health Dashboard which should open in a new tab. Then switch back to original RUM tab, and then use the Open Application Overview link, or click on the name of the app to launch the Application Overview dashboard.\nWe will looking at the Application Overview and Browser App Health Dashboards in detail in the following sections.\n2. Application Overview The RUM Application Overview Dashboard is focused on providing you with at a glance overview of the status of your application.\n2.1. Page Views / Errors \u0026 Network Requests / Errors The first section shows Page Views / Errors, \u0026 Network Requests and Errors charts show the quantity and trend of these issues in your application. This could be Javascript errors, or failed network calls to back end services.\nIn the example above you can see that there are no failed network calls in the Network chart, but in the Page View chart you can see that a number of pages do experience some errors. These are often not visible for regular users, but can seriously impact the performance of your web site.\nYou can see the count of the Page Views / Network Requests / Errors by hovering over the charts.\n2.2. JavaScript Errors With the second section of the RUM Application Summary Dashboard we are showing you an overview of the JavaScript errors occurring in your application, along with a count of each error.\nIn the example above you can see there are three JavaScript errors, one that appears 29 times in the selected time slot, and the other two each appear 12 times.\nIf you click on one of the errors a pop-out opens that will show a summary (below) of the errors over time, along with a Stack Trace of the JavaScript error, giving you an indication of where the problems occurred. (We will see this in more detail in one of the following sections)\n2.3. Web Vitals The third section of the RUM Application Summary Dashboard is showing you the crucial (google) Web Vitals, three metrics, that are used by Google in its ranking system, and give a very good indication of the speed of your site for your end users.\nAs you can see our site is well behaved and scores Good for all three Metrics. These metrics can be used to identify the effect changes to your application have, and help you improve the performance of your site.\nIf you click on any of the Metrics shown in the Web Vitals pane you will be taken to the corresponding Tag Spotlight Dashboard. e.g. clicking on the Largest Contentful Paint (LCP) chartlet, you will be taken to a dashboard similar to the screen shot below, that gives you timeline and table views for how this metric has performed. This should allow you to spot trends and identify where the problem may be more common, such as an OS or browser version, .\n2.4. Most Recent Detectors The fourth and final section of the RUM Application Summary Dashboard is focused on providing you an overview of any detector that has triggered for your application. We have created a detector for this screen shot but your pane will be empty for now, but we will add some detectors to your site and make sure they are triggered in one of the next sections.\nIn the screen shot you can see we have a critical alert for the RUM Aggregated View Detector, and a Count, how often this alert has triggered in the selected time window. If you happen to have an alert listed, you can click on the name of the Alert (that is shown as a blue link) and you will be taken to the Alert Overview page showing the details of the alert (Note: this will move you away from the current page, Please use the Back option of your browser to return to the overview page).\nPlease take a few minutes to experiment with the RUM Application Summary Dashboard and the underlying chart and dashboards before going on to the next section.",
    "description": "Get familiar with the UI and options available from this landing page Identify Page Views/Errors and Request/Errors and Java Script Errors in a single view Check the Web Vitals metrics and any Detector that has fired for in relation to your Browser Application 1. Application Summary Dashboard Overview 1.1. Header Bar As seen in the previous section the RUM Application Summary Dashboard consists of 5 major sections. The first section is the selection header, where you can collapse the Pane via the Browser icon or the \u003e in front of the application name, which is jmcj-store in the example below. It also provides access to the Application Overview page if you click the link with your application name which is jmcj-store in the example below.",
    "tags": [],
    "title": "4. Check Browser Applications health at a glance",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/4-browser-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Solving Problems with O11y Cloud",
    "content": "Index Tags To use advanced features in Splunk Observability Cloud such as Tag Spotlight, we’ll need to first index one or more tags.\nTo do this, navigate to Settings -\u003e MetricSets and ensure the APM tab is selected. Then click the + Add Custom MetricSet button.\nLet’s index the credit.score.category tag by entering the following details (note: since everyone in the workshop is using the same organization, the instructor will do this step on your behalf):\nClick Start Analysis to proceed.\nThe tag will appear in the list of Pending MetricSets while analysis is performed.\nOnce analysis is complete, click on the checkmark in the Actions column.\nTroubleshooting vs. Monitoring MetricSets You may have noticed that, to index this tag, we created something called a Troubleshooting MetricSet. It’s named this way because a Troubleshooting MetricSet, or TMS, allows us to troubleshoot issues with this tag using features such as Tag Spotlight.\nYou may have also noticed that there’s another option which we didn’t choose called a Monitoring MetricSet (or MMS). Monitoring MetricSets go beyond troubleshooting and allow us to use tags for alerting and dashboards. While we won’t be exploring this capability as part of this workshop, it’s a powerful feature that I encourage you to explore on your own.",
    "description": "Index Tags To use advanced features in Splunk Observability Cloud such as Tag Spotlight, we’ll need to first index one or more tags.\nTo do this, navigate to Settings -\u003e MetricSets and ensure the APM tab is selected. Then click the + Add Custom MetricSet button.\nLet’s index the credit.score.category tag by entering the following details (note: since everyone in the workshop is using the same organization, the instructor will do this step on your behalf):",
    "tags": [],
    "title": "Create a Troubleshooting MetricSet",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/9-solving-problems-with-o11y-cloud/4-create-troubleshooting-metricset/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM",
    "content": "Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules 1. Introduction Splunk Observability Cloud uses detectors, events, alerts, and notifications to keep you informed when certain criteria are met. For example, you might want a message sent to a Slack channel or an email address for the Ops team when CPU Utilization has reached 95%, or when the number of concurrent users is approaching a limit that might require you to spin up an additional AWS instance.\nThese conditions are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical.\n2. Creating a Detector In Dashboards click on your Custom Dashboard Group (that you created in the previous module) and then click on the dashboard name.\nWe are now going to create a new detector from a chart on this dashboard. Click on the bell icon on the Latency vs Load chart, and then click New Detector From Chart.\nIn the text field next to Detector Name, ADD YOUR INITIALS before the proposed detector name.\nNaming the detector It’s important that you add your initials in front of the proposed detector name.\nIt should be something like this: XYZ’s Latency Chart Detector.\nClick on Create Alert Rule\nIn the Detector window, inside Alert signal, the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert.\nClick on Proceed to Alert Condition\n3. Setting Alert condition In Alert condition, click on Static Threshold and then on Proceed to Alert Settings\nIn Alert Settings, enter the value 290 in the Threshold field. In the same window change Time on top right to past day (-1d).\n4. Alert pre-flight check A pre-flight check will take place after 5 seconds. See the Estimated alert count. Based on the current alert settings, the amount of alerts we would have received in 1 day would have been 3.\nAbout pre-flight checks Once you set an alert condition, the UI estimates how many alerts you might get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day.\nImmediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guesswork from configuring alerts in a simple but very powerful way, only available using the Splunk Observability Cloud.\nTo read more about detector previewing, please visit this link Preview detector alerts.\nClick on Proceed to Alert Message\n5. Alert message In Alert message, under Severity choose Major.\nClick on Proceed to Alert Recipients\nClick on Add Recipient and then on your email address displayed as the first option.\nNotification Services That’s the same as entering that email address OR you can enter another email address by clicking on E-mail….\nThis is just one example of the many Notification Services the platform has available. You can check this out by going to the Integrations tab of the top menu, and see Notification Services.\n6. Alert Activation Click on Proceed to Alert Activation\nIn Activate… click on Activate Alert Rule\nIf you want to get alerts quicker you edit the rule and lower the value from 290 to say 280.\nIf you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metrics from the last 1 hour.\nClick on the in the navbar and then click on Detectors. You can optionally filter for your initials. You will see you detector listed here. If you don’t then please refresh your browser.\nCongratulations! You have created your first detector and activated it!",
    "description": "Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules 1. Introduction Splunk Observability Cloud uses detectors, events, alerts, and notifications to keep you informed when certain criteria are met. For example, you might want a message sent to a Slack channel or an email address for the Ops team when CPU Utilization has reached 95%, or when the number of concurrent users is approaching a limit that might require you to spin up an additional AWS instance.",
    "tags": [],
    "title": "Working with Detectors",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/detectors/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop",
    "content": "Now that we’ve captured several tags from our application, let’s explore some of the trace data we’ve captured that include this additional context, and see if we can identify what’s causing a poor user experience in some cases.\nUse Trace Analyzer Navigate to APM, then select Traces. This takes us to the Trace Analyzer, where we can add filters to search for traces of interest. For example, we can filter on traces where the credit score starts with 7:\nIf you load one of these traces, you’ll see that the credit score indeed starts with seven.\nWe can apply similar filters for the customer number, credit score category, and credit score result.\nExplore Traces With Errors Let’s remove the credit score filter and toggle Errors only to on, which results in a list of only those traces where an error occurred:\nClick on a few of these traces, and look at the tags we captured. Do you notice any patterns?\nNext, toggle Errors only to off, and sort traces by duration. Look at a few of the slowest running traces, and compare them to the fastest running traces. Do you notice any patterns?\nIf you found a pattern that explains the slow performance and errors - great job! But keep in mind that this is a difficult way to troubleshoot, as it requires you to look through many traces and mentally keep track of what you saw, so you can identify a pattern.\nThankfully, Splunk Observability Cloud provides a more efficient way to do this, which we’ll explore next.",
    "description": "Now that we’ve captured several tags from our application, let’s explore some of the trace data we’ve captured that include this additional context, and see if we can identify what’s causing a poor user experience in some cases.\nUse Trace Analyzer Navigate to APM, then select Traces. This takes us to the Trace Analyzer, where we can add filters to search for traces of interest. For example, we can filter on traces where the credit score starts with 7:",
    "tags": [],
    "title": "Explore Trace Data",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/4-explore-trace-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Horizontal Pod Autoscaling",
    "content": "1. Kubernetes Resources Especially in Production Kubernetes Clusters, CPU and Memory are considered precious resources. Cluster Operators will normally require you to specify the amount of CPU and Memory your Pod or Service will require in the deployment, so they can have the Cluster automatically manage on which Node(s) your solution will be placed.\nYou do this by placing a Resource section in the deployment of your application/Pod\nExample:\nresources: limits: # Maximum amount of CPU \u0026 memory for peek use cpu: \"8\" # Maximum of 8 cores of CPU allowed at for peek use memory: \"8Mi\" # Maximum allowed 8Mb of memory requests: # Request are the expected amount of CPU \u0026 memory for normal use cpu: \"6\" # Requesting 4 cores of a CPU memory: \"4Mi\" # Requesting 4Mb of memory More information can be found here: Resource Management for Pods and Containers\nIf your application or Pod will go over the limits set in your deployment, Kubernetes will kill and restart your Pod to protect the other applications on the Cluster.\nAnother scenario that you will run into is when there is not enough Memory or CPU on a Node. In that case, the Cluster will try to reschedule your Pod(s) on a different Node with more space.\nIf that fails, or if there is not enough space when you deploy your application, the Cluster will put your workload/deployment in schedule mode until there is enough room on any of the available Nodes to deploy the Pods according to their limits.\n2. Fix PHP/Apache Deployment Workshop Question Before we start, let’s check the current status of the PHP/Apache deployment. Under Alerts \u0026 Detectors which detector has fired? Where else can you find this information?\nTo fix the PHP/Apache StatefulSet, edit ~/workshop/k3s/php-apache.yaml using the following commands to reduce the CPU resources:\nvim ~/workshop/k3s/php-apache.yaml Find the resources section and reduce the CPU limits to 1 and the CPU requests to 0.5:\nresources: limits: cpu: \"1\" memory: \"8Mi\" requests: cpu: \"0.5\" memory: \"4Mi\" Save the changes you have made. (Hint: Use Esc followed by :wq! to save your changes).\nNow, we must delete the existing StatefulSet and re-create it. StatefulSets are immutable, so we must delete the existing one and re-create it with the new changes.\nkubectl delete statefulset php-apache -n apache Now, deploy your changes:\nkubectl apply -f ~/workshop/k3s/php-apache.yaml -n apache 3. Validate the changes You can validate the changes have been applied by running the following command:\nkubectl describe statefulset php-apache -n apache Validate the Pod is now running in Splunk Observability Cloud.\nWorkshop Question Is the Apache Web Servers dashboard showing any data now?\nTip: Don’t forget to use filters and time frames to narrow down your data.\nMonitor the Apache web servers Navigator dashboard for a few minutes.\nWorkshop Question What is happening with the # Hosts reporting chart?\n4. Fix the memory issue If you navigate back to the Apache dashboard, you will notice that metrics are no longer coming in. We have another resource issue and this time we are Out of Memory. Let’s edit the stateful set and increase the memory to what is shown in the image below:\nkubectl edit statefulset php-apache -n apache resources: limits: cpu: \"1\" memory: 16Mi requests: cpu: 500m memory: 12Mi Save the changes you have made.\nHint kubectl edit will open the contents in the vi editor, use Esc followed by :wq! to save your changes.\nBecause StatefulSets are immutable, we must delete the existing Pod and let the StatefulSet re-create it with the new changes.\nkubectl delete pod php-apache-0 -n apache Validate the changes have been applied by running the following command:\nkubectl describe statefulset php-apache -n apache",
    "description": "1. Kubernetes Resources Especially in Production Kubernetes Clusters, CPU and Memory are considered precious resources. Cluster Operators will normally require you to specify the amount of CPU and Memory your Pod or Service will require in the deployment, so they can have the Cluster automatically manage on which Node(s) your solution will be placed.\nYou do this by placing a Resource section in the deployment of your application/Pod\nExample:\nresources: limits: # Maximum amount of CPU \u0026 memory for peek use cpu: \"8\" # Maximum of 8 cores of CPU allowed at for peek use memory: \"8Mi\" # Maximum allowed 8Mb of memory requests: # Request are the expected amount of CPU \u0026 memory for normal use cpu: \"6\" # Requesting 4 cores of a CPU memory: \"4Mi\" # Requesting 4Mb of memory More information can be found here: Resource Management for Pods and Containers",
    "tags": [],
    "title": "Fix PHP/Apache Issue",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/2-hpa/4-fix-apache/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "Download the Splunk Distribution of OpenTelemetry For this workshop, we’ll install the Splunk Distribution of OpenTelemetry manually rather than using the NuGet packages.\nWe’ll start by downloading the latest splunk-otel-dotnet-install.sh file, which we’ll use to instrument our .NET application:\ncd ~/workshop/docker-k8s-otel/helloworld curl -sSfL https://github.com/signalfx/splunk-otel-dotnet/releases/latest/download/splunk-otel-dotnet-install.sh -O Refer to Install the Splunk Distribution of OpenTelemetry .NET manually for further details on the installation process.\nInstall the Distribution In the terminal, install the distribution as follows\n​ Script Example Output sh ./splunk-otel-dotnet-install.sh Downloading v1.8.0 for linux-glibc (/tmp/tmp.m3tSdtbmge/splunk-opentelemetry-dotnet-linux-glibc-x64.zip)... Note: we may need to include the ARCHITECTURE environment when running the command above:\nARCHITECTURE=x64 sh ./splunk-otel-dotnet-install.sh Activate the Instrumentation Next, we can activate the OpenTelemetry instrumentation:\n. $HOME/.splunk-otel-dotnet/instrument.sh Set the Deployment Environment Let’s set the deployment environment, to ensure our data flows into its own environment within Splunk Observability Cloud:\nexport OTEL_RESOURCE_ATTRIBUTES=deployment.environment=otel-$INSTANCE Run the Application with Instrumentation We can run the application as follows:\ndotnet run A Challenge For You How can we see what traces are being exported by the .NET application from our Linux instance?\nClick here to see the answer There are two ways we can do this:\nWe could add OTEL_TRACES_EXPORTER=otlp,console at the start of the dotnet run command, which ensures that traces are both written to collector via OTLP as well as the console. OTEL_TRACES_EXPORTER=otlp,console dotnet run Alternatively, we could add the debug exporter to the collector configuration, and add it to the traces pipeline, which ensures the traces are written to the collector logs. exporters: debug: verbosity: detailed service: pipelines: traces: receivers: [jaeger, otlp, zipkin] processors: - memory_limiter - batch - resourcedetection exporters: [otlphttp, signalfx, debug] Access the Application Once the application is running, use a second SSH terminal and access it using curl:\ncurl http://localhost:8080/hello As before, it should return Hello, World!.\nIf you enabled trace logging, you should see a trace written the console or collector logs such as the following:\ninfo: Program[0] /hello endpoint invoked anonymously Activity.TraceId: c7bbf57314e4856447508cd8addd49b0 Activity.SpanId: 1c92ac653c3ece27 Activity.TraceFlags: Recorded Activity.ActivitySourceName: Microsoft.AspNetCore Activity.DisplayName: GET /hello/{name?} Activity.Kind: Server Activity.StartTime: 2024-12-20T00:45:25.6551267Z Activity.Duration: 00:00:00.0006464 Activity.Tags: server.address: localhost server.port: 8080 http.request.method: GET url.scheme: http url.path: /hello network.protocol.version: 1.1 user_agent.original: curl/7.81.0 http.route: /hello/{name?} http.response.status_code: 200 Resource associated with Activity: splunk.distro.version: 1.8.0 telemetry.distro.name: splunk-otel-dotnet telemetry.distro.version: 1.8.0 service.name: helloworld os.type: linux os.description: Ubuntu 22.04.5 LTS os.build_id: 6.8.0-1021-aws os.name: Ubuntu os.version: 22.04 host.name: derek-1 host.id: 20cf15fcc7054b468647b73b8f87c556 process.owner: splunk process.pid: 16997 process.runtime.description: .NET 8.0.11 process.runtime.name: .NET process.runtime.version: 8.0.11 container.id: 2 telemetry.sdk.name: opentelemetry telemetry.sdk.language: dotnet telemetry.sdk.version: 1.9.0 deployment.environment: otel-derek-1 View your application in Splunk Observability Cloud Now that the setup is complete, let’s confirm that traces are sent to Splunk Observability Cloud. Note that when the application is deployed for the first time, it may take a few minutes for the data to appear.\nNavigate to APM, then use the Environment dropdown to select your environment (i.e. otel-instancename).\nIf everything was deployed correctly, you should see helloworld displayed in the list of services:\nClick on Service Map on the right-hand side to view the service map.\nNext, click on Traces on the right-hand side to see the traces captured for this application.\nAn individual trace should look like the following:\nPress Ctrl + C to quit your Helloworld app before moving to the next step.",
    "description": "Download the Splunk Distribution of OpenTelemetry For this workshop, we’ll install the Splunk Distribution of OpenTelemetry manually rather than using the NuGet packages.\nWe’ll start by downloading the latest splunk-otel-dotnet-install.sh file, which we’ll use to instrument our .NET application:\ncd ~/workshop/docker-k8s-otel/helloworld curl -sSfL https://github.com/signalfx/splunk-otel-dotnet/releases/latest/download/splunk-otel-dotnet-install.sh -O Refer to Install the Splunk Distribution of OpenTelemetry .NET manually for further details on the installation process.\nInstall the Distribution In the terminal, install the distribution as follows",
    "tags": [],
    "title": "Instrument a .NET Application with OpenTelemetry",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/4-instrument-app-with-otel/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e GDI (OTel \u0026 UF)",
    "content": "1. Use Data Setup to instrument a Python application Within the O11y Cloud UI:\nData Management -\u003e Add Integration -\u003e Monitor Applications -\u003e Python (traces) -\u003e Add Integration\nProvide the following to the Configure Integration Wizard:\nService: review\nDjango: no\ncollector endpoint: http://localhost:4317\nEnvironment: rtapp-workshop-[YOURNAME]\nKubernetes: yes\nLegacy Agent: no\nWe are instructed to:\nInstall the instrumentation packages for your Python environment. pip install splunk-opentelemetry[all] splunk-py-trace-bootstrap Configure the Downward API to expose environment variables to Kubernetes resources.\nFor example, update a Deployment to inject environment variables by adding .spec.template.spec.containers.env like:\napiVersion: apps/v1 kind: Deployment spec: selector: matchLabels: app: your-application template: spec: containers: - name: myapp env: - name: SPLUNK_OTEL_AGENT valueFrom: fieldRef: fieldPath: status.hostIP - name: OTEL_EXPORTER_OTLP_ENDPOINT value: \"http://$(SPLUNK_OTEL_AGENT):4317\" - name: OTEL_SERVICE_NAME value: \"review\" - name: OTEL_RESOURCE_ATTRIBUTES value: \"deployment.environment=rtapp-workshop-stevel\" Enable the Splunk OTel Python agent by editing your Python service command.\nsplunk-py-trace python3 main.py --port=8000 The actions we must perform include:\nUpdate the Dockerfile to install the splunk-opentelemetry packages Update the deployment.yaml for each service to include these environment variables which will be used by the pod and container. Update our Dockerfile for REVIEW so that our program is bootstrapped with splunk-py-trace Note We will accomplish this by:\ngenerating a new requirements.txt file generating a new container image with an updated Dockerfile for REVIEW and then update the review.deployment.yaml to capture all of these changes. 2. Update the REVIEW container Generate a new container image\nUpdate the Dockerfile for REVIEW (/workshop/flask_apps_finish/review)\nFROM python:3.10-slim WORKDIR /app COPY requirements.txt /app RUN pip install -r requirements.txt RUN pip install splunk-opentelemetry[all] RUN splk-py-trace-bootstrap COPY ./review.py /app EXPOSE 5000 ENTRYPOINT [ \"splunk-py-trace\" ] CMD [ \"python\", \"review.py\" ] Note Note that the only lines, in bold, added to the Dockerfile\nGenerate a new container image with docker build in the ‘finished’ directory Notice that I have changed the repository name from localhost:8000/review:0.01 to localhost:8000/review-splkotel:0.01 Ensure you are in the correct directory.\npwd ./workshop/flask_apps_finish/review ​ docker build docker build Output docker build -f Dockerfile.review -t localhost:8000/review-splkotel:0.01 . [+] Building 27.1s (12/12) FINISHED =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 364B 0.0s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 2B 0.0s =\u003e [internal] load metadata for docker.io/library/python:3.10-slim 1.6s =\u003e [auth] library/python:pull token for registry-1.docker.io 0.0s =\u003e [1/6] FROM docker.io/library/python:3.10-slim@sha256:54956d6c929405ff651516d5ebbc204203a6415c9d2757aad 0.0s =\u003e [internal] load build context 0.3s =\u003e =\u003e transferring context: 1.91kB 0.3s =\u003e CACHED [2/6] WORKDIR /app 0.0s =\u003e [3/6] COPY requirements.txt /app 0.0s =\u003e [4/6] RUN pip install -r requirements.txt 15.3s =\u003e [5/6] RUN splk-py-trace-bootstrap 9.0s =\u003e [6/6] COPY ./review.py /app 0.0s =\u003e exporting to image 0.6s =\u003e =\u003e exporting layers 0.6s =\u003e =\u003e writing image sha256:164977dd860a17743b8d68bcc50c691082bd3bfb352d1025dc3a54b15d5f4c4d 0.0s =\u003e =\u003e naming to docker.io/localhost:8000/review-splkotel:0.01 0.0s Push the image to Docker Hub with docker push command ​ docker push docker push Output docker push localhost:8000/review-splkotel:0.01 The push refers to repository [docker.io/localhost:8000/review-splkotel] 682f0e550f2c: Pushed dd7dfa312442: Pushed 917fd8334695: Pushed e6782d51030d: Pushed c6b19a64e528: Mounted from localhost:8000/review 8f52e3bfc0ab: Mounted from localhost:8000/review f90b85785215: Mounted from localhost:8000/review d5c0beb90ce6: Mounted from localhost:8000/review 3759be374189: Mounted from localhost:8000/review fd95118eade9: Mounted from localhost:8000/review 0.01: digest: sha256:3b251059724dbb510ea81424fc25ed03554221e09e90ef965438da33af718a45 size: 2412 3. Update the REVIEW deployment in Kubernetes review.deployment.yaml must be updated with the following changes:\nLoad the new container image on Docker Hub Add environment variables so traces can be emitted to the OTEL collector The deployment must be replaced using the updated review.deployment.yaml\nUpdate review.deployment.yaml (updates highlighted in bold)\napiVersion: apps/v1 kind: Deployment metadata: name: review labels: app: review spec: replicas: 1 selector: matchLabels: app: review template: metadata: labels: app: review spec: imagePullSecrets: - name: regcred containers: - image: localhost:8000/review-splkotel:0.01 name: review volumeMounts: - mountPath: /var/appdata name: appdata env: - name: SPLUNK_OTEL_AGENT valueFrom: fieldRef: fieldPath: status.hostIP - name: OTEL_SERVICE_NAME value: 'review' - name: SPLUNK_METRICS_ENDPOINT value: \"http://$(SPLUNK_OTEL_AGENT):9943\" - name: OTEL_EXPORTER_OTLP_ENDPOINT value: \"http://$(SPLUNK_OTEL_AGENT):4317\" - name: OTEL_RESOURCE_ATTRIBUTES value: 'deployment.environment=rtapp-workshop-stevel' volumes: - name: appdata hostPath: path: /var/appdata Apply review.deployment.yaml. Kubernetes will automatically pick up the changes to the deployment and redeploy new pods with these updates\nNotice that the review-* pod has been restarted kubectl apply -f review.deployment.yaml kubectl get pods NAME READY STATUS RESTARTS AGE kafka-client 0/1 Unknown 0 155d curl 0/1 Unknown 0 155d kafka-zookeeper-0 1/1 Running 0 26h kafka-2 2/2 Running 0 26h kafka-exporter-647bddcbfc-h9gp5 1/1 Running 2 26h mongodb-6f6c78c76-kl4vv 2/2 Running 0 26h kafka-1 2/2 Running 1 26h kafka-0 2/2 Running 1 26h splunk-otel-collector-1653114277-agent-n4dfn 2/2 Running 0 26h splunk-otel-collector-1653114277-k8s-cluster-receiver-5f48v296j 1/1 Running 0 26h splunk-otel-collector-1653114277-agent-jqxhh 2/2 Running 0 26h review-6686859bd7-4pf5d 1/1 Running 0 11s review-5dd8cfd77b-52jbd 0/1 Terminating 0 2d10h kubectl get pods NAME READY STATUS RESTARTS AGE kafka-client 0/1 Unknown 0 155d curl 0/1 Unknown 0 155d kafka-zookeeper-0 1/1 Running 0 26h kafka-2 2/2 Running 0 26h kafka-exporter-647bddcbfc-h9gp5 1/1 Running 2 26h mongodb-6f6c78c76-kl4vv 2/2 Running 0 26h kafka-1 2/2 Running 1 26h kafka-0 2/2 Running 1 26h splunk-otel-collector-1653114277-agent-n4dfn 2/2 Running 0 26h splunk-otel-collector-1653114277-k8s-cluster-receiver-5f48v296j 1/1 Running 0 26h splunk-otel-collector-1653114277-agent-jqxhh 2/2 Running 0 26h review-6686859bd7-4pf5d 1/1 Running 0 15s",
    "description": "1. Use Data Setup to instrument a Python application Within the O11y Cloud UI:\nData Management -\u003e Add Integration -\u003e Monitor Applications -\u003e Python (traces) -\u003e Add Integration\nProvide the following to the Configure Integration Wizard:\nService: review\nDjango: no\ncollector endpoint: http://localhost:4317\nEnvironment: rtapp-workshop-[YOURNAME]\nKubernetes: yes\nLegacy Agent: no\nWe are instructed to:\nInstall the instrumentation packages for your Python environment. pip install splunk-opentelemetry[all] splunk-py-trace-bootstrap Configure the Downward API to expose environment variables to Kubernetes resources.",
    "tags": [],
    "title": "Instrument REVIEWS for Tracing",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/8-gdi/4-instrument/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Monolith Workshop",
    "content": "For the Splunk Log Observer component, the Splunk OpenTelemetry Collector automatically collects logs from the Spring PetClinic application and sends them to Splunk Observability Cloud using the OTLP exporter, anotating the log events with trace_id, span_id and trace flags.\nLog Observer provides a real-time view of logs from your applications and infrastructure. It allows you to search, filter, and analyze logs to troubleshoot issues and monitor your environment.\nGo back to the PetClinic web application and click on the Error link several times. This will generate some log messages in the PetClinic application logs.\nFrom the left-hand menu click on Log Observer and ensure Index is set to splunk4rookies-workshop.\nNext, click Add Filter search for the field service.name select the value \u003cINSTANCE\u003e-petclinic-service and click = (include). You should now see only the log messages from your PetClinic application.\nSelect one of the log entries that were generated by clicking on the Error link in the PetClinic application. You will see the log message and the trace metadata that was automatically injected into the log message. Also, you will notice that Related Content is available for APM and Infrastructure.\nThis is the end of the workshop and we have certainly covered a lot of ground. At this point, you should have metrics, traces (APM \u0026 RUM), logs, database query performance and code profiling being reported into Splunk Observability Cloud and all without having to modify the PetClinic application code (well except for RUM).\nCongratulations!",
    "description": "For the Splunk Log Observer component, the Splunk OpenTelemetry Collector automatically collects logs from the Spring PetClinic application and sends them to Splunk Observability Cloud using the OTLP exporter, anotating the log events with trace_id, span_id and trace flags.\nLog Observer provides a real-time view of logs from your applications and infrastructure. It allows you to search, filter, and analyze logs to troubleshoot issues and monitor your environment.\nGo back to the PetClinic web application and click on the Error link several times. This will generate some log messages in the PetClinic application logs.",
    "tags": [],
    "title": "4. Log Observer",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/1-petclinic-monolith/5-log-observer/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "Log Observer Connect allows you to seamlessly bring in the same log data from your Splunk Platform into an intuitive and no-code interface designed to help you find and fix problems quickly. You can easily perform log-based analysis and seamlessly correlate your logs with Splunk Infrastructure Monitoring’s real-time metrics and Splunk APM traces in one place.\nEnd-to-end visibility: By combining the powerful logging capabilities of Splunk Platform with Splunk Observability Cloud’s traces and real-time metrics for deeper insights and more context of your hybrid environment.\nPerform quick and easy log-based investigations: By reusing logs that are already ingested in Splunk Cloud Platform or Enterprise in a simplified and intuitive interface (no need to know SPL!) with customizable and out-of-the-box dashboards\nAchieve higher economies of scale and operational efficiency: By centralizing log management across teams, breaking down data and team silos, and getting better overall support",
    "description": "Log Observer Connect allows you to seamlessly bring in the same log data from your Splunk Platform into an intuitive and no-code interface designed to help you find and fix problems quickly. You can easily perform log-based analysis and seamlessly correlate your logs with Splunk Infrastructure Monitoring’s real-time metrics and Splunk APM traces in one place.\nEnd-to-end visibility: By combining the powerful logging capabilities of Splunk Platform with Splunk Observability Cloud’s traces and real-time metrics for deeper insights and more context of your hybrid environment.\nPerform quick and easy log-based investigations: By reusing logs that are already ingested in Splunk Cloud Platform or Enterprise in a simplified and intuitive interface (no need to know SPL!) with customizable and out-of-the-box dashboards\nAchieve higher economies of scale and operational efficiency: By centralizing log management across teams, breaking down data and team silos, and getting better overall support",
    "tags": [],
    "title": "Log Observer Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/4-log-observer-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 5. Splunk Log Observer",
    "content": "The next chart type that can be used with logs is the Log View chart type. This chart will allow us to see log messages based on predefined filters.\nAs with the previous Log Timeline chart, we will add a version of this chart to our Customer Health Service Dashboard:\nExercise After the previous exercise make sure you are still in Log Observer. The filters should be the same as the previous exercise, with the time picker set to the Last 15 minutes and filtering on severity=error, sf_service=wire-transfer-service and sf_environment=[WORKSHOPNAME]. Make sure we have the header with just the fields we wanted. Click again on Save and then Save to Dashboard. This will again provide you with the Chart creation dialog. For the Chart name use Log View. This time Click Select Dashboard and search for the Dashboard you created in the previous exercise. You can start by typing your initials in the search box (1). Click on your dashboard name to highlight it (2) and click OK (3). This will return you to the create chart dialog. Ensure Log View is selected as the Chart Type. To see your dashboard click Save and go to dashboard. The result should be similar to the dashboard below: As the last step in this exercise, let us add your dashboard to your workshop team page, this will make it easy to find later in the workshop. At the top of the page, click on the … to the left of your dashboard name. Select Link to teams from the drop-down. In the following Link to teams dialog box, find the Workshop team that your instructor will have provided for you and click Done.",
    "description": "The next chart type that can be used with logs is the Log View chart type. This chart will allow us to see log messages based on predefined filters.\nAs with the previous Log Timeline chart, we will add a version of this chart to our Customer Health Service Dashboard:\nExercise After the previous exercise make sure you are still in Log Observer. The filters should be the same as the previous exercise, with the time picker set to the Last 15 minutes and filtering on severity=error, sf_service=wire-transfer-service and sf_environment=[WORKSHOPNAME]. Make sure we have the header with just the fields we wanted. Click again on Save and then Save to Dashboard. This will again provide you with the Chart creation dialog. For the Chart name use Log View. This time Click Select Dashboard and search for the Dashboard you created in the previous exercise. You can start by typing your initials in the search box (1). Click on your dashboard name to highlight it (2) and click OK (3). This will return you to the create chart dialog. Ensure Log View is selected as the Chart Type. To see your dashboard click Save and go to dashboard. The result should be similar to the dashboard below: As the last step in this exercise, let us add your dashboard to your workshop team page, this will make it easy to find later in the workshop. At the top of the page, click on the … to the left of your dashboard name. Select Link to teams from the drop-down. In the following Link to teams dialog box, find the Workshop team that your instructor will have provided for you and click Done.",
    "tags": [],
    "title": "4. Log View Chart",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/5-log-observer/4-log-view-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 7. Splunk Log Observer",
    "content": "The next chart type that can be used with logs is the Log View chart type. This chart will allow us to see log messages based on predefined filters.\nAs with the previous Log Timeline chart, we will add a version of this chart to our Customer Health Service Dashboard:\nExercise After the previous exercise make sure you are still in Log Observer. The filters should be the same as the previous exercise, with the time picker set to the Last 15 minutes and filtering on severity=error, sf_service=paymentservice and sf_environment=[WORKSHOPNAME]. Make sure we have the header with just the fields we wanted. Click again on Save and then Save to Dashboard. This will again provide you with the Chart creation dialog. For the Chart name use Log View. This time Click Select Dashboard and search for the Dashboard you created in the previous exercise. You can start by typing your initials in the search box (1). Click on your dashboard name to highlight it (2) and click OK (3). This will return you to the create chart dialog. Ensure Log View is selected as the Chart Type. To see your dashboard click Save and go to dashboard. The result should be similar to the dashboard below: As the last step in this exercise, let us add your dashboard to your workshop team page, this will make it easy to find later in the workshop. At the top of the page, click on the … to the left of your dashboard name. Select Link to teams from the drop-down. In the following Link to teams dialog box, find the Workshop team that your instructor will have provided for you and click Done. In the next session, we will look at Splunk Synthetics and see how we can automate the testing of web-based applications.",
    "description": "The next chart type that can be used with logs is the Log View chart type. This chart will allow us to see log messages based on predefined filters.\nAs with the previous Log Timeline chart, we will add a version of this chart to our Customer Health Service Dashboard:\nExercise After the previous exercise make sure you are still in Log Observer. The filters should be the same as the previous exercise, with the time picker set to the Last 15 minutes and filtering on severity=error, sf_service=paymentservice and sf_environment=[WORKSHOPNAME]. Make sure we have the header with just the fields we wanted. Click again on Save and then Save to Dashboard. This will again provide you with the Chart creation dialog. For the Chart name use Log View. This time Click Select Dashboard and search for the Dashboard you created in the previous exercise. You can start by typing your initials in the search box (1). Click on your dashboard name to highlight it (2) and click OK (3). This will return you to the create chart dialog. Ensure Log View is selected as the Chart Type. To see your dashboard click Save and go to dashboard. The result should be similar to the dashboard below: As the last step in this exercise, let us add your dashboard to your workshop team page, this will make it easy to find later in the workshop. At the top of the page, click on the … to the left of your dashboard name. Select Link to teams from the drop-down. In the following Link to teams dialog box, find the Workshop team that your instructor will have provided for you and click Done. In the next session, we will look at Splunk Synthetics and see how we can automate the testing of web-based applications.",
    "tags": [],
    "title": "4. Log View Chart",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/7-log-observer/4-log-view-chart/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Lambda Tracing",
    "content": "The second part of our workshop will focus on demonstrating how manual instrumentation with OpenTelemetry empowers us to enhance telemetry collection. More specifically, in our case, it will enable us to propagate trace context data from the producer-lambda function to the consumer-lambda function, thus enabling us to see the relationship between the two functions, even across Kinesis Stream, which currently does not support automatic context propagation.\nThe Manual Instrumentation Workshop Directory \u0026 Contents Once again, we will first start by taking a look at our operating directory, and some of its files. This time, it will be workshop/lambda/manual directory. This is where all the content for the manual instrumentation portion of our workshop resides.\nThe manual directory Run the following command to get into the workshop/lambda/manual directory:\ncd ~/workshop/lambda/manual Inspect the contents of this directory with the ls command:\nls The output should include the following files and directories:\nhandler outputs.tf terraform.tf variables.tf main.tf send_message.py terraform.tfvars Workshop Question Do you see any difference between this directory and the auto directory when you first started?\nCompare auto and manual files Let’s make sure that all these files that LOOK the same, are actually the same.\nCompare the main.tf files in the auto and manual directories:\ndiff ~/workshop/lambda/auto/main.tf ~/workshop/lambda/manual/main.tf There is no difference! (Well, there shouldn’t be. Ask your workshop facilitator to assist you if there is) Now, let’s compare the producer.mjs files:\ndiff ~/workshop/lambda/auto/handler/producer.mjs ~/workshop/lambda/manual/handler/producer.mjs There’s quite a few differences here! You may wish to view the entire file and examine its content\ncat ~/workshop/lambda/manual/handler/producer.mjs Notice how we are now importing some OpenTelemetry objects directly into our function to handle some of the manual instrumentation tasks we require. import { context, propagation, trace, } from \"@opentelemetry/api\"; We are importing the following objects from @opentelemetry/api to propagate our context in our producer function: context propagation trace Finally, compare the consumer.mjs files:\ndiff ~/workshop/lambda/auto/handler/consumer.mjs ~/workshop/lambda/manual/handler/consumer.mjs Here also, there are a few differences of note. Let’s take a closer look\ncat handler/consumer.mjs In this file, we are importing the following @opentelemetry/api objects: propagation trace ROOT_CONTEXT We use these to extract the trace context that was propagated from the producer function Then to add new span attributes based on our name and superpower to the extracted trace context Propagating the Trace Context from the Producer Function The below code executes the following steps inside the producer function:\nGet the tracer for this trace Initialize a context carrier object Inject the context of the active span into the carrier object Modify the record we are about to pu on our Kinesis stream to include the carrier that will carry the active span’s context to the consumer ... import { context, propagation, trace, } from \"@opentelemetry/api\"; ... const tracer = trace.getTracer('lambda-app'); ... return tracer.startActiveSpan('put-record', async(span) =\u003e { let carrier = {}; propagation.inject(context.active(), carrier); const eventBody = Buffer.from(event.body, 'base64').toString(); const data = \"{\\\"tracecontext\\\": \" + JSON.stringify(carrier) + \", \\\"record\\\": \" + eventBody + \"}\"; console.log( `Record with Trace Context added: ${data}` ); try { await kinesis.send( new PutRecordCommand({ StreamName: streamName, PartitionKey: \"1234\", Data: data, }), message = `Message placed in the Event Stream: ${streamName}` ) ... span.end(); Extracting Trace Context in the Consumer Function The below code executes the following steps inside the consumer function:\nExtract the context that we obtained from producer-lambda into a carrier object. Extract the tracer from current context. Start a new span with the tracer within the extracted context. Bonus: Add extra attributes to your span, including custom ones with the values from your message! Once completed, end the span. import { propagation, trace, ROOT_CONTEXT } from \"@opentelemetry/api\"; ... const carrier = JSON.parse( message ).tracecontext; const parentContext = propagation.extract(ROOT_CONTEXT, carrier); const tracer = trace.getTracer(process.env.OTEL_SERVICE_NAME); const span = tracer.startSpan(\"Kinesis.getRecord\", undefined, parentContext); span.setAttribute(\"span.kind\", \"server\"); const body = JSON.parse( message ).record; if (body.name) { span.setAttribute(\"custom.tag.name\", body.name); } if (body.superpower) { span.setAttribute(\"custom.tag.superpower\", body.superpower); } ... span.end(); Now let’s see the different this makes!",
    "description": "The second part of our workshop will focus on demonstrating how manual instrumentation with OpenTelemetry empowers us to enhance telemetry collection. More specifically, in our case, it will enable us to propagate trace context data from the producer-lambda function to the consumer-lambda function, thus enabling us to see the relationship between the two functions, even across Kinesis Stream, which currently does not support automatic context propagation.\nThe Manual Instrumentation Workshop Directory \u0026 Contents Once again, we will first start by taking a look at our operating directory, and some of its files. This time, it will be workshop/lambda/manual directory. This is where all the content for the manual instrumentation portion of our workshop resides.",
    "tags": [],
    "title": "Manual Instrumentation",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/6-lambda-kinesis/4-manual-instrumentation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Lambda Tracing and Kinesis",
    "content": "Manual Instrumentation Navigate to the manual directory that contains manually instrumentated code.\n​ Command cd ~/o11y-lambda-lab/manual Inspect the contents of the files in this directory. Take a look at the serverless.yml template.\n​ Command cat serverless.yml Workshop Question Do you see any difference from the same file in your auto directory?\nYou can try to compare them with a diff command:\n​ Diff Command Expected Output diff ~/o11y-lambda-lab/auto/serverless.yml ~/o11y-lambda-lab/manual/serverless.yml 19c19 \u003c #====================================== --- \u003e #====================================== There is no difference! (Well, there shouldn’t be. Ask your lab facilitator to assist you if there is)\nNow compare handler.js it with the same file in auto directory using the diff command:\n​ Diff Command diff ~/o11y-lambda-lab/auto/handler.js ~/o11y-lambda-lab/manual/handler.js Look at all these differences!\nYou may wish to view the entire file with cat handler.js command and examine its content.\nNotice how we are now importing some OpenTelemetry libraries directly into our function to handle some of the manual instrumenation tasks we require.\nconst otelapi = require('@opentelemetry/api'); const otelcore = require('@opentelemetry/core'); We are using https://www.npmjs.com/package/@opentelemetry/api to manipulate the tracing logic in our functions. We are using https://www.npmjs.com/package/@opentelemetry/core to access the Propagator objects that we will use to manually propagate our context with.\nInject Trace Context in Producer Function The below code executes the following steps inside the Producer function:\nGet the current Active Span. Create a Propagator. Initialize a context carrier object. Inject the context of the active span into the carrier object. Modify the record we are about to put on our Kinesis stream to include the carrier that will carry the active span’s context to the consumer. const activeSpan = otelapi.trace.getSpan(otelapi.context.active()); const propagator = new otelcore.W3CTraceContextPropagator(); let carrier = {}; propagator.inject(otelapi.trace.setSpanContext(otelapi.ROOT_CONTEXT, activeSpan.spanContext()), carrier, otelapi.defaultTextMapSetter ); const data = \"{\\\"tracecontext\\\": \" + JSON.stringify(carrier) + \", \\\"record\\\":\" + event.body + \"}\"; console.log(`Record with Trace Context added: ${data}`); Extract Trace Context in Consumer Function The bellow code executes the following steps inside the Consumer function:\nExtract the context that we obtained from the Producer into a carrier object. Create a Propagator. Extract the context from the carrier object in Customer function’s parent span context. Start a new span with the parent span context. Bonus: Add extra attributes to your span, including custom ones with the values from your message! Once completed, end the span. const carrier = JSON.parse( message ).tracecontext; const propagator = new otelcore.W3CTraceContextPropagator(); const parentContext = propagator.extract(otelapi.ROOT_CONTEXT, carrier, otelapi.defaultTextMapGetter); const tracer = otelapi.trace.getTracer(process.env.OTEL_SERVICE_NAME); const span = tracer.startSpan(\"Kinesis.getRecord\", undefined, parentContext); span.setAttribute(\"span.kind\", \"server\"); const body = JSON.parse( message ).record; if (body.name) { span.setAttribute(\"custom.tag.name\", body.name); } if (body.superpower) { span.setAttribute(\"custom.tag.superpower\", body.superpower); } --- function does some work span.end(); Now let’s see the difference this makes.",
    "description": "Manual Instrumentation Navigate to the manual directory that contains manually instrumentated code.\n​ Command cd ~/o11y-lambda-lab/manual Inspect the contents of the files in this directory. Take a look at the serverless.yml template.\n​ Command cat serverless.yml Workshop Question Do you see any difference from the same file in your auto directory?",
    "tags": [],
    "title": "Manual Instrumentation",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/6-lambda-kinesis/4-manual-instrumentation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e NodeJS Zero-Config Workshop",
    "content": "1. Patching the Payment Service Finally, we will patch the paymentservice deployment with an annotation to inject the NodeJS auto instrumentation. This will allow us to see the paymentservice service in APM.\nkubectl patch deployment opentelemetry-demo-paymentservice -n otel-demo -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"instrumentation.opentelemetry.io/inject-nodejs\":\"default/splunk-otel-collector\"}}}} }' This will cause the opentelemetry-demo-paymentservice pod to restart and after a few minutes, you should see the paymentservice service in APM.",
    "description": "1. Patching the Payment Service Finally, we will patch the paymentservice deployment with an annotation to inject the NodeJS auto instrumentation. This will allow us to see the paymentservice service in APM.\nkubectl patch deployment opentelemetry-demo-paymentservice -n otel-demo -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"instrumentation.opentelemetry.io/inject-nodejs\":\"default/splunk-otel-collector\"}}}} }' This will cause the opentelemetry-demo-paymentservice pod to restart and after a few minutes, you should see the paymentservice service in APM.",
    "tags": [],
    "title": "Zero Configuration - Payment Service",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/3-nodejs-kubernetes/4-payment-service/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts",
    "content": "Processors are run on data between being received and being exported. Processors are optional though some are recommended. There are a large number of processors included in the OpenTelemetry contrib Collector.\n%%{ init:{ \"theme\":\"base\", \"themeVariables\": { \"primaryColor\": \"#ffffff\", \"clusterBkg\": \"#eff2fb\", \"defaultLinkColor\": \"#333333\" } } }%% flowchart LR; style Processors fill:#e20082,stroke:#333,stroke-width:4px,color:#fff subgraph Collector A[OTLP] --\u003e M(Receivers) B[JAEGER] --\u003e M(Receivers) C[Prometheus] --\u003e M(Receivers) end subgraph Processors M(Receivers) --\u003e H(Filters, Attributes, etc) E(Extensions) end subgraph Exporters H(Filters, Attributes, etc) --\u003e S(OTLP) H(Filters, Attributes, etc) --\u003e T(JAEGER) H(Filters, Attributes, etc) --\u003e U(Prometheus) end",
    "description": "Processors are run on data between being received and being exported. Processors are optional though some are recommended. There are a large number of processors included in the OpenTelemetry contrib Collector.\n%%{ init:{ \"theme\":\"base\", \"themeVariables\": { \"primaryColor\": \"#ffffff\", \"clusterBkg\": \"#eff2fb\", \"defaultLinkColor\": \"#333333\" } } }%% flowchart LR; style Processors fill:#e20082,stroke:#333,stroke-width:4px,color:#fff subgraph Collector A[OTLP] --\u003e M(Receivers) B[JAEGER] --\u003e M(Receivers) C[Prometheus] --\u003e M(Receivers) end subgraph Processors M(Receivers) --\u003e H(Filters, Attributes, etc) E(Extensions) end subgraph Exporters H(Filters, Attributes, etc) --\u003e S(OTLP) H(Filters, Attributes, etc) --\u003e T(JAEGER) H(Filters, Attributes, etc) --\u003e U(Prometheus) end",
    "tags": [],
    "title": "OpenTelemetry Collector Processors",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/4-processors/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "View Service Map Next step is open your Observability UI, accessing the proper org ( where you sent the traces to ) and click APM and Access the Environemnnt that matches the username you put in .env.\nPlease note it may take 4-5 minutes or more for traces to show up and you will see full map “form” as traces are coming in, so you may have to refresh the page a few times each time we Build and Deploy.\nIt is recommended to use a -15m look back during this lab. You may need to change it from time to time (for example to -5m or a custom -10m) to make sure you are only looking at the newer traces after your changes.\nNOTE: You may have to refresh the page several times to see your Environment Tag in the UI. The prefix of the Environment tag should match what you entered for SHOP_USER in the .env file.\nNOTE: Typically, to identify root cause and route an issue, an SRE or alert responder would check metrics and logs to determine if it is a software or hardware related issue, and thus route to the correct party. In this excercise we are ONLY handling software issues, so we are skipping the metrics and logs parts of normal triage.\nIf your instrumentation was successful, the service-map will show latency from the shop service to the products service.\nOk let’s triage this SOFTWARE ISSUE and skip directly to the traces.\nClick on shop service Click Traces (on the right side) Sort by Duration Select the longest duration trace Or one of the obvious much longer ones Now we can see the long latency occurred in the products service and if we click on products: /products we can see the offending method was products:ProductResource.getAllProducts\nOur next step here would be to send that trace to a developer by clicking download trace and they will have to debug the function. Since we will be the developer there is no need to download the trace. Just remember that is normal workflow for alert responders to route an issue to the “Repairers” while providing trace data.\nBefore we do that please take note of the Tags available for the developer to leverage to find root cause. We see standard out of the box Otel tags on the span, environmental information, but no indications of data specific to something inside custom code (which is where the problem often is).",
    "description": "View Service Map Next step is open your Observability UI, accessing the proper org ( where you sent the traces to ) and click APM and Access the Environemnnt that matches the username you put in .env.\nPlease note it may take 4-5 minutes or more for traces to show up and you will see full map “form” as traces are coming in, so you may have to refresh the page a few times each time we Build and Deploy.",
    "tags": [],
    "title": "Review APM in the UI",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/4-review-apm/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop \u003e 5. APM Features",
    "content": "Splunk APM provide Service Centric Views that provide engineers a deep understanding of service performance in one centralized view. Now, across every service, engineers can quickly identify errors or bottlenecks from a service’s underlying infrastructure, pinpoint performance degradation from new deployments, and visualize the health of every third party dependency.\nTo see this dashboard for the api-gateway, click on APM from the left-hand menu and then click on the api-gateway service in the list. This will bring you to the Service Centric View dashboard:\nThis view, which is available for each of your instrumented services, offers an overview of Service metrics, Error breakdown, Runtime metrics (Java) and Infrastructure metrics.",
    "description": "Splunk APM provide Service Centric Views that provide engineers a deep understanding of service performance in one centralized view. Now, across every service, engineers can quickly identify errors or bottlenecks from a service’s underlying infrastructure, pinpoint performance degradation from new deployments, and visualize the health of every third party dependency.\nTo see this dashboard for the api-gateway, click on APM from the left-hand menu and then click on the api-gateway service in the list. This will bring you to the Service Centric View dashboard:",
    "tags": [],
    "title": "Service Centric View",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/5-traces/4-red-metrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Advanced Synthetics",
    "content": "The simple settings allow you to configure the basics of the test:\nName: The name of the test (e.g. RWC - Online Boutique). Details: Locations: The locations where the test will run from. Device: Emulate different devices and connection speeds. Also, the viewport will be adjusted to match the chosen device. Frequency: How often the test will run. Round-robin: If multiple locations are selected, the test will run from one location at a time, rather than all locations at once. Active: Set the test to active or inactive. For this workshop, we will configure the locations that we wish to monitor from. Click in the Locations field and you will be presented with a list of global locations (over 50 in total).\nSelect the following locations:\nAWS - N. Virginia AWS - London AWS - Melbourne Once complete, scroll down and click on Click on Submit to save the test.\nThe test will now be scheduled to run every 5 minutes from the 3 locations that we have selected. This does take a few minutes for the schedule to be created.\nSo while we wait for the test to be scheduled, click on Edit test so we can go through the Advanced settings.",
    "description": "The simple settings allow you to configure the basics of the test:\nName: The name of the test (e.g. RWC - Online Boutique). Details: Locations: The locations where the test will run from. Device: Emulate different devices and connection speeds. Also, the viewport will be adjusted to match the chosen device. Frequency: How often the test will run. Round-robin: If multiple locations are selected, the test will run from one location at a time, rather than all locations at once. Active: Set the test to active or inactive. For this workshop, we will configure the locations that we wish to monitor from. Click in the Locations field and you will be presented with a list of global locations (over 50 in total).",
    "tags": [],
    "title": "Test settings",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/3-advanced-synthetics/4-edit-test-settings/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "Persona You are a hip urban professional, longing to buy your next novelty items in the famous Online Boutique shop. You have heard that the Online Boutique is the place to go for all your hipster needs.\nThe purpose of this exercise is for you to interact with the Online Boutique web application. This is a sample application that is used to demonstrate the capabilities of Splunk Observability Cloud. The application is a simple e-commerce site that allows you to browse items, add them to your cart, and then checkout.\nThe application will already be deployed for you and your instructor will provide you with a link to the Online Boutique website e.g:\nhttp://\u003cs4r-workshop-i-xxx.splunk\u003e.show:81/. The application is also running on ports 80 \u0026 443 if you prefer to use those or port 81 is unreachable. Exercise - Let’s go shopping Once you have the link to the Online Boutique, have a browse through a few items, add them to your cart and then, finally, do a checkout. Repeat this exercise a few times and if possible use different browsers, mobile devices or tablets as this will generate more data for you to explore. Tip While you are waiting for pages to load, please move your mouse cursor around the page. This will generate more data for us to explore at a later date in this workshop.\nExercise (cont.) Did you notice anything about the checkout process? Did it seem to take a while to complete, but it did ultimately complete? When this happens please copy the Order Confirmation ID and save it locally somewhere as we will need it later. Close the browser sessions you used to shop. This is what a poor user experience can feel like and since this is a potential customer satisfaction issue we had better jump on this and troubleshoot.\nLet’s go take a look at what the data looks like in Splunk RUM.",
    "description": "Interact with the Online Boutique web application to generate data for Splunk Observability Cloud.",
    "tags": [],
    "title": "Let's go shopping 💶",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/4-online-boutique/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud",
    "content": "Persona You are a back-end developer and you have been called in to help investigate an issue found by the SRE. The SRE has identified a poor user experience and has asked you to investigate the issue.\nGetting to the root cause of a problem in cloud-native environments requires engineers to navigate through immense complexity within a distributed system. Oftentimes, you didn’t write the code and you lack the background and context to quickly understand what’s going on when a problem occurs. That is where Splunk APM comes in.",
    "description": "In this section, we will use APM to drill down and identify where the problem is.",
    "tags": [],
    "title": "Splunk APM",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/4-apm/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 8. Splunk Synthetics",
    "content": "Given you can run these tests 24/7, it is an ideal tool to get warned early if your tests are failing or starting to run longer than your agreed SLA instead of getting informed by social media, or Uptime websites.\nTo stop that from happening let’s detect if our test is taking more than 1.1 minutes.\nExercise Go back to the Synthetics home page via the menu on the left\nSelect the workshop test again and click the Create Detector button at the top of the page.\nEdit the text New Synthetics Detector (1) and replace it with INITIALS - [WORKSHOPNAME]`.\nEnsure that Run duration and Static threshold are selected.\nSet the Trigger threshold (2) to be around 65,000 to 68,000 and hit enter to update the chart. Make sure you have more than one spike cutting through the threshold line as shown above (you may have to adjust the threshold value a bit to match your actual latency).\nLeave the rest as default.\nNote that there is now a row of red and white triangles appearing below the spikes (3). The red triangles let you know that your detector found that your test was above the given threshold \u0026 the white triangle indicates that the result returned below the threshold. Each red triangle will trigger an alert.\nYou can change the Alerts criticality (4) by changing the drop-down to a different level, as well as the method of alerting. Make sure you do NOT add a Recipient as this could lead to you being subjected to an alert storm!\nClick Activate to deploy your detector.\nTo see your new created detector click Edit Test button\nAt the bottom of the page is a list of active detectors.\nIf you can’t find yours, but see one called New Synthetic Detector, you may not have saved it correctly with your name. Click on the New Synthetic Detector link, and redo the rename.\nClick on the Close button to exit the edit mode.",
    "description": "Given you can run these tests 24/7, it is an ideal tool to get warned early if your tests are failing or starting to run longer than your agreed SLA instead of getting informed by social media, or Uptime websites.\nTo stop that from happening let’s detect if our test is taking more than 1.1 minutes.\nExercise Go back to the Synthetics home page via the menu on the left",
    "tags": [],
    "title": "4. Synthetics Detector",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/8-synthetics/4-synthetics-detector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Workshop Setup",
    "content": "Log Observer Connect The workshops require a Log Observer Connect configuration to be setup. This is required to be able to send logs from the workshop instances to Splunk Cloud/Enterprise. SWiPE provides a simple form to configure this and is available here.\nLog Field Aliasing Log Field Aliasing configuration is required to ensure Related Content works correctly in the Splunk Observability Cloud UI. Configure as per the following:\nAPM MetricSets APM MetricSets need to be configured for the tags in the screenshot below.",
    "description": "Log Observer Connect The workshops require a Log Observer Connect configuration to be setup. This is required to be able to send logs from the workshop instances to Splunk Cloud/Enterprise. SWiPE provides a simple form to configure this and is available here.\nLog Field Aliasing Log Field Aliasing configuration is required to ensure Related Content works correctly in the Splunk Observability Cloud UI. Configure as per the following:",
    "tags": [],
    "title": "4. Trial Org Configuration",
    "uri": "/observability-workshop/v6.5/en/workshop-setup/4-org-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 5. Splunk RUM",
    "content": "Exercise Close the RUM Session Replay by clicking on the X in the top right corner. Note the length of the span, this is the time it took to complete the order, not good! Scroll down the page and you will see the Tags metadata (which is used in Tag Spotlight). After the tags, we come to the waterfall which shows the page objects that have been loaded (HTML, CSS, images, JavaScript etc.) Keep scrolling down the page until you come to a blue APM link (the one with /cart/checkout at the end of the URL) and hover over it. This brings up the APM Performance Summary. Having this end-to-end (RUM to APM) view is very useful when troubleshooting issues.\nExercise You will see paymentservice and checkoutservice are in an error state as per the screenshot above. Under Workflow Name click on front-end:/cart/checkout, this will bring up the APM Service Map.",
    "description": "Exercise Close the RUM Session Replay by clicking on the X in the top right corner. Note the length of the span, this is the time it took to complete the order, not good! Scroll down the page and you will see the Tags metadata (which is used in Tag Spotlight). After the tags, we come to the waterfall which shows the page objects that have been loaded (HTML, CSS, images, JavaScript etc.) Keep scrolling down the page until you come to a blue APM link (the one with /cart/checkout at the end of the URL) and hover over it.",
    "tags": [],
    "title": "4. User Sessions",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/5-rum/4-user-sessions/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 4. Building Resilience",
    "content": "In this exercise, we’ll test how the OpenTelemetry Collector recovers from a network outage by restarting the Gateway collector. When the gateway becomes available again, the agent will resume sending data from its last checkpointed state, ensuring no data loss.\nExercise Restart the Gateway: In the Gateway terminal window run:\n​ Gateway ../otelcol --config=gateway.yaml Restart the Agent: In the Agent terminal window run:\n​ Start the Agent ../otelcol --config=agent.yaml After the agent is up and running, the File_Storage extension will detect buffered data in the checkpoint folder.\nIt will start to dequeue the stored spans from the last checkpoint folder, ensuring no data is lost.\nExercise Verify the Agent Debug output\nNote that the Agent Debug Screen does NOT change and still shows the following line indicating no new data is being exported.\n2025-02-07T13:40:12.195+0100 info service@v0.120.0/service.go:253 Everything is ready. Begin running and processing data. Watch the Gateway Debug output\nYou should see from the gateway debug screen, it has started receiving the previously missed traces without requiring any additional action on your part.\n2025-02-07T12:44:32.651+0100 info service@v0.120.0/service.go:253 Everything is ready. Begin running and processing data. 2025-02-07T12:47:46.721+0100 info Traces {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\", \"resource spans\": 4, \"spans\": 4} 2025-02-07T12:47:46.721+0100 info ResourceSpans #0 Resource SchemaURL: https://opentelemetry.io/schemas/1.6.1 Resource attributes: Check the gateway-traces.out file\nUsing jq, count the number of traces in the recreated gateway-traces.out. It should match the number you send when the gateway was down.\n​ Check Gateway Traces Out File Example output jq '.resourceSpans | length | \"\\(.) resourceSpans found\"' gateway-traces.out \"5 resourceSpans found\" Important Stop the agent and the gateway processes by pressing Ctrl-C in their respective terminals.\nConclusion This exercise demonstrated how to enhance the resilience of the OpenTelemetry Collector by configuring the file_storage extension, enabling retry mechanisms for the otlp exporter, and using a file-backed queue for temporary data storage.\nBy implementing file-based checkpointing and queue persistence, you ensure the telemetry pipeline can gracefully recover from temporary interruptions, making it a more robust and reliable for production environments.",
    "description": "In this exercise, we’ll test how the OpenTelemetry Collector recovers from a network outage by restarting the Gateway collector. When the gateway becomes available again, the agent will resume sending data from its last checkpointed state, ensuring no data loss.\nExercise Restart the Gateway: In the Gateway terminal window run:\n​ Gateway ../otelcol --config=gateway.yaml Restart the Agent: In the Agent terminal window run:",
    "tags": [],
    "title": "4.4 Recovery",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/4-building-resilience/4-4-recovery/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 6. Service",
    "content": "Attributes Processor Also in the Processors section of this workshop, we added the attributes/conf processor so that the collector will insert a new attribute called participant.name to all the metrics. We now need to enable this under the metrics pipeline.\nUpdate the processors section to include attributes/conf under the metrics pipeline:\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch, resourcedetection/system, resourcedetection/ec2, attributes/conf] exporters: [debug]",
    "description": "Attributes Processor Also in the Processors section of this workshop, we added the attributes/conf processor so that the collector will insert a new attribute called participant.name to all the metrics. We now need to enable this under the metrics pipeline.\nUpdate the processors section to include attributes/conf under the metrics pipeline:\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch, resourcedetection/system, resourcedetection/ec2, attributes/conf] exporters: [debug]",
    "tags": [],
    "title": "OpenTelemetry Collector Service",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/6-service/4-attributes/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences",
    "content": "Go to Dashboards and find the End User Experiences dashboard group.\nClick the three dots on the top right to open the dashboard menu, and select Save As, and include your team name and initials in the dashboard name.\nSave to the dashboard group that matches your email address. Now you have your own copy of this dashboard to customize!",
    "description": "Go to Dashboards and find the End User Experiences dashboard group.\nClick the three dots on the top right to open the dashboard menu, and select Save As, and include your team name and initials in the dashboard name.\nSave to the dashboard group that matches your email address. Now you have your own copy of this dashboard to customize!",
    "tags": [],
    "title": "Frontend Dashboards",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/4-dashboards/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources",
    "content": "Introduction When deploying OpenTelemetry in a large organization, it’s critical to define a standardized naming convention for tagging, and a governance process to ensure the convention is adhered to.\nThis ensures that MELT data collected via OpenTelemetry can be efficiently utilized for alerting, dashboarding, and troubleshooting purposes. It also ensures that users of Splunk Observability Cloud can quickly find the data they’re looking for.\nNaming conventions also ensure that data can be aggregated effectively. For example, if we wanted to count the number of unique hosts by environment, then we must use a standardized convention for capturing the host and environment names.\nAttributes vs. Tags Before we go further, let’s make a note regarding terminology. Tags in OpenTelemetry are called “attributes”. Attributes can be attached to metrics, logs, and traces, either via manual instrumentation or automated instrumentation.\nAttributes can also be attached to metrics, logs, and traces at the OpenTelemetry collector level, using various processors such as the Resource Detection processor.\nOnce traces with attributes are ingested into Splunk Observability Cloud, they are available as “tags”. Optionally, attributes collected as part of traces can be used to create Troubleshooting Metric Sets, which can in turn be used with various features such as Tag Spotlight.\nAlternatively, attributes can be used to create Monitoring Metric Sets, which can be used to drive alerting.\nResource Semantic Conventions OpenTelemetry resource semantic conventions should be used as a starting point when determining which attributes an organization should standardize on. In the following sections, we’ll review some of the more commonly used attributes.\nService Attributes A number of attributes are used to describe the service being monitored.\nservice.name is a required attribute that defines the logical name of the service. It’s added automatically by the OpenTelemetry SDK but can be customized. It’s best to keep this simple (i.e. inventoryservice would be better than inventoryservice-prod-hostxyz, as other attributes can be utilized to capture other aspects of the service instead).\nThe following service attributes are recommended:\nservice.namespace this attribute could be utilized to identify the team that owns the service service.instance.id is used to identify a unique instance of the service service.version is used to identify the version of the service Telemetry SDK These attributes are set automatically by the SDK, to record information about the instrumentation libraries being used:\ntelemetry.sdk.name is typically set to opentelemetry telemetry.sdk.language is the language of the SDK, such as java telemetry.sdk.version identifies which version of the SDK is utilized Containers For services running in containers, there are numerous attributes used to describe the container runtime, such as container.id, container.name, and container.image.name. The full list can be found here.\nHosts These attributes describe the host where the service is running, and include attributes such as host.id, host.name, and host.arch. The full list can be found here.\nDeployment Environment The deployment.environment attribute is used to identify the environment where the service is deployed, such as staging or production.\nSplunk Observability Cloud uses this attribute to enable related content (as described here), so it’s important to include it.\nCloud There are also attributes to capture information for services running in public cloud environments, such as AWS. Attributes include cloud.provider, cloud.account.id, and cloud.region.\nThe full list of attributes can be found here.\nSome cloud providers, such as GCP, define semantic conventions specific to their offering.\nKubernetes There are a number of standardized attributes for applications running in Kubernetes as well. Many of these are added automatically by Splunk’s distribution of the OpenTelemetry collector, as described here.\nThese attributes include k8s.cluster.name, k8s.node.name, k8s.pod.name, k8s.namespace.name, and kubernetes.workload.name.\nBest Practices for Creating Custom Attributes Many organizations require attributes that go beyond what’s defined in OpenTelemetry’s resource semantic conventions.\nIn this case, it’s important to avoid naming conflicts with attribute names already included in the semantic conventions. So it’s a good idea to check the semantic conventions before deciding on a particular attribute name for your naming convention.\nIn addition to a naming convention for attribute names, you also need to consider attribute values. For example, if you’d like to capture the particular business unit with which an application belongs, then you’ll also want to have a standardized list of business unit values to choose from, to facilitate effective filtering.\nThe OpenTelemetry community provides guidelines that should be followed when naming attributes as well, which can be found here.\nThe Recommendations for Application Developers section is most relevant to our discussion.\nThey recommend:\nPrefixing the attribute name with your company’s domain name, e.g. com.acme.shopname (if the attribute may be used outside your company as well as inside). Prefixing the attribute name with the application name, if the attribute is unique to a particular application and is only used within your organization. Not using existing OpenTelemetry semantic convention names as a prefix for your attribute name. Consider submitting a proposal to add your attribute name to the OpenTelemetry specification, if there’s a general need for it across different organizations and industries. Avoid having attribute names start with otel.*, as this is reserved for OpenTelemetry specification usage. Metric Cardinality Considerations One final thing to keep in mind when deciding on naming standards for attribute names and values is related to metric cardinality.\nMetric cardinality is defined as **the number of unique metric time series (MTS) produced by a combination of metric names and their associated dimensions.\nA metric has high cardinality when it has a high number of dimension keys and a high number of possible unique values for those dimension keys.\nFor example, suppose your application sends in data for a metric named custom.metric. In the absence of any attributes, custom.metric would generate a single metric time series (MTS).\nOn the other hand, if custom.metric includes an attribute named customer.id, and there are thousands of customer ID values, this would generate thousands of metric time series, which may impact costs and query performance.\nSplunk Observability Cloud provides a report that allows for the management of metrics usage. And rules can be created to drop undesirable dimensions. However, the first line of defence is understanding how attribute name and value combinations can drive increased metric cardinality.\nSummary In this document, we highlighted the importance of defining naming conventions for OpenTelemetry tags, preferably before starting a large rollout of OpenTelemetry instrumentation.\nWe discussed how OpenTelemetry’s resource semantic conventions define the naming conventions for several attributes, many of which are automatically collected via the OpenTelemetry SDKs, as well as processors that run within the OpenTelemetry collector.\nFinally, we shared some best practices for creating your attribute names, for situations where the resource semantic conventions are not sufficient for your organization’s needs.",
    "description": "When deploying OpenTelemetry in a large organization, it’s critical to define a standardized naming convention for tagging, and a governance process to ensure the convention is adhered to.",
    "tags": [],
    "title": "Naming Conventions for Tagging with OpenTelemetry and Splunk",
    "uri": "/observability-workshop/v6.5/en/resources/otel-tagging/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios",
    "content": "How can we use Splunk Observability to get insight into end user experience, and proactively test scenarios to improve that experience?\nSections:\nSet up basic Synthetic tests to understand availability and performance ASAP Uptime test API test Single page Browser test Explore RUM to understand our real users Write advanced Synthetics tests based on what we’ve learned about our users and what we need them to do Customize dashboard charts to capture our KPIs, show trends, and show data in context of our events Create Detectors to alert on our KPIs Tip Keep in mind throughout the workshop: how can I prioritize activities strategically to get the fastest time to value for my end users and for myself/ my developers? Context As a reminder, we need frontend performance monitoring to capture everything that goes into our end user experience. If we’re just monitoring the backend, we’re missing all of the other resources that are critical to our users’ success. Read What the Fastly Outage Can Teach Us About Observability for a real world example. Click the image below to zoom in. References Throughout this workshop, we will see references to resources to help further understand end user experience and how to optimize it. In addition to Splunk Docs for supported features and Lantern for tips and tricks, Google’s web.dev and Mozilla are great resources.\nRemember that the specific libraries, platforms, and CDNs you use often also have their own specific resources. For example React, Wordpress, and Cloudflare all have their own tips to improve performance.",
    "description": "Use Splunk Real User Monitoring (RUM) and Synthetics to get insight into end user experience, and proactively test scenarios to improve that experience.",
    "tags": [],
    "title": "Optimize End User Experiences",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "Dimension, Properties and TagsOne conversation that frequently comes up is Dimensions vs Properties and when you should use one verus the other.\nOpenTelemetry TaggingWhen deploying OpenTelemetry in a large organization, it’s critical to define a standardized naming convention for tagging, and a governance process to ensure the convention is adhered to.\nSPLUNK ARCADE - PLAY. LEARN. OBSERVE!SPLUNK ARCADE - Where Retro Gaming Meets Real-World Observability\nLocal HostingResources for setting up a locally hosted workshop environment.",
    "description": "Resources for learning about Splunk Observability Cloud",
    "tags": [],
    "title": "Resources",
    "uri": "/observability-workshop/v6.5/en/resources/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources",
    "content": "Unlock the Power of Telemetry Through Play Imagine stepping into an 80s-inspired arcade, where every high score, every power-up, and every epic fail isn’t just fun—it’s real data feeding into Splunk! Welcome to Splunk Arcade, an interactive, gamified learning experience that transforms classic arcade action into a hands-on observability workshop.\nHere’s the deal: You play. Splunk observes. Every move you make in these custom-built HTML games generates metrics, traces, and logs—just like your real-world applications. The difference? Instead of debugging a dull enterprise system, you’re blasting aliens and hunting bugs in a neon-lit, pixel-perfect throwback to the golden age of gaming.\n🎮 GAMES THAT TEACH AS YOU PLAY 🛸 SPACE IMVADERS – The Observability Invasion (Inspired by Space Invaders)\nMission: Defend Earth from waves of alien bugs!\nObservability Lesson: Every laser blast, every hit, and every lost life generates real-time telemetry data—error rates, and request latency. Watch as your Splunk dashboards light up with performance insights, showing you exactly how “system health” translates into gameplay. But the real alien bugs are the ones in your own code! Can you and Splunk save the Earth (and your job!) from destruction?!\n🐸 LOGGER – Hop Through the Data Stream (Inspired by Frogger)\nMission: Navigate a chaotic river of scrolling logs, avoiding errors and latency spikes!\nObservability Lesson: As you leap from log to log, Splunk captures end-to-end traces, helping you visualize the user journey—just like tracking a real application’s flow. Miss a jump? That’s a failed transaction. Make it across? That’s a successful trace. Help your team of frogs across and see your customers win!\n🔫 BUGHUNT – Shoot Down Glitches Before They Crash Your System (Inspired by Duck Hunt)\nMission: Take aim at flying bugs—but these aren’t just pixels; they’re real application errors in disguise!\nObservability Lesson: Every shot fires a custom event log into Splunk. Missed bugs? That’s an outage waiting to happen. Perfect accuracy? You’ve just optimized your system’s resilience. Use Splunk to shoot down problems before they escape to your customers!\nWHY SPLUNK ARCADE? Observability doesn’t have to be dry and technical. Splunk Arcade makes learning fun, competitive, and unforgettable by:\n✅ Bridging Gaming \u0026 Real-World Telemetry – See how your in-game actions mirror real IT system behavior.\n✅ Teaching Through Experience – No slides, no lectures—just play, learn, and observe.\n✅ Providing Instant Feedback – Watch dashboards update in real-time as you play, making abstract concepts tangible and engaging.\nWHO IS THIS FOR? ✔ DevOps \u0026 SREs – Test your skills in a stress-free, nostalgic environment.\n✔ New Splunk Users – Learn observability fundamentals without drowning in complexity or code.\n✔ Gamers \u0026 Tech Enthusiasts – Who says enterprise software can’t be fun?\nREADY TO PLAY? Scan the QR code below to launch Splunk Arcade and start sending real telemetry data through gameplay.\n(Insert QR code linking to Splunk Arcade here)\n🔌 POWERED BY SPLUNK OBSERVABILITY CLOUD\nBecause even fun should be monitored, measured, and optimized.\nGame Over? Not a chance! 🕹️ Press START to level up your observability skills!",
    "description": "SPLUNK ARCADE - Where Retro Gaming Meets Real-World Observability",
    "tags": [],
    "title": "SPLUNK ARCADE - PLAY. LEARN. OBSERVE!",
    "uri": "/observability-workshop/v6.5/en/resources/splunk-arcade/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "Proactively monitor the performance of your web app before problems affect your users. With Splunk Synthetic Monitoring, technical and business teams create detailed tests to proactively monitor the speed and reliability of websites, web apps, and resources over time, at any stage in the development cycle.\nSplunk Synthetic Monitoring offers the most comprehensive and in-depth capabilities for uptime and web performance optimization as part of the only complete observability suite, Splunk Observability Cloud.\nEasily set up monitoring for APIs, service endpoints and end-user experience. With Splunk Synthetic Monitoring, go beyond basic uptime and performance monitoring and focus on proactively finding and fixing issues, optimizing web performance, and ensuring customers get the best user experience.\nWith Splunk Synthetic Monitoring you can:\nDetect and resolve issues fast across critical user flows, business transactions and API endpoints Prevent web performance issues from affecting customers with an intelligent web optimization engine And improve the performance of all page resources and third-party dependencies",
    "description": "Proactively find and fix performance issues across user flows, business transactions and APIs to deliver better digital experiences.",
    "tags": [],
    "title": "Splunk Synthetic Scripting",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 1. Real Browser Test",
    "content": "Click on Advanced, these settings are optional and can be used to further configure the test.\nNote In the case of this workshop, we will not be using any of these settings as this is for informational purposes only.\nSecurity: TLS/SSL validation: When activated, this feature is used to enforce the validation of expired, invalid hostname, or untrusted issuer on SSL/TLS certificates. Authentication: Add credentials to authenticate with sites that require additional security protocols, for example from within a corporate network. By using concealed global variables in the Authentication field, you create an additional layer of security for your credentials and simplify the ability to share credentials across checks. Custom Content: Custom headers: Specify custom headers to send with each request. For example, you can add a header in your request to filter out requests from analytics on the back end by sending a specific header in the requests. You can also use custom headers to set cookies. Cookies: Set cookies in the browser before the test starts. For example, to prevent a popup modal from randomly appearing and interfering with your test, you can set cookies. Any cookies that are set will apply to the domain of the starting URL of the check. Splunk Synthetics Monitoring uses the public suffix list to determine the domain. Host overrides: Add host override rules to reroute requests from one host to another. For example, you can create a host override to test an existing production site against page resources loaded from a development site or a specific CDN edge node. Next, we will edit the test steps to provide more meaningful names for each step.",
    "description": "Click on Advanced, these settings are optional and can be used to further configure the test.\nNote In the case of this workshop, we will not be using any of these settings as this is for informational purposes only.\nSecurity: TLS/SSL validation: When activated, this feature is used to enforce the validation of expired, invalid hostname, or untrusted issuer on SSL/TLS certificates. Authentication: Add credentials to authenticate with sites that require additional security protocols, for example from within a corporate network. By using concealed global variables in the Authentication field, you create an additional layer of security for your credentials and simplify the ability to share credentials across checks. Custom Content: Custom headers: Specify custom headers to send with each request. For example, you can add a header in your request to filter out requests from analytics on the back end by sending a specific header in the requests. You can also use custom headers to set cookies. Cookies: Set cookies in the browser before the test starts. For example, to prevent a popup modal from randomly appearing and interfering with your test, you can set cookies. Any cookies that are set will apply to the domain of the starting URL of the check. Splunk Synthetics Monitoring uses the public suffix list to determine the domain. Host overrides: Add host override rules to reroute requests from one host to another. For example, you can create a host override to test an existing production site against page resources loaded from a development site or a specific CDN edge node. Next, we will edit the test steps to provide more meaningful names for each step.",
    "tags": [],
    "title": "1.5 Advanced Settings",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/1-real-browser-test/5-advanced-settings/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 2. API Test",
    "content": "View results Wait for a few minutes for the test to provision and run. Once you see the test has run successfully, click on the run to view the test results:\n6. Resources How to Create an API Test\nAPI Test Overview",
    "description": "View results Wait for a few minutes for the test to provision and run. Once you see the test has run successfully, click on the run to view the test results:\n6. Resources How to Create an API Test\nAPI Test Overview",
    "tags": [],
    "title": "View results",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/2-api-test/5-view-results/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration \u003e 2. Gateway Setup",
    "content": "Tip Introduction to the otlphttp Exporter The otlphttp exporter is now the default method for sending metrics and traces to Splunk Observability Cloud. This exporter provides a standardized and efficient way to transmit telemetry data using the OpenTelemetry Protocol (OTLP) over HTTP.\nWhen deploying the Splunk Distribution of the OpenTelemetry Collector in host monitoring (agent) mode, the otlphttp exporter is included by default. This replaces older exporters such as sapm and signalfx, which are gradually being phased out.\nConfiguring Splunk Access Tokens To authenticate and send data to Splunk Observability Cloud, you need to configure access tokens properly. In OpenTelemetry, authentication is handled via HTTP headers. To pass an access token, use the headers: key with the sub-key X-SF-Token:. This configuration works in both agent and gateway mode.\nExample:\nexporters: otlphttp: endpoint: \"https://ingest.\u003crealm\u003e.signalfx.com\" headers: X-SF-Token: \"your-access-token\" Pass-Through Mode If you need to forward headers through the pipeline, enable pass-through mode by setting include_metadata: to true in the OTLP receiver configuration. This ensures that any authentication headers received by the collector are retained and forwarded along with the data.\nExample:\nreceivers: otlp: protocols: http: include_metadata: true This is particularly useful in gateway mode, where data from multiple agents may pass through a centralized gateway before being sent to Splunk.\nUnderstanding Batch Processing The Batch Processor is a key component in optimizing data transmission efficiency. It groups traces, metrics, and logs into batches before sending them to the backend. Batching improves performance by:\nReducing the number of outgoing requests. Improving compression efficiency. Lowering network overhead. Configuring the Batch Processor To enable batching, configure the batch: section and include the X-SF-Token: key. This ensures that data is grouped correctly before being sent to Splunk Observability Cloud.\nExample:\nprocessors: batch: metadata_keys: [X-SF-Token] # Array of metadata keys to batch send_batch_size: 100 timeout: 5s Best Practices for Batch Processing For optimal performance, it is recommended to use the Batch Processor in every collector deployment. The best placement for the Batch Processor is after the memory limiter and sampling processors. This ensures that only necessary data is batched, avoiding unnecessary processing of dropped data.\nGateway Configuration with Batch Processor When deploying a gateway, ensure that the Batch Processor is included in the pipeline:\nservice: pipelines: traces: processors: [memory_limiter, tail_sampling, batch] Conclusion The otlphttp exporter is now the preferred method for sending telemetry data to Splunk Observability Cloud. Properly configuring Splunk Access Tokens ensures secure data transmission, while the Batch Processor helps optimize performance by reducing network overhead. By implementing these best practices, you can efficiently collect and transmit observability data at scale.",
    "description": "Tip Introduction to the otlphttp Exporter The otlphttp exporter is now the default method for sending metrics and traces to Splunk Observability Cloud. This exporter provides a standardized and efficient way to transmit telemetry data using the OpenTelemetry Protocol (OTLP) over HTTP.\nWhen deploying the Splunk Distribution of the OpenTelemetry Collector in host monitoring (agent) mode, the otlphttp exporter is included by default. This replaces older exporters such as sapm and signalfx, which are gradually being phased out.",
    "tags": [],
    "title": "2.5 Addendum - Info on Access Tokens and Batch Processing",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/2-gateway/2-5-addendum/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Synthetics \u003e 2. API Test",
    "content": "Wait for a few minutes for the test to provision and run. Once you see the test has run successfully, click on the run to view the results:\nResources How to Create an API Test API Test Overview",
    "description": "Wait for a few minutes for the test to provision and run. Once you see the test has run successfully, click on the run to view the results:\nResources How to Create an API Test API Test Overview",
    "tags": [],
    "title": "View results",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/1-synthetics/2-api-test/5-view-results/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "In this section, we will explore how to use the Filter Processor to selectively drop spans based on certain conditions.\nSpecifically, we will drop traces based on the span name, which is commonly used to filter out unwanted spans such as health checks or internal communication traces. In this case, we will be filtering out spans that contain \"/_healthz\", typically associated with health check requests and usually are quite “noisy”.\nExercise Important Change ALL terminal windows to the 3-dropping-spans directory and run the clear command.\nCopy *.yaml from the 2-building-resilience directory into 3-dropping-spans. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Next, we will configure the filter processor and the respective pipelines.",
    "description": "In this section, we will explore how to use the Filter Processor to selectively drop spans based on certain conditions.\nSpecifically, we will drop traces based on the span name, which is commonly used to filter out unwanted spans such as health checks or internal communication traces. In this case, we will be filtering out spans that contain \"/_healthz\", typically associated with health check requests and usually are quite “noisy”.\nExercise Important Change ALL terminal windows to the 3-dropping-spans directory and run the clear command.",
    "tags": [],
    "title": "3. Dropping Spans",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/3-dropping-spans/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "In this section, we will explore how to use the Filter Processor to selectively drop spans based on certain conditions.\nSpecifically, we will drop traces based on the span name, which is commonly used to filter out unwanted spans such as health checks or internal communication traces. In this case, we will be filtering out spans that contain \"/_healthz\", typically associated with health check requests and usually are quite “noisy”.\nExercise Important Change ALL terminal windows to the 3-dropping-spans directory and run the clear command.\nCopy *.yaml from the 2-building-resilience directory into 3-dropping-spans. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Next, we will configure the filter processor and the respective pipelines.",
    "description": "In this section, we will explore how to use the Filter Processor to selectively drop spans based on certain conditions.\nSpecifically, we will drop traces based on the span name, which is commonly used to filter out unwanted spans such as health checks or internal communication traces. In this case, we will be filtering out spans that contain \"/_healthz\", typically associated with health check requests and usually are quite “noisy”.\nExercise Important Change ALL terminal windows to the 3-dropping-spans directory and run the clear command.",
    "tags": [],
    "title": "3. Dropping Spans",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/3-dropping-spans/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "The FileLog Receiver in the OpenTelemetry Collector is used to ingest logs from files.\nIt monitors specified files for new log entries and streams those logs into the Collector for further processing or exporting. It is also useful for testing and development purposes.\nFor this part of the workshop, the loadgen will generate logs using random quotes:\nlotrQuotes := []string{ \"One does not simply walk into Mordor.\", \"Even the smallest person can change the course of the future.\", \"All we have to decide is what to do with the time that is given us.\", \"There is some good in this world, and it's worth fighting for.\", } starWarsQuotes := []string{ \"Do or do not, there is no try.\", \"The Force will be with you. Always.\", \"I find your lack of faith disturbing.\", \"In my experience, there is no such thing as luck.\", } The FileLog receiver in the agent collector will read these log lines and send them to the gateway.\nExercise In the Logs terminal window, change into the [WORKSHOP] directory and create a new subdirectory named 3-filelog. Next, copy *.yaml from 2-gateway into 3-filelog. Important Change ALL terminal windows to the [WORKSHOP]/3-filelog directory.\nStart the loadgen and this will begin writing lines to a file named quotes.log:\n​ Log Load Generator Log Load Generator Output ../loadgen -logs Writing logs to quotes.log. Press Ctrl+C to stop. ​ Updated Directory Structure . ├── agent.yaml ├── gateway.yaml └── quotes.yaml",
    "description": "The FileLog Receiver in the OpenTelemetry Collector is used to ingest logs from files.\nIt monitors specified files for new log entries and streams those logs into the Collector for further processing or exporting. It is also useful for testing and development purposes.\nFor this part of the workshop, the loadgen will generate logs using random quotes:\nlotrQuotes := []string{ \"One does not simply walk into Mordor.\", \"Even the smallest person can change the course of the future.\", \"All we have to decide is what to do with the time that is given us.\", \"There is some good in this world, and it's worth fighting for.\", } starWarsQuotes := []string{ \"Do or do not, there is no try.\", \"The Force will be with you. Always.\", \"I find your lack of faith disturbing.\", \"In my experience, there is no such thing as luck.\", } The FileLog receiver in the agent collector will read these log lines and send them to the gateway.",
    "tags": [],
    "title": "3. FileLog Setup",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/3-filelog/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Ingest Processor for Observability Cloud \u003e 3. Create an Ingest Pipeline",
    "content": "Now that an Ingest Pipeline has been configured to convert Kubernetes Audit Logs into metrics and send them to Splunk Observability Cloud the metrics should be available. To confirm the metrics are being collected complete the following steps:\nExercise: Confirm Metrics in Splunk Observability Cloud 1. Login to the Splunk Observability Cloud organization you were invited for the workshop. In the upper-right corner, click the + Icon → Chart to create a new chart.\n2. In the Plot Editor of the newly created chart enter the metric name you used while configuring the Ingest Pipeline.\nInfo You should see the metric you created in the Ingest Pipeline. Keep this tab open as it will be used again in the next section.\nIn the next step you will update the ingest pipeline to add dimensions to the metric, so you have additional context for alerting and troubleshooting.",
    "description": "Now that an Ingest Pipeline has been configured to convert Kubernetes Audit Logs into metrics and send them to Splunk Observability Cloud the metrics should be available. To confirm the metrics are being collected complete the following steps:\nExercise: Confirm Metrics in Splunk Observability Cloud 1. Login to the Splunk Observability Cloud organization you were invited for the workshop. In the upper-right corner, click the + Icon → Chart to create a new chart.",
    "tags": [],
    "title": "Confirm Metrics in Observability Cloud",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/3-create-an-ingest-pipeline/4-confirm-metrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 3. Dashboards",
    "content": "1. Save to existing dashboard Check that you have YOUR_NAME-Dashboard: YOUR_NAME-Dashboard in the top left corner. This means you chart will be saved in this Dashboard.\nName the Chart Latency History and add a Chart Description if you wish.\nClick on Save And Close. This returns you to your dashboard that now has two charts!\nNow let’s quickly add another Chart based on the previous one.\n2. Copy and Paste a chart Click on the three dots ... on the Latency History chart in your dashboard and then on Copy.\nYou see the chart being copied, and you should now have a red circle with a white 1 next to the + on the top left of the page.\nClick on the plus icon the top of the page, and then in the menu on Paste Charts (There should also be a red dot with a 1 visible at the end of the line).\nThis will place a copy of the previous chart in your dashboard.\n3. Edit the pasted chart Click on the three dots ... on one of the Latency History charts in your dashboard and then on Open (or you can click on the name of the chart which here is Latency History).\nThis will bring you to the editor environment again.\nFirst set the time for the chart to -1 hour in the Time box at the top right of the chart. Then to make this a different chart, click on the eye icon in front of signal “A” to make it visible again, and then hide signal “C” via the eye icon and change the name for Latency history to Latency vs Load.\nClick on the Add Metric Or Event button. This will bring up the box for a new signal. Type and select demo.trans.count for Signal D.\nThis will add a new Signal D to your chart, It shows the number of active requests. Add the filter for the demo_datacenter:Paris, then change the Rollup type by clicking on the Configure Plot button and changing the roll-up from Auto (Delta) to Rate/sec. Change the name from demo.trans.count to Latency vs Load.\nFinally press the Save And Close button. This returns you to your dashboard that now has three different charts!\nLet’s add an “instruction” note and arrange the charts!",
    "description": "1. Save to existing dashboard Check that you have YOUR_NAME-Dashboard: YOUR_NAME-Dashboard in the top left corner. This means you chart will be saved in this Dashboard.\nName the Chart Latency History and add a Chart Description if you wish.\nClick on Save And Close. This returns you to your dashboard that now has two charts!\nNow let’s quickly add another Chart based on the previous one.",
    "tags": [],
    "title": "Adding charts to dashboards",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/dashboards/adding-charts/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "In this section, we will enable automatic discovery and configuration for the Java services running in Kubernetes. This means that the OpenTelemetry Collector will look for Pod annotations that indicate that the Java application should be instrumented with the Splunk OpenTelemetry Java agent. This will allow us to get traces, spans, and profiling data from Java services running on the cluster.\nautomatic discovery and configuration It is important to understand that automatic discovery and configuration is designed to get trace, span \u0026 profiling data out of your application without requiring code changes or recompilation.\nThis is a great way to get started with APM, but it is not a replacement for manual instrumentation. Manual instrumentation allows you to add custom spans, tags, and logs to your application, which can provide more context and detail to your traces.\nFor Java applications, the OpenTelemetry Collector will look for the annotation instrumentation.opentelemetry.io/inject-java.\nThe annotation can have the value set to true or to the namespace/daemonset of the OpenTelemetry Collector e.g. default/splunk-otel-collector. This allows working across namespaces and is what we will use in this workshop.\nUsing the deployment.yaml If you want your Pods to send traces automatically, you can add the annotation to the deployment.yaml as shown below. This will add the instrumentation library during the initial deployment. To speed things up we have done that for the following Pods:\nadmin-server config-server discovery-server apiVersion: apps/v1 kind: Deployment metadata: name: admin-server labels: app.kubernetes.io/part-of: spring-petclinic spec: selector: matchLabels: app: admin-server template: metadata: labels: app: admin-server annotations: instrumentation.opentelemetry.io/inject-java: \"default/splunk-otel-collector\"",
    "description": "In this section, we will enable automatic discovery and configuration for the Java services running in Kubernetes. This means that the OpenTelemetry Collector will look for Pod annotations that indicate that the Java application should be instrumented with the Splunk OpenTelemetry Java agent. This will allow us to get traces, spans, and profiling data from Java services running on the cluster.\nautomatic discovery and configuration It is important to understand that automatic discovery and configuration is designed to get trace, span \u0026 profiling data out of your application without requiring code changes or recompilation.",
    "tags": [],
    "title": "Setting up automatic discovery and configuration for APM",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/4-apm/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Profiling Workshop",
    "content": "Now that our game startup slowness has been resolved, let’s play several rounds of the Door Game and ensure the rest of the game performs quickly.\nAs you play the game, do you notice any other slowness? Let’s look at the data in Splunk Observability Cloud to put some numbers on what we’re seeing.\nReview Game Performance in Splunk Observability Cloud Navigate to APM then click on Traces on the right-hand side of the screen. Sort the traces by Duration in descending order:\nWe can see that a few of the traces with an operation of GET /game/:uid/picked/:picked/outcome have a duration of just over five seconds. This explains why we’re still seeing some slowness when we play the app (note that the slowness is no longer on the game startup operation, GET /new-game, but rather a different operation used while actually playing the game).\nLet’s click on one of the slow traces and take a closer look. Since profiling is still enabled, call stacks have been captured as part of this trace. Click on the child span in the waterfall view, then click CPU Stack Traces:\nAt the bottom of the call stack, we can see that the thread was busy sleeping:\ncom.splunk.profiling.workshop.ServiceMain$$Lambda$.handle(Unknown Source:0) com.splunk.profiling.workshop.ServiceMain.lambda$main$(ServiceMain.java:34) com.splunk.profiling.workshop.DoorGame.getOutcome(DoorGame.java:41) com.splunk.profiling.workshop.DoorChecker.isWinner(DoorChecker.java:14) com.splunk.profiling.workshop.DoorChecker.checkDoorTwo(DoorChecker.java:30) com.splunk.profiling.workshop.DoorChecker.precheck(DoorChecker.java:36) com.splunk.profiling.workshop.Util.sleep(Util.java:9) java.util.concurrent.TimeUnit.sleep(Unknown Source:0) java.lang.Thread.sleep(Unknown Source:0) java.lang.Thread.sleep(Native Method:0) The call stack tells us a story – reading from the bottom up, it lets us describe what is happening inside the service code. A developer, even one unfamiliar with the source code, should be able to look at this call stack to craft a narrative like:\nWe are getting the outcome of a game. We leverage the DoorChecker to see if something is the winner, but the check for door two somehow issues a precheck() that, for some reason, is deciding to sleep for a long time.\nOur workshop application is left intentionally simple – a real-world service might see the thread being sampled during a database call or calling into an un-traced external service. It is also possible that a slow span is executing a complicated business process, in which case maybe none of the stack traces relate to each other at all.\nThe longer a method or process is, the greater chance we will have call stacks sampled during its execution.\nLet’s Fix That Bug By using the profiling tool, we were able to determine that our application is slow when issuing the DoorChecker.precheck() method from inside DoorChecker.checkDoorTwo(). Let’s open the doorgame/src/main/java/com/splunk/profiling/workshop/DoorChecker.java source file in our editor.\nBy quickly glancing through the file, we see that there are methods for checking each door, and all of them call precheck(). In a real service, we might be uncomfortable simply removing the precheck() call because there could be unseen/unaccounted side effects.\nDown on line 29 we see the following:\nprivate boolean checkDoorTwo(GameInfo gameInfo) { precheck(2); return gameInfo.isWinner(2); } private void precheck(int doorNum) { long extra = (int)Math.pow(70, doorNum); sleep(300 + extra); } With our developer hat on, we notice that the door number is zero based, so the first door is 0, the second is 1, and the 3rd is 2 (this is conventional). The extra value is used as extra/additional sleep time, and it is computed by taking 70^doorNum (Math.pow performs an exponent calculation). That’s odd, because this means:\ndoor 0 =\u003e 70^0 =\u003e 1ms door 1 =\u003e 70^1 =\u003e 70ms door 2 =\u003e 70^2 =\u003e 4900ms We’ve found the root cause of our slow bug! This also explains why the first two doors weren’t ever very slow.\nWe have a quick chat with our product manager and team lead, and we agree that the precheck() method must stay but that the extra padding isn’t required. Let’s remove the extra variable and make precheck now read like this:\nprivate void precheck(int doorNum) { sleep(300); } Now all doors will have a consistent behavior. Save your work and then rebuild and redeploy the application using the following command:\ncd workshop/profiling ./5-redeploy-doorgame.sh Once the application has been redeployed successfully, visit The Door Game again to confirm that your fix is in place: http://\u003cyour IP address\u003e:81\nWhat did we accomplish? We found another performance issue with our application that impacts game play. We used the CPU call stacks included in the trace to understand application behavior. We learned how the call stack can tell us a story and point us to suspect lines of code. We identified the slow code and fixed it to make it faster.",
    "description": "Now that our game startup slowness has been resolved, let’s play several rounds of the Door Game and ensure the rest of the game performs quickly.\nAs you play the game, do you notice any other slowness? Let’s look at the data in Splunk Observability Cloud to put some numbers on what we’re seeing.\nReview Game Performance in Splunk Observability Cloud Navigate to APM then click on Traces on the right-hand side of the screen. Sort the traces by Duration in descending order:",
    "tags": [],
    "title": "Fix In Game Slowness",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/profiling/5-fix-ingame-slowness/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Advanced Synthetics",
    "content": "Click on Advanced, these settings are optional and can be used to further configure the test.\nNote In the case of this workshop, we will not be using any of these settings; this is for informational purposes only.\nSecurity: TLS/SSL validation: When activated, this feature is used to enforce the validation of expired, invalid hostname, or untrusted issuer on SSL/TLS certificates. Authentication: Add credentials to authenticate with sites that require additional security protocols, for example from within a corporate network. By using concealed global variables in the Authentication field, you create an additional layer of security for your credentials and simplify the ability to share credentials across checks. Custom Content: Custom headers: Specify custom headers to send with each request. For example, you can add a header in your request to filter out requests from analytics on the back end by sending a specific header in the requests. You can also use custom headers to set cookies. Cookies: Set cookies in the browser before the test starts. For example, to prevent a popup modal from randomly appearing and interfering with your test, you can set cookies. Any cookies that are set will apply to the domain of the starting URL of the check. Splunk Synthetics Monitoring uses the public suffix list to determine the domain. Host overrides: Add host override rules to reroute requests from one host to another. For example, you can create a host override to test an existing production site against page resources loaded from a development site or a specific CDN edge node. See the Advanced Settings for Browser Tests section of the Docs for more information.\nNext, we will edit the test steps to provide more meaningful names for each step.",
    "description": "Click on Advanced, these settings are optional and can be used to further configure the test.\nNote In the case of this workshop, we will not be using any of these settings; this is for informational purposes only.\nSecurity: TLS/SSL validation: When activated, this feature is used to enforce the validation of expired, invalid hostname, or untrusted issuer on SSL/TLS certificates. Authentication: Add credentials to authenticate with sites that require additional security protocols, for example from within a corporate network. By using concealed global variables in the Authentication field, you create an additional layer of security for your credentials and simplify the ability to share credentials across checks. Custom Content: Custom headers: Specify custom headers to send with each request. For example, you can add a header in your request to filter out requests from analytics on the back end by sending a specific header in the requests. You can also use custom headers to set cookies. Cookies: Set cookies in the browser before the test starts. For example, to prevent a popup modal from randomly appearing and interfering with your test, you can set cookies. Any cookies that are set will apply to the domain of the starting URL of the check. Splunk Synthetics Monitoring uses the public suffix list to determine the domain. Host overrides: Add host override rules to reroute requests from one host to another. For example, you can create a host override to test an existing production site against page resources loaded from a development site or a specific CDN edge node. See the Advanced Settings for Browser Tests section of the Docs for more information.",
    "tags": [],
    "title": "Advanced Test Settings",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/3-advanced-synthetics/5-advanced-settings/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e RUM",
    "content": "See RUM Metrics and Session information in the RUM UI See correlated APM traces in the RUM \u0026 APM UI 1. RUM Overview Pages From your RUM Application Summary Dashboard you can see detailed information by opening the Application Overview Page via the tripple dot menu on the right by selecting Open Application Overview or by clicking the link with your application name which is jmcj-rum-app in the example below.\nThis will take you to the RUM Application Overview Page screen as shown below.\n2. RUM Browser Overview 2.1. Header The RUM UI consists of five major sections. The first is the selection header, where you can set/filter a number of options:\nA drop down for the time window you’re reviewing (You are looking at the past 15 minutes in this case) A drop down to select the Comparison window (You are comparing current performance on a rolling window - in this case compared to 1 hour ago) A drop down with the available Environments to view A drop down list with the Various Web apps Optionally a drop down to select Browser or Mobile metrics (Might not be available in your workshop) 2.2. UX Metrics By default, RUM prioritizes the metrics that most directly reflect the experience of the end user.\nAdditional Tags All of the dashboard charts allow us to compare trends over time, create detectors, and click through to further diagnose issues.\nFirst, we see page load and route change information, which can help us understand if something unexpected is impacting user traffic trends.\nNext, Google has defined Core Web Vitals to quantify the user experience as measured by loading, interactivity, and visual stability. Splunk RUM builds in Google’s thresholds into the UI, so you can easily see if your metrics are in an acceptable range.\nLargest Contentful Paint (LCP), measures loading performance. How long does it take for the largest block of content in the viewport to load? To provide a good user experience, LCP should occur within 2.5 seconds of when the page first starts loading. First Input Delay (FID), measures interactivity. How long does it take to be able to interact with the app? To provide a good user experience, pages should have a FID of 100 milliseconds or less. Cumulative Layout Shift (CLS), measures visual stability. How much does the content move around after the initial load? To provide a good user experience, pages should maintain a CLS of 0.1. or less. Improving Web Vitals is a key component to optimizing your end user experience, so being able to quickly understand them and create detectors if they exceed a threshold is critical.\nGoogle has some great resources if you want to learn more, for example the business impact of Core Web Vitals.\n2.3. Front-end health Common causes of frontend issues are javascript errors and long tasks, which can especially affect interactivity. Creating detectors on these indicators helps us investigate interactivity issues sooner than our users report it, allowing us to build workarounds or roll back related releases faster if needed. Learn more about optimizing long tasks for better end user experience!\n2.4. Back-end health Common back-end issues affecting user experience are network issues and resource requests. In this example, we clearly see a spike in Time To First Byte that lines up with a resource request spike, so we already have a good starting place to investigate.\nTime To First Byte (TTFB), measures how long it takes for a client’s browser to receive the first byte of the response from the server. The longer it takes for the server to process the request and send a response, the slower your visitors’ browser is at displaying your page.",
    "description": "See RUM Metrics and Session information in the RUM UI See correlated APM traces in the RUM \u0026 APM UI 1. RUM Overview Pages From your RUM Application Summary Dashboard you can see detailed information by opening the Application Overview Page via the tripple dot menu on the right by selecting Open Application Overview or by clicking the link with your application name which is jmcj-rum-app in the example below.",
    "tags": [],
    "title": "Analyzing RUM Metrics",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/2-rum/5-analyzing-metrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "See RUM Metrics and Session information in the RUM UI See correlated APM traces in the RUM \u0026 APM UI 1. RUM Overview Pages From your RUM Application Summary Dashboard you can see detailed information by opening the Application Overview Page via the tripple dot menu on the right by selecting Open Application Overview or by clicking the link with your application name which is jmcj-store in the example below.\nThis will take you to the RUM Application Overview Page screen as shown below.\n2. RUM Browser Overview 2.1. Header The RUM UI consists of 6 major sections. The first is the selection header, where you can set/filter a number of options:\nA drop down for the time window you’re reviewing (You are looking at the past hour in this case) A drop down to select the Comparison window (You are comparing current performance on a rolling window - in this case compared to 1 hour ago) A drop down with the available Environments to view: (Choose the one provided by the workshop host or All like in the example) A drop down list with the Various Web apps (You can use the one provided by the workshop host or use All) Optionally a drop down to select Browser or Mobile metrics (Might not be available in your workshop) 2.2. Overview Pane The Overview Panes, down the left hand side of the page, give you a summary of the pages which have increased load times.\nIn the example here you can see that the checkout and cart pages have errors due to the yellow triangles, and you can see that the load time has increased by 2.38 to 5.50 seconds.\nYou also see an overview of the number of Front end Error and Backend Errors per minute, and we appear to have three JavaScript errors on our site.\nThe last two panes show you the Top Page Views and the Top Network Requests.\n2.3. Key Metrics Pane The Key Metrics View is the location where you will find the metrics for the number of JavaScript Errors per second, Network Errors per second an the Backend/Resource Request Duration. These Metrics are very useful to guide you to the location of an issue if you are experiencing problems with your site.\n2.4. Web Vitals Pane The Web Vitals view is the location where you go if you wish to get insight into the experience you are delivering to your End users based on Web Vitals metrics. Web Vitals is an initiative by Google to provide unified guidance for quality signals that are essential to delivering a great user experience on the web and focuses on three key parameters:\nLargest Contentful Paint (LCP), measures loading performance. To provide a good user experience, LCP should occur within 2.5 seconds of when the page first starts loading. First Input Delay (FID), measures interactivity. To provide a good user experience, pages should have a FID of 100 milliseconds or less. Cumulative Layout Shift (CLS), measures visual stability. To provide a good user experience, pages should maintain a CLS of 0.1. or less. 2.5. Other Metrics Pane The Other Metrics Pane is the location where you find other performance metrics, with a focus on initial load time of your page or tasks that are taking too long to complete.\nTime To First Byte (TTFB), measures how long it takes for a client’s browser to receive the first byte of the response from the server. The longer it takes for the server to process the request and send a response, the slower your visitors’ browser is at displaying your page. Long Task Duration, a performance metric that can be used help developers to understand the bad user experience on the website, or can be an indication of a problem. Long Task Count, a metric to indicate how often a long task occurs, again used for exploring user experiences or problem detection. 2.6. Custom Event Pane The Custom Event View is the location where you will find the metrics for any event you may have added yourself to the web pages you are monitoring.\nAs we have seen in the RUM enabled website, we have added the following two lines:\nconst Provider = SplunkRum.provider; var tracer=Provider.getTracer('appModuleLoader'); These lines will automatically create custom Events for every new Page, and you can also add these to pieces of custom code that are not part of a framework or an event you created so you can better understand the flow though your application. We support Custom Event Requests, Custom Event Error Rates and Custom Event Latency metrics.",
    "description": "See RUM Metrics and Session information in the RUM UI See correlated APM traces in the RUM \u0026 APM UI 1. RUM Overview Pages From your RUM Application Summary Dashboard you can see detailed information by opening the Application Overview Page via the tripple dot menu on the right by selecting Open Application Overview or by clicking the link with your application name which is jmcj-store in the example below.",
    "tags": [],
    "title": "5. Analyzing RUM Metrics",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/5-analyzing-metrics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 4. Splunk APM",
    "content": "As Splunk APM provides a NoSample end-to-end visibility of every service Splunk APM captures every trace. For this workshop, the wire transfer orderId is available as a tag. This means that we can use this to search for the exact trace of the poor user experience encountered by users.\nTrace Analyzer Splunk Observability Cloud provides several tools for exploring application monitoring data. Trace Analyzer is suited to scenarios where you have high-cardinality, high-granularity searches and explorations to research unknown or new issues.\nExercise With the outer box of the wire-transfer-service selected, in the right-hand pane, click on Traces. Set Time Range to Last 15 minutes. Ensure the Sample Ratio is set to 1:1 and not 1:10. The Trace \u0026 error count view shows the total traces and traces with errors in a stacked bar chart. You can use your mouse to select a specific period within the available time frame.\nExercise Click on the dropdown menu that says Trace \u0026 error count, and change it to Trace duration The Trace Duration view shows a heatmap of traces by duration. The heatmap represents 3 dimensions of data:\nTime on the x-axis Trace duration on the y-axis The traces (or requests) per second are represented by the heatmap shades You can use your mouse to select an area on the heatmap, to focus on a specific time period and trace duration range.\nExercise Switch from Trace duration back to Trace \u0026 Error count. In the time picker select Last 1 hour. Note, that most of our traces have errors (red) and there are only a limited amount of traces that are error-free (blue). Make sure the Sample Ratio is set to 1:1 and not 1:10. Click on Add filters, type in orderId and select orderId from the list. Find and select the orderId provided by your workshop leader and hit enter. We have now filtered down to the exact trace where users reported a poor experience with a very long processing wait.\nA secondary benefit to viewing this trace is that the trace will be accessible for up to 13 months. This will allow developers to come back to this issue at a later stage and still view this trace for example.\nExercise Click on the trace in the list. Next, we will walk through the trace waterfall.",
    "description": "As Splunk APM provides a NoSample end-to-end visibility of every service Splunk APM captures every trace. For this workshop, the wire transfer orderId is available as a tag. This means that we can use this to search for the exact trace of the poor user experience encountered by users.\nTrace Analyzer Splunk Observability Cloud provides several tools for exploring application monitoring data. Trace Analyzer is suited to scenarios where you have high-cardinality, high-granularity searches and explorations to research unknown or new issues.",
    "tags": [],
    "title": "5. APM Trace Analyzer",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/4-apm/5-apm-trace-analyzer/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6. Splunk APM",
    "content": "As Splunk APM provides a NoSample end-to-end visibility of every service Splunk APM captures every trace. For this workshop, the Order Confirmation ID is available as a tag. This means that we can use this to search for the exact trace of the poor user experience you encountered earlier in the workshop.\nTrace Analyzer Splunk Observability Cloud provides several tools for exploring application monitoring data. Trace Analyzer is suited to scenarios where you have high-cardinality, high-granularity searches and explorations to research unknown or new issues.\nExercise With the outer box of the paymentservice selected, in the right-hand pane, click on Traces. Set Time Range to Last 15 minutes. Ensure the Sample Ratio is set to 1:1 and not 1:10. The Trace \u0026 error count view shows the total traces and traces with errors in a stacked bar chart. You can use your mouse to select a specific period within the available time frame.\nExercise Click on the dropdown menu that says Trace \u0026 error count, and change it to Trace duration The Trace Duration view shows a heatmap of traces by duration. The heatmap represents 3 dimensions of data:\nTime on the x-axis Trace duration on the y-axis The traces (or requests) per second are represented by the heatmap shades You can use your mouse to select an area on the heatmap, to focus on a specific time period and trace duration range.\nExercise Switch from Trace duration back to Trace \u0026 Error count. In the time picker select Last 1 hour. Note, that most of our traces have errors (red) and there are only a limited amount of traces that are error-free (blue). Make sure the Sample Ratio is set to 1:1 and not 1:10. Click on Add filters, type in orderId and select orderId from the list. Paste in your Order Confirmation ID from when you went shopping earlier in the workshop and hit enter. If you didn’t capture one, please ask your instructor for one. We have now filtered down to the exact trace where you encountered a poor user experience with a very long checkout wait.\nA secondary benefit to viewing this trace is that the trace will be accessible for up to 13 months. This will allow developers to come back to this issue at a later stage and still view this trace for example.\nExercise Click on the trace in the list. Next, we will walk through the trace waterfall.",
    "description": "As Splunk APM provides a NoSample end-to-end visibility of every service Splunk APM captures every trace. For this workshop, the Order Confirmation ID is available as a tag. This means that we can use this to search for the exact trace of the poor user experience you encountered earlier in the workshop.\nTrace Analyzer Splunk Observability Cloud provides several tools for exploring application monitoring data. Trace Analyzer is suited to scenarios where you have high-cardinality, high-granularity searches and explorations to research unknown or new issues.",
    "tags": [],
    "title": "5. APM Trace Analyzer",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/6-apm/5-apm-trace-analyzer/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e NodeJS Zero-Config Workshop",
    "content": "1. AlwaysOn Profiling for the Payment Service AlwaysOn Profiling is a feature of the Splunk Distribution of OpenTelemetry Collector that allows you to collect CPU and Memory profiling data for your services without having to modify your code. This is useful for troubleshooting performance issues in your services. Here are some of the benefits of AlwaysOn Profiling:\nPerform continuous profiling of your applications. The profiler is always on once you activate it. Collect code performance context and link it to trace data. Explore memory usage and garbage collection of your application. Analyze code bottlenecks that impact service performance. Identify inefficiencies that increase the need for scaling up cloud resources. With the opentelemetry-demo-paymentservice selected, click on AlwaysOn Profiling to view the code profiling data for the service.\nHere you can see the CPU and Memory profiling data for the paymentservice service. You can also see the CPU and Memory profiling data for the frontend service by selecting the opentelemetry-demofrontend service from the Service dropdown.",
    "description": "1. AlwaysOn Profiling for the Payment Service AlwaysOn Profiling is a feature of the Splunk Distribution of OpenTelemetry Collector that allows you to collect CPU and Memory profiling data for your services without having to modify your code. This is useful for troubleshooting performance issues in your services. Here are some of the benefits of AlwaysOn Profiling:\nPerform continuous profiling of your applications. The profiler is always on once you activate it. Collect code performance context and link it to trace data. Explore memory usage and garbage collection of your application. Analyze code bottlenecks that impact service performance. Identify inefficiencies that increase the need for scaling up cloud resources. With the opentelemetry-demo-paymentservice selected, click on AlwaysOn Profiling to view the code profiling data for the service.",
    "tags": [],
    "title": "Code Profiling - Payment Service",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/3-nodejs-kubernetes/5-profiling/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Now let’s play the role of the developer As a developer we must debug the function products:ProductResource.getAllProducts to find the problem.\nDebugging 101, the Line by Line method Without anything to go on other than “BAD FUNCTION”, a Developer must then look at code visually line by line to find and fix the problem. To make this worse, functions call other functions, and it can get very messy in bad code scenarios.\nWe will do the visual inspection mehtod next.\nUsing Nano: nano products/src/main/java/com/shabushabu/javashop/products/resources/ProductResource.java Search in Nano: [CTRL]-w Enter in: getAllProducts [Enter] You will be taken here: @GET public Response getAllProducts(@DefaultValue(\"California\") @QueryParam(\"location\") String location) { // STEP X: All we know right now is somewhere in this function, latency was introduced. myCoolFunction1(location); myCoolFunction2(location); myCoolFunction10(location); myCoolFunction13(location); myCoolFunction5(location); myCoolFunction6(location); We can see here in getAllProducts, the first call is to myCoolFunction1(), so as may have guessed our next step is to go look at myCoolFunction1().\nSearch in Nano: [CTRL]-w Enter in: myCoolFunction1 [Enter] Find the next occurrence: [CTRL]-w [Enter] Keep repeating [CTRL]-w [Enter] until you get to the actual function definition It looks like this:\nprivate void myCoolFunction1(String location) { // Generate a FAST sleep of 0 time ! int sleepy = lookupLocation1(location); try{ Thread.sleep(sleepy); } catch (Exception e){ } } Now, myCoolFunction1 calls lookupLocation1(location)\nSearch in Nano: [CTRL]-w Enter in: lookupLocation1 [Enter] I think you get the picture by now, you have no choice but to inspect every line of code and every function called and visually inspect them for problems. This can be a VERY long process and kills our customers Mean Time to Repair. This happens quite often to our customers with our competition beacsue they can’t provide all the traces 100% of the time and most can’t scale to add more data, via Custom Attributes on top of that!\nRemember, without Full Fidelty, you have to either reproduce errors / latency in another environment or inspect code line by line.\nSo they are stuck where we are, quite often.\nOK, enough fun. Let’s make this easier for our developer, and show off some Splunk APM Scale!\nExit your editor: Exit nano: [CTRL]-X Optional: If it asks you to save, hit N",
    "description": "Now let’s play the role of the developer As a developer we must debug the function products:ProductResource.getAllProducts to find the problem.\nDebugging 101, the Line by Line method Without anything to go on other than “BAD FUNCTION”, a Developer must then look at code visually line by line to find and fix the problem. To make this worse, functions call other functions, and it can get very messy in bad code scenarios.",
    "tags": [],
    "title": "Debugging 101",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/5-debugging101/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Horizontal Pod Autoscaling",
    "content": "Now let’s apply some load against the php-apache pod. To do this, you will need to start a different Pod to act as a client. The container within the client Pod runs in an infinite loop, sending HTTP GETs to the php-apache service.\n1. Review loadgen YAML Inspect the YAML file ~/workshop/k3s/loadgen.yaml and validate the contents using the following command:\ncat ~/workshop/k3s/loadgen.yaml This file contains the configuration for the load generator and will create a new ReplicaSet with two replicas of the load generator image.\napiVersion: apps/v1 kind: ReplicaSet metadata: name: loadgen labels: app: loadgen spec: replicas: 2 selector: matchLabels: app: loadgen template: metadata: name: loadgen labels: app: loadgen spec: containers: - name: infinite-calls image: busybox command: - /bin/sh - -c - \"while true; do wget -q -O- http://php-apache-svc.apache.svc.cluster.local; done\" 2. Create a new namespace kubectl create namespace loadgen 3. Deploy the loadgen YAML kubectl apply -f ~/workshop/k3s/loadgen.yaml --namespace loadgen Once you have deployed the load generator, you can see the Pods running in the loadgen namespace. Use previous similar commands to check the status of the Pods from the command line.\nWorkshop Question Which metrics in the Apache Navigator have now significantly increased?\n4. Scale the load generator A ReplicaSet is a process that runs multiple instances of a Pod and keeps the specified number of Pods constant. Its purpose is to maintain the specified number of Pod instances running in a cluster at any given time to prevent users from losing access to their application when a Pod fails or is inaccessible.\nReplicaSet helps bring up a new instance of a Pod when the existing one fails, scale it up when the running instances are not up to the specified number, and scale down or delete Pods if another instance with the same label is created. A ReplicaSet ensures that a specified number of Pod replicas are running continuously and helps with load-balancing in case of an increase in resource usage.\nLet’s scale our ReplicaSet to 4 replicas using the following command:\nkubectl scale replicaset/loadgen --replicas 4 -n loadgen Validate the replicas are running from both the command line and Splunk Observability Cloud:\nkubectl get replicaset loadgen -n loadgen Workshop Question What impact can you see in the Apache Navigator?\nLet the load generator run for around 2-3 minutes and keep observing the metrics in the Kubernetes Navigator and the Apache Navigator.",
    "description": "Now let’s apply some load against the php-apache pod. To do this, you will need to start a different Pod to act as a client. The container within the client Pod runs in an infinite loop, sending HTTP GETs to the php-apache service.\n1. Review loadgen YAML Inspect the YAML file ~/workshop/k3s/loadgen.yaml and validate the contents using the following command:\ncat ~/workshop/k3s/loadgen.yaml This file contains the configuration for the load generator and will create a new ReplicaSet with two replicas of the load generator image.",
    "tags": [],
    "title": "Deploy Load Generator",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/2-hpa/5-deploy-loadgen/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "Later on in this workshop, we’re going to deploy our .NET application into a Kubernetes cluster.\nBut how do we do that?\nThe first step is to create a Docker image for our application. This is known as “dockerizing” and application, and the process begins with the creation of a Dockerfile.\nBut first, let’s define some key terms.\nKey Terms What is Docker? “Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security lets you run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you don’t need to rely on what’s installed on the host.”\nSource: https://docs.docker.com/get-started/docker-overview/\nWhat is a container? “Containers are isolated processes for each of your app’s components. Each component …runs in its own isolated environment, completely isolated from everything else on your machine.”\nSource: https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-container/\nWhat is a container image? “A container image is a standardized package that includes all of the files, binaries, libraries, and configurations to run a container.”\nDockerfile “A Dockerfile is a text-based document that’s used to create a container image. It provides instructions to the image builder on the commands to run, files to copy, startup command, and more.”\nCreate a Dockerfile Let’s create a file named Dockerfile in the /home/splunk/workshop/docker-k8s-otel/helloworld directory.\ncd /home/splunk/workshop/docker-k8s-otel/helloworld You can use vi or nano to create the file. We will show an example using vi:\nvi Dockerfile Copy and paste the following content into the newly opened file:\nPress ‘i’ to enter into insert mode in vi before pasting the text below.\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base USER app WORKDIR /app EXPOSE 8080 FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build ARG BUILD_CONFIGURATION=Release WORKDIR /src COPY [\"helloworld.csproj\", \"helloworld/\"] RUN dotnet restore \"./helloworld/./helloworld.csproj\" WORKDIR \"/src/helloworld\" COPY . . RUN dotnet build \"./helloworld.csproj\" -c $BUILD_CONFIGURATION -o /app/build FROM build AS publish ARG BUILD_CONFIGURATION=Release RUN dotnet publish \"./helloworld.csproj\" -c $BUILD_CONFIGURATION -o /app/publish /p:UseAppHost=false FROM base AS final WORKDIR /app COPY --from=publish /app/publish . ENTRYPOINT [\"dotnet\", \"helloworld.dll\"] To save your changes in vi, press the esc key to enter command mode, then type :wq! followed by pressing the enter/return key.\nWhat does all this mean? Let’s break it down.\nWalking through the Dockerfile We’ve used a multi-stage Dockerfile for this example, which separates the Docker image creation process into the following stages:\nBase Build Publish Final While a multi-stage approach is more complex, it allows us to create a lighter-weight runtime image for deployment. We’ll explain the purpose of each of these stages below.\nThe Base Stage The base stage defines the user that will be running the app, the working directory, and exposes the port that will be used to access the app. It’s based off of Microsoft’s mcr.microsoft.com/dotnet/aspnet:8.0 image:\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base USER app WORKDIR /app EXPOSE 8080 Note that the mcr.microsoft.com/dotnet/aspnet:8.0 image includes the .NET runtime only, rather than the SDK, so is relatively lightweight. It’s based off of the Debian 12 Linux distribution. You can find more information about the ASP.NET Core Runtime Docker images in GitHub.\nThe Build Stage The next stage of the Dockerfile is the build stage. For this stage, the mcr.microsoft.com/dotnet/sdk:8.0 image is used, which is also based off of Debian 12 but includes the full .NET SDK rather than just the runtime.\nThis stage copies the .csproj file to the build image, and then uses dotnet restore to download any dependencies used by the application.\nIt then copies the application code to the build image and uses dotnet build to build the project and its dependencies into a set of .dll binaries:\nFROM mcr.microsoft.com/dotnet/sdk:8.0 AS build ARG BUILD_CONFIGURATION=Release WORKDIR /src COPY [\"helloworld.csproj\", \"helloworld/\"] RUN dotnet restore \"./helloworld/./helloworld.csproj\" WORKDIR \"/src/helloworld\" COPY . . RUN dotnet build \"./helloworld.csproj\" -c $BUILD_CONFIGURATION -o /app/build The Publish Stage The third stage is publish, which is based on build stage image rather than a Microsoft image. In this stage, dotnet publish is used to package the application and its dependencies for deployment:\nFROM build AS publish ARG BUILD_CONFIGURATION=Release RUN dotnet publish \"./helloworld.csproj\" -c $BUILD_CONFIGURATION -o /app/publish /p:UseAppHost=false The Final Stage The fourth stage is our final stage, which is based on the base stage image (which is lighter-weight than the build and publish stages). It copies the output from the publish stage image and defines the entry point for our application:\nFROM base AS final WORKDIR /app COPY --from=publish /app/publish . ENTRYPOINT [\"dotnet\", \"helloworld.dll\"] Build a Docker Image Now that we have the Dockerfile, we can use it to build a Docker image containing our application:\n​ Script Example Output docker build -t helloworld:1.0 . DEPRECATED: The legacy builder is deprecated and will be removed in a future release. Install the buildx component to build images with BuildKit: https://docs.docker.com/go/buildx/ Sending build context to Docker daemon 281.1kB Step 1/19 : FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base 8.0: Pulling from dotnet/aspnet af302e5c37e9: Pull complete 91ab5e0aabf0: Pull complete 1c1e4530721e: Pull complete 1f39ca6dcc3a: Pull complete ea20083aa801: Pull complete 64c242a4f561: Pull complete Digest: sha256:587c1dd115e4d6707ff656d30ace5da9f49cec48e627a40bbe5d5b249adc3549 Status: Downloaded newer image for mcr.microsoft.com/dotnet/aspnet:8.0 ---\u003e 0ee5d7ddbc3b Step 2/19 : USER app etc, This tells Docker to build an image using a tag of helloworld:1.0 using the Dockerfile in the current directory.\nWe can confirm it was created successfully with the following command:\n​ Script Example Output docker images REPOSITORY TAG IMAGE ID CREATED SIZE helloworld 1.0 db19077b9445 20 seconds ago 217MB Test the Docker Image Before proceeding, ensure the application we started before is no longer running on your instance.\nWe can run our application using the Docker image as follows:\ndocker run --name helloworld \\ --detach \\ --expose 8080 \\ --network=host \\ helloworld:1.0 Note: we’ve included the --network=host parameter to ensure our Docker container is able to access resources on our instance, which is important later on when we need our application to send data to the collector running on localhost.\nLet’s ensure that our Docker container is running:\n​ Script Example Output docker ps $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5f5b9cd56ac5 helloworld:1.0 \"dotnet helloworld.d…\" 2 mins ago Up 2 mins helloworld We can access our application as before:\n​ Script Example Output curl http://localhost:8080/hello/Docker Hello, Docker! Congratulations, if you’ve made it this far, you’ve successfully Dockerized a .NET application.",
    "description": "Later on in this workshop, we’re going to deploy our .NET application into a Kubernetes cluster.\nBut how do we do that?\nThe first step is to create a Docker image for our application. This is known as “dockerizing” and application, and the process begins with the creation of a Dockerfile.\nBut first, let’s define some key terms.\nKey Terms What is Docker? “Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security lets you run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you don’t need to rely on what’s installed on the host.”",
    "tags": [],
    "title": "Dockerize the Application",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/5-dockerize-app/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts",
    "content": "An exporter, which can be push or pull-based, is how you send data to one or more backends/destinations. Exporters may support one or more data sources.\nFor this workshop, we will be using the otlphttp exporter. The OpenTelemetry Protocol (OTLP) is a vendor-neutral, standardised protocol for transmitting telemetry data. The OTLP exporter sends data to a server that implements the OTLP protocol. The OTLP exporter supports both gRPC and HTTP/JSON protocols.\n%%{ init:{ \"theme\":\"base\", \"themeVariables\": { \"primaryColor\": \"#ffffff\", \"clusterBkg\": \"#eff2fb\", \"defaultLinkColor\": \"#333333\" } } }%% flowchart LR; style Exporters fill:#e20082,stroke:#333,stroke-width:4px,color:#fff subgraph Collector A[OTLP] --\u003e M(Receivers) B[JAEGER] --\u003e M(Receivers) C[Prometheus] --\u003e M(Receivers) end subgraph Processors M(Receivers) --\u003e H(Filters, Attributes, etc) E(Extensions) end subgraph Exporters H(Filters, Attributes, etc) --\u003e S(OTLP) H(Filters, Attributes, etc) --\u003e T(JAEGER) H(Filters, Attributes, etc) --\u003e U(Prometheus) end",
    "description": "An exporter, which can be push or pull-based, is how you send data to one or more backends/destinations. Exporters may support one or more data sources.\nFor this workshop, we will be using the otlphttp exporter. The OpenTelemetry Protocol (OTLP) is a vendor-neutral, standardised protocol for transmitting telemetry data. The OTLP exporter sends data to a server that implements the OTLP protocol. The OTLP exporter supports both gRPC and HTTP/JSON protocols.\n%%{ init:{ \"theme\":\"base\", \"themeVariables\": { \"primaryColor\": \"#ffffff\", \"clusterBkg\": \"#eff2fb\", \"defaultLinkColor\": \"#333333\" } } }%% flowchart LR; style Exporters fill:#e20082,stroke:#333,stroke-width:4px,color:#fff subgraph Collector A[OTLP] --\u003e M(Receivers) B[JAEGER] --\u003e M(Receivers) C[Prometheus] --\u003e M(Receivers) end subgraph Processors M(Receivers) --\u003e H(Filters, Attributes, etc) E(Extensions) end subgraph Exporters H(Filters, Attributes, etc) --\u003e S(OTLP) H(Filters, Attributes, etc) --\u003e T(JAEGER) H(Filters, Attributes, etc) --\u003e U(Prometheus) end",
    "tags": [],
    "title": "OpenTelemetry Collector Exporters",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/5-exporters/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop",
    "content": "Index Tags To use advanced features in Splunk Observability Cloud such as Tag Spotlight, we’ll need to first index one or more tags.\nTo do this, navigate to Settings -\u003e APM MetricSets. Then click the + New MetricSet button.\nLet’s index the credit.score.category tag by entering the following details (note: since everyone in the workshop is using the same organization, the instructor will do this step on your behalf):\nClick Start Analysis to proceed.\nThe tag will appear in the list of Pending MetricSets while analysis is performed.\nOnce analysis is complete, click on the checkmark in the Actions column.\nHow to choose tags for indexing Why did we choose to index the credit.score.category tag and not the others?\nTo understand this, let’s review the primary use cases for tags:\nFiltering Grouping Filtering With the filtering use case, we can use the Trace Analyzer capability of Splunk Observability Cloud to filter on traces that match a particular tag value.\nWe saw an example of this earlier, when we filtered on traces where the credit score started with 7.\nOr if a customer calls in to complain about slow service, we could use Trace Analyzer to locate all traces with that particular customer number.\nTags used for filtering use cases are generally high-cardinality, meaning that there could be thousands or even hundreds of thousands of unique values. In fact, Splunk Observability Cloud can handle an effectively infinite number of unique tag values! Filtering using these tags allows us to rapidly locate the traces of interest.\nNote that we aren’t required to index tags to use them for filtering with Trace Analyzer.\nGrouping With the grouping use case, we can use Trace Analyzer to group traces by a particular tag.\nBut we can also go beyond this and surface trends for tags that we collect using the powerful Tag Spotlight feature in Splunk Observability Cloud, which we’ll see in action shortly.\nTags used for grouping use cases should be low to medium-cardinality, with hundreds of unique values.\nFor custom tags to be used with Tag Spotlight, they first need to be indexed.\nWe decided to index the credit.score.category tag because it has a few distinct values that would be useful for grouping. In contrast, the customer number and credit score tags have hundreds or thousands of unique values, and are more valuable for filtering use cases rather than grouping.\nTroubleshooting vs. Monitoring MetricSets You may have noticed that, to index this tag, we created something called a Troubleshooting MetricSet. It’s named this way because a Troubleshooting MetricSet, or TMS, allows us to troubleshoot issues with this tag using features such as Tag Spotlight.\nYou may have also noticed that there’s another option which we didn’t choose called a Monitoring MetricSet (or MMS). Monitoring MetricSets go beyond troubleshooting and allow us to use tags for alerting and dashboards. We’ll explore this concept later in the workshop.",
    "description": "Index Tags To use advanced features in Splunk Observability Cloud such as Tag Spotlight, we’ll need to first index one or more tags.\nTo do this, navigate to Settings -\u003e APM MetricSets. Then click the + New MetricSet button.\nLet’s index the credit.score.category tag by entering the following details (note: since everyone in the workshop is using the same organization, the instructor will do this step on your behalf):",
    "tags": [],
    "title": "Index Tags",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/5-index-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM",
    "content": "Use Terraform1 to manage Observability Cloud Dashboards and Detectors Initialize the Terraform Splunk Provider2. Run Terraform to create detectors and dashboards from code using the Splunk Terraform Provider. See how Terraform can also delete detectors and dashboards. 1. Initial setup Monitoring as code adopts the same approach as infrastructure as code. You can manage monitoring the same way you do applications, servers, or other infrastructure components.\nYou can use monitoring as code to build out your visualizations, what to monitor, and when to alert, among other things. This means your monitoring setup, processes, and rules can be versioned, shared, and reused.\nFull documentation for the Splunk Terraform Provider is available here.\nRemaining in your AWS/EC2 instance, change into the o11y-cloud-jumpstart directory\n​ Change directory cd ~/observability-content-contrib/integration-examples/terraform-jumpstart Initialize Terraform and upgrade to the latest version of the Splunk Terraform Provider.\nNote: Upgrading the SignalFx Terraform Provider You will need to run the command below each time a new version of the Splunk Terraform Provider is released. You can track the releases on GitHub.\n​ Initialise Terraform Initialise Output terraform init -upgrade Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kafka in modules/kafka - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - rum_and_synthetics_dashboard in modules/dashboards/rum_and_synthetics - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Finding latest version of splunk-terraform/signalfx... - Installing splunk-terraform/signalfx v6.20.0... - Installed splunk-terraform/signalfx v6.20.0 (self-signed, key ID CE97B6074989F138) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. 2. Create execution plan The terraform plan command creates an execution plan. By default, creating a plan consists of:\nReading the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date. Comparing the current configuration to the prior state and noting any differences. Proposing a set of change actions that should, if applied, make the remote objects match the configuration. The plan command alone will not actually carry out the proposed changes, and so you can use this command to check whether the proposed changes match what you expected before you apply the changes\n​ Execution Plan Execution Plan Output terraform plan -var=\"api_token=$API_TOKEN\" -var=\"realm=$REALM\" -var=\"o11y_prefix=[$INSTANCE]\" Plan: 146 to add, 0 to change, 0 to destroy. If the plan executes successfully, we can go ahead and apply:\n3. Apply execution plan The terraform apply command executes the actions proposed in the Terraform plan above.\nThe most straightforward way to use terraform apply is to run it without any arguments at all, in which case it will automatically create a new execution plan (as if you had run terraform plan) and then prompt you to provide the API Token, Realm (the prefix defaults to Splunk) and approve the plan, before taking the indicated actions.\nDue to this being a workshop it is required that the prefix is to be unique so you need to run the terraform apply below.\n​ Apply Plan Apply Plan Output terraform apply -var=\"api_token=$API_TOKEN\" -var=\"realm=$REALM\" -var=\"o11y_prefix=[$INSTANCE]\" Apply complete! Resources: 146 added, 0 changed, 0 destroyed. Once the apply has been completed, validate that the detectors were created, under the Alerts \u0026 Detectors and click on the Detectors tab. They will be prefixed by the instance name. To check the prefix value run:\necho $INSTANCE You will see a list of the new detectors and you can search for the prefix that was output from above.\n3. Destroy all your hard work The terraform destroy command is a convenient way to destroy all remote objects managed by your Terraform configuration.\nWhile you will typically not want to destroy long-lived objects in a production environment, Terraform is sometimes used to manage ephemeral infrastructure for development purposes, in which case you can use terraform destroy to conveniently clean up all of those temporary objects once you are finished with your work.\nNow go and destroy all the Detectors and Dashboards that were previously applied!\n​ Destroy Destroy Output terraform destroy -var=\"api_token=$API_TOKEN\" -var=\"realm=$REALM\" Destroy complete! Resources: 146 destroyed. Validate all the detectors have been removed by navigating to Alerts → Detectors\nTerraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. ↩︎\nA provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. Alibaba Cloud, AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. Splunk, Terraform Cloud, DNSimple, Cloudflare). ↩︎",
    "description": "Use Terraform1 to manage Observability Cloud Dashboards and Detectors Initialize the Terraform Splunk Provider2. Run Terraform to create detectors and dashboards from code using the Splunk Terraform Provider. See how Terraform can also delete detectors and dashboards. 1. Initial setup Monitoring as code adopts the same approach as infrastructure as code. You can manage monitoring the same way you do applications, servers, or other infrastructure components.\nYou can use monitoring as code to build out your visualizations, what to monitor, and when to alert, among other things. This means your monitoring setup, processes, and rules can be versioned, shared, and reused.",
    "tags": [],
    "title": "Monitoring as Code",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/monitoring-as-code/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Lambda Tracing and Kinesis",
    "content": "Re-deploy your Lambdas While remaining in your manual directory, run the following commandd to re-deploy your Lambda Functions:\n​ Deploy Producer Code Expected Output sls deploy -f producer Deploying function producer to stage dev (us-east-1) ✔ Function code deployed (6s) Configuration did not change. Configuration update skipped. (6s) ​ Deploy Consumer Code Expected Output sls deploy -f consumer Deploying function consumer to stage dev (us-east-1) ✔ Function code deployed (6s) Configuration did not change. Configuration update skipped. (6s) Note that this deployment now only updates the code changes within the function. Our configuration remains the same.\nCheck the details of your serverless functions:\n​ Command sls info You endpoint value should remain the same:\nSend some Traffic again Use the curl command to send a payload to your producer function. Note the command option -d is followed by your message payload.\nTry changing the value of name to your name and telling the Lambda function about your superpower. Replace YOUR_ENDPOINT with the endpoint from your previous step.\n​ Command curl -d '{ \"name\": \"CHANGE_ME\", \"superpower\": \"CHANGE_ME\" }' YOUR_ENDPOINT For example:\ncurl -d '{ \"name\": \"Kate\", \"superpower\": \"Distributed Tracing\" }' https://xvq043lj45.execute-api.us-east-1.amazonaws.com/dev/producer You should see the following output if your message is successful:\n{\"message\":\"Message placed in the Event Stream: hostname-eventSteam\"} If unsuccessful, you will see:\n{\"message\": \"Internal server error\"} If this occurs, ask one of the lab facilitators for assistance.\nIf you see a success message, generate more load: re-send that messate 5+ times. You should keep seeing a success message after each send.\nCheck the lambda logs output:\n​ Producer Function Logs sls logs -f producer ​ Consumer Function Logs sls logs -f consumer Examine the logs carefully.\nWorkshop Question Do you notice the difference?\nNote that we are logging our Record together with the Trace context that we have added to it. Copy one of the underlined sub-sections of your trace parent context, and save it for later.",
    "description": "Re-deploy your Lambdas While remaining in your manual directory, run the following commandd to re-deploy your Lambda Functions:\n​ Deploy Producer Code Expected Output sls deploy -f producer Deploying function producer to stage dev (us-east-1) ✔ Function code deployed (6s) Configuration did not change. Configuration update skipped. (6s) ​ Deploy Consumer Code Expected Output sls deploy -f consumer Deploying function consumer to stage dev (us-east-1) ✔ Function code deployed (6s) Configuration did not change. Configuration update skipped. (6s) Note that this deployment now only updates the code changes within the function. Our configuration remains the same.",
    "tags": [],
    "title": "Redeploy Lambdas",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/6-lambda-kinesis/5-redeploy-lambdas/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Lambda Tracing",
    "content": "Now that we know how to apply manual instrumentation to the functions and services we wish to capture trace data for, let’s go about deploying our Lambda functions again, and generating traffic against our producer-lambda endpoint.\nInitialize Terraform in the manual directory Seeing as we’re in a new directory, we will need to initialize Terraform here once again.\nEnsure you are in the manual directory:\npwd The expected output would be ~/workshop/lambda/manual If you are not in the manual directory, run the following command:\ncd ~/workshop/lambda/manual Run the following command to initialize Terraform in this directory\nterraform init Deploy the Lambda functions and other AWS resources Let’s go ahead and deploy those resources again as well!\nRun the terraform plan command, ensuring there are no issues.\nterraform plan Follow up with the terraform apply command to deploy the Lambda functions and other supporting resources from the main.tf file:\nterraform apply Respond yes when you see the Enter a value: prompt\nThis will result in the following outputs:\nOutputs: base_url = \"https://______.amazonaws.com/serverless_stage/producer\" consumer_function_name = \"_____-consumer\" consumer_log_group_arn = \"arn:aws:logs:us-east-1:############:log-group:/aws/lambda/______-consumer\" consumer_log_group_name = \"/aws/lambda/______-consumer\" environment = \"______-lambda-shop\" lambda_bucket_name = \"lambda-shop-______-______\" producer_function_name = \"______-producer\" producer_log_group_arn = \"arn:aws:logs:us-east-1:############:log-group:/aws/lambda/______-producer\" producer_log_group_name = \"/aws/lambda/______-producer\" As you can tell, aside from the first portion of the base_url and the log gropu ARNs, the output should be largely the same as when you ran the auto-instrumentation portion of this workshop up to this same point.\nSend some traffic to the producer-lambda endpoint (base_url) Once more, we will send our name and superpower as a message to our endpoint. This will then be added to a record in our Kinesis Stream, along with our trace context.\nEnsure you are in the manual directory:\npwd The expected output would be ~/workshop/lambda/manual If you are not in the manual directory, run the following command:\ncd ~/workshop/lambda/manual Run the send_message.py script as a background process:\nnohup ./send_message.py --name CHANGEME --superpower CHANGEME \u0026 Next, check the contents of the response.logs file for successful calls to ourproducer-lambda endpoint:\ncat response.logs You should see the following output among the lines printed to your screen if your message is successful:\n{\"message\": \"Message placed in the Event Stream: hostname-eventStream\"} If unsuccessful, you will see:\n{\"message\": \"Internal server error\"} Important If this occurs, ask one of the workshop facilitators for assistance.\nView the Lambda Function Logs Let’s see what our logs look like now.\nCheck the producer.logs file:\ncat producer.logs And the consumer.logs file:\ncat consumer.logs Examine the logs carefully.\nWorkshop Question Do you notice the difference?\nCopy the Trace ID from the consumer.logs file This time around, we can see that the consumer-lambda log group is logging our message as a record together with the tracecontext that we propagated.\nTo copy the Trace ID:\nTake a look at one of the Kinesis Message logs. Within it, there is a data dictionary Take a closer look at data to see the nested tracecontext dictionary Within the tracecontext dictionary, there is a traceparent key-value pair The traceparent key-value pair holds the Trace ID we seek There are 4 groups of values, separated by -. The Trace ID is the 2nd group of characters Copy the Trace ID, and save it. We will need it for a later step in this workshop",
    "description": "Now that we know how to apply manual instrumentation to the functions and services we wish to capture trace data for, let’s go about deploying our Lambda functions again, and generating traffic against our producer-lambda endpoint.\nInitialize Terraform in the manual directory Seeing as we’re in a new directory, we will need to initialize Terraform here once again.\nEnsure you are in the manual directory:\npwd The expected output would be ~/workshop/lambda/manual If you are not in the manual directory, run the following command:",
    "tags": [],
    "title": "Deploying Lambda Functions \u0026 Generating Trace Data",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/6-lambda-kinesis/5-redeploy-lambdas/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud",
    "content": "Persona Remaining in your back-end developer role, you need to inspect the logs from your application to determine the root cause of the issue.\nUsing the content related to the APM trace (logs) we will now use Splunk Log Observer to drill down further to understand exactly what the problem is.\nRelated Content is a powerful feature that allows you to jump from one component to another and is available for metrics, traces and logs.",
    "description": "In this section, we will use Log Observer to drill down and identify what the problem is.",
    "tags": [],
    "title": "Splunk Log Observer",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/5-log-observer/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "Persona You are a frontend engineer, or an SRE tasked to do the first triage of a performance issue. You have been asked to investigate a potential customer satisfaction issue with the Online Boutique application.\nWe are going to examine the real user data that has been provided by the telemetry received from all participants’ browser sessions. The goal is to find a browser, mobile or tablet session that performed poorly and begin the troubleshooting process.",
    "description": "This section helps you understand how to use Splunk RUM to monitor the performance of your applications from the end user's perspective.",
    "tags": [],
    "title": "Splunk RUM",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/5-rum/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "Splunk Synthetic Monitoring provides visibility across URLs, APIs and critical web services to solve problems faster. IT Operations and engineering teams can easily detect, alert and prioritize issues, simulate multi-step user journeys, measure business impact from new code deployments and optimize web performance with guided step-by-step recommendations to ensure better digital experiences.\nEnsure Availability: Proactively monitor and alert on the health and availability of critical services, URLs and APIs with customizable browser tests to simulate multi-step workflows that make up the user experience.\nImprove Metrics: Core Web Vitals and modern performance metrics allow users to view all their performance defects in one place, measure and improve page load, interactivity and visual stability, and find and fix JavaScript errors to improve page performance.\nfront-end to back-end: Integrations with Splunk APM, Infrastructure Monitoring, On-Call and ITSI help teams view endpoint uptime against back-end services, the underlying infrastructure and within their incident response coordination so they can troubleshoot across their entire environment, in a single UI.\nDetect and Alert: Monitor and simulate end-user experiences to detect, communicate and resolve issues for APIs, service endpoints and critical business transactions before they impact customers.\nBusiness Performance: Easily define multi-step user flows for key business transactions and start recording and testing your critical user journeys in minutes. Track and report SLAs and SLOs for uptime and performance.\nFilmstrips and Video Playback: View screen recordings, film strips, and screenshots alongside modern performance scores, competitive benchmarking, and metrics to visualize artificial end-user experiences. Optimize how fast you deliver visual content, and improve page stability and interactivity to deploy better digital experiences.",
    "description": "Splunk Synthetic Monitoring provides visibility across URLs, APIs and critical web services to solve problems faster. IT Operations and engineering teams can easily detect, alert and prioritize issues, simulate multi-step user journeys, measure business impact from new code deployments and optimize web performance with guided step-by-step recommendations to ensure better digital experiences.\nEnsure Availability: Proactively monitor and alert on the health and availability of critical services, URLs and APIs with customizable browser tests to simulate multi-step workflows that make up the user experience.\nImprove Metrics: Core Web Vitals and modern performance metrics allow users to view all their performance defects in one place, measure and improve page load, interactivity and visual stability, and find and fix JavaScript errors to improve page performance.\nfront-end to back-end: Integrations with Splunk APM, Infrastructure Monitoring, On-Call and ITSI help teams view endpoint uptime against back-end services, the underlying infrastructure and within their incident response coordination so they can troubleshoot across their entire environment, in a single UI.\nDetect and Alert: Monitor and simulate end-user experiences to detect, communicate and resolve issues for APIs, service endpoints and critical business transactions before they impact customers.\nBusiness Performance: Easily define multi-step user flows for key business transactions and start recording and testing your critical user journeys in minutes. Track and report SLAs and SLOs for uptime and performance.\nFilmstrips and Video Playback: View screen recordings, film strips, and screenshots alongside modern performance scores, competitive benchmarking, and metrics to visualize artificial end-user experiences. Optimize how fast you deliver visual content, and improve page stability and interactivity to deploy better digital experiences.",
    "tags": [],
    "title": "Synthetics Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/5-synthetics-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Solving Problems with O11y Cloud",
    "content": "Explore APM Data Let’s explore some of the APM data we’ve captured to see how our application is performing.\nNavigate to APM, then use the Environment dropdown to select your environment (i.e. tagging-workshop-instancename).\nYou should see creditprocessorservice and creditcheckservice displayed in the list of services:\nClick on Service Map on the right-hand side to view the service map. We can see that the creditcheckservice makes calls to the creditprocessorservice, with an average response time of at least 3 seconds:\nNext, click on Traces on the right-hand side to see the traces captured for this application. You’ll see that some traces run relatively fast (i.e. just a few milliseconds), whereas others take a few seconds.\nClick on one of the longer running traces. In this example, the trace took five seconds, and we can see that most of the time was spent calling the /runCreditCheck operation, which is part of the creditprocessorservice:\nBut why are some traces slow, and others are relatively quick?\nClose the trace and return to the Trace Analyzer. If you toggle Errors only to on, you’ll also notice that some traces have errors:\nIf we look at one of the error traces, we can see that the error occurs when the creditprocessorservice attempts to call another service named otherservice. But why do some requests result in a call to otherservice, and others don’t?\nTo determine why some requests perform slowly, and why some requests result in errors, we could look through the traces one by one and try to find a pattern behind the issues.\nSplunk Observability Cloud provides a better way to find the root cause of an issue. We’ll explore this next.\nUsing Tag Spotlight Since we indexed the credit.score.category tag, we can use it with Tag Spotlight to troubleshoot our application.\nNavigate to APM then click on Tag Spotlight on the right-hand side. Ensure the creditcheckservice service is selected from the Service drop-down (if not already selected).\nWith Tag Spotlight, we can see 100% of credit score requests that result in a score of impossible have an error, yet requests for all other credit score types have no errors at all!\nThis illustrates the power of Tag Spotlight! Finding this pattern would be time-consuming without it, as we’d have to manually look through hundreds of traces to identify the pattern (and even then, there’s no guarantee we’d find it).\nWe’ve looked at errors, but what about latency? Let’s click on the Requests \u0026 errors distribution dropdown and change it to Latency distribution.\nIMPORTANT: Click on the settings icon beside Cards display to add the P50 and P99 metrics.\nHere, we can see that the requests with a poor credit score request are running slowly, with P50, P90, and P99 times of around 3 seconds, which is too long for our users to wait, and much slower than other requests.\nWe can also see that some requests with an exceptional credit score request are running slowly, with P99 times of around 5 seconds, though the P50 response time is relatively quick.\nUsing Dynamic Service Maps Now that we know the credit score category associated with the request can impact performance and error rates, let’s explore another feature that utilizes indexed tags: Dynamic Service Maps.\nWith Dynamic Service Maps, we can breakdown a particular service by a tag. For example, let’s click on APM, then click Service Map to view the service map.\nClick on creditcheckservice. Then, on the right-hand menu, click on the drop-down that says Breakdown, and select the credit.score.category tag.\nAt this point, the service map is updated dynamically, and we can see the performance of requests hitting creditcheckservice broken down by the credit score category:\nThis view makes it clear that performance for good and fair credit scores is excellent, while poor and exceptional scores are much slower, and impossible scores result in errors.\nOur Findings Tag Spotlight has uncovered several interesting patterns for the engineers that own this service to explore further:\nWhy are all the impossible credit score requests resulting in error? Why are all the poor credit score requests running slowly? Why do some of the exceptional requests run slowly? As an SRE, passing this context to the engineering team would be extremely helpful for their investigation, as it would allow them to track down the issue much more quickly than if we simply told them that the service was “sometimes slow”.\nIf you’re curious, have a look at the source code for the creditprocessorservice. You’ll see that requests with impossible, poor, and exceptional credit scores are handled differently, thus resulting in the differences in error rates and latency that we uncovered.\nThe behavior we saw with our application is typical for modern cloud-native applications, where different inputs passed to a service lead to different code paths, some of which result in slower performance or errors. For example, in a real credit check service, requests resulting in low credit scores may be sent to another downstream service to further evaluate risk, and may perform more slowly than requests resulting in higher scores, or encounter higher error rates.",
    "description": "Explore APM Data Let’s explore some of the APM data we’ve captured to see how our application is performing.\nNavigate to APM, then use the Environment dropdown to select your environment (i.e. tagging-workshop-instancename).\nYou should see creditprocessorservice and creditcheckservice displayed in the list of services:\nClick on Service Map on the right-hand side to view the service map. We can see that the creditcheckservice makes calls to the creditprocessorservice, with an average response time of at least 3 seconds:",
    "tags": [],
    "title": "Troubleshoot a Problem Using Tag Spotlight",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/9-solving-problems-with-o11y-cloud/5-troubleshoot-using-tag-spotlight/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e GDI (OTel \u0026 UF)",
    "content": "Objective: Learn how to monitor Linux system logs with the Universal Forwarder sending logs to Splunk Enterprise\nDuration: 10 Minutes\nScenario You’ve been tasked with monitoring the OS logs of the host running your Kubernetes cluster. We are going to utilize a script that will autodeploy the Splunk Universal Forwarder. You will then configure the Universal Forwarder to send logs to the Splunk Enterprise instance assigned to you.\n1. Ensure You’re in the Correct Directory we will need to be in /home/splunk/session-2 cd /home/splunk/session-2 2. Review the Universal Forwarder Install Script Let’s take a look at the script that will install the Universal Forwarder and Linux TA automatically for you. This script is primarily used for remote instances. Note we are not using a deployment server in this lab, however it is recommended in production we do that. What user are we installing Splunk as? #!/bin/sh # This EXAMPLE script shows how to deploy the Splunk universal forwarder # to many remote hosts via ssh and common Unix commands. # For \"real\" use, this script needs ERROR DETECTION AND LOGGING!! # --Variables that you must set ----- # Set username using by splunkd to run. SPLUNK_RUN_USER=\"ubuntu\" # Populate this file with a list of hosts that this script should install to, # with one host per line. This must be specified in the form that should # be used for the ssh login, ie. username@host # # Example file contents: # splunkuser@10.20.13.4 # splunkker@10.20.13.5 HOSTS_FILE=\"myhost.txt\" # This should be a WGET command that was *carefully* copied from splunk.com!! # Sign into splunk.com and go to the download page, then look for the wget # link near the top of the page (once you have selected your platform) # copy and paste your wget command between the \"\" WGET_CMD=\"wget -O splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz 'https://download.splunk.com/products/universalforwarder/releases/9.0.3/linux/splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz'\" # Set the install file name to the name of the file that wget downloads # (the second argument to wget) INSTALL_FILE=\"splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz\" # After installation, the forwarder will become a deployment client of this # host. Specify the host and management (not web) port of the deployment server # that will be managing these forwarder instances. # Example 1.2.3.4:8089 # DEPLOY_SERVER=\"x.x.x.x:8089\" # After installation, the forwarder can have additional TA's added to the # /app directory please provide the local where TA's will be. TA_INSTALL_DIRECTORY=\"/home/splunk/session-2\" # Set the seed app folder name for deploymentclien.conf # DEPLOY_APP_FOLDER_NAME=\"seed_all_deploymentclient\" # Set the new Splunk admin password PASSWORD=\"buttercup\" REMOTE_SCRIPT_DEPLOY=\" cd /opt sudo $WGET_CMD sudo tar xvzf $INSTALL_FILE sudo rm $INSTALL_FILE #sudo useradd $SPLUNK_RUN_USER sudo find $TA_INSTALL_DIRECTORY -name '*.tgz' -exec tar xzvf {} --directory /opt/splunkforwarder/etc/apps \\; sudo chown -R $SPLUNK_RUN_USER:$SPLUNK_RUN_USER /opt/splunkforwarder echo \\\"[user_info] USERNAME = admin PASSWORD = $PASSWORD\\\" \u003e /opt/splunkforwarder/etc/system/local/user-seed.conf #sudo cp $TA_INSTALL_DIRECTORY/*.tgz /opt/splunkforwader/etc/apps/ #sudo find /opt/splunkforwarder/etc/apps/ -name '*.tgz' -exec tar xzvf {} \\; #sudo -u splunk /opt/splunkforwarder/bin/splunk start --accept-license --answer-yes --auto-ports --no-prompt /opt/splunkforwarder/bin/splunk start --accept-license --answer-yes --auto-ports --no-prompt #sudo /opt/splunkforwarder/bin/splunk enable boot-start -user $SPLUNK_RUN_USER /opt/splunkforwarder/bin/splunk enable boot-start -user $SPLUNK_RUN_USER #sudo cp $TA_INSTALL_DIRECTORY/*.tgz /opt/splunkforwarder/etc/apps/ exit \" DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" \u003e/dev/null \u0026\u0026 pwd )\" #=============================================================================================== echo \"In 5 seconds, will run the following script on each remote host:\" echo echo \"====================\" echo \"$REMOTE_SCRIPT_DEPLOY\" echo \"====================\" echo sleep 5 echo \"Reading host logins from $HOSTS_FILE\" echo echo \"Starting.\" for DST in `cat \"$DIR/$HOSTS_FILE\"`; do if [ -z \"$DST\" ]; then continue; fi echo \"---------------------------\" echo \"Installing to $DST\" echo \"Initial UF deployment\" sudo ssh -t \"$DST\" \"$REMOTE_SCRIPT_DEPLOY\" done echo \"---------------------------\" echo \"Done\" echo \"Please use the following app folder name to override deploymentclient.conf options: $DEPLOY_APP_FOLDER_NAME\" 3. Run the install script We will run the install script now. You will see some Warnings at the end. This is totally normal. The script is built for use on remote machines, however for todays lab you will be using localhost.\n./install.sh You will be asked Are you sure you want to continue connecting (yes/no/[fingerprint])? Answer Yes.\nEnter your ssh password when prompted.\n4. Verify installation of the Universal Forwarader We need to verify that the Splunk Universal Forwarder is installed and running. You should see a couple PID’s return and a “Splunk is currently running.” message. /opt/splunkforwarder/bin/splunk status 5. Configure the Universal Forwarder to Send Data to Splunk Enterprise We will be able to send the data to our Splunk Enterprise environment easily by entering one line into the cli. Universal Forwarder Config Guide /opt/splunkforwarder/bin/splunk add forward-server \u003cyour_splunk_enterprise_ip\u003e:9997 6. Verify the Data in Your Splunk Enterprise Environment We are now going to take a look at the Splunk Enterprise environment to verify logs are coming in.\nLogs will be coming into index=main Open your web browser and navigate to: http://\u003cyour_splunk_enterprise_ip:8000\nYou will use the credentials admin:\u003cyour_ssh_password\u003e In the search bar, type in the following:\nindex=main host=\u003cyour_host_name\u003e\nYou should see data from your host. Take note of the interesting fields and the different data sources flowing in.",
    "description": "Objective: Learn how to monitor Linux system logs with the Universal Forwarder sending logs to Splunk Enterprise\nDuration: 10 Minutes\nScenario You’ve been tasked with monitoring the OS logs of the host running your Kubernetes cluster. We are going to utilize a script that will autodeploy the Splunk Universal Forwarder. You will then configure the Universal Forwarder to send logs to the Splunk Enterprise instance assigned to you.\n1. Ensure You’re in the Correct Directory we will need to be in /home/splunk/session-2 cd /home/splunk/session-2 2. Review the Universal Forwarder Install Script Let’s take a look at the script that will install the Universal Forwarder and Linux TA automatically for you. This script is primarily used for remote instances. Note we are not using a deployment server in this lab, however it is recommended in production we do that. What user are we installing Splunk as? #!/bin/sh # This EXAMPLE script shows how to deploy the Splunk universal forwarder # to many remote hosts via ssh and common Unix commands. # For \"real\" use, this script needs ERROR DETECTION AND LOGGING!! # --Variables that you must set ----- # Set username using by splunkd to run. SPLUNK_RUN_USER=\"ubuntu\" # Populate this file with a list of hosts that this script should install to, # with one host per line. This must be specified in the form that should # be used for the ssh login, ie. username@host # # Example file contents: # splunkuser@10.20.13.4 # splunkker@10.20.13.5 HOSTS_FILE=\"myhost.txt\" # This should be a WGET command that was *carefully* copied from splunk.com!! # Sign into splunk.com and go to the download page, then look for the wget # link near the top of the page (once you have selected your platform) # copy and paste your wget command between the \"\" WGET_CMD=\"wget -O splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz 'https://download.splunk.com/products/universalforwarder/releases/9.0.3/linux/splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz'\" # Set the install file name to the name of the file that wget downloads # (the second argument to wget) INSTALL_FILE=\"splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz\" # After installation, the forwarder will become a deployment client of this # host. Specify the host and management (not web) port of the deployment server # that will be managing these forwarder instances. # Example 1.2.3.4:8089 # DEPLOY_SERVER=\"x.x.x.x:8089\" # After installation, the forwarder can have additional TA's added to the # /app directory please provide the local where TA's will be. TA_INSTALL_DIRECTORY=\"/home/splunk/session-2\" # Set the seed app folder name for deploymentclien.conf # DEPLOY_APP_FOLDER_NAME=\"seed_all_deploymentclient\" # Set the new Splunk admin password PASSWORD=\"buttercup\" REMOTE_SCRIPT_DEPLOY=\" cd /opt sudo $WGET_CMD sudo tar xvzf $INSTALL_FILE sudo rm $INSTALL_FILE #sudo useradd $SPLUNK_RUN_USER sudo find $TA_INSTALL_DIRECTORY -name '*.tgz' -exec tar xzvf {} --directory /opt/splunkforwarder/etc/apps \\; sudo chown -R $SPLUNK_RUN_USER:$SPLUNK_RUN_USER /opt/splunkforwarder echo \\\"[user_info] USERNAME = admin PASSWORD = $PASSWORD\\\" \u003e /opt/splunkforwarder/etc/system/local/user-seed.conf #sudo cp $TA_INSTALL_DIRECTORY/*.tgz /opt/splunkforwader/etc/apps/ #sudo find /opt/splunkforwarder/etc/apps/ -name '*.tgz' -exec tar xzvf {} \\; #sudo -u splunk /opt/splunkforwarder/bin/splunk start --accept-license --answer-yes --auto-ports --no-prompt /opt/splunkforwarder/bin/splunk start --accept-license --answer-yes --auto-ports --no-prompt #sudo /opt/splunkforwarder/bin/splunk enable boot-start -user $SPLUNK_RUN_USER /opt/splunkforwarder/bin/splunk enable boot-start -user $SPLUNK_RUN_USER #sudo cp $TA_INSTALL_DIRECTORY/*.tgz /opt/splunkforwarder/etc/apps/ exit \" DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" \u003e/dev/null \u0026\u0026 pwd )\" #=============================================================================================== echo \"In 5 seconds, will run the following script on each remote host:\" echo echo \"====================\" echo \"$REMOTE_SCRIPT_DEPLOY\" echo \"====================\" echo sleep 5 echo \"Reading host logins from $HOSTS_FILE\" echo echo \"Starting.\" for DST in `cat \"$DIR/$HOSTS_FILE\"`; do if [ -z \"$DST\" ]; then continue; fi echo \"---------------------------\" echo \"Installing to $DST\" echo \"Initial UF deployment\" sudo ssh -t \"$DST\" \"$REMOTE_SCRIPT_DEPLOY\" done echo \"---------------------------\" echo \"Done\" echo \"Please use the following app folder name to override deploymentclient.conf options: $DEPLOY_APP_FOLDER_NAME\" 3. Run the install script We will run the install script now. You will see some Warnings at the end. This is totally normal. The script is built for use on remote machines, however for todays lab you will be using localhost.",
    "tags": [],
    "title": "Monitor System Logs with Splunk Universal Forwarder",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/8-gdi/5-forwarder/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize Cloud Monitoring \u003e 5. Improve Timeliness of Alerts",
    "content": "Splunk Observability Cloud provides detectors, events, alerts, and notifications to keep you informed when certain criteria are met. There are a number of pre-built AutoDetect Detectors that automatically surface when common problem patterns occur, such as when an EC2 instance’s CPU utilization is expected to reach its limit. Additionally, you can also create custom detectors if you want something more optimized or specific. For example, you want a message sent to a Slack channel or to an email address for the Ops team that manages this Kubernetes cluster when Memory Utilization on their pods has reached 85%.\nExercise: Create Custom Detector In this section you’ll create a detector on Pod Memory Utilization which will trigger if utilization surpasses 85%\nOn the Kubernetes Pods Dashboard you cloned in section 3.2 Dashboard Cloning, click the Get Alerts button (bell icon) for the Memory usage (%) chart -\u003e Click New detector from chart.\nIn the Create detector add your initials to the detector name.\nClick Create alert rule.\nThese conditions are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Importantly, multiple rules can be included in the same detector configuration which minimizes the total number of alerts that need to be created and maintained. You can see which signal this detector will alert on by the bell icon in the Alert On column. In this case, this detector will alert on the Memory Utilization for the pods running in this Kubernetes cluster.\nClick Proceed To Alert Conditions.\nMany pre-built alert conditions can be applied to the metric you want to alert on. This could be as simple as a static threshold or something more complex, for example, is memory usage deviating from the historical baseline across any of your 50,000 containers?\nSelect Static Threshold.\nClick Proceed To Alert Settings.\nIn this case, you want the alert to trigger if any pods exceed 85% memory utilization. Once you’ve set the alert condition, the configuration is back-tested against the historical data so you can confirm that the alert configuration is accurate, meaning will the alert trigger on the criteria you’ve defined? This is also a great way to confirm if the alert generates too much noise.\nEnter 85 in the Threshold field.\nClick Proceed To Alert Message.\nNext, you can set the severity for this alert, you can include links to runbooks and short tips on how to respond, and you can customize the message that is included in the alert details. The message can include parameterized fields from the actual data, for example, in this case, you may want to include which Kubernetes node the pod is running on, or the store.location configured when you deployed the application, to provide additional context.\nClick Proceed To Alert Recipients.\nYou can choose where you want this alert to be sent when it triggers. This could be to a team, specific email addresses, or to other systems such as ServiceNow, Slack, Splunk On-Call or Splunk ITSI. You can also have the alert execute a webhook which enables me to leverage automation or to integrate with many other systems such as homegrown ticketing tools. For the purpose of this workshop do not include a recipient\nClick Proceed To Alert Activation.\nClick Activate Alert.\nYou will receive a warning because no recipients were included in the Notification Policy for this detector. This can be warning can be dismissed.\nClick Save.\nYou will be taken to your newly created detector where you can see any triggered alerts.\nIn the upper right corner, Click Close to close the Detector.\nThe detector status and any triggered alerts will automatically be included in the chart because this detector was configured for this chart.\nCongratulations! You’ve successfully created a detector that will trigger if pod memory utilization exceeds 85%. After a few minutes, the detector should trigger some alerts. You can click the detector name in the chart to view the triggered alerts.",
    "description": "Splunk Observability Cloud provides detectors, events, alerts, and notifications to keep you informed when certain criteria are met. There are a number of pre-built AutoDetect Detectors that automatically surface when common problem patterns occur, such as when an EC2 instance’s CPU utilization is expected to reach its limit. Additionally, you can also create custom detectors if you want something more optimized or specific. For example, you want a message sent to a Slack channel or to an email address for the Ops team that manages this Kubernetes cluster when Memory Utilization on their pods has reached 85%.",
    "tags": [],
    "title": "Create Custom Detector",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-monitoring/5-improve-alert-timeliness/1-create-custom-detector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 6. Service",
    "content": "OTLP HTTP Exporter In the Exporters section of the workshop, we configured the otlphttp exporter to send metrics to Splunk Observability Cloud. We now need to enable this under the metrics pipeline.\nUpdate the exporters section to include otlphttp/splunk under the metrics pipeline:\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch, resourcedetection/system, resourcedetection/ec2, attributes/conf] exporters: [debug, otlphttp/splunk] Ninja: Observing the collector internals The collector captures internal signals about its behavior this also includes additional signals from running components. The reason for this is that components that make decisions about the flow of data need a way to surface that information as metrics or traces.\nWhy monitor the collector? This is somewhat of a chicken and egg problem of, “Who is watching the watcher?”, but it is important that we can surface this information. Another interesting part of the collector’s history is that it existed before the Go metrics’ SDK was considered stable so the collector exposes a Prometheus endpoint to provide this functionality for the time being.\nConsiderations Monitoring the internal usage of each running collector in your organization can contribute a significant amount of new Metric Time Series (MTS). The Splunk distribution has curated these metrics for you and would be able to help forecast the expected increases.\nThe Ninja Zone To expose the internal observability of the collector, some additional settings can be adjusted:\n​ telemetry schema example-config.yml service: telemetry: logs: level: \u003cinfo|warn|error\u003e development: \u003ctrue|false\u003e encoding: \u003cconsole|json\u003e disable_caller: \u003ctrue|false\u003e disable_stacktrace: \u003ctrue|false\u003e output_paths: [\u003cstdout|stderr\u003e, paths...] error_output_paths: [\u003cstdout|stderr\u003e, paths...] initial_fields: key: value metrics: level: \u003cnone|basic|normal|detailed\u003e # Address binds the promethues endpoint to scrape address: \u003chostname:port\u003e service: telemetry: logs: level: info encoding: json disable_stacktrace: true initial_fields: instance.name: ${env:INSTANCE} metrics: address: localhost:8888 References https://opentelemetry.io/docs/collector/configuration/#service Final configuration Check-inReview your final configuration ​ config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 # To limit exposure to denial of service attacks, change the host in endpoints below from 0.0.0.0 to a specific network interface. # See https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/security-best-practices.md#safeguards-against-denial-of-service-attacks extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 receivers: hostmetrics: collection_interval: 10s scrapers: # CPU utilization metrics cpu: # Disk I/O metrics disk: # File System utilization metrics filesystem: # Memory utilization metrics memory: # Network interface I/O metrics \u0026 TCP connection metrics network: # CPU load metrics load: # Paging/Swap space utilization and I/O metrics paging: # Process count metrics processes: # Per process CPU, Memory and Disk I/O metrics. Disabled by default. # process: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 opencensus: endpoint: 0.0.0.0:55678 # Collect own metrics prometheus/internal: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 10s static_configs: - targets: ['0.0.0.0:8888'] jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_binary: endpoint: 0.0.0.0:6832 thrift_compact: endpoint: 0.0.0.0:6831 thrift_http: endpoint: 0.0.0.0:14268 zipkin: endpoint: 0.0.0.0:9411 processors: batch: resourcedetection/system: detectors: [system] system: hostname_sources: [os] resourcedetection/ec2: detectors: [ec2] attributes/conf: actions: - key: participant.name action: insert value: \"INSERT_YOUR_NAME_HERE\" exporters: debug: verbosity: normal otlphttp/splunk: metrics_endpoint: https://ingest.${env:REALM}.signalfx.com/v2/datapoint/otlp headers: X-SF-Token: ${env:ACCESS_TOKEN} service: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch, resourcedetection/system, resourcedetection/ec2, attributes/conf] exporters: [debug, otlphttp/splunk] logs: receivers: [otlp] processors: [batch] exporters: [debug] extensions: [health_check, pprof, zpages] Tip It is recommended that you validate your configuration file before restarting the collector. You can do this by pasting the contents of your config.yaml file into otelbin.io.\nScreenshotOTelBin Now that we have a working configuration, let’s start the collector and then check to see what zPages is reporting.\n​ Command otelcol-contrib --config=file:/etc/otelcol-contrib/config.yaml Open up zPages in your browser: http://localhost:55679/debug/pipelinez (change localhost to reflect your own environment).",
    "description": "OTLP HTTP Exporter In the Exporters section of the workshop, we configured the otlphttp exporter to send metrics to Splunk Observability Cloud. We now need to enable this under the metrics pipeline.\nUpdate the exporters section to include otlphttp/splunk under the metrics pipeline:\nservice: pipelines: traces: receivers: [otlp, opencensus, jaeger, zipkin] processors: [batch] exporters: [debug] metrics: receivers: [hostmetrics, otlp, opencensus, prometheus/internal] processors: [batch, resourcedetection/system, resourcedetection/ec2, attributes/conf] exporters: [debug, otlphttp/splunk] Ninja: Observing the collector internals The collector captures internal signals about its behavior this also includes additional signals from running components. The reason for this is that components that make decisions about the flow of data need a way to surface that information as metrics or traces.",
    "tags": [],
    "title": "OpenTelemetry Collector Service",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/6-service/5-otlphttp/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences",
    "content": "After we have a good understanding of our performance baseline, we can start to create Detectors so that we receive alerts when our KPIs are unexpected. If we create detectors before understanding our baseline, we run the risk of generating unnecessary alert noise.\nFor RUM and Synthetics, we will explore how to create detectors:\non a single Synthetic test on a single KPI in RUM on a dashboard chart For more Detector resources, please see our Observability docs, Lantern, and consider an Education course if you’d like to go more in depth with instructor guidance.",
    "description": "After we have a good understanding of our performance baseline, we can start to create Detectors so that we receive alerts when our KPIs are unexpected. If we create detectors before understanding our baseline, we run the risk of generating unnecessary alert noise.\nFor RUM and Synthetics, we will explore how to create detectors:\non a single Synthetic test on a single KPI in RUM on a dashboard chart For more Detector resources, please see our Observability docs, Lantern, and consider an Education course if you’d like to go more in depth with instructor guidance.",
    "tags": [],
    "title": "Detectors",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/5-detectors/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Resources",
    "content": "Info Please disable any VPNs or proxies before setting up an instance e.g:\nZScaler Cisco AnyConnect These tools will prevent the instance from being created properly.\nLocal Hosting with MultipassLearn how to create a local hosting environment with Multipass - Windows/Linux/Mac(Intel)\nLocal Hosting with OrbStackLearn how to create a local hosting environment with OrbStack - Mac (Apple Silicon)\nLocal Hosting with ProxmoxLearn how to create a local hosting environment in Proxmox VE\nRunning Demo-in-a-BoxLearn how to use Demo-in-a-Box to manage demos and otel collectors in an easy-to-use web interface.",
    "description": "Resources for setting up a locally hosted workshop environment.",
    "tags": [],
    "title": "Local Hosting",
    "uri": "/observability-workshop/v6.5/en/resources/local-hosting/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 1. Real Browser Test",
    "content": "To edit the steps click on the + Edit steps or synthetic transactions button. From here, we are going to give meaningful names to each step.\nFor each of the four steps, we are going to give them a meaningful name.\nStep 1 replace the text Go to URL with HomePage - Online Boutique Step 2 enter the text Select Vintage Camera Lens. Step 3 enter Add to Cart. Step 4 enter Place Order. Click \u003c Return to test to return to the test configuration page and click Save to save the test.\nYou will be returned to the test dashboard where you will see test results start to appear.\nCongratulations! You have successfully created a Real Browser Test in Splunk Synthetic Monitoring. Next, we will look into a test result in more detail.",
    "description": "To edit the steps click on the + Edit steps or synthetic transactions button. From here, we are going to give meaningful names to each step.\nFor each of the four steps, we are going to give them a meaningful name.\nStep 1 replace the text Go to URL with HomePage - Online Boutique Step 2 enter the text Select Vintage Camera Lens. Step 3 enter Add to Cart. Step 4 enter Place Order.",
    "tags": [],
    "title": "1.6 Edit test steps",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/1-real-browser-test/6-edit-steps/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM \u003e 3. Dashboards",
    "content": "1. Adding Notes Often on dashboards it makes sense to place a short “instruction” pane that helps users of a dashboard. Lets add one now by clicking on the New Text Note Button.\nThis will open the notes editor.\nTo allow you to add more then just text to you notes, Splunk is allowing you to use Markdown in these notes/panes. Markdown is a lightweight markup language for creating formatted text using plain-text often used in Webpages.\nThis includes (but not limited to):\nHeaders. (in various sizes) Emphasis styles. Lists and Tables. Links. These can be external webpages (for documentation for example) or directly to other Splunk IM Dashboards Below is an example of above Markdown options you can use in your note.\n​ Sample Markdown text # h1 Big headings ###### h6 To small headings ##### Emphasis **This is bold text**, *This is italic text* , ~~Strikethrough~~ ##### Lists Unordered + Create a list by starting a line with `+`, `-`, or `*` - Sub-lists are made by indenting 2 spaces: - Marker character change forces new list start: * Ac tristique libero volutpat at + Facilisis in pretium nisl aliquet * Very easy! Ordered 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa ##### Tables | Option | Description | | ------ | ----------- | | chart | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | #### Links [link to webpage](https://www.splunk.com) [link to dashboard with title](https://app.eu0.signalfx.com/#/dashboard/EaJHrbPAEAA?groupId=EaJHgrsAIAA\u0026configId=EaJHsHzAEAA \"Link to the Sample chart Dashboard!\") Copy the above by using the copy button and paste it in the Edit box. the preview will show you how it will look.\n2. Saving our chart Give the Note chart a name, in our example we used Example text chart, then press the Save And Close Button.\nThis will bring you back to you Dashboard, that now includes the note.\n3. Ordering \u0026 sizing of charts If you do not like the default order and sizes of your charts you can simply use window dragging technique to move and size them to the desired location.\nGrab the top border of a chart and you should see the mouse pointer change to a drag icon (see picture below).\nNow drag the Latency vs Load chart to sit between the Latency History Chart and the Example text chart.\nYou can also resize windows by dragging from the left, right and bottom edges.\nAs a last exercise reduce the width of the note chart to about a third of the other charts. The chart will automatically snap to one of the sizes it supports. Widen the 3 other charts to about a third of the Dashboard. Drag the notes to the right of the others and resize it to match it to the 3 others. Set the Time to -1h and you should have the following dashboard!\nOn to Detectors!",
    "description": "1. Adding Notes Often on dashboards it makes sense to place a short “instruction” pane that helps users of a dashboard. Lets add one now by clicking on the New Text Note Button.\nThis will open the notes editor.\nTo allow you to add more then just text to you notes, Splunk is allowing you to use Markdown in these notes/panes. Markdown is a lightweight markup language for creating formatted text using plain-text often used in Webpages.",
    "tags": [],
    "title": "Adding Notes and Dashboard Layout",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/dashboards/notes-and-layout/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "The OpenTelemetry Collector’s FileStorage Extension enhances the resilience of your telemetry pipeline by providing reliable checkpointing, managing retries, and handling temporary failures effectively.\nWith this extension enabled, the OpenTelemetry Collector can store intermediate states on disk, preventing data loss during network disruptions and allowing it to resume operations seamlessly.\nNote This solution will work for metrics as long as the connection downtime is brief—up to 15 minutes. If the downtime exceeds this, Splunk Observability Cloud will drop data due to datapoints being out of order.\nFor logs, there are plans to implement a more enterprise-ready solution in one of the upcoming Splunk OpenTelemetry Collector releases.\nExercise Inside the [WORKSHOP] directory, create a new subdirectory named 4-resilience. Next, copy *.yaml from the 3-filelog directory into 4-resilience. Important Change ALL terminal windows to the [WORKSHOP]/4-resilience directory.\nYour updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml",
    "description": "The OpenTelemetry Collector’s FileStorage Extension enhances the resilience of your telemetry pipeline by providing reliable checkpointing, managing retries, and handling temporary failures effectively.\nWith this extension enabled, the OpenTelemetry Collector can store intermediate states on disk, preventing data loss during network disruptions and allowing it to resume operations seamlessly.\nNote This solution will work for metrics as long as the connection downtime is brief—up to 15 minutes. If the downtime exceeds this, Splunk Observability Cloud will drop data due to datapoints being out of order.",
    "tags": [],
    "title": "4. Building In Resilience",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/4-building-resilience/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "Splunk Infrastructure Monitoring (IM) is a market-leading monitoring and observability service for hybrid cloud environments. Built on a patented streaming architecture, it provides a real-time solution for engineering teams to visualize and analyze performance across infrastructure, services, and applications in a fraction of the time and with greater accuracy than traditional solutions.\nOpenTelemetry standardization: Gives you full control over your data — freeing you from vendor lock-in and implementing proprietary agents.\nSplunk’s OTel Collector: Seamless installation and dynamic configuration, auto-discovers your entire stack in seconds for visibility across clouds, services, and systems.\n300+ Easy-to-use OOTB content: Pre-built navigators and dashboards, deliver immediate visualizations of your entire environment so that you can interact with all your data in real time.\nKubernetes navigator: Provides an instant, comprehensive out-of-the-box hierarchical view of nodes, pods, and containers. Ramp up even the most novice Kubernetes user with easy-to-understand interactive cluster maps.\nAutoDetect alerts and detectors: Automatically identify the most important metrics, out-of-the-box, to create alert conditions for detectors that accurately alert from the moment telemetry data is ingested and use real-time alerting capabilities for important notifications in seconds.\nLog views in dashboards: Combine log messages and real-time metrics on one page with common filters and time controls for faster in-context troubleshooting.\nMetrics pipeline management: Control metrics volume at the point of ingest without re-instrumentation with a set of aggregation and data-dropping rules to store and analyze only the needed data. Reduce metrics volume and optimize observability spend.",
    "description": "Splunk Infrastructure Monitoring (IM) is a market-leading monitoring and observability service for hybrid cloud environments. Built on a patented streaming architecture, it provides a real-time solution for engineering teams to visualize and analyze performance across infrastructure, services, and applications in a fraction of the time and with greater accuracy than traditional solutions.\nOpenTelemetry standardization: Gives you full control over your data — freeing you from vendor lock-in and implementing proprietary agents.",
    "tags": [],
    "title": "Infrastructure Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/4-infrastructure-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "In this section, you’ll learn how to configure the OpenTelemetry Collector to remove specific tags and redact sensitive data from telemetry spans. This is crucial for protecting sensitive information such as credit card numbers, personal data, or other security-related details that must be anonymized before being processed or exported.\nWe’ll walk through configuring key processors in the OpenTelemetry Collector, including:\nAttributes Processor: Modifies or removes specific span attributes. Redaction Processor: Ensures sensitive data is sanitized before being stored or transmitted. Exercise Important Change ALL terminal windows to the 4-sensitive-data directory and run the clear command.\nCopy *.yaml from the 3-dropping-spans directory into 4-sensitive-data. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml",
    "description": "In this section, you’ll learn how to configure the OpenTelemetry Collector to remove specific tags and redact sensitive data from telemetry spans. This is crucial for protecting sensitive information such as credit card numbers, personal data, or other security-related details that must be anonymized before being processed or exported.\nWe’ll walk through configuring key processors in the OpenTelemetry Collector, including:\nAttributes Processor: Modifies or removes specific span attributes. Redaction Processor: Ensures sensitive data is sanitized before being stored or transmitted. Exercise Important Change ALL terminal windows to the 4-sensitive-data directory and run the clear command.",
    "tags": [],
    "title": "4. Redacting Sensitive Data",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/4-sensitive-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "In this section, you’ll learn how to configure the OpenTelemetry Collector to remove specific tags and redact sensitive data from telemetry spans. This is crucial for protecting sensitive information such as credit card numbers, personal data, or other security-related details that must be anonymized before being processed or exported.\nWe’ll walk through configuring key processors in the OpenTelemetry Collector, including:\nAttributes Processor: Modifies or removes specific span attributes. Redaction Processor: Ensures sensitive data is sanitized before being stored or transmitted. Exercise Important Change ALL terminal windows to the 4-sensitive-data directory and run the clear command.\nCopy *.yaml from the 3-dropping-spans directory into 4-sensitive-data. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml",
    "description": "In this section, you’ll learn how to configure the OpenTelemetry Collector to remove specific tags and redact sensitive data from telemetry spans. This is crucial for protecting sensitive information such as credit card numbers, personal data, or other security-related details that must be anonymized before being processed or exported.\nWe’ll walk through configuring key processors in the OpenTelemetry Collector, including:\nAttributes Processor: Modifies or removes specific span attributes. Redaction Processor: Ensures sensitive data is sanitized before being stored or transmitted. Exercise Important Change ALL terminal windows to the 4-sensitive-data directory and run the clear command.",
    "tags": [],
    "title": "4. Redacting Sensitive Data",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/4-sensitive-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "As we have seen in the previous section, once you enable automatic discovery and configuration on your services, traces are sent to Splunk Observability Cloud.\nWith these traces, Splunk will automatically generate Service Maps and RED Metrics. These are the first steps in understanding the behavior of your services and how they interact with each other.\nIn this next section, we are going to examine the traces themselves and the information they provide to help you understand the behavior of your services – all without touching your code!",
    "description": "As we have seen in the previous section, once you enable automatic discovery and configuration on your services, traces are sent to Splunk Observability Cloud.\nWith these traces, Splunk will automatically generate Service Maps and RED Metrics. These are the first steps in understanding the behavior of your services and how they interact with each other.\nIn this next section, we are going to examine the traces themselves and the information they provide to help you understand the behavior of your services – all without touching your code!",
    "tags": [],
    "title": "APM Features",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/5-traces/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Profiling Workshop",
    "content": "In this workshop, we accomplished the following:\nWe deployed our application and captured traces with Splunk Observability Cloud. We used Database Query Performance to find a slow-running query that impacted the game startup time. We enabled AlwaysOn Profiling and used it to confirm which line of code was causing the increased startup time and memory usage. We found another application performance issue and used AlwaysOn Profiling again to find the problematic line of code. We applied fixes for both of these issues and verified the result using Splunk Observability Cloud. Enabling AlwaysOn Profiling and utilizing Database Query Performance for your applications will let you get even more value from the data you’re sending to Splunk Observability Cloud.\nNow that you’ve completed this workshop, you have the knowledge you need to start collecting deeper diagnostic data from your own applications!\nTo get started with Database Query Performance today, check out Monitor Database Query Performance.\nAnd to get started with AlwaysOn Profiling, check out Introduction to AlwaysOn Profiling for Splunk APM\nFor more help, feel free to ask a Splunk Expert.",
    "description": "In this workshop, we accomplished the following:\nWe deployed our application and captured traces with Splunk Observability Cloud. We used Database Query Performance to find a slow-running query that impacted the game startup time. We enabled AlwaysOn Profiling and used it to confirm which line of code was causing the increased startup time and memory usage. We found another application performance issue and used AlwaysOn Profiling again to find the problematic line of code. We applied fixes for both of these issues and verified the result using Splunk Observability Cloud. Enabling AlwaysOn Profiling and utilizing Database Query Performance for your applications will let you get even more value from the data you’re sending to Splunk Observability Cloud.",
    "tags": [],
    "title": "Summary",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/profiling/6-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "Now that we’ve successfully Dockerized our application, let’s add in OpenTelemetry instrumentation.\nThis is similar to the steps we took when instrumenting the application running on Linux, but there are some key differences to be aware of.\nUpdate the Dockerfile Let’s update the Dockerfile in the /home/splunk/workshop/docker-k8s-otel/helloworld directory.\nAfter the .NET application is built in the Dockerfile, we want to:\nAdd dependencies needed to download and execute splunk-otel-dotnet-install.sh Download the Splunk OTel .NET installer Install the distribution We can add the following to the build stage of the Dockerfile. Let’s open the Dockerfile in vi:\nvi /home/splunk/workshop/docker-k8s-otel/helloworld/Dockerfile Press the i key to enter edit mode in vi\nPaste the lines marked with ‘NEW CODE’ into your Dockerfile in the build stage section:\n# CODE ALREADY IN YOUR DOCKERFILE: FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build ARG BUILD_CONFIGURATION=Release WORKDIR /src COPY [\"helloworld.csproj\", \"helloworld/\"] RUN dotnet restore \"./helloworld/./helloworld.csproj\" WORKDIR \"/src/helloworld\" COPY . . RUN dotnet build \"./helloworld.csproj\" -c $BUILD_CONFIGURATION -o /app/build # NEW CODE: add dependencies for splunk-otel-dotnet-install.sh RUN apt-get update \u0026\u0026 \\ apt-get install -y unzip # NEW CODE: download Splunk OTel .NET installer RUN curl -sSfL https://github.com/signalfx/splunk-otel-dotnet/releases/latest/download/splunk-otel-dotnet-install.sh -O # NEW CODE: install the distribution RUN sh ./splunk-otel-dotnet-install.sh Next, we’ll update the final stage of the Dockerfile with the following changes:\nCopy the /root/.splunk-otel-dotnet/ from the build image to the final image Copy the entrypoint.sh file as well Set the OTEL_SERVICE_NAME and OTEL_RESOURCE_ATTRIBUTES environment variables Set the ENTRYPOINT to entrypoint.sh It’s easiest to simply replace the entire final stage with the following:\nIMPORTANT replace $INSTANCE in your Dockerfile with your instance name, which can be determined by running echo $INSTANCE.\n# CODE ALREADY IN YOUR DOCKERFILE FROM base AS final # NEW CODE: Copy instrumentation file tree WORKDIR \"//home/app/.splunk-otel-dotnet\" COPY --from=build /root/.splunk-otel-dotnet/ . # CODE ALREADY IN YOUR DOCKERFILE WORKDIR /app COPY --from=publish /app/publish . # NEW CODE: copy the entrypoint.sh script COPY entrypoint.sh . # NEW CODE: set OpenTelemetry environment variables ENV OTEL_SERVICE_NAME=helloworld ENV OTEL_RESOURCE_ATTRIBUTES='deployment.environment=otel-$INSTANCE' # NEW CODE: replace the prior ENTRYPOINT command with the following two lines ENTRYPOINT [\"sh\", \"entrypoint.sh\"] CMD [\"dotnet\", \"helloworld.dll\"] To save your changes in vi, press the esc key to enter command mode, then type :wq! followed by pressing the enter/return key.\nAfter all of these changes, the Dockerfile should look like the following:\nIMPORTANT if you’re going to copy and paste this content into your own Dockerfile, replace $INSTANCE in your Dockerfile with your instance name, which can be determined by running echo $INSTANCE.\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base USER app WORKDIR /app EXPOSE 8080 FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build ARG BUILD_CONFIGURATION=Release WORKDIR /src COPY [\"helloworld.csproj\", \"helloworld/\"] RUN dotnet restore \"./helloworld/./helloworld.csproj\" WORKDIR \"/src/helloworld\" COPY . . RUN dotnet build \"./helloworld.csproj\" -c $BUILD_CONFIGURATION -o /app/build # NEW CODE: add dependencies for splunk-otel-dotnet-install.sh RUN apt-get update \u0026\u0026 \\ apt-get install -y unzip # NEW CODE: download Splunk OTel .NET installer RUN curl -sSfL https://github.com/signalfx/splunk-otel-dotnet/releases/latest/download/splunk-otel-dotnet-install.sh -O # NEW CODE: install the distribution RUN sh ./splunk-otel-dotnet-install.sh FROM build AS publish ARG BUILD_CONFIGURATION=Release RUN dotnet publish \"./helloworld.csproj\" -c $BUILD_CONFIGURATION -o /app/publish /p:UseAppHost=false FROM base AS final # NEW CODE: Copy instrumentation file tree WORKDIR \"//home/app/.splunk-otel-dotnet\" COPY --from=build /root/.splunk-otel-dotnet/ . WORKDIR /app COPY --from=publish /app/publish . # NEW CODE: copy the entrypoint.sh script COPY entrypoint.sh . # NEW CODE: set OpenTelemetry environment variables ENV OTEL_SERVICE_NAME=helloworld ENV OTEL_RESOURCE_ATTRIBUTES='deployment.environment=otel-$INSTANCE' # NEW CODE: replace the prior ENTRYPOINT command with the following two lines ENTRYPOINT [\"sh\", \"entrypoint.sh\"] CMD [\"dotnet\", \"helloworld.dll\"] Create the entrypoint.sh file We also need to create a file named entrypoint.sh in the /home/splunk/workshop/docker-k8s-otel/helloworld folder with the following content:\nvi /home/splunk/workshop/docker-k8s-otel/helloworld/entrypoint.sh Then paste the following code into the newly created file:\n#!/bin/sh # Read in the file of environment settings . /$HOME/.splunk-otel-dotnet/instrument.sh # Then run the CMD exec \"$@\" To save your changes in vi, press the esc key to enter command mode, then type :wq! followed by pressing the enter/return key.\nThe entrypoint.sh script is required for sourcing environment variables from the instrument.sh script, which is included with the instrumentation. This ensures the correct setup of environment variables for each platform.\nYou may be wondering, why can’t we just include the following command in the Dockerfile to do this, like we did when activating OpenTelemetry .NET instrumentation on our Linux host?\nRUN . $HOME/.splunk-otel-dotnet/instrument.sh The problem with this approach is that each Dockerfile RUN step runs a new container and a new shell. If you try to set an environment variable in one shell, it will not be visible later on. This problem is resolved by using an entry point script, as we’ve done here. Refer to this Stack Overflow post for further details on this issue.\nBuild the Docker Image Let’s build a new Docker image that includes the OpenTelemetry .NET instrumentation:\ndocker build -t helloworld:1.1 . Note: we’ve used a different version (1.1) to distinguish the image from our earlier version. To clean up the older versions, run the following command to get the container id:\ndocker ps -a Then run the following command to delete the container:\ndocker rm \u003cold container id\u003e --force Now we can get the container image id:\ndocker images | grep 1.0 Finally, we can run the following command to delete the old image:\ndocker image rm \u003cold image id\u003e Run the Application Let’s run the new Docker image:\ndocker run --name helloworld \\ --detach \\ --expose 8080 \\ --network=host \\ helloworld:1.1 We can access the application using:\ncurl http://localhost:8080/hello Execute the above command a few times to generate some traffic.\nAfter a minute or so, confirm that you see new traces in Splunk Observability Cloud.\nRemember to look for traces in your particular Environment.\nTroubleshooting If you don’t see traces appear in Splunk Observability Cloud, here’s how you can troubleshoot.\nFirst, open the collector config file for editing:\nsudo vi /etc/otel/collector/agent_config.yaml Next, add the debug exporter to the traces pipeline, which ensures the traces are written to the collector logs:\nservice: extensions: [health_check, http_forwarder, zpages, smartagent] pipelines: traces: receivers: [jaeger, otlp, zipkin] processors: - memory_limiter - batch - resourcedetection #- resource/add_environment # NEW CODE: add the debug exporter here exporters: [otlphttp, signalfx, debug] Then, restart the collector to apply the configuration changes:\nsudo systemctl restart splunk-otel-collector We can then view the collector logs using journalctl:\nPress Ctrl + C to exit out of tailing the log.\nsudo journalctl -u splunk-otel-collector -f -n 100",
    "description": "Now that we’ve successfully Dockerized our application, let’s add in OpenTelemetry instrumentation.\nThis is similar to the steps we took when instrumenting the application running on Linux, but there are some key differences to be aware of.\nUpdate the Dockerfile Let’s update the Dockerfile in the /home/splunk/workshop/docker-k8s-otel/helloworld directory.\nAfter the .NET application is built in the Dockerfile, we want to:\nAdd dependencies needed to download and execute splunk-otel-dotnet-install.sh Download the Splunk OTel .NET installer Install the distribution We can add the following to the build stage of the Dockerfile. Let’s open the Dockerfile in vi:",
    "tags": [],
    "title": "Add Instrumentation to Dockerfile",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/6-add-instrumentation-to-dockerfile/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 4. Splunk APM",
    "content": "We have arrived at the Trace Waterfall from the Trace Analyzer. A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services.\nEach span in Splunk APM captures a single operation. Splunk APM considers a span to be an error span if the operation that the span captures results in an error.\nExercise Click on the ! next to any of the wire-transfer-service spans in the waterfall. ​ Question Answer What is the error message and version being reported in the Span Details?\nInvalid request and v350.10.\nNow that we have identified the version of the wire-transfer-service that is causing the issue, let’s see if we can find out more information about the error. This is where Related Logs come in.\nRelated Content relies on specific metadata that allow APM, Infrastructure Monitoring, and Log Observer to pass filters around Observability Cloud. For related logs to work, you need to have the following metadata in your logs:\nservice.name deployment.environment host.name trace_id span_id Exercise At the very bottom of the Trace Waterfall click on Logs (1). This highlights that there are Related Logs for this trace. Click on the Logs for trace xxx entry in the pop-up, this will open the logs for the complete trace in Log Observer. Next, let’s find out more about the error in the logs.",
    "description": "We have arrived at the Trace Waterfall from the Trace Analyzer. A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services.\nEach span in Splunk APM captures a single operation. Splunk APM considers a span to be an error span if the operation that the span captures results in an error.",
    "tags": [],
    "title": "6. APM Waterfall",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/4-apm/6-apm-waterfall/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 6. Splunk APM",
    "content": "We have arrived at the Trace Waterfall from the Trace Analyzer. A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services.\nEach span in Splunk APM captures a single operation. Splunk APM considers a span to be an error span if the operation that the span captures results in an error.\nExercise Click on the ! next to any of the paymentservice:grpc.hipstershop.PaymentService/Charge spans in the waterfall. ​ Question Answer What is the error message and version being reported in the Span Details?\nInvalid request and v350.10.\nNow that we have identified the version of the paymentservice that is causing the issue, let’s see if we can find out more information about the error. This is where Related Logs come in.\nRelated Content relies on specific metadata that allow APM, Infrastructure Monitoring, and Log Observer to pass filters around Observability Cloud. For related logs to work, you need to have the following metadata in your logs:\nservice.name deployment.environment host.name trace_id span_id Exercise At the very bottom of the Trace Waterfall click on Logs (1). This highlights that there are Related Logs for this trace. Click on the Logs for trace xxx entry in the pop-up, this will open the logs for the complete trace in Log Observer. Next, let’s find out more about the error in the logs.",
    "description": "We have arrived at the Trace Waterfall from the Trace Analyzer. A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services.\nEach span in Splunk APM captures a single operation. Splunk APM considers a span to be an error span if the operation that the span captures results in an error.",
    "tags": [],
    "title": "6. APM Waterfall",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/6-apm/6-apm-waterfall/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Custom Attribution (Custom Tags) To take a deeper look at this issue and make this much easier to debug we will implement Custom Attributes via Opentelemetry Manual Instrumentation.\nTo speed up manual instrumentation in Java you can leverage OpenTelemetry Annotations, which automatically create a span around a method without modifying the actual code inside the method. This can be very valuable if you are working with an SRE that may have limited access to source code changes.\nTo add even more information to help our developers find the root cause faster, OpenTelemetry Annotations can be used to generate span tags with parameter values for the method in question. This is incredibly important to mean time to Repair, because as a Developer, if I know the parameter values of a function at the time of latency or error, I can debug this without having to reproduce an issue in another environment if I have Full Fidelity Tracing.\nSplunk Full Fidelity APM allows our customers development teams to debug their code as it ran in production, 100% of the time. Add with Custom Attribution, you are providing repeatable Mean Time to Repair reduction… 100% of the time, only with Full Fidelity tracing.\nTo expedite manual instrumentation implementation for this exercise, we have provided a tool which will annotate the entirety of the “shop” and “products” services with OpenTelemetry standard annotations for every function call without having to write any code. This “annotator” tool will also tag every parameter in every function, which adds a span tag with Parameter=Value.\nRun Manual Instrumentation Tool ./AutomateManualInstrumentation.sh Rebuild and Deploy the Application ./BuildAndDeploy.sh Visualize in Splunk APM Now that we have rebuilt and deployed our application, traffic is being sent once again.\nGo back to the Splunk Observability UI and let’s see if these annotations help us narrow down the root cause more quickly.\nLet’s try to find our latency root cause again, this time with every function and every parameter spanned and tagged respectively…. You will see exactly what this means and how it benefits developers in a moment.\nClick on shop service Click Traces ( on the right side ) Sort by Duration Select the longest duration trace ( or one of the obvious much longer ones ) We can see that the actual function call that has the latency was not ProductResource.getAllProducts but the function call ProductResource.myCoolFunction234234234.\nSince we now have the parameter tagged as part of our span metadata the actual root cause is seemingly related to the “location” Colorado. And it appears the one custom attribute that was tagged for function ProductResource.myCoolFunction234234234 was myInt with a value of 999. With this information a developer can debug very quickly.\nNote We added additional span information, which we call Custom Attributes here at Splunk. In this case our Custom Attributes are Parameter Values at Time of Latency.\nIn our exmaple the myInt tag was created due our handy Annotator, that did the OpenTelemetry Annotations for us.\nUsing nano: nano products/src/main/java/com/shabushabu/javashop/products/resources/ProductResource.java Search in Nano: [CTRL]-w Enter in: 999 [Enter] We can see here, someone wrote some very bad code in the form of a long Sleep!\nif (999==myInt) Thread.sleep( sleepy.nextInt(5000 - 3000) + 3000);",
    "description": "Custom Attribution (Custom Tags) To take a deeper look at this issue and make this much easier to debug we will implement Custom Attributes via Opentelemetry Manual Instrumentation.\nTo speed up manual instrumentation in Java you can leverage OpenTelemetry Annotations, which automatically create a span around a method without modifying the actual code inside the method. This can be very valuable if you are working with an SRE that may have limited access to source code changes.",
    "tags": [],
    "title": "Custom Tags",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/6-custom-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Advanced Synthetics",
    "content": "To edit the steps click on the + Edit steps or synthetic transactions button. From here, we are going to give meaningful names to each step.\nFor each step, we are going to give them a meaningful, readable name. That could look like:\nStep 1 replace the text Go to URL with Go to Homepage Step 2 enter the text Select Typewriter. Step 3 enter Add to Cart. Step 4 enter Place Order. Note If you’d like, group the test steps into Transactions and edit the transaction names as seen above. This is especially useful for Single Page Apps (SPAs), where the resource waterfall is not split by URL. We can also create charts and alerts based on transactions.\nClick \u003c Return to test to return to the test configuration page and click Save to save the test.\nYou will be returned to the test dashboard where you will see test results start to appear.\nCongratulations! You have successfully created a Real Browser Test in Splunk Synthetic Monitoring. Next, we will look into a test result in more detail.",
    "description": "To edit the steps click on the + Edit steps or synthetic transactions button. From here, we are going to give meaningful names to each step.\nFor each step, we are going to give them a meaningful, readable name. That could look like:\nStep 1 replace the text Go to URL with Go to Homepage Step 2 enter the text Select Typewriter. Step 3 enter Add to Cart. Step 4 enter Place Order.",
    "tags": [],
    "title": "Edit test steps",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/3-advanced-synthetics/6-edit-steps/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour",
    "content": "Splunk Infrastructure Monitoring (IM) is a market-leading monitoring and observability service for hybrid cloud environments. Built on a patented streaming architecture, it provides a real-time solution for engineering teams to visualize and analyze performance across infrastructure, services, and applications in a fraction of the time and with greater accuracy than traditional solutions.\nOpenTelemetry standardization: Gives you full control over your data — freeing you from vendor lock-in and implementing proprietary agents.\nSplunk’s OTel Collector: Seamless installation and dynamic configuration, auto-discovers your entire stack in seconds for visibility across clouds, services, and systems.\n300+ Easy-to-use OOTB content: Pre-built navigators and dashboards, deliver immediate visualizations of your entire environment so that you can interact with all your data in real time.\nKubernetes navigator: Provides an instant, comprehensive out-of-the-box hierarchical view of nodes, pods, and containers. Ramp up even the most novice Kubernetes user with easy-to-understand interactive cluster maps.\nAutoDetect alerts and detectors: Automatically identify the most important metrics, out-of-the-box, to create alert conditions for detectors that accurately alert from the moment telemetry data is ingested and use real-time alerting capabilities for important notifications in seconds.\nLog views in dashboards: Combine log messages and real-time metrics on one page with common filters and time controls for faster in-context troubleshooting.\nMetrics pipeline management: Control metrics volume at the point of ingest without re-instrumentation with a set of aggregation and data-dropping rules to store and analyze only the needed data. Reduce metrics volume and optimize observability spend.",
    "description": "Splunk Infrastructure Monitoring (IM) is a market-leading monitoring and observability service for hybrid cloud environments. Built on a patented streaming architecture, it provides a real-time solution for engineering teams to visualize and analyze performance across infrastructure, services, and applications in a fraction of the time and with greater accuracy than traditional solutions.\nOpenTelemetry standardization: Gives you full control over your data — freeing you from vendor lock-in and implementing proprietary agents.\nSplunk’s OTel Collector: Seamless installation and dynamic configuration, auto-discovers your entire stack in seconds for visibility across clouds, services, and systems.\n300+ Easy-to-use OOTB content: Pre-built navigators and dashboards, deliver immediate visualizations of your entire environment so that you can interact with all your data in real time.\nKubernetes navigator: Provides an instant, comprehensive out-of-the-box hierarchical view of nodes, pods, and containers. Ramp up even the most novice Kubernetes user with easy-to-understand interactive cluster maps.\nAutoDetect alerts and detectors: Automatically identify the most important metrics, out-of-the-box, to create alert conditions for detectors that accurately alert from the moment telemetry data is ingested and use real-time alerting capabilities for important notifications in seconds.\nLog views in dashboards: Combine log messages and real-time metrics on one page with common filters and time controls for faster in-context troubleshooting.\nMetrics pipeline management: Control metrics volume at the point of ingest without re-instrumentation with a set of aggregation and data-dropping rules to store and analyze only the needed data. Reduce metrics volume and optimize observability spend.",
    "tags": [],
    "title": "Infrastructure Overview",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/6-infrastructure-home/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e NodeJS Zero-Config Workshop",
    "content": "1. Viewing the logs for the Payment Service Navigate back to APM from the main menu and under Services click on opentelemetry-demo-paymentservice. This will open up the Service map for the paymentservice service only.\nAt the bottom of the page, click on the Logs(1) tab to view the logs for the paymentservice service.\nOnce in Log Observer select one of the log entries to view the metadata for the log entry.",
    "description": "1. Viewing the logs for the Payment Service Navigate back to APM from the main menu and under Services click on opentelemetry-demo-paymentservice. This will open up the Service map for the paymentservice service only.\nAt the bottom of the page, click on the Logs(1) tab to view the logs for the paymentservice service.\nOnce in Log Observer select one of the log entries to view the metadata for the log entry.",
    "tags": [],
    "title": "Logs - Payment Service",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/3-nodejs-kubernetes/6-logs/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts",
    "content": "The Service section is used to configure what components are enabled in the Collector based on the configuration found in the receivers, processors, exporters, and extensions sections.\nInfo If a component is configured, but not defined within the Service section then it is not enabled.\nThe service section consists of three sub-sections:\nextensions pipelines telemetry In the default configuration, the extension section has been configured to enable health_check, pprof and zpages, which we configured in the Extensions module earlier.\nservice: extensions: [health_check, pprof, zpages] So lets configure our Metric Pipeline!",
    "description": "The Service section is used to configure what components are enabled in the Collector based on the configuration found in the receivers, processors, exporters, and extensions sections.\nInfo If a component is configured, but not defined within the Service section then it is not enabled.\nThe service section consists of three sub-sections:\nextensions pipelines telemetry In the default configuration, the extension section has been configured to enable health_check, pprof and zpages, which we configured in the Extensions module earlier.",
    "tags": [],
    "title": "OpenTelemetry Collector Service",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/6-service/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk IM",
    "content": "How to keep track of the usage of Observability Cloud in your organization Learn how to keep track of spend by exploring the Subscription Usage interface Creating Teams Adding notification rules to Teams Controlling usage 1. Understanding engagement To fully understand Observability Cloud engagement inside your organization, click on the » bottom left and select the Settings → Organization Overview, this will provide you with the following dashboards that show you how your Observability Cloud organization is being used:\nYou will see various dashboards such as Throttling, System Limits, Entitlements \u0026 Engagement. The workshop organization you’re using now may have less data to work with as this is cleared down after each workshop.\nTake a minute to explore the various dashboards and charts in the Organization Overview of this workshop instance.\n2. Subscription Usage If you want to see what your usage is against your subscription you can select Subscription Usage.\nThis screen may take a few seconds to load whilst it calculates and pulls in the usage.\n3. Understanding usage You will see a screen similar to the one below that will give you an overview of the current usage, the average usage and your entitlement per category: Hosts, Containers, Custom Metrics and High Resolution Metrics.\nFor more information about these categories please refer to Monitor Splunk Infrastructure Monitoring subscription usage.\n4. Examine usage in detail The top chart shows you the current subscription levels per category (shown by the red arrows at the top in the screenshot below).\nAlso, your current usage of the four categories is displayed (shown in the red lines at the bottom of the chart).\nIn this example, you can see that there are 25 Hosts, 0 Containers, 100 Custom Metrics and 0 High Resolution Metrics.\nIn the bottom chart, you can see the usage per category for the current period (shown in the drop-down box on the top right of the chart).\nThe blue line marked Average Usage indicates what Observability Cloud will use to calculate your average usage for the current Subscription Usage Period.\nInfo As you can see from the screenshot, Observability Cloud does not use High Watermark or P95% for cost calculation but the actual average hourly usage, allowing you to do performance testing or Blue/Green style deployments etc. without the risk of overage charges.\nTo get a feel for the options you can change the metric displayed by selecting the different options from the Usage Metric drop-down on the left, or change the Subscription Usage Period with the drop-down on the right.\nPlease take a minute to explore the different time periods \u0026 categories and their views.\nFinally, the pane on the right shows you information about your Subscription.",
    "description": "How to keep track of the usage of Observability Cloud in your organization Learn how to keep track of spend by exploring the Subscription Usage interface Creating Teams Adding notification rules to Teams Controlling usage 1. Understanding engagement To fully understand Observability Cloud engagement inside your organization, click on the » bottom left and select the Settings → Organization Overview, this will provide you with the following dashboards that show you how your Observability Cloud organization is being used:",
    "tags": [],
    "title": "Service Bureau",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/1-imt/servicebureau/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud",
    "content": "Persona As the SRE hat suits you let’s keep it on as you have been asked to build a custom Service Health Dashboard for the wire-transfer-service. The requirement is to display RED metrics, logs and Synthetic test duration results.\nIt is common for development and SRE teams to require a summary of the health of their applications and/or services. More often or not these are displayed on wall-mounted TVs. Splunk Observability Cloud has the perfect solution for this by creating custom dashboards.\nIn this section we are going to build a Service Health Dashboard we can use to display on teams’ monitors or TVs.",
    "description": "In this section, you will learn how to build a custom Service Health Dashboard to monitor the health of your services.",
    "tags": [],
    "title": "Custom Service Health Dashboard 🏥",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/6-custom-dashboard/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Horizontal Pod Autoscaling",
    "content": "In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), to automatically scale the workload to match demand.\nHorizontal scaling means that the response to increased load is to deploy more Pods. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that are already running for the workload.\nIf the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.\n1. Setup HPA Inspect the ~/workshop/k3s/hpa.yaml file and validate the contents using the following command:\ncat ~/workshop/k3s/hpa.yaml This file contains the configuration for the Horizontal Pod Autoscaler and will create a new HPA for the php-apache deployment.\napiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: php-apache namespace: apache spec: maxReplicas: 4 metrics: - type: Resource resource: name: cpu target: averageUtilization: 50 type: Utilization - type: Resource resource: name: memory target: averageUtilization: 75 type: Utilization minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: StatefulSet name: php-apache Once deployed, php-apache will autoscale when either the average CPU usage goes above 50% or the average memory usage for the deployment goes above 75%, with a minimum of 1 pod and a maximum of 4 pods.\nkubectl apply -f ~/workshop/k3s/hpa.yaml 2. Validate HPA kubectl get hpa -n apache Go to the Workloads or Node Detail tab in Kubernetes and check the HPA deployment.\nWorkshop Question How many additional php-apache-x pods have been created?\nWorkshop Question Which metrics in the Apache Navigator have significantly increased again?\n3. Increase the HPA replica count Increase the maxReplicas to 8\nkubectl edit hpa php-apache -n apache Save the changes you have made. (Hint: Use Esc followed by :wq! to save your changes).\nWorkshop Questions How many pods are now running?\nHow many are pending?\nWhy are they pending?\nCongratulations! You have completed the workshop.",
    "description": "In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), to automatically scale the workload to match demand.\nHorizontal scaling means that the response to increased load is to deploy more Pods. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that are already running for the workload.\nIf the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.",
    "tags": [],
    "title": "Setup Horizontal Pod Autoscaling (HPA)",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/2-hpa/6-setup-hpa/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "Persona You are a back-end developer and you have been called in to help investigate an issue found by the SRE. The SRE has identified a poor user experience and has asked you to investigate the issue.\nDiscover the power of full end-to-end visibility by jumping from a RUM trace (front-end) to an APM trace (back-end). All the services are sending telemetry (traces and spans) that Splunk Observability Cloud can visualize, analyze and use to detect anomalies and errors.\nRUM and APM are two sides of the same coin. RUM is the client-side view of the application and APM is the server-side view. In this section, we will use APM to drill down and identify where the problem is.",
    "description": "In this section, we will use APM to drill down and identify where the problem is.",
    "tags": [],
    "title": "Splunk APM",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/6-apm/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Solving Problems with O11y Cloud",
    "content": "This workshop provided hands-on experience with the following concepts:\nHow to deploy the Splunk Distribution of the OpenTelemetry Collector using Helm. How instrument an application with OpenTelemetry. How to capture tags of interest from your application using an OpenTelemetry SDK. How to index tags in Splunk Observability Cloud using Troubleshooting MetricSets. How to utilize tags in Splunk Observability Cloud to find “unknown unknowns” using the Tag Spotlight and Dynamic Service Map features. Collecting tags aligned with the best practices shared in this workshop will let you get even more value from the data you’re sending to Splunk Observability Cloud. Now that you’ve completed this workshop, you have the knowledge you need to start collecting tags from your own applications!\nTo get started with capturing tags today, check out how to add tags in various supported languages, and then how to use them to create Troubleshooting MetricSets so they can be analyzed in Tag Spotlight. For more help, feel free to ask a Splunk Expert.\nAnd to see how other languages and environments are instrumented with OpenTelemetry, explore the Splunk OpenTelemetry Examples GitHub repository.\nTip for Workshop Facilitator(s) Once the workshop is complete, remember to delete the APM MetricSet you created earlier for the credit.score.category tag.",
    "description": "This workshop provided hands-on experience with the following concepts:\nHow to deploy the Splunk Distribution of the OpenTelemetry Collector using Helm. How instrument an application with OpenTelemetry. How to capture tags of interest from your application using an OpenTelemetry SDK. How to index tags in Splunk Observability Cloud using Troubleshooting MetricSets. How to utilize tags in Splunk Observability Cloud to find “unknown unknowns” using the Tag Spotlight and Dynamic Service Map features. Collecting tags aligned with the best practices shared in this workshop will let you get even more value from the data you’re sending to Splunk Observability Cloud. Now that you’ve completed this workshop, you have the knowledge you need to start collecting tags from your own applications!",
    "tags": [],
    "title": "Summary",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/9-solving-problems-with-o11y-cloud/6-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Lambda Tracing and Kinesis",
    "content": "Navigate back to APM in Splunk Observabilty Cloud\nGo back to your Service Dependency map.\nWorkshop Question Notice the difference?\nYou should be able to see the consumer-lambda now clearly connected to the producer-lambda.\nRemember the value you copied from your producer logs? You can run sls logs -f consumer command again on your EC2 lab host to fetch one.\nTake that value, and paste it into trace search:\nClick on Go and you should be able to find the logged Trace:\nNotice that the Trace ID is something that makes up the trace context that we propagated.\nYou can read up on the two common propagation standards:\nW3C: https://www.w3.org/TR/trace-context/#traceparent-header B3: https://github.com/openzipkin/b3-propagation#overall-process Workshop Question Which one are we using?\nIt should be self-explanatory from the Propagator we are creating in the Functions\nWorkshop Question Bonus Question: What happens if we mix and match the W3C and B3 headers?\nExpand the consumer-lambda span.\nWorkshop Question Can you find the attributes from your message?",
    "description": "Navigate back to APM in Splunk Observabilty Cloud\nGo back to your Service Dependency map.\nWorkshop Question Notice the difference?\nYou should be able to see the consumer-lambda now clearly connected to the producer-lambda.\nRemember the value you copied from your producer logs? You can run sls logs -f consumer command again on your EC2 lab host to fetch one.",
    "tags": [],
    "title": "Updated Lambdas in Splunk APM",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/6-lambda-kinesis/6-updated-in-splunk/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Lambda Tracing",
    "content": "In order to see the result of our context propagation outside of the logs, we’ll once again consult the Splunk APM UI.\nView your Lambda Functions in the Splunk APM Service Map Let’s take a look at the Service Map for our environment in APM once again.\nIn Splunk Observability Cloud:\nClick on the APM Button in the Main Menu.\nSelect your APM Environment from the Environment: dropdown.\nClick the Service Map Button on the right side of the APM Overview page. This will take you to your Service Map view.\nNote Reminder: It may take a few minutes for your traces to appear in Splunk APM. Try hitting refresh on your browser until you find your environment name in the list of environments.\nWorkshop Question Notice the difference?\nYou should be able to see the producer-lambda and consumer-lambda functions linked by the propagated context this time! Explore a Lambda Trace by Trace ID Next, we will take another look at a trace related to our Environment.\nPaste the Trace ID you copied from the consumer function’s logs into the View Trace ID search box under Traces and click Go Note The Trace ID was a part of the trace context that we propagated.\nYou can read up on two of the most common propagation standards:\nW3C B3 Workshop Question Which one are we using?\nThe Splunk Distribution of Opentelemetry JS, which supports our NodeJS functions, defaults to the W3C standard Workshop Question Bonus Question: What happens if we mix and match the W3C and B3 headers?\nClick on the consumer-lambda span.\nWorkshop Question Can you find the attributes from your message?\nClean Up We are finally at the end of our workshop. Kindly clean up after yourself!\nKill the send_message If the send_message.py script is still running, stop it with the follwing commands:\nfg This brings your background process to the foreground. Next you can hit [CONTROL-C] to kill the process. Destroy all AWS resources Terraform is great at managing the state of our resources individually, and as a deployment. It can even update deployed resources with any changes to their definitions. But to start afresh, we will destroy the resources and redeploy them as part of the manual instrumentation portion of this workshop.\nPlease follow these steps to destroy your resources:\nEnsure you are in the manual directory:\npwd The expected output would be ~/workshop/lambda/manual If you are not in the manual directory, run the following command:\ncd ~/workshop/lambda/manual Destroy the Lambda functions and other AWS resources you deployed earlier:\nterraform destroy respond yes when you see the Enter a value: prompt This will result in the resources being destroyed, leaving you with a clean environment",
    "description": "In order to see the result of our context propagation outside of the logs, we’ll once again consult the Splunk APM UI.\nView your Lambda Functions in the Splunk APM Service Map Let’s take a look at the Service Map for our environment in APM once again.\nIn Splunk Observability Cloud:\nClick on the APM Button in the Main Menu.\nSelect your APM Environment from the Environment: dropdown.\nClick the Service Map Button on the right side of the APM Overview page. This will take you to your Service Map view.",
    "tags": [],
    "title": "Splunk APM, Lambda Functions and Traces, Again!",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/6-lambda-kinesis/6-updated-lambdas/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop",
    "content": "Using Tag Spotlight Now that we’ve indexed the credit.score.category tag, we can use it with Tag Spotlight to troubleshoot our application.\nNavigate to APM then click on Tag Spotlight on the right-hand side. Ensure the creditcheckservice service is selected from the Service drop-down (if not already selected).\nWith Tag Spotlight, we can see 100% of credit score requests that result in a score of impossible have an error, yet requests for all other credit score types have no errors at all!\nThis illustrates the power of Tag Spotlight! Finding this pattern would be time-consuming without it, as we’d have to manually look through hundreds of traces to identify the pattern (and even then, there’s no guarantee we’d find it).\nWe’ve looked at errors, but what about latency? Let’s click on Latency near the top of the screen to find out.\nHere, we can see that the requests with a poor credit score request are running slowly, with P50, P90, and P99 times of around 3 seconds, which is too long for our users to wait, and much slower than other requests.\nWe can also see that some requests with an exceptional credit score request are running slowly, with P99 times of around 5 seconds, though the P50 response time is relatively quick.\nUsing Dynamic Service Maps Now that we know the credit score category associated with the request can impact performance and error rates, let’s explore another feature that utilizes indexed tags: Dynamic Service Maps.\nWith Dynamic Service Maps, we can breakdown a particular service by a tag. For example, let’s click on APM, then click Explore to view the service map.\nClick on creditcheckservice. Then, on the right-hand menu, click on the drop-down that says Breakdown, and select the credit.score.category tag.\nAt this point, the service map is updated dynamically, and we can see the performance of requests hitting creditcheckservice broken down by the credit score category:\nThis view makes it clear that performance for good and fair credit scores is excellent, while poor and exceptional scores are much slower, and impossible scores result in errors.\nSummary Tag Spotlight has uncovered several interesting patterns for the engineers that own this service to explore further:\nWhy are all the impossible credit score requests resulting in error? Why are all the poor credit score requests running slowly? Why do some of the exceptional requests run slowly? As an SRE, passing this context to the engineering team would be extremely helpful for their investigation, as it would allow them to track down the issue much more quickly than if we simply told them that the service was “sometimes slow”.\nIf you’re curious, have a look at the source code for the creditprocessorservice. You’ll see that requests with impossible, poor, and exceptional credit scores are handled differently, thus resulting in the differences in error rates and latency that we uncovered.\nThe behavior we saw with our application is typical for modern cloud-native applications, where different inputs passed to a service lead to different code paths, some of which result in slower performance or errors. For example, in a real credit check service, requests resulting in low credit scores may be sent to another downstream service to further evaluate risk, and may perform more slowly than requests resulting in higher scores, or encounter higher error rates.",
    "description": "Using Tag Spotlight Now that we’ve indexed the credit.score.category tag, we can use it with Tag Spotlight to troubleshoot our application.\nNavigate to APM then click on Tag Spotlight on the right-hand side. Ensure the creditcheckservice service is selected from the Service drop-down (if not already selected).\nWith Tag Spotlight, we can see 100% of credit score requests that result in a score of impossible have an error, yet requests for all other credit score types have no errors at all!",
    "tags": [],
    "title": "Use Tags for Troubleshooting",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/6-use-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e RUM",
    "content": "Look into the Metrics views for the various endpoints and use the Tags sent via the Tag spotlight for deeper analysis 1. Find an url for the Cart endpoint From the RUM Overview page, please select the url for the Cart endpoint to dive deeper into the information available for this endpoint.\nOnce you have selected and clicked on the blue url, you will find yourself in the Tag Spotlight overview\nHere you will see all of the tags that have been sent to Splunk RUM as part of the RUM traces. The tags displayed will be relevant to the overview that you have selected. These are generic Tags created automatically when the Trace was sent, and additional Tags you have added to the trace as part of the configuration of your website.\nAdditional Tags We are already sending two additional tags, you have seen them defined in the Beacon url that was added to your website: app: \"[nodename]-store\", environment: \"[nodename]-workshop\" in the first section of this workshop! You can add additional tags in a similar way.\nIn our example we have selected the Page Load view as shown here:\nYou can select any of the following Tag views, each focused on a specific metric.\n2. Explore the information in the Tag Spotlight view The Tag spotlight is designed to help you identify problems, either through the chart view,, where you may quickly identify outliers or via the TAGs.\nIn the Page Load view, if you look at the Browser, Browser Version \u0026 OS Name Tag views,you can see the various browser types and versions, as well as for the underlying OS.\nThis makes it easy to identify problems related to specific browser or OS versions, as they would be highlighted.\nIn the above example you can see that Firefox had the slowest response, various Browser versions ( Chrome) that have different response times and the slow response of the Android devices.\nA further example are the regional Tags that you can use to identify problems related to ISP or locations etc. Here you should be able to find the location you have been using to access the Online Boutique. Drill down by selecting the town or country you are accessing the Online Boutique from by clicking on the name as shown below (City of Amsterdam):\nThis will select only the sessions relevant to the city selected as shown below:\nBy selecting the various Tag you build up a filter, you can see the current selection below\nTo clear the filter and see every trace click on Clear All at the top right of the page.\nIf the overview page is empty or shows , no traces have been received in the selected timeslot. You need to increase the time window at the top left. You can start with the Last 12 hours for example.\nYou can then use your mouse to select the time slot you want like show in the view below and activate that time filter by clicking on the little spyglass icon.",
    "description": "Look into the Metrics views for the various endpoints and use the Tags sent via the Tag spotlight for deeper analysis 1. Find an url for the Cart endpoint From the RUM Overview page, please select the url for the Cart endpoint to dive deeper into the information available for this endpoint.\nOnce you have selected and clicked on the blue url, you will find yourself in the Tag Spotlight overview",
    "tags": [],
    "title": "Analyzing RUM Tags in the Tag Spotlight view",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/2-rum/6-tag-spotlight/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "Look into the Metrics views for the various endpoints and use the Tags sent via the Tag spotlight for deeper analysis 1. Find an url for the Cart endpoint From the RUM Overview page, please select the url for the Cart endpoint to dive deeper into the information available for this endpoint.\nOnce you have selected and clicked on the blue url, you will find yourself in the Tag Spotlight overview\nHere you will see all of the tags that have been sent to Splunk RUM as part of the RUM traces. The tags displayed will be relevant to the overview that you have selected. These are generic Tags created automatically when the Trace was sent, and additional Tags you have added to the trace as part of the configuration of your website.\nAdditional Tags We are already sending two additional tags, you have seen them defined in the Beacon url that was added to your website: app: \"[nodename]-store\", environment: \"[nodename]-workshop\" in the first section of this workshop! You can add additional tags in a similar way.\nIn our example we have selected the Document Load Latency view as shown here:\nYou can select any of the following Tag views, each focused on a specific metric.\n2. Explore the information in the Tag Spotlight view The Tag spotlight is designed to help you identify problems, either through the chart view,, where you may quickly identify outliers or via the TAGs.\nIn the Document Load Latency view, if you look at the Browser, Browser Version \u0026 OS Name Tag views,you can see the various browser types and versions, as well as for the underlying OS.\nThis makes it easy to identify problems related to specific browser or OS versions, as they would be highlighted.\nIn the above example you can see that Firefox had the slowest response, various Browser versions ( Chrome) that have different response times and the slow response of the Android devices.\nA further example are the regional Tags that you can use to identify problems related to ISP or locations etc. Here you should be able to find the location you have been using to access the Online Boutique. Drill down by selecting the town or country you are accessing the Online Boutique from by clicking on the name as shown below (City of Amsterdam):\nThis will select only the traces relevant to the city selected as shown below:\nBy selecting the various Tag you build up a filter, you can see the current selection below\nTo clear the filter and see every trace click on Clear All at the top right of the page.\nIf the overview page is empty or shows , no traces have been received in the selected timeslot. You need to increase the time window at the top left. You can start with the Last 12 hours for example.\nYou can then use your mouse to select the time slot you want like show in the view below and activate that time filter by clicking on the little spyglass icon.",
    "description": "Look into the Metrics views for the various endpoints and use the Tags sent via the Tag spotlight for deeper analysis 1. Find an url for the Cart endpoint From the RUM Overview page, please select the url for the Cart endpoint to dive deeper into the information available for this endpoint.\nOnce you have selected and clicked on the blue url, you will find yourself in the Tag Spotlight overview",
    "tags": [],
    "title": "6. Analyzing RUM Tags in the Tag Spotlight view",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/6-tag-spotlight/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "This workshop will equip you to build a distributed trace for a small serverless application that runs on AWS Lambda, producing and consuming a message via AWS Kinesis.\nFirst, we will see how OpenTelemetry’s auto-instrumentation captures traces and exports them to your target of choice.\nThen, we will see how we can enable context propagation with manual instrumentation.\nFor this workshop Splunk has prepared an Ubuntu Linux instance in AWS/EC2 all pre-configured for you. To get access to that instance, please visit the URL provided by the workshop leader.",
    "description": "This workshop will enable you to build a distributed trace for a small serverless application that runs on AWS Lambda, producing and consuming a message via AWS Kinesis",
    "tags": [],
    "title": "Distributed Tracing for AWS Lambda Functions",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/6-lambda-kinesis/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops",
    "content": "This workshop will equip you with how a distributed trace is constructed for a small serverless application that runs on AWS Lambda, producing and consuming a message via AWS Kinesis.\nWe will see how auto-instrumentation works with manual steps to force a Producer function’s context to be sent to Consumer function via a Record put on a Kinesis stream.\nFor this workshop Splunk has prepared an Ubuntu Linux instance in AWS/EC2 all pre-configured for you.\nTo get access to the instance that you will be using in the workshop, please visit the URL provided by the workshop leader.d",
    "description": "This workshop will equip you with how a distributed trace is constructed for a small serverless application that runs on AWS Lambda, producing and consuming a message via AWS Kinesis",
    "tags": [],
    "title": "Build a Distributed Trace in Lambda and Kinesis",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/6-lambda-kinesis/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences",
    "content": "In this workshop, we learned the following:\nHow to create simple synthetic tests so that we can quickly begin to understand the availability and performance of our application How to understand what RUM shows us about the end user experience, including specific user sessions How to write advanced synthetic browser tests to proactively test our most important user actions How to visualize our frontend performance data in context with events on dashboards How to set up detectors so we don’t have to wait to hear about issues from our end users How all of the above, plus Splunk and Google’s resources, helps us optimize end user experience There is a lot more we can do with front end performance monitoring. If you have extra time, be sure to play with the charts, detectors, and do some more synthetic testing. Remember our resources such as Lantern, Splunk Docs, and experiment with apps for Mobile RUM.\nThis is just the beginning! If you need more time to trial Splunk Observability, or have any other questions, reach out to a Splunk Expert.",
    "description": "In this workshop, we learned the following:\nHow to create simple synthetic tests so that we can quickly begin to understand the availability and performance of our application How to understand what RUM shows us about the end user experience, including specific user sessions How to write advanced synthetic browser tests to proactively test our most important user actions How to visualize our frontend performance data in context with events on dashboards How to set up detectors so we don’t have to wait to hear about issues from our end users How all of the above, plus Splunk and Google’s resources, helps us optimize end user experience There is a lot more we can do with front end performance monitoring. If you have extra time, be sure to play with the charts, detectors, and do some more synthetic testing. Remember our resources such as Lantern, Splunk Docs, and experiment with apps for Mobile RUM.",
    "tags": [],
    "title": "Summary",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/6-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Splunk Synthetic Scripting \u003e 1. Real Browser Test",
    "content": "In the Scatterplot from the previous step, click on one of the dots to drill into the test run data. Preferably, select the most recent test run (farthest to the right).",
    "description": "In the Scatterplot from the previous step, click on one of the dots to drill into the test run data. Preferably, select the most recent test run (farthest to the right).",
    "tags": [],
    "title": "1.7 View test results",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/4-synthetics-scripting/1-real-browser-test/7-view-test-results/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "In this section, we will explore how to use the Filter Processor to selectively drop spans based on certain conditions.\nSpecifically, we will drop traces based on the span name, which is commonly used to filter out unwanted spans such as health checks or internal communication traces. In this case, we will be filtering out spans whose name is \"/_healthz\", typically associated with health check requests and usually are quite “noisy”.\nExercise Inside the [WORKSHOP] directory, create a new subdirectory named 5-dropping-spans. Next, copy *.yaml from the 4-resilience directory into 5-dropping-spans. Important Change ALL terminal windows to the [WORKSHOP]/5-dropping-spans directory.\nYour updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Next, we will configure the filter processor and the respective pipelines.",
    "description": "In this section, we will explore how to use the Filter Processor to selectively drop spans based on certain conditions.\nSpecifically, we will drop traces based on the span name, which is commonly used to filter out unwanted spans such as health checks or internal communication traces. In this case, we will be filtering out spans whose name is \"/_healthz\", typically associated with health check requests and usually are quite “noisy”.",
    "tags": [],
    "title": "5. Dropping Spans",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/5-dropping-spans/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "The Transform Processor lets you modify telemetry data—logs, metrics, and traces—as it flows through the pipeline. Using the OpenTelemetry Transformation Language (OTTL), you can filter, enrich, and transform data on the fly without touching your application code.\nIn this exercise we’ll update gateway.yaml to include a Transform Processor that will:\nFilter log resource attributes. Parse JSON structured log data into attributes. Set log severity levels based on the log message body. You may have noticed that in previous logs, fields like SeverityText and SeverityNumber were undefined. This is typical of the filelog receiver. However, the severity is embedded within the log body e.g.:\nSeverityText: SeverityNumber: Unspecified(0) Body: Str(2025-01-31 15:49:29 [WARN] - Do or do not, there is no try.) Logs often contain structured data encoded as JSON within the log body. Extracting these fields into attributes allows for better indexing, filtering, and querying. Instead of manually parsing JSON in downstream systems, OTTL enables automatic transformation at the telemetry pipeline level.\nExercise Important Change ALL terminal windows to the 5-transform-data directory and run the clear command.\nCopy *.yaml from the 4-sensitve-data directory into 5-transform-data. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml",
    "description": "The Transform Processor lets you modify telemetry data—logs, metrics, and traces—as it flows through the pipeline. Using the OpenTelemetry Transformation Language (OTTL), you can filter, enrich, and transform data on the fly without touching your application code.\nIn this exercise we’ll update gateway.yaml to include a Transform Processor that will:\nFilter log resource attributes. Parse JSON structured log data into attributes. Set log severity levels based on the log message body. You may have noticed that in previous logs, fields like SeverityText and SeverityNumber were undefined. This is typical of the filelog receiver. However, the severity is embedded within the log body e.g.:",
    "tags": [],
    "title": "5. Transform Data",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/5-transform-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "The Transform Processor lets you modify telemetry data—logs, metrics, and traces—as it flows through the pipeline. Using the OpenTelemetry Transformation Language (OTTL), you can filter, enrich, and transform data on the fly without touching your application code.\nIn this exercise we’ll update gateway.yaml to include a Transform Processor that will:\nFilter log resource attributes. Parse JSON structured log data into attributes. Set log severity levels based on the log message body. You may have noticed that in previous logs, fields like SeverityText and SeverityNumber were undefined. This is typical of the filelog receiver. However, the severity is embedded within the log body e.g.:\nSeverityText: SeverityNumber: Unspecified(0) Body: Str(2025-01-31 15:49:29 [WARN] - Do or do not, there is no try.) Logs often contain structured data encoded as JSON within the log body. Extracting these fields into attributes allows for better indexing, filtering, and querying. Instead of manually parsing JSON in downstream systems, OTTL enables automatic transformation at the telemetry pipeline level.\nExercise Important Change ALL terminal windows to the 5-transform-data directory and run the clear command.\nCopy *.yaml from the 4-sensitve-data directory into 5-transform-data. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml",
    "description": "The Transform Processor lets you modify telemetry data—logs, metrics, and traces—as it flows through the pipeline. Using the OpenTelemetry Transformation Language (OTTL), you can filter, enrich, and transform data on the fly without touching your application code.\nIn this exercise we’ll update gateway.yaml to include a Transform Processor that will:\nFilter log resource attributes. Parse JSON structured log data into attributes. Set log severity levels based on the log message body. You may have noticed that in previous logs, fields like SeverityText and SeverityNumber were undefined. This is typical of the filelog receiver. However, the severity is embedded within the log body e.g.:",
    "tags": [],
    "title": "5. Transform Data",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/5-transform-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "As we have seen in the previous chapter, you can trace your interactions between the various services using APM without touching your code, allowing you to identify issues faster.\nIn addition to tracing, automatic discovery and configuration offers additional features out of the box that can help you find issues even faster! In this section we are going to look at two of them:\nAlways-on Profiling and Java Metrics Database Query Performance If you want to dive deeper into Always-on Profiling or DB-Query performance, we have a separate Ninja Workshop called Debug Problems in Microservices that you can follow.",
    "description": "As we have seen in the previous chapter, you can trace your interactions between the various services using APM without touching your code, allowing you to identify issues faster.\nIn addition to tracing, automatic discovery and configuration offers additional features out of the box that can help you find issues even faster! In this section we are going to look at two of them:\nAlways-on Profiling and Java Metrics Database Query Performance If you want to dive deeper into Always-on Profiling or DB-Query performance, we have a separate Ninja Workshop called Debug Problems in Microservices that you can follow.",
    "tags": [],
    "title": "Always-On Profiling \u0026 DB Query Performance",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/6-profiling-db-query/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e RUM",
    "content": "Dive into RUM Session information in the RUM UI Identify Javascript errors in the Span of an user interaction 1. Drill down in the Sessions After you have analyzed the information and drilled down via the Tag Spotlight to a subset of the traces, you can view the actual session as it was run by the end-user’s browser.\nYou do this by clicking on the link User Sessions as shown below:\nThis will give you a list of sessions that matched both the time filter and the subset selected in the Tag Profile.\nSelect one by clicking on the session ID, It is a good idea to select one that has the longest duration (preferably over 700 ms).\nOnce you have selected the session, you will be taken to the session details page. As you are selecting a specific action that is part of the session, you will likely arrive somewhere in the middle of the session, at the moment of the interaction.\nYou can see the URL that you selected earlier is where we are focusing on in the waterfall.\nScroll down a little bit on the page, so you see the end of the operation as shown below.\nYou can see that we have received a few Javascript Console errors that may not have been detected or visible to the end users. To examine these in more detail click on the middle one that says: *Cannot read properties of undefined (reading ‘Prcie’)\nThis will cause the page to expand and show the Span detail for this interaction, It will contain a detailed error.stack you can pass on the developer to solve the issue. You may have noticed when buying in the Online Boutique that the final total always was $0.00.",
    "description": "Dive into RUM Session information in the RUM UI Identify Javascript errors in the Span of an user interaction 1. Drill down in the Sessions After you have analyzed the information and drilled down via the Tag Spotlight to a subset of the traces, you can view the actual session as it was run by the end-user’s browser.\nYou do this by clicking on the link User Sessions as shown below:",
    "tags": [],
    "title": "Analyzing RUM Sessions",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/2-rum/7-rum-sessions/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "Dive into RUM Session information in the RUM UI Identify Javascript errors in the Span of an user interaction 1. Again select the cart URL After you have focussed the time slot with the time selector, you need to reselect the cart url from Url Name view, as shown below:\nIn the example above we selected http://34.246.124.162:81/cart\n2. Drill down in the Sessions After you have analyzed the information and drilled down via the Tag Spotlight to a subset of the traces, you can view the actual session as it was run by the end-user’s browser.\nYou do this by clicking on the link Example Sessions as shown below:\nThis will give you a list of sessions that matched both the time filter and the subset selected in the Tag Profile.\nSelect one by clicking on the session ID, It is a good idea to select one that has the longest duration (preferably over 700 ms).\nOnce you have selected the session, you will be taken to the session details page. As you are selecting a specific action that is part of the session, you will likely arrive somewhere in the middle of the session, at the moment of the interaction.\nYou can see the URL http://34.246.124.162:81/cart, the one you selected earlier, is where we are focusing on in the session stream.\nScroll down a little bit on the page, so you see the end of the operation as shown below.\nYou can see that we have received a few Javascript Console errors that may not have been detected or visible to the end users. To examine these in more detail click on the middle one that says: *Cannot read properties of undefined (reading ‘Prcie’)\nThis will cause the page to expand and show the Span detail for this interaction, It will contain a detailed error.stack you can pass on the developer to solve the issue. You may have noticed when buying in the Online Boutique that the final total always was $0.00.",
    "description": "Dive into RUM Session information in the RUM UI Identify Javascript errors in the Span of an user interaction 1. Again select the cart URL After you have focussed the time slot with the time selector, you need to reselect the cart url from Url Name view, as shown below:\nIn the example above we selected http://34.246.124.162:81/cart\n2. Drill down in the Sessions After you have analyzed the information and drilled down via the Tag Spotlight to a subset of the traces, you can view the actual session as it was run by the end-user’s browser.",
    "tags": [],
    "title": "7. Analyzing RUM Sessions",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/7-rum-sessions/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Lambda Tracing",
    "content": "Congratulations on finishing the Lambda Tracing Workshop! You have seen how we can complement auto-instrumentation with manual steps to have the producer-lambda function’s context be sent to the consumer-lambda function via a record in a Kinesis stream. This allowed us to build the expected Distributed Trace, and to contextualize the relationship between both functions in Splunk APM.\nYou can now build out a trace manually by linking two different functions together. This comes in handy when your auto-instrumentation, or 3rd-party systems, do not support context propagation out of the box, or when you wish to add custom attributes to a trace for more relevant trace analaysis.",
    "description": "Congratulations on finishing the Lambda Tracing Workshop! You have seen how we can complement auto-instrumentation with manual steps to have the producer-lambda function’s context be sent to the consumer-lambda function via a record in a Kinesis stream. This allowed us to build the expected Distributed Trace, and to contextualize the relationship between both functions in Splunk APM.",
    "tags": [],
    "title": "Conclusion",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/6-lambda-kinesis/7-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Finding the Problem How did we get here? How did the 999 end up in the trace as a Custom Attribute?\nTake a look at the function signature\nprivate void myCoolFunction234234234(@SpanAttribute(\"myInt\") int myInt) @SpanAttribute(\"myint\") is an OpenTelemetry Annotation that was added by our Java Otel Annotator tool.\nFixing the code Change this: private void myCoolFunction234234234(@SpanAttribute(\"myInt\") int myInt) { // Generate a FAST sleep of 0 time ! Random sleepy = new Random(); try{ if (999==myInt) Thread.sleep( sleepy.nextInt(5000 - 3000) + 3000); } catch (Exception e){ to this: private void myCoolFunction234234234(@SpanAttribute(\"myInt\") int myInt) { // Generate a FAST sleep of 0 time ! Random sleepy = new Random(); try{ // if (999==myInt) // Thread.sleep( // sleepy.nextInt(5000 - 3000) // + 3000); } catch (Exception e){ which is basically placing comments (//) before the lines in myCoolFunction234234234 that are causing the slowness:\n// if (999==myInt) // Thread.sleep( // sleepy.nextInt(5000 - 3000) // + 3000); Save the changes: [CTRL]-o [Enter] Exit: [CTRL]-x Note Until we rebuild and restart our application this change isn’t implemented.\nFind Other Issues Let’s go see if our manual instrumentation uncovered any other issues we did not see before\nSo you may be asking yourself: “How does manual instrumentation alone show us “more problems” than before? Latency is latency, isn’t it?”\nThe answer is with auto-instrumentation you are in most situations NOT covering functions our customers’ development teams wrote, which is of course the bulk of what developers do. They write functions…. and of course this is where the bulk of software problems occur.\nYou may have noticed a new exception in our trace that was not present with Auto-Instrumentation during our latency fix use-case. Since we skipped over this, let’s walk you through it.\nReturn to the service map Click on shop service Click Traces (on the right side) Click Errors Only Click on a trace with an error present We can see our exception is InvalidLocaleException.\nThe real problem must be related to the new data associated with Sri Lanka as the Exception says “Non English Characters found in Instrument Data”.\nThis exception had not surfaced in previous traces based on ONLY Auto-Instrumentation because the method where it was thrown was NOT covered with Auto-Instrumentation.\nOnce we completed the Manual Instrumentation via the Otel Annotator, this method was instrumented and we can now see we had a buried Exception being thrown.\nFixing more code We already know exactly what file to look in and what method to look at as it is called out in the trace.\nUsing nano: nano shop/src/main/java/com/shabushabu/javashop/shop/model/Instrument.java Search in Nano: [CTRL]-w Enter in: buildForLocale [Enter] Look just above the buildForLocale function.\nNotice the Annotation @WithSpan? @WithSpan is an OpenTelemetry Annotation for Java that automatically generates a span around a the function that follows.\n@WithSpan public Instrument buildForLocale( @SpanAttribute(\"id\") long id, ... @SpanAttribute(\"seller_type\") ... this.id= id; Now let’s fix this code. We are going to simply comment this out for now and see if it fixes our latency issue.\nChange this: if (!isEnglish(title)) { throw new InvalidLocaleException(\"Non English Characters found in Instrument Data\"); } else { System.out.println(\"Characters OK \"); } to this (comment out if block): // if (!isEnglish(title)) { // throw new InvalidLocaleException(\"Non English Characters found in Instrument Data\"); // } else { // System.out.println(\"Characters OK \"); // } Save the changes: [CTRL]-o [Enter] Exit: [CTRL]-x",
    "description": "Finding the Problem How did we get here? How did the 999 end up in the trace as a Custom Attribute?\nTake a look at the function signature\nprivate void myCoolFunction234234234(@SpanAttribute(\"myInt\") int myInt) @SpanAttribute(\"myint\") is an OpenTelemetry Annotation that was added by our Java Otel Annotator tool.\nFixing the code Change this: private void myCoolFunction234234234(@SpanAttribute(\"myInt\") int myInt) { // Generate a FAST sleep of 0 time ! Random sleepy = new Random(); try{ if (999==myInt) Thread.sleep( sleepy.nextInt(5000 - 3000) + 3000); } catch (Exception e){ to this: private void myCoolFunction234234234(@SpanAttribute(\"myInt\") int myInt) { // Generate a FAST sleep of 0 time ! Random sleepy = new Random(); try{ // if (999==myInt) // Thread.sleep( // sleepy.nextInt(5000 - 3000) // + 3000); } catch (Exception e){ which is basically placing comments (//) before the lines in myCoolFunction234234234 that are causing the slowness:",
    "tags": [],
    "title": "Fixing Code",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/7-fixing-code/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "Recap of Part 1 of the Workshop At this point in the workshop, we’ve successfully:\nDeployed the Splunk distribution of the OpenTelemetry Collector on our Linux Host Configured it to send traces and metrics to Splunk Observability Cloud Deployed a .NET application and instrumented it with OpenTelemetry Dockerized the .NET application and ensured traces are flowing to o11y cloud If you haven’t completed the steps listed above, please execute the following commands before proceeding with the remainder of the workshop:\ncp /home/splunk/workshop/docker-k8s-otel/docker/Dockerfile /home/splunk/workshop/docker-k8s-otel/helloworld/ cp /home/splunk/workshop/docker-k8s-otel/docker/entrypoint.sh /home/splunk/workshop/docker-k8s-otel/helloworld/ IMPORTANT once these files are copied, open /home/splunk/workshop/docker-k8s-otel/helloworld/Dockerfile\nwith an editor and replace $INSTANCE in your Dockerfile with your instance name, which can be determined by running echo $INSTANCE.\nIntroduction to Part 2 of the Workshop In the next part of the workshop, we want to run the application in Kubernetes, so we’ll need to deploy the Splunk distribution of the OpenTelemetry Collector in our Kubernetes cluster.\nLet’s define some key terms first.\nKey Terms What is Kubernetes? “Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.”\nSource: https://kubernetes.io/docs/concepts/overview/\nWe’ll deploy the Docker image we built earlier for our application into our Kubernetes cluster, after making a small modification to the Dockerfile.\nWhat is Helm? Helm is a package manager for Kubernetes.\n“It helps you define, install, and upgrade even the most complex Kubernetes application.”\nSource: https://helm.sh/\nWe’ll use Helm to deploy the OpenTelemetry collector in our K8s cluster.\nBenefits of Helm Manage Complexity deal with a single values.yaml file rather than dozens of manifest files Easy Updates in-place upgrades Rollback support Just use helm rollback to roll back to an older version of a release Uninstall the Host Collector Before moving forward, let’s remove the collector we installed earlier on the Linux host:\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh \u003e /tmp/splunk-otel-collector.sh; sudo sh /tmp/splunk-otel-collector.sh --uninstall Install the Collector using Helm Let’s use the command line rather than the in-product wizard to create our own helm command to install the collector.\nWe first need to add the helm repo:\nhelm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart And ensure the repo is up-to-date:\nhelm repo update To configure the helm chart deployment, let’s create a new file named values.yaml in the /home/splunk directory:\n# swith to the /home/splunk dir cd /home/splunk # create a values.yaml file in vi vi values.yaml Press ‘i’ to enter into insert mode in vi before pasting the text below.\nThen paste the following contents:\nlogsEngine: otel agent: config: receivers: hostmetrics: collection_interval: 10s root_path: /hostfs scrapers: cpu: null disk: null filesystem: exclude_mount_points: match_type: regexp mount_points: - /var/* - /snap/* - /boot/* - /boot - /opt/orbstack/* - /mnt/machines/* - /Users/* load: null memory: null network: null paging: null processes: null To save your changes in vi, press the esc key to enter command mode, then type :wq! followed by pressing the enter/return key.\nNow we can use the following command to install the collector:\n​ Script Example Output helm install splunk-otel-collector --version 0.132.0 \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-cluster\" \\ --set=\"environment=otel-$INSTANCE\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ -f values.yaml \\ splunk-otel-collector-chart/splunk-otel-collector NAME: splunk-otel-collector LAST DEPLOYED: Fri Dec 20 01:01:43 2024 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Observability realm us1. Confirm the Collector is Running We can confirm whether the collector is running with the following command:\n​ Script Example Output kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-agent-8xvk8 1/1 Running 0 49s splunk-otel-collector-k8s-cluster-receiver-d54857c89-tx7qr 1/1 Running 0 49s Confirm your K8s Cluster is in O11y Cloud In Splunk Observability Cloud, navigate to Infrastructure -\u003e Kubernetes -\u003e Kubernetes Clusters, and then search for your cluster name (which is $INSTANCE-cluster):",
    "description": "Recap of Part 1 of the Workshop At this point in the workshop, we’ve successfully:\nDeployed the Splunk distribution of the OpenTelemetry Collector on our Linux Host Configured it to send traces and metrics to Splunk Observability Cloud Deployed a .NET application and instrumented it with OpenTelemetry Dockerized the .NET application and ensured traces are flowing to o11y cloud If you haven’t completed the steps listed above, please execute the following commands before proceeding with the remainder of the workshop:",
    "tags": [],
    "title": "Install the OpenTelemetry Collector in K8s",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/7-install-collector-k8s/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "Persona Remaining in your back-end developer role, you need to inspect the logs from your application to determine the root cause of the issue.\nUsing the content related to the APM trace (logs) we will now use Splunk Log Observer to drill down further to understand exactly what the problem is.\nRelated Content is a powerful feature that allows you to jump from one component to another and is available for metrics, traces and logs.",
    "description": "In this section, we will use Log Observer to drill down and identify what the problem is.",
    "tags": [],
    "title": "Splunk Log Observer",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/7-log-observer/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Lambda Tracing and Kinesis",
    "content": "Before you Go Please kindly clean up your lab using the following command:\n​ Remove Functions sls remove Conclusion Congratuations on finishing the lab. You have seen how we complement auto-instrumentation with manual steps to force Producer function’s context to be sent to Consumer function via a Record put on a Kinesis stream. This allowed us to build the expected Distributed Trace.\nYou can now build out a Trace manually by linking two different functions together. This is very powerful when your auto-instrumenation, or third-party systems, do not support context propagation out of the box.",
    "description": "Before you Go Please kindly clean up your lab using the following command:\n​ Remove Functions sls remove Conclusion Congratuations on finishing the lab. You have seen how we complement auto-instrumentation with manual steps to force Producer function’s context to be sent to Consumer function via a Record put on a Kinesis stream. This allowed us to build the expected Distributed Trace.",
    "tags": [],
    "title": "Summary",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/6-lambda-kinesis/7-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop",
    "content": "Earlier, we created a Troubleshooting Metric Set on the credit.score.category tag, which allowed us to use Tag Spotlight with that tag and identify a pattern to explain why some users received a poor experience.\nIn this section of the workshop, we’ll explore a related concept: Monitoring MetricSets.\nWhat are Monitoring MetricSets? Monitoring MetricSets go beyond troubleshooting and allow us to use tags for alerting, dashboards and SLOs.\nCreate a Monitoring MetricSet (note: your workshop instructor will do the following for you, but observe the steps)\nLet’s navigate to Settings -\u003e APM MetricSets, and click the edit button (i.e. the little pencil) beside the MetricSet for credit.score.category.\nCheck the box beside Also create Monitoring MetricSet then click Start Analysis\nThe credit.score.category tag appears again as a Pending MetricSet. After a few moments, a checkmark should appear. Click this checkmark to enable the Pending MetricSet.\nUsing Monitoring MetricSets This mechanism creates a new dimension from the tag on a bunch of metrics that can be used to filter these metrics based on the values of that new dimension. Important: To differentiate between the original and the copy, the dots in the tag name are replaced by underscores for the new dimension. With that the metrics become a dimension named credit_score_category and not credit.score.category.\nNext, let’s explore how we can use this Monitoring MetricSet.",
    "description": "Earlier, we created a Troubleshooting Metric Set on the credit.score.category tag, which allowed us to use Tag Spotlight with that tag and identify a pattern to explain why some users received a poor experience.\nIn this section of the workshop, we’ll explore a related concept: Monitoring MetricSets.\nWhat are Monitoring MetricSets? Monitoring MetricSets go beyond troubleshooting and allow us to use tags for alerting, dashboards and SLOs.\nCreate a Monitoring MetricSet (note: your workshop instructor will do the following for you, but observe the steps)",
    "tags": [],
    "title": "Use Tags for Monitoring",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/7-alerting-dashboards-slos/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences \u003e Advanced Synthetics",
    "content": "1. Click into a spike or failure in your test run results.\n2. What can you learn about this test run? If it failed, use the error message, filmstrip, video replay, and waterfall to understand what happened.\n3. What do you see in the resources? Make sure to click through all of the page (or transaction) tabs.\nWorkshop Question Do you see anything interesting? Common issues to find and fix include: unexpected response codes, duplicate requests, forgotten third parties, large or slow files, and long gaps between requests.\nWant to learn more about specific performance improvements? Google and Mozilla have great resources to help understand what goes into frontend performance as well as in-depth details of how to optimize it.",
    "description": "1. Click into a spike or failure in your test run results.\n2. What can you learn about this test run? If it failed, use the error message, filmstrip, video replay, and waterfall to understand what happened.",
    "tags": [],
    "title": "View test results",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/3-advanced-synthetics/7-view-test-results/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts",
    "content": "Splunk Observability Cloud Now that we have configured the OpenTelemetry Collector to send metrics to Splunk Observability Cloud, let’s take a look at the data in Splunk Observability Cloud. If you have not received an invite to Splunk Observability Cloud, your instructor will provide you with login credentials.\nBefore that, let’s make things a little more interesting and run a stress test on the instance. This in turn will light up the dashboards.\nsudo apt install stress while true; do stress -c 2 -t 40; stress -d 5 -t 40; stress -m 20 -t 40; done Once you are logged into Splunk Observability Cloud, using the left-hand navigation, navigate to Dashboards from the main menu. This will take you to the Teams view. At the top of this view click on All Dashboards :\nIn the search box, search for OTel Contrib:\nInfo If the dashboard does not exist, then your instructor will be able to quickly add it. If you are not attending a Splunk hosted version of this workshop then the Dashboard Group to import can be found at the bottom of this page.\nClick on the OTel Contrib Dashboard dashboard to open it, next click in the Participant Name box, at the top of the dashboard, and select the name you configured for participant.name in the config.yaml in the drop-down list or start typing the name to search for it:\nYou can now see the host metrics for the host upon which you configured the OpenTelemetry Collector.\nDownload Dashboard Group JSON for importing OpenTelemetry-Contrib-Dashboard-Group.json (40 KB)",
    "description": "Splunk Observability Cloud Now that we have configured the OpenTelemetry Collector to send metrics to Splunk Observability Cloud, let’s take a look at the data in Splunk Observability Cloud. If you have not received an invite to Splunk Observability Cloud, your instructor will provide you with login credentials.\nBefore that, let’s make things a little more interesting and run a stress test on the instance. This in turn will light up the dashboards.",
    "tags": [],
    "title": "Data Visualisations",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/7-visualisation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud",
    "content": "Congratulations, you have completed the Splunk4Rookies - Observability Cloud Workshop. Today, you have become familiar with how to use Splunk Observability Cloud to monitor your applications and infrastructure.\nCelebrate your achievement by adding this certificate to your LinkedIn profile.\nLet’s recap what we have learned and what you can do next.",
    "description": "Congratulations, you have completed the Splunk4Rookies - Observability Cloud Workshop. Today, you have become familiar with how to use Splunk Observability Cloud to monitor your applications and infrastructure.",
    "tags": [],
    "title": "Workshop Wrap-up 🎁",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/7-wrap-up/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "Welcome to the workshop on Dashboards in the Splunk Observability Cloud. This session is designed to help you get hands-on experience with building insightful visualizations.\nWe’ll be working with existing demo data available in the Splunk Observability Suite. You can follow along using any trial or production Splunk Observability organization that you have access to.\nWhat You’ll Learn Dashboards In the first part of the workshop, we’ll focus on dashboards and charts:\nUnderstanding dashboards and the role of charts Editing existing charts and creating new ones Applying filters and using analytical functions Building and customizing formulas Saving charts into a dashboard for reuse An introduction to SignalFlow for advanced charting",
    "description": "Dashboard",
    "tags": [],
    "title": "Dashboard Workshop",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/7-dashboards-detectors/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops",
    "content": "This workshop will:\nFor this workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2 all pre-configured for you.\nTo get access to the instance that you will be using in the workshop, please visit the URL provided by the workshop leader.\nSplunk Observability Cloud You must have the ability to send traces to Splunk Observability Cloud. If you do not have an account already, please start a trial here: https://www.splunk.com/en_us/download/apm-free-trial.html\nWorkshop Story Here at Splunk Instruments, our business is expanding!\nWe have recently added 3 new locations: Two locations in the USA (Colorado and Chicago) and one international location in Sri Lanka.\nOur technical staff has already on-boarded the data from these new locations and incorporated them into our inventory application and it is our job to review these improvements and send any issues back to our developers for repairs.\nWe now have a total of 6 locations!",
    "description": "This workshop offers a method for automatically adding annotations to your application.",
    "tags": [],
    "title": "Improving MTTR with Custom Tags",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "The Routing Connector in OpenTelemetry is a powerful feature that allows you to direct data (traces, metrics, or logs) to different pipelines/destinations based on specific criteria. This is especially useful in scenarios where you want to apply different processing or exporting logic to subsets of your telemetry data.\nFor example, you might want to send production data to one exporter while directing test or development data to another. Similarly, you could route certain spans based on their attributes, such as service name, environment, or span name, to apply custom processing or storage logic.\nExercise Important Change ALL terminal windows to the 6-routing-data directory and run the clear command.\nCopy *.yaml from the 5-transform-data directory into 6-routing-data. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Next, we will configure the routing connector and the respective pipelines.",
    "description": "The Routing Connector in OpenTelemetry is a powerful feature that allows you to direct data (traces, metrics, or logs) to different pipelines/destinations based on specific criteria. This is especially useful in scenarios where you want to apply different processing or exporting logic to subsets of your telemetry data.\nFor example, you might want to send production data to one exporter while directing test or development data to another. Similarly, you could route certain spans based on their attributes, such as service name, environment, or span name, to apply custom processing or storage logic.",
    "tags": [],
    "title": "6. Routing Data",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/6-routing-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "The Routing Connector in OpenTelemetry is a powerful feature that allows you to direct data (traces, metrics, or logs) to different pipelines/destinations based on specific criteria. This is especially useful in scenarios where you want to apply different processing or exporting logic to subsets of your telemetry data.\nFor example, you might want to send production data to one exporter while directing test or development data to another. Similarly, you could route certain spans based on their attributes, such as service name, environment, or span name, to apply custom processing or storage logic.\nExercise Important Change ALL terminal windows to the 6-routing-data directory and run the clear command.\nCopy *.yaml from the 5-transform-data directory into 6-routing-data. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Next, we will configure the routing connector and the respective pipelines.",
    "description": "The Routing Connector in OpenTelemetry is a powerful feature that allows you to direct data (traces, metrics, or logs) to different pipelines/destinations based on specific criteria. This is especially useful in scenarios where you want to apply different processing or exporting logic to subsets of your telemetry data.\nFor example, you might want to send production data to one exporter while directing test or development data to another. Similarly, you could route certain spans based on their attributes, such as service name, environment, or span name, to apply custom processing or storage logic.",
    "tags": [],
    "title": "6. Routing Data",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/6-routing-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "In this section, you’ll learn how to configure the OpenTelemetry Collector to remove specific tags and redact sensitive data from telemetry spans. This is crucial for protecting sensitive information such as credit card numbers, personal data, or other security-related details that must be anonymized before being processed or exported.\nWe’ll walk through configuring key processors in the OpenTelemetry Collector, including:\nAttributes Processor: Modifies or removes specific span attributes. Redaction Processor: Ensures sensitive data is sanitized before being stored or transmitted. Exercise Inside the [WORKSHOP] directory, create a new subdirectory named 6-sensitive-data. Next, copy *.yaml from the 5-dropping-spans directory into 6-sensitive-data. Important Change ALL terminal windows to the [WORKSHOP]/6-sensitive-data directory.\nYour updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml",
    "description": "In this section, you’ll learn how to configure the OpenTelemetry Collector to remove specific tags and redact sensitive data from telemetry spans. This is crucial for protecting sensitive information such as credit card numbers, personal data, or other security-related details that must be anonymized before being processed or exported.\nWe’ll walk through configuring key processors in the OpenTelemetry Collector, including:\nAttributes Processor: Modifies or removes specific span attributes. Redaction Processor: Ensures sensitive data is sanitized before being stored or transmitted. Exercise Inside the [WORKSHOP] directory, create a new subdirectory named 6-sensitive-data. Next, copy *.yaml from the 5-dropping-spans directory into 6-sensitive-data. Important Change ALL terminal windows to the [WORKSHOP]/6-sensitive-data directory.",
    "tags": [],
    "title": "6. Redacting Sensitive Data",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/6-sensitive-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "Up until this point, there have been no code changes, yet tracing, profiling and Database Query Performance data is being sent to Splunk Observability Cloud.\nNext, we will work with Splunk Log Observer to obtain log data from the Spring PetClinic application.\nThe Splunk OpenTelemetry Collector automatically collects logs from the Spring PetClinic application and sends them to Splunk Observability Cloud using the OTLP exporter, annotating the log events with trace_id, span_id and trace flags.\nSplunk Log Observer is then used to view the logs, automatically correlating log information with services and traces.\nThis feature is called Related Content.",
    "description": "Up until this point, there have been no code changes, yet tracing, profiling and Database Query Performance data is being sent to Splunk Observability Cloud.\nNext, we will work with Splunk Log Observer to obtain log data from the Spring PetClinic application.\nThe Splunk OpenTelemetry Collector automatically collects logs from the Spring PetClinic application and sends them to Splunk Observability Cloud using the OTLP exporter, annotating the log events with trace_id, span_id and trace flags.",
    "tags": [],
    "title": "Log Observer",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/7-log-observer-connect/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "Continue with the RUM Session information in the RUM UI See correlated APM traces and logs in the APM \u0026 Log Observer UI 1. Finding backend service issues Click on the to close the Span view. Now continue to scroll down and find the POST /cart/checkout line.\nClick on the blue link, this should pop up a dialog showing information on the backend services that were part of the checkout action taken by the end user.\nIn this popup, there are multiple sections available, providing you with a quick insight in the behavior of your backend services. For example the Performance Summary section will tell you where the time was spent during the backend call.\nIn the above example you can see that more than 77,9% was spent in external services.\nIf you scroll down to the bottom of the dialog, you can see the complete Trace and Services section like shown below:\nin the Services map, you can see two services flagged red, the Checkout Service and the Payment Service in both in dark red. Light red means it received an error and dark red means an error originated from that service.\nSo already it is obvious there is a problem in the back end services.\nLet’s investigate!\n2. Follow the Trace to the Backend service You can now click on the Trace Id link:\nThis will bring you to the Waterfall APM view that will show you what occurred in detail in a call to the backend services. On the right you see the Trace Id: and again the Performance Summary, as we saw before. In the waterfall, you can identify the various backend services that were part of this call from the frontend.\nAs you can see there are red error indicators before the Checkout Service and the Payment Service.\nClick on the after the paymentservice: grpc.hipstershop.PaymentService/Charge line.\nThis will open the span detail page to show you the detailed information about this service call. You wil see that the call returned a 401 error code or Invalid Request.\n3. Use the Related Content - Logs As the Splunk Observability cloud suite correlates trace metrics and logs automatically, the system will show you in the related content bar at the bottom of the page, the corresponding logs for this trace.\nClick on the Log link to see the logs.\nWhen the logs are shown, notice that the filter at the top of the page contains the logs for the trace. Next select one of the lines indicating an error for the payment service. This should open the log message on the right.\nIt clearly shows the reason why the payment service fails: we are using an invalid token towards the service:\n*Failed payment processing through ButtercupPayments: Invalid API Token (test-20e26e90-356b-432e-a2c6-956fc03f5609)\n4. Conclusion In the workshop, you have seen how to add RUM functionality to you website. We investigate the performance of your Website using RUM Metrics. Using the Tag profile, you have searched for your own session, and with the session waterfall, you identified two problems:\nA Java script error that caused your price calculation to be zero. An issue in the payment backend service that caused payments to fail. Using the ability to correlate RUM traces with the Backend APM trace and Logs, you have found the reason for the payment failure.\nThis concludes the RUM workshop.",
    "description": "Continue with the RUM Session information in the RUM UI See correlated APM traces and logs in the APM \u0026 Log Observer UI 1. Finding backend service issues Click on the to close the Span view. Now continue to scroll down and find the POST /cart/checkout line.\nClick on the blue link, this should pop up a dialog showing information on the backend services that were part of the checkout action taken by the end user.",
    "tags": [],
    "title": "8. Correlate between Splunk RUM and APM backend services",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/8-apm-correlation/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "Update the Dockerfile With Kubernetes, environment variables are typically managed in the .yaml manifest files rather than baking them into the Docker image. So let’s remove the following two environment variables from the Dockerfile:\nvi /home/splunk/workshop/docker-k8s-otel/helloworld/Dockerfile Then remove the following two environment variables:\nENV OTEL_SERVICE_NAME=helloworld ENV OTEL_RESOURCE_ATTRIBUTES='deployment.environment=otel-$INSTANCE' To save your changes in vi, press the esc key to enter command mode, then type :wq! followed by pressing the enter/return key.\nBuild a new Docker Image Let’s build a new Docker image that excludes the environment variables:\ncd /home/splunk/workshop/docker-k8s-otel/helloworld docker build -t helloworld:1.2 . Note: we’ve used a different version (1.2) to distinguish the image from our earlier version. To clean up the older versions, run the following command to get the container id:\ndocker ps -a Then run the following command to delete the container:\ndocker rm \u003cold container id\u003e --force Now we can get the container image id:\ndocker images | grep 1.1 Finally, we can run the following command to delete the old image:\ndocker image rm \u003cold image id\u003e Import the Docker Image to Kubernetes Normally we’d push our Docker image to a repository such as Docker Hub. But for this session, we’ll use a workaround to import it to k3s directly.\ncd /home/splunk # Export the image from docker docker save --output helloworld.tar helloworld:1.2 # Import the image into k3s sudo k3s ctr images import helloworld.tar Deploy the .NET Application Hint: To enter edit mode in vi, press the ‘i’ key. To save changes, press the esc key to enter command mode, then type :wq! followed by pressing the enter/return key.\nTo deploy our .NET application to K8s, let’s create a file named deployment.yaml in /home/splunk:\nvi /home/splunk/deployment.yaml And paste in the following:\napiVersion: apps/v1 kind: Deployment metadata: name: helloworld spec: selector: matchLabels: app: helloworld replicas: 1 template: metadata: labels: app: helloworld spec: containers: - name: helloworld image: docker.io/library/helloworld:1.2 imagePullPolicy: Never ports: - containerPort: 8080 env: - name: PORT value: \"8080\" What is a Deployment in Kubernetes? The deployment.yaml file is a kubernetes config file that is used to define a deployment resource. This file is the cornerstone of managing applications in Kubernetes! The deployment config defines the deployment’s desired state and Kubernetes then ensures the actual state matches it. This allows application pods to self-heal and also allows for easy updates or roll backs to applications.\nThen, create a second file in the same directory named service.yaml:\nvi /home/splunk/service.yaml And paste in the following:\napiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: type: ClusterIP selector: app: helloworld ports: - port: 8080 protocol: TCP What is a Service in Kubernetes? A Service in Kubernetes is an abstraction layer, working like a middleman, giving you a fixed IP address or DNS name to access your Pods, which stays the same, even if Pods are added, removed, or replaced over time.\nWe can then use these manifest files to deploy our application:\n​ Script Example Output # create the deployment kubectl apply -f deployment.yaml # create the service kubectl apply -f service.yaml deployment.apps/helloworld created service/helloworld created Test the Application To access our application, we need to first get the IP address:\n​ Script Example Output kubectl describe svc helloworld | grep IP: IP: 10.43.102.103 Then we can access the application by using the Cluster IP that was returned from the previous command. For example:\ncurl http://10.43.102.103:8080/hello/Kubernetes Configure OpenTelemetry The .NET OpenTelemetry instrumentation was already baked into the Docker image. But we need to set a few environment variables to tell it where to send the data.\nAdd the following to deployment.yaml file you created earlier:\nIMPORTANT replace $INSTANCE in the YAML below with your instance name, which can be determined by running echo $INSTANCE.\nenv: - name: PORT value: \"8080\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP - name: OTEL_EXPORTER_OTLP_ENDPOINT value: \"http://$(NODE_IP):4318\" - name: OTEL_SERVICE_NAME value: \"helloworld\" - name: OTEL_RESOURCE_ATTRIBUTES value: \"deployment.environment=otel-$INSTANCE\" The complete deployment.yaml file should be as follows (with your instance name rather than $INSTANCE):\napiVersion: apps/v1 kind: Deployment metadata: name: helloworld spec: selector: matchLabels: app: helloworld replicas: 1 template: metadata: labels: app: helloworld spec: containers: - name: helloworld image: docker.io/library/helloworld:1.2 imagePullPolicy: Never ports: - containerPort: 8080 env: - name: PORT value: \"8080\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP - name: OTEL_EXPORTER_OTLP_ENDPOINT value: \"http://$(NODE_IP):4318\" - name: OTEL_SERVICE_NAME value: \"helloworld\" - name: OTEL_RESOURCE_ATTRIBUTES value: \"deployment.environment=otel-$INSTANCE\" Apply the changes with:\n​ Script Example Output kubectl apply -f deployment.yaml deployment.apps/helloworld configured Then use curl to generate some traffic.\nAfter a minute or so, you should see traces flowing in the o11y cloud. But, if you want to see your trace sooner, we have …\nA Challenge For You If you are a developer and just want to quickly grab the trace id or see console feedback, what environment variable could you add to the deployment.yaml file?\nClick here to see the answer If you recall in our challenge from Section 4, Instrument a .NET Application with OpenTelemetry, we showed you a trick to write traces to the console using the OTEL_TRACES_EXPORTER environment variable. We can add this variable to our deployment.yaml, redeploy our application, and tail the logs from our helloworld app so that we can grab the trace id to then find the trace in Splunk Observability Cloud. (In the next section of our workshop, we will also walk through using the debug exporter, which is how you would typically debug your application in a K8s environment.)\nFirst, open the deployment.yaml file in vi:\nvi deployment.yaml Then, add the OTEL_TRACES_EXPORTER environment variable:\nenv: - name: PORT value: \"8080\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP - name: OTEL_EXPORTER_OTLP_ENDPOINT value: \"http://$(NODE_IP):4318\" - name: OTEL_SERVICE_NAME value: \"helloworld\" - name: OTEL_RESOURCE_ATTRIBUTES value: \"deployment.environment=YOURINSTANCE\" # NEW VALUE HERE: - name: OTEL_TRACES_EXPORTER value: \"otlp,console\" Save your changes then redeploy the application:\n​ Script Example Output kubectl apply -f deployment.yaml deployment.apps/helloworld configured Tail the helloworld logs:\n​ Script Example Output kubectl logs -l app=helloworld -f info: HelloWorldController[0] /hello endpoint invoked by K8s9 Activity.TraceId: 5bceb747cc7b79a77cfbde285f0f09cb Activity.SpanId: ac67afe500e7ad12 Activity.TraceFlags: Recorded Activity.ActivitySourceName: Microsoft.AspNetCore Activity.DisplayName: GET hello/{name?} Activity.Kind: Server Activity.StartTime: 2025-02-04T15:22:48.2381736Z Activity.Duration: 00:00:00.0027334 Activity.Tags: server.address: 10.43.226.224 server.port: 8080 http.request.method: GET url.scheme: http url.path: /hello/K8s9 network.protocol.version: 1.1 user_agent.original: curl/7.81.0 http.route: hello/{name?} http.response.status_code: 200 Resource associated with Activity: splunk.distro.version: 1.8.0 telemetry.distro.name: splunk-otel-dotnet telemetry.distro.version: 1.8.0 os.type: linux os.description: Debian GNU/Linux 12 (bookworm) os.build_id: 6.2.0-1018-aws os.name: Debian GNU/Linux os.version: 12 host.name: helloworld-69f5c7988b-dxkwh process.owner: app process.pid: 1 process.runtime.description: .NET 8.0.12 process.runtime.name: .NET process.runtime.version: 8.0.12 container.id: 39c2061d7605d8c390b4fe5f8054719f2fe91391a5c32df5684605202ca39ae9 telemetry.sdk.name: opentelemetry telemetry.sdk.language: dotnet telemetry.sdk.version: 1.9.0 service.name: helloworld deployment.environment: otel-jen-tko-1b75 Then, in your other terminal window, generate a trace with your curl command. You will see the trace id in the console in which you are tailing the logs. Copy the Activity.TraceId: value and paste it into the Trace search field in APM.",
    "description": "Update the Dockerfile With Kubernetes, environment variables are typically managed in the .yaml manifest files rather than baking them into the Docker image. So let’s remove the following two environment variables from the Dockerfile:\nvi /home/splunk/workshop/docker-k8s-otel/helloworld/Dockerfile Then remove the following two environment variables:\nENV OTEL_SERVICE_NAME=helloworld ENV OTEL_RESOURCE_ATTRIBUTES='deployment.environment=otel-$INSTANCE' To save your changes in vi, press the esc key to enter command mode, then type :wq! followed by pressing the enter/return key.",
    "tags": [],
    "title": "Deploy Application to K8s",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/8-deploy-app-k8s/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts",
    "content": "Developing a custom component Building a component for the Open Telemetry Collector requires three key parts:\nThe Configuration - What values are exposed to the user to configure The Factory - Make the component using the provided values The Business Logic - What the component needs to do For this, we will use the example of building a component that works with Jenkins so that we can track important DevOps metrics of our project(s).\nThe metrics we are looking to measure are:\nLead time for changes - “How long it takes for a commit to get into production” Change failure rate - “The percentage of deployments causing a failure in production” Deployment frequency - “How often a [team] successfully releases to production” Mean time to recover - “How long does it take for a [team] to recover from a failure in production” These indicators were identified Google’s DevOps Research and Assesment (DORA)[^1] team to help show performance of a software development team. The reason for choosing Jenkins CI is that we remain in the same Open Source Software ecosystem which we can serve as the example for the vendor managed CI tools to adopt in future.\nInstrument Vs Component There is something to consider when improving level of Observability within your organisation since there are some trade offs that get made.\nPros Cons (Auto) Instrumented Does not require an external API to be monitored in order to observe the system. Changing instrumentation requires changes to the project. Gives system owners/developers to make changes in their observability. Requires additional runtime dependancies. Understands system context and can corrolate captured data with Exemplars. Can impact performance of the system. Component - Changes to data names or semantics can be rolled out independently of the system’s release cycle. Breaking API changes require a coordinated release between system and collector. Updating/extending data collected is a seemless user facing change. Captured data semantics can unexpectedly break that does not align with a new system release. Does not require the supporting teams to have a deep understanding of observability practice. Strictly external / exposed information can be surfaced from the system.",
    "description": "Developing a custom component Building a component for the Open Telemetry Collector requires three key parts:\nThe Configuration - What values are exposed to the user to configure The Factory - Make the component using the provided values The Business Logic - What the component needs to do For this, we will use the example of building a component that works with Jenkins so that we can track important DevOps metrics of our project(s).",
    "tags": [],
    "title": "OpenTelemetry Collector Development",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/8-develop/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Rebuild and Deploy Run ./BuildAndDeploy.sh Wait a few minutes . . . Return to the service map If you do NOT see RED in your service map, you have completed the Latency Repair for the Colorado Location!\nNow let’s check for our exception in the traces.\nClick on shop service Click Traces (on the right side) Click Errors Only If you do not have red in your service map and you do not see Errors in traces, you have successfully completed our Inventory application review for Sri Lanka and Colorado locations.\nWell done!",
    "description": "Rebuild and Deploy Run ./BuildAndDeploy.sh Wait a few minutes . . . Return to the service map If you do NOT see RED in your service map, you have completed the Latency Repair for the Colorado Location!\nNow let’s check for our exception in the traces.\nClick on shop service Click Traces (on the right side) Click Errors Only If you do not have red in your service map and you do not see Errors in traces, you have successfully completed our Inventory application review for Sri Lanka and Colorado locations.",
    "tags": [],
    "title": "Rebuild and Deploy",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/8-rebuild-and-deploy/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "Persona Putting your SRE hat back on, you have been asked to set up monitoring for the Online Boutique. You need to ensure that the application is available and performing well 24 hours a day, 7 days a week.\nWouldn’t it be great if we could have 24/7 monitoring of our application, and be alerted when there is a problem? This is where Synthetics comes in. We will show you a simple test that runs every 1 minute and checks the performance and availability of a typical user journey through the Online Boutique.",
    "description": "In this section, you will learn how to use Splunk Synthetics to monitor the performance and availability of your applications.",
    "tags": [],
    "title": "Splunk Synthetics",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/8-synthetics/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Debug Problems in Microservices \u003e Tagging Workshop",
    "content": "In this workshop, we learned the following:\nWhat are tags and why are they such a critical part of making an application observable? How to use OpenTelemetry to capture tags of interest from your application. How to index tags in Splunk Observability Cloud and the differences between Troubleshooting MetricSets and Monitoring MetricSets. How to utilize tags in Splunk Observability Cloud to find “unknown unknowns” using the Tag Spotlight and Dynamic Service Map features. How to utilize tags for dashboards, alerting and service level objectives. Collecting tags aligned with the best practices shared in this workshop will let you get even more value from the data you’re sending to Splunk Observability Cloud. Now that you’ve completed this workshop, you have the knowledge you need to start collecting tags from your own applications!\nTo get started with capturing tags today, check out how to add tags in various supported languages, and then how to use them to create Troubleshooting MetricSets so they can be analyzed in Tag Spotlight. For more help, feel free to ask a Splunk Expert.\nTip for Workshop Facilitator(s) Once the workshop is complete, remember to delete the APM MetricSet you created earlier for the credit.score.category tag.",
    "description": "In this workshop, we learned the following:\nWhat are tags and why are they such a critical part of making an application observable? How to use OpenTelemetry to capture tags of interest from your application. How to index tags in Splunk Observability Cloud and the differences between Troubleshooting MetricSets and Monitoring MetricSets. How to utilize tags in Splunk Observability Cloud to find “unknown unknowns” using the Tag Spotlight and Dynamic Service Map features. How to utilize tags for dashboards, alerting and service level objectives. Collecting tags aligned with the best practices shared in this workshop will let you get even more value from the data you’re sending to Splunk Observability Cloud. Now that you’ve completed this workshop, you have the knowledge you need to start collecting tags from your own applications!",
    "tags": [],
    "title": "Summary",
    "uri": "/observability-workshop/v6.5/en/scenarios/debug-problems/tagging/8-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops",
    "content": "During this technical workshop, you will learn how to:\nEfficiently deploy complex environments Capture metrics from these environments to Splunk Observability Cloud Auto-instrument a Python application Enable OS logging to Splunk Enterprise via Universal Forwarder To simplify the workshop modules, a pre-configured AWS EC2 instance is provided.\nBy the end of this technical workshop, you will have an approach to demonstrating metrics collection for complex environments and services.",
    "description": "Learn how to get data into Splunk Observability Cloud with OpenTelemetry and the Splunk Universal Forwarder.",
    "tags": [],
    "title": "Getting Data In (GDI) with OTel and UF",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/8-gdi/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "In this workshop, you’ll get hands-on experience with the following:\nPractice deploying the collector and instrumenting a .NET application with the Splunk distribution of OpenTelemetry .NET in Linux and Kubernetes environments. Practice “dockerizing” a .NET application, running it in Docker, and then adding Splunk OpenTelemetry instrumentation. Practice deploying the Splunk distro of the collector in a K8s environment using Helm. Then customize the collector config and troubleshoot an issue. The workshop uses a simple .NET application to illustrate these concepts. Let’s get started!\nTip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "By the end of this workshop you'll have gotten hands-on experience instrumenting a .NET application with OpenTelemetry, then Dockerizing the application and deploying it to Kubernetes.  You’ll also gain experience deploying the OpenTelemetry collector using Helm, customizing the collector configuration, and troubleshooting collector configuration issues.",
    "tags": [],
    "title": "Hands-On OpenTelemetry, Docker, and K8s",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "In this section, we’ll explore how to use the Count Connector to extract attribute values from logs and convert them into meaningful metrics.\nSpecifically, we’ll use the Count Connector to track the number of “Star Wars” and “Lord of the Rings” quotes appearing in our logs, turning them into measurable data points.\nExercise Important Change ALL terminal windows to the 7-sum-count directory and run the clear command.\nCopy *.yaml from the 6-routing-data directory into 7-sum-count. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Update the agent.yaml to change the frequency that we read logs. Find the filelog/quotes receiver in the agent.yaml and add a poll_interval attribute: filelog/quotes: # Receiver Type/Name poll_interval: 10s # Only read every ten seconds The reason for the delay is that the Count Connector in the OpenTelemetry Collector counts logs only within each processing interval. This means that every time the data is read, the count resets to zero for the next interval. With the default Filelog reciever interval of 200ms, it reads every line the loadgen writes, giving us counts of 1. With this interval we make sure we have multiple entries to count.\nThe Collector can maintain a running count for each read interval by omitting conditions, as shown below. However, it’s best practice to let your backend handle running counts since it can track them over a longer time period.\nExercise Add the Count Connector Include the Count Connector in the connector’s section of your configuration and define the metrics counters we want to use:\nconnectors: count: logs: logs.full.count: description: \"Running count of all logs read in interval\" logs.sw.count: description: \"StarWarsCount\" conditions: - attributes[\"movie\"] == \"SW\" logs.lotr.count: description: \"LOTRCount\" conditions: - attributes[\"movie\"] == \"LOTR\" logs.error.count: description: \"ErrorCount\" conditions: - attributes[\"level\"] == \"ERROR\" Explanation of the Metrics Counters\nlogs.full.count: Tracks the total number of logs processed during each read interval.\nSince this metric has no filtering conditions, every log that passes through the system is included in the count. logs.sw.count Counts logs that contain a quote from a Star Wars movie. logs.lotr.count: Counts logs that contain a quote from a Lord of the Rings movie. logs.error.count: Represents a real-world scenario by counting logs with a severity level of ERROR for the read interval. Configure the Count Connector in the pipelines\nIn the pipeline configuration below, the connector exporter is added to the logs section, while the connector receiver is added to the metrics section.\npipelines: traces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes - redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp metrics: receivers: - count # Count Connector that receives count metric from logs count exporter in logs pipeline. - otlp #- hostmetrics # Host Metrics Receiver processors: - memory_limiter - resourcedetection - resource/add_mode - batch exporters: - debug - otlphttp logs: receivers: - otlp - filelog/quotes processors: - memory_limiter - resourcedetection - resource/add_mode - transform/logs # Transform logs processor - batch exporters: - count # Count Connector that exports count as a metric to metrics pipeline. - debug - otlphttp We count logs based on their attributes. If your log data is stored in the log body instead of attributes, you’ll need to use a Transform processor in your pipeline to extract key/value pairs and add them as attributes.\nIn this workshop, we’ve already added merge_maps(attributes, cache, \"upsert\") in the 05-transform-data section. This ensures that all relevant data is included in the log attributes for processing.\nWhen selecting fields to create attributes from, be mindful—adding all fields indiscriminately is generally not ideal for production environments. Instead, choose only the fields that are truly necessary to avoid unnecessary data clutter.\nExercise Validate the agent configuration using otelbin.io. For reference, the logs and metrics: sections of your pipelines will look like this: %%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(otlp\u003cbr\u003efa:fa-download):::receiver REC2(filelog\u003cbr\u003efa:fa-download\u003cbr\u003equotes):::receiver REC3(otlp\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO5(batch\u003cbr\u003efa:fa-microchip):::processor PRO6(batch\u003cbr\u003efa:fa-microchip):::processor PRO7(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO8(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO9(transfrom\u003cbr\u003efa:fa-microchip\u003cbr\u003elogs):::processor EXP1(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026emsp;\u0026emsp;otlphttp\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload):::exporter EXP3(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP4(\u0026emsp;\u0026emsp;otlphttp\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload):::exporter ROUTE1(\u0026nbsp;count\u0026nbsp;\u003cbr\u003efa:fa-route):::con-export ROUTE2(\u0026nbsp;count\u0026nbsp;\u003cbr\u003efa:fa-route):::con-receive %% Links subID1:::sub-logs subID2:::sub-metrics subgraph \" \" direction LR subgraph subID1[**Logs**] direction LR REC1 --\u003e PRO1 REC2 --\u003e PRO1 PRO1 --\u003e PRO7 PRO7 --\u003e PRO3 PRO3 --\u003e PRO9 PRO9 --\u003e PRO5 PRO5 --\u003e ROUTE1 PRO5 --\u003e EXP1 PRO5 --\u003e EXP2 end subgraph subID2[**Metrics**] direction LR ROUTE1 --\u003e ROUTE2 ROUTE2 --\u003e PRO2 REC3 --\u003e PRO2 PRO2 --\u003e PRO8 PRO8 --\u003e PRO4 PRO4 --\u003e PRO6 PRO6 --\u003e EXP3 PRO6 --\u003e EXP4 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3; classDef sub-metrics stroke:#38bdf8,stroke-width:1px, color:#38bdf8,stroke-dasharray: 3 3;",
    "description": "In this section, we’ll explore how to use the Count Connector to extract attribute values from logs and convert them into meaningful metrics.\nSpecifically, we’ll use the Count Connector to track the number of “Star Wars” and “Lord of the Rings” quotes appearing in our logs, turning them into measurable data points.\nExercise Important Change ALL terminal windows to the 7-sum-count directory and run the clear command.",
    "tags": [],
    "title": "7. Create metrics with Count Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/7-sum-count/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "The Transform Processor lets you modify telemetry data—logs, metrics, and traces—as it flows through the pipeline. Using the OpenTelemetry Transformation Language (OTTL), you can filter, enrich, and transform data on the fly without touching your application code.\nIn this exercise we’ll update agent.yaml to include a Transform Processor that will:\nFilter log resource attributes. Parse JSON structured log data into attributes. Set log severity levels based on the log message body. You may have noticed that in previous logs, fields like SeverityText and SeverityNumber were undefined. This is typical of the filelog receiver. However, the severity is embedded within the log body:\n\u003csnip\u003e SeverityText: SeverityNumber: Unspecified(0) Body: Str(2025-01-31 15:49:29 [WARN] - Do or do not, there is no try.) \u003c/snip\u003e Logs often contain structured data encoded as JSON within the log body. Extracting these fields into attributes allows for better indexing, filtering, and querying. Instead of manually parsing JSON in downstream systems, OTTL enables automatic transformation at the telemetry pipeline level.\nExercise Inside the [WORKSHOP] directory, create a new subdirectory named 7-transform-data. Next, copy *.yaml from the 6-sensitve-data directory into 7-transform-data. Important Change ALL terminal windows to the [WORKSHOP]/7-transform-data directory.\nYour updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml",
    "description": "The Transform Processor lets you modify telemetry data—logs, metrics, and traces—as it flows through the pipeline. Using the OpenTelemetry Transformation Language (OTTL), you can filter, enrich, and transform data on the fly without touching your application code.\nIn this exercise we’ll update agent.yaml to include a Transform Processor that will:\nFilter log resource attributes. Parse JSON structured log data into attributes. Set log severity levels based on the log message body. You may have noticed that in previous logs, fields like SeverityText and SeverityNumber were undefined. This is typical of the filelog receiver. However, the severity is embedded within the log body:",
    "tags": [],
    "title": "7. Transform Data",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/7-transform-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk .conf25 Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Wrap-up",
    "uri": "/observability-workshop/v6.5/en/conf/1-advanced-collector/9-wrap-up/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "To enable Real User Monitoring (RUM) instrumentation for an application, you need to add the Open Telemetry Javascript https://github.com/signalfx/splunk-otel-js-web snippet to the code base.\nThe Spring PetClinic application uses a single index HTML page, that is reused across all views of the application. This is the perfect location to insert the Splunk RUM instrumentation library as it will be loaded for all pages automatically.\nThe api-gateway service is already running the instrumentation and sending RUM traces to Splunk Observability Cloud and we will review the data in the next section.\nIf you’d like to verify the snippet, you can view the page source in your browser by right-clicking on the page and selecting View Page Source.\n\u003cscript src=\"/env.js\"\u003e\u003c/script\u003e \u003cscript src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e \u003cscript src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web-session-recorder.js\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e \u003cscript\u003e var realm = env.RUM_REALM; console.log('Realm:', realm); var auth = env.RUM_AUTH; var appName = env.RUM_APP_NAME; var environmentName = env.RUM_ENVIRONMENT if (realm \u0026\u0026 auth) { SplunkRum.init({ realm: realm, rumAccessToken: auth, applicationName: appName, deploymentEnvironment: environmentName, version: '1.0.0', }); SplunkSessionRecorder.init({ app: appName, realm: realm, rumAccessToken: auth }); const Provider = SplunkRum.provider; var tracer=Provider.getTracer('appModuleLoader'); } else { // Realm or auth is empty, provide default values or skip initialization console.log(\"Realm or auth is empty. Skipping Splunk Rum initialization.\"); } \u003c/script\u003e \u003c!-- Section added for RUM --\u003e",
    "description": "To enable Real User Monitoring (RUM) instrumentation for an application, you need to add the Open Telemetry Javascript https://github.com/signalfx/splunk-otel-js-web snippet to the code base.\nThe Spring PetClinic application uses a single index HTML page, that is reused across all views of the application. This is the perfect location to insert the Splunk RUM instrumentation library as it will be loaded for all pages automatically.\nThe api-gateway service is already running the instrumentation and sending RUM traces to Splunk Observability Cloud and we will review the data in the next section.",
    "tags": [],
    "title": "Real User Monitoring",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/8-rum/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 8. Develop",
    "content": "Project Setup Ninja Note The time to finish this section of the workshop can vary depending on experience.\nA complete solution can be found here in case you’re stuck or want to follow along with the instructor.\nTo get started developing the new Jenkins CI receiver, we first need to set up a Golang project. The steps to create your new Golang project is:\nCreate a new directory named ${HOME}/go/src/jenkinscireceiver and change into it The actual directory name or location is not strict, you can choose your own development directory to make it in. Initialize the golang module by going go mod init splunk.conf/workshop/example/jenkinscireceiver This will create a file named go.mod which is used to track our direct and indirect dependencies Eventually, there will be a go.sum which is the checksum value of the dependencies being imported. Check-inReview your go.mod module splunk.conf/workshop/example/jenkinscireceiver go 1.20",
    "description": "Project Setup Ninja Note The time to finish this section of the workshop can vary depending on experience.\nA complete solution can be found here in case you’re stuck or want to follow along with the instructor.\nTo get started developing the new Jenkins CI receiver, we first need to set up a Golang project. The steps to create your new Golang project is:",
    "tags": [],
    "title": "OpenTelemetry Collector Development",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/8-develop/1-project-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Splunk RUM",
    "content": "Use RUM Metrics to set up Alerts to be warned in case of an issue Create a Custom Chart based on RUM Metrics 1. Overview The fact that Splunk’s RUM is designed as a full-fidelity solution, and thus can take 100% of your traces, allows it to detect and alert you to any change to the behavior of your website. It also gives you the ability to give you accurate insight into how your website is behaving by allowing you to create custom Charts and Dashboards. This allows you to combine data from your Website, Backend service and underlying Infrastructure. Allowing you to observe the complete stack that makes up your application/solution.\nCreating charts or alerts for RUM Metrics is done in the same way as we do for Infrastructure Metrics. In this section, we will create a simple chart, detector and alert.\nIf you previously done the Splunk IM Part of the Workshop, you will find this section very familiar. If you have not done the Splunk IM workshop before, it is recommended that you run through the Dashboards and Detectors modules after completing the RUM workshop to get a better understanding of the capabilities.\n2. Create an alert on one of the RUM Metrics From the top left hamburger menu icon click Alerts in the menu and then select Detectors.\n3. Create a Chart based on Rum Metrics 3.1 Overview Creating charts or alerts for RUM Metrics is done in the same way as we do for Infrastructure Metrics. In this section we will create a simple chart, detector and alert If you previously done the Splunk IM Part of the Workshop, you will find this section very familiar.\nYou have added to the trace as part of the configuration of your website.\nAdditional Tags We are already sending two additional tags, you have seen them defined in the Beacon URL that was added to your website in the first section of this workshop! You can similarly add additional tags.\napp: \"[nodename]-store\", environment: \"[nodename]-workshop\"",
    "description": "Use RUM Metrics to set up Alerts to be warned in case of an issue Create a Custom Chart based on RUM Metrics 1. Overview The fact that Splunk’s RUM is designed as a full-fidelity solution, and thus can take 100% of your traces, allows it to detect and alert you to any change to the behavior of your website. It also gives you the ability to give you accurate insight into how your website is behaving by allowing you to create custom Charts and Dashboards. This allows you to combine data from your Website, Backend service and underlying Infrastructure. Allowing you to observe the complete stack that makes up your application/solution.",
    "tags": [],
    "title": "9. Custom alerts and charts based on RUM Metrics",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/9-alerting/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "We deployed the Splunk Distribution of the OpenTelemetry Collector in our K8s cluster using the default configuration. In this section, we’ll walk through several examples showing how to customize the collector config.\nGet the Collector Configuration Before we customize the collector config, how do we determine what the current configuration looks like?\nIn a Kubernetes environment, the collector configuration is stored using a Config Map.\nWe can see which config maps exist in our cluster with the following command:\n​ Script Example Output kubectl get cm -l app=splunk-otel-collector NAME DATA AGE splunk-otel-collector-otel-k8s-cluster-receiver 1 3h37m splunk-otel-collector-otel-agent 1 3h37m Why are there two config maps?\nWe can then view the config map of the collector agent as follows:\n​ Script Example Output kubectl describe cm splunk-otel-collector-otel-agent Name: splunk-otel-collector-otel-agent Namespace: default Labels: app=splunk-otel-collector app.kubernetes.io/instance=splunk-otel-collector app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=splunk-otel-collector app.kubernetes.io/version=0.113.0 chart=splunk-otel-collector-0.113.0 helm.sh/chart=splunk-otel-collector-0.113.0 heritage=Helm release=splunk-otel-collector Annotations: meta.helm.sh/release-name: splunk-otel-collector meta.helm.sh/release-namespace: default Data ==== relay: ---- exporters: otlphttp: headers: X-SF-Token: ${SPLUNK_OBSERVABILITY_ACCESS_TOKEN} metrics_endpoint: https://ingest.us1.signalfx.com/v2/datapoint/otlp traces_endpoint: https://ingest.us1.signalfx.com/v2/trace/otlp (followed by the rest of the collector config in yaml format) How to Update the Collector Configuration in K8s In our earlier example running the collector on a Linux instance, the collector configuration was available in the /etc/otel/collector/agent_config.yaml file. If we needed to make changes to the collector config in that case, we’d simply edit this file, save the changes, and then restart the collector.\nIn K8s, things work a bit differently. Instead of modifying the agent_config.yaml directly, we’ll instead customize the collector configuration by making changes to the values.yaml file used to deploy the helm chart.\nThe values.yaml file in GitHub describes the customization options that are available to us.\nLet’s look at an example.\nAdd Infrastructure Events Monitoring For our first example, let’s enable infrastructure events monitoring for our K8s cluster.\nThis will allow us to see Kubernetes events as part of the Events Feed section in charts. The cluster receiver will be configured with a Smart Agent receiver using the kubernetes-events monitor to send custom events. See Collect Kubernetes events for further details.\nThis is done by adding the following line to the values.yaml file:\nHint: steps to open and save in vi are in previous steps.\nlogsEngine: otel splunkObservability: infrastructureMonitoringEventsEnabled: true agent: ... Once the file is saved, we can apply the changes with:\n​ Script Example Output helm upgrade splunk-otel-collector \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-cluster\" \\ --set=\"environment=otel-$INSTANCE\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ -f values.yaml \\ splunk-otel-collector-chart/splunk-otel-collector Release \"splunk-otel-collector\" has been upgraded. Happy Helming! NAME: splunk-otel-collector LAST DEPLOYED: Fri Dec 20 01:17:03 2024 NAMESPACE: default STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Observability realm us1. We can then view the config map and ensure the changes were applied:\n​ Script Example Output kubectl describe cm splunk-otel-collector-otel-k8s-cluster-receiver Ensure smartagent/kubernetes-events is included in the agent config now:\nsmartagent/kubernetes-events: alwaysClusterReporter: true type: kubernetes-events whitelistedEvents: - involvedObjectKind: Pod reason: Created - involvedObjectKind: Pod reason: Unhealthy - involvedObjectKind: Pod reason: Failed - involvedObjectKind: Job reason: FailedCreate Note that we specified the cluster receiver config map since that’s where these particular changes get applied.\nAdd the Debug Exporter Suppose we want to see the traces and logs that are sent to the collector, so we can inspect them before sending them to Splunk. We can use the debug exporter for this purpose, which can be helpful for troubleshooting OpenTelemetry-related issues.\nLet’s add the debug exporter to the bottom of the values.yaml file as follows:\nlogsEngine: otel splunkObservability: infrastructureMonitoringEventsEnabled: true agent: config: receivers: ... exporters: debug: verbosity: detailed service: pipelines: traces: exporters: - debug logs: exporters: - debug Once the file is saved, we can apply the changes with:\n​ Script Example Output helm upgrade splunk-otel-collector \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-cluster\" \\ --set=\"environment=otel-$INSTANCE\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ -f values.yaml \\ splunk-otel-collector-chart/splunk-otel-collector Release \"splunk-otel-collector\" has been upgraded. Happy Helming! NAME: splunk-otel-collector LAST DEPLOYED: Fri Dec 20 01:32:03 2024 NAMESPACE: default STATUS: deployed REVISION: 3 TEST SUITE: None NOTES: Splunk OpenTelemetry Collector is installed and configured to send data to Splunk Observability realm us1. Exercise the application a few times using curl, then tail the agent collector logs with the following command:\nkubectl logs -l component=otel-collector-agent -f You should see traces written to the agent collector logs such as the following:\n2024-12-20T01:43:52.929Z\tinfo\tTraces\t{\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\", \"resource spans\": 1, \"spans\": 2} 2024-12-20T01:43:52.929Z\tinfo\tResourceSpans #0 Resource SchemaURL: https://opentelemetry.io/schemas/1.6.1 Resource attributes: -\u003e splunk.distro.version: Str(1.8.0) -\u003e telemetry.distro.name: Str(splunk-otel-dotnet) -\u003e telemetry.distro.version: Str(1.8.0) -\u003e os.type: Str(linux) -\u003e os.description: Str(Debian GNU/Linux 12 (bookworm)) -\u003e os.build_id: Str(6.8.0-1021-aws) -\u003e os.name: Str(Debian GNU/Linux) -\u003e os.version: Str(12) -\u003e host.name: Str(derek-1) -\u003e process.owner: Str(app) -\u003e process.pid: Int(1) -\u003e process.runtime.description: Str(.NET 8.0.11) -\u003e process.runtime.name: Str(.NET) -\u003e process.runtime.version: Str(8.0.11) -\u003e container.id: Str(78b452a43bbaa3354a3cb474010efd6ae2367165a1356f4b4000be031b10c5aa) -\u003e telemetry.sdk.name: Str(opentelemetry) -\u003e telemetry.sdk.language: Str(dotnet) -\u003e telemetry.sdk.version: Str(1.9.0) -\u003e service.name: Str(helloworld) -\u003e deployment.environment: Str(otel-derek-1) -\u003e k8s.pod.ip: Str(10.42.0.15) -\u003e k8s.pod.labels.app: Str(helloworld) -\u003e k8s.pod.name: Str(helloworld-84865965d9-nkqsx) -\u003e k8s.namespace.name: Str(default) -\u003e k8s.pod.uid: Str(38d39bc6-1309-4022-a569-8acceef50942) -\u003e k8s.node.name: Str(derek-1) -\u003e k8s.cluster.name: Str(derek-1-cluster) And log entries such as:\n2024-12-20T01:43:53.215Z\tinfo\tLogs\t{\"kind\": \"exporter\", \"data_type\": \"logs\", \"name\": \"debug\", \"resource logs\": 1, \"log records\": 2} 2024-12-20T01:43:53.215Z\tinfo\tResourceLog #0 Resource SchemaURL: https://opentelemetry.io/schemas/1.6.1 Resource attributes: -\u003e splunk.distro.version: Str(1.8.0) -\u003e telemetry.distro.name: Str(splunk-otel-dotnet) -\u003e telemetry.distro.version: Str(1.8.0) -\u003e os.type: Str(linux) -\u003e os.description: Str(Debian GNU/Linux 12 (bookworm)) -\u003e os.build_id: Str(6.8.0-1021-aws) -\u003e os.name: Str(Debian GNU/Linux) -\u003e os.version: Str(12) -\u003e host.name: Str(derek-1) -\u003e process.owner: Str(app) -\u003e process.pid: Int(1) -\u003e process.runtime.description: Str(.NET 8.0.11) -\u003e process.runtime.name: Str(.NET) -\u003e process.runtime.version: Str(8.0.11) -\u003e container.id: Str(78b452a43bbaa3354a3cb474010efd6ae2367165a1356f4b4000be031b10c5aa) -\u003e telemetry.sdk.name: Str(opentelemetry) -\u003e telemetry.sdk.language: Str(dotnet) -\u003e telemetry.sdk.version: Str(1.9.0) -\u003e service.name: Str(helloworld) -\u003e deployment.environment: Str(otel-derek-1) -\u003e k8s.node.name: Str(derek-1) -\u003e k8s.cluster.name: Str(derek-1-cluster) If you return to Splunk Observability Cloud though, you’ll notice that traces and logs are no longer being sent there by the application.\nWhy do you think that is? We’ll explore it in the next section.",
    "description": "We deployed the Splunk Distribution of the OpenTelemetry Collector in our K8s cluster using the default configuration. In this section, we’ll walk through several examples showing how to customize the collector config.\nGet the Collector Configuration Before we customize the collector config, how do we determine what the current configuration looks like?\nIn a Kubernetes environment, the collector configuration is stored using a Config Map.\nWe can see which config maps exist in our cluster with the following command:",
    "tags": [],
    "title": "Customize the OpenTelemetry Collector Configuration",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/9-customize-collector-config/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Don’t forget Chicago We are nearly done, one more location to go… Chicago.\nSince we have been having so many issues related to “location” and we have added that custom attribute via Opentelemetry Manual Instrumentation, lets go to the Splunk Observability UI and look at an APM metric set around that tag that I created for us.\nOpen a browser and navigate to http://[EC2-Address]:8010 Replace [EC2-Address] with the ip address of your host Select a few locations and hit the Login button. Make sure to also select the Chicago Location and hit the Login button. Uh oh! We received a 500 error, something is wrong there as well.\nReturn to the Splunk Observability UI and lets look once again at our Service Map Select the Instruments Service Click the Breakdowns dropdown on the right and select location We see there was an un-handled exception thrown in the Instruments service, and some latency from our database that is related to the Chicago location!\nClick on Traces on the right Click Errors Only Click one of the traces We can see the exception was thrown by Hibernate, however it was thrown in our method instruments: InstrumentRepository.findInstruments\nLet’s play developer again Edit the file instruments: InstrumentRepository.findInstruments using nano: nano instruments/src/main/java/com/shabushabu/javashop/instruments/repositories/FindInstrumentRepositoryImpl.java Find the method: findInstruments You know how to do this now, right? We can see the developer accidently added the Instruments database with the Chicago Instruments database!\nLet’s change the query and fix this, remove instruments_for_sale from our query.\nChange this: public Object findInstruments() { LOGGER.info(\"findInstruments Called (All)\"); Object obj = entityManager.createNativeQuery( \"SELECT * FROM instruments_for_sale, instruments_for_sale_chicago\").getResultList(); return obj; } to this: public Object findInstruments() { LOGGER.info(\"findInstruments Called (All)\"); Object obj = entityManager.createNativeQuery( \"SELECT * FROM instruments_for_sale_chicago\").getResultList(); return obj; } Save the changes: [CTRL]-o [Enter]\nExit: [CTRL]-x\nBuild and Deploy Application\n./BuildAndDeploy.sh Now let’s test the Chicago location once again\nOpen a browser and navigate to http://[EC2-Address]:8010 Select the Chicago location and Login We now see the 500 error is gone!\nLet’s confirm a clean Service Map: If you see a clean service map, free of errors and Latency you have successfully completed the Java Instrumentation Workshop!\nCongratulations!!!",
    "description": "Don’t forget Chicago We are nearly done, one more location to go… Chicago.\nSince we have been having so many issues related to “location” and we have added that custom attribute via Opentelemetry Manual Instrumentation, lets go to the Splunk Observability UI and look at an APM metric set around that tag that I created for us.\nOpen a browser and navigate to http://[EC2-Address]:8010 Replace [EC2-Address] with the ip address of your host Select a few locations and hit the Login button. Make sure to also select the Chicago Location and hit the Login button.",
    "tags": [],
    "title": "Don't Forget Chicago",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/9-chicago/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "Persona As the SRE hat suits you let’s keep it on as you have been asked to build a custom Service Health Dashboard for the paymentservice. The requirement is to display RED metrics, logs and Synthetic test duration results.\nIt is common for development and SRE teams to require a summary of the health of their applications and/or services. More often or not these are displayed on wall-mounted TVs. Splunk Observability Cloud has the perfect solution for this by creating custom dashboards.\nIn this section we are going to build a Service Health Dashboard we can use to display on teams’ monitors or TVs.",
    "description": "In this section, you will learn how to build a custom Service Health Dashboard to monitor the health of your services.",
    "tags": [],
    "title": "Custom Service Health Dashboard 🏥",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/9-custom-dashboard/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Automatic Discovery Workshops \u003e PetClinic Kubernetes Workshop",
    "content": "Congratulations, you have completed the Get the Most Out of Your Existing Kubernetes Java Applications Using Automatic Discovery and Configuration With OpenTelemetry workshop.\nToday, you have learned how easy it is to add Tracing, Code Profiling and Database Query Performance to your existing Java application in Kubernetes.\nYou immediately improved the observability of the application and infrastructure without touching a line of code or configuration using Automatic Discovery and Configuration.\nYou also learned that with simple configuration changes, you can add even more observability (logging and RUM) to the application in order to provide end-to-end observability.",
    "description": "Congratulations, you have completed the Get the Most Out of Your Existing Kubernetes Java Applications Using Automatic Discovery and Configuration With OpenTelemetry. Today, you have become familiar with how easy it is to add tracing, Code Profiling and Database Query Performance to your existing Java application in Kubernetes to immediately improve the observability of your applications and infrastructure.",
    "tags": [],
    "title": "Workshop Wrap-up 🎁",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/1-automatic-discovery/2-petclinic-kubernetes/9-wrap-up/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "In this workshop, you’ll get hands-on experience with the following:\nDeploy the OpenTelemetry Collector and customize the collector config Deploy an application and instrument it with OpenTelemetry See how tags are captured using an OpenTelemetry SDK Create a Troubleshooting MetricSet Troubleshoot a problem and determine root cause using Tag Spotlight Let’s get started!\nTip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "By the end of this workshop you'll have gotten hands-on experience deploying the OpenTelemetry Collector, instrumenting an application with OpenTelemetry, capturing tags from the application, and using Troubleshooting MetricSets and Tag Spotlight to determine the root cause of an issue.",
    "tags": [],
    "title": "Solving Problems with O11y Cloud",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/9-solving-problems-with-o11y-cloud/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops",
    "content": "Splunk RUM is the industry’s only end-to-end, full-fidelity Real User Monitoring solution. It is built to optimize performance and aid in faster troubleshooting, giving you full visibility into end-user experiences.\nSplunk RUM allows you to identify performance problems in your Web and or Mobile applications that impact the customer experience. We support benchmarking and measuring page performance with core web vitals. This includes but is not limited to W3C timings, the ability to identify long-running tasks, along with anything that can impact your page load.\nWith Splunk’s end-to-end monitoring capabilities, you can view the latency between all of the services that make up your application, from the service itself through to infrastructure metrics such as database calls and everything in between. Our full-fidelity end-to-end monitoring solution captures 100% of your span data. We do not sample, we are framework agnostic and Open Telemetry standardized.\nMore often than not we find that the frontend and backend application’s performance are interdependent. Fully understanding and being able to visualize the link between your backend services and your user experience is increasingly important. To see the full picture, Splunk RUM provides a seamless correlation between our front-end and back-end microservices. If your users are experiencing less than optimal conditions on your web-based application due to an issue related to your microservice or infrastructure, Splunk will be able to detect this issue and alert you.\nTo complete the picture and offer full visibility, Splunk is also able to show in-context logs and events to enable deeper troubleshooting and root-cause analysis.",
    "description": "End-to-end visibility helps you pinpoint customer-impacting issues from web browsers and native mobile apps to your backend services.",
    "tags": [],
    "title": "Splunk RUM",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/9-rum/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "In the previous section, we added the debug exporter to the collector configuration, and made it part of the pipeline for traces and logs. We see the debug output written to the agent collector logs as expected.\nHowever, traces are no longer sent to o11y cloud. Let’s figure out why and fix it.\nReview the Collector Config Whenever a change to the collector config is made via a values.yaml file, it’s helpful to review the actual configuration applied to the collector by looking at the config map:\nkubectl describe cm splunk-otel-collector-otel-agent Let’s review the pipelines for logs and traces in the agent collector config. They should look like this:\npipelines: logs: exporters: - debug processors: - memory_limiter - k8sattributes - filter/logs - batch - resourcedetection - resource - resource/logs - resource/add_environment receivers: - filelog - fluentforward - otlp ... traces: exporters: - debug processors: - memory_limiter - k8sattributes - batch - resourcedetection - resource - resource/add_environment receivers: - otlp - jaeger - smartagent/signalfx-forwarder - zipkin Do you see the problem? Only the debug exporter is included in the traces and logs pipelines. The otlphttp and signalfx exporters that were present in the traces pipeline configuration previously are gone. This is why we no longer see traces in o11y cloud. And for the logs pipeline, the splunk_hec/platform_logs exporter has been removed.\nHow did we know what specific exporters were included before? To find out, we could have reverted our earlier customizations and then checked the config map to see what was in the traces pipeline originally. Alternatively, we can refer to the examples in the GitHub repo for splunk-otel-collector-chart which shows us what default agent config is used by the Helm chart.\nHow did these exporters get removed? Let’s review the customizations we added to the values.yaml file:\nlogsEngine: otel splunkObservability: infrastructureMonitoringEventsEnabled: true agent: config: receivers: ... exporters: debug: verbosity: detailed service: pipelines: traces: exporters: - debug logs: exporters: - debug When we applied the values.yaml file to the collector using helm upgrade, the custom configuration got merged with the previous collector configuration. When this happens, the sections of the yaml configuration that contain lists, such as the list of exporters in the pipeline section, get replaced with what we included in the values.yaml file (which was only the debug exporter).\nLet’s Fix the Issue So when customizing an existing pipeline, we need to fully redefine that part of the configuration. Our values.yaml file should thus be updated as follows:\nlogsEngine: otel splunkObservability: infrastructureMonitoringEventsEnabled: true agent: config: receivers: ... exporters: debug: verbosity: detailed service: pipelines: traces: exporters: - otlphttp - signalfx - debug logs: exporters: - splunk_hec/platform_logs - debug Let’s apply the changes:\nhelm upgrade splunk-otel-collector \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$INSTANCE-cluster\" \\ --set=\"environment=otel-$INSTANCE\" \\ --set=\"splunkPlatform.token=$HEC_TOKEN\" \\ --set=\"splunkPlatform.endpoint=$HEC_URL\" \\ --set=\"splunkPlatform.index=splunk4rookies-workshop\" \\ -f values.yaml \\ splunk-otel-collector-chart/splunk-otel-collector And then check the agent config map:\nkubectl describe cm splunk-otel-collector-otel-agent This time, we should see a fully defined exporters pipeline for both logs and traces:\npipelines: logs: exporters: - splunk_hec/platform_logs - debug processors: ... traces: exporters: - otlphttp - signalfx - debug processors: ... Reviewing the Log Output The Splunk Distribution of OpenTelemetry .NET automatically exports logs enriched with tracing context from applications that use Microsoft.Extensions.Logging for logging (which our sample app does).\nApplication logs are enriched with tracing metadata and then exported to a local instance of the OpenTelemetry Collector in OTLP format.\nLet’s take a closer look at the logs that were captured by the debug exporter to see if that’s happening.\nTo tail the collector logs, we can use the following command:\nkubectl logs -l component=otel-collector-agent -f Once we’re tailing the logs, we can use curl to generate some more traffic. Then we should see something like the following:\n2024-12-20T21:56:30.858Z\tinfo\tLogs\t{\"kind\": \"exporter\", \"data_type\": \"logs\", \"name\": \"debug\", \"resource logs\": 1, \"log records\": 1} 2024-12-20T21:56:30.858Z\tinfo\tResourceLog #0 Resource SchemaURL: https://opentelemetry.io/schemas/1.6.1 Resource attributes: -\u003e splunk.distro.version: Str(1.8.0) -\u003e telemetry.distro.name: Str(splunk-otel-dotnet) -\u003e telemetry.distro.version: Str(1.8.0) -\u003e os.type: Str(linux) -\u003e os.description: Str(Debian GNU/Linux 12 (bookworm)) -\u003e os.build_id: Str(6.8.0-1021-aws) -\u003e os.name: Str(Debian GNU/Linux) -\u003e os.version: Str(12) -\u003e host.name: Str(derek-1) -\u003e process.owner: Str(app) -\u003e process.pid: Int(1) -\u003e process.runtime.description: Str(.NET 8.0.11) -\u003e process.runtime.name: Str(.NET) -\u003e process.runtime.version: Str(8.0.11) -\u003e container.id: Str(5bee5b8f56f4b29f230ffdd183d0367c050872fefd9049822c1ab2aa662ba242) -\u003e telemetry.sdk.name: Str(opentelemetry) -\u003e telemetry.sdk.language: Str(dotnet) -\u003e telemetry.sdk.version: Str(1.9.0) -\u003e service.name: Str(helloworld) -\u003e deployment.environment: Str(otel-derek-1) -\u003e k8s.node.name: Str(derek-1) -\u003e k8s.cluster.name: Str(derek-1-cluster) ScopeLogs #0 ScopeLogs SchemaURL: InstrumentationScope HelloWorldController LogRecord #0 ObservedTimestamp: 2024-12-20 21:56:28.486804 +0000 UTC Timestamp: 2024-12-20 21:56:28.486804 +0000 UTC SeverityText: Information SeverityNumber: Info(9) Body: Str(/hello endpoint invoked by {name}) Attributes: -\u003e name: Str(Kubernetes) Trace ID: 78db97a12b942c0252d7438d6b045447 Span ID: 5e9158aa42f96db3 Flags: 1 {\"kind\": \"exporter\", \"data_type\": \"logs\", \"name\": \"debug\"} In this example, we can see that the Trace ID and Span ID were automatically written to the log output by the OpenTelemetry .NET instrumentation. This allows us to correlate logs with traces in Splunk Observability Cloud.\nYou might remember though that if we deploy the OpenTelemetry collector in a K8s cluster using Helm, and we include the log collection option, then the OpenTelemetry collector will use the File Log receiver to automatically capture any container logs.\nThis would result in duplicate logs being captured for our application. For example, in the following screenshot we can see two log entries for each request made to our service:\nHow do we avoid this?\nAvoiding Duplicate Logs in K8s To avoid capturing duplicate logs, we can set the OTEL_LOGS_EXPORTER environment variable to none, to tell the Splunk Distribution of OpenTelemetry .NET to avoid exporting logs to the collector using OTLP. We can do this by adding the OTEL_LOGS_EXPORTER environment variabl to the deployment.yaml file:\nenv: - name: PORT value: \"8080\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP - name: OTEL_EXPORTER_OTLP_ENDPOINT value: \"http://$(NODE_IP):4318\" - name: OTEL_SERVICE_NAME value: \"helloworld\" - name: OTEL_RESOURCE_ATTRIBUTES value: \"deployment.environment=otel-$INSTANCE\" - name: OTEL_LOGS_EXPORTER value: \"none\" And then running:\n# update the deployment kubectl apply -f deployment.yaml Setting the OTEL_LOGS_EXPORTER environment variable to none is straightforward. However, the Trace ID and Span ID are not written to the stdout logs generated by the application, which would prevent us from correlating logs with traces.\nTo resolve this, we will need to define a custom logger, such as the example defined in\n/home/splunk/workshop/docker-k8s-otel/helloworld/SplunkTelemetryConfigurator.cs.\nWe could include this in our application by updating the Program.cs file as follows:\nusing SplunkTelemetry; using Microsoft.Extensions.Logging.Console; var builder = WebApplication.CreateBuilder(args); builder.Services.AddControllers(); SplunkTelemetryConfigurator.ConfigureLogger(builder.Logging); var app = builder.Build(); app.MapControllers(); app.Run(); Then we’ll build a new Docker image that includes the custom logging configuration:\ncd /home/splunk/workshop/docker-k8s-otel/helloworld docker build -t helloworld:1.3 . And then we’ll import the updated image into Kubernetes:\ncd /home/splunk # Export the image from docker docker save --output helloworld.tar helloworld:1.3 # Import the image into k3s sudo k3s ctr images import helloworld.tar Finally, we’ll need to update the `deployment.yaml’ file to use the 1.3 version of the container image:\nspec: containers: - name: helloworld image: docker.io/library/helloworld:1.3 And then apply the changes:\n# update the deployment kubectl apply -f deployment.yaml Now we can see that the duplicate log entries have been eliminated. And the remaining log entries have been formatted as JSON, and include the span and trace IDs:",
    "description": "In the previous section, we added the debug exporter to the collector configuration, and made it part of the pipeline for traces and logs. We see the debug output written to the agent collector logs as expected.\nHowever, traces are no longer sent to o11y cloud. Let’s figure out why and fix it.\nReview the Collector Config Whenever a change to the collector config is made via a values.yaml file, it’s helpful to review the actual configuration applied to the collector by looking at the config map:",
    "tags": [],
    "title": "Troubleshoot OpenTelemetry Collector Issues",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/10-troubleshoot-collector/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud",
    "content": "Congratulations, you have completed the Splunk4Rookies - Observability Cloud Workshop. Today, you have become familiar with how to use Splunk Observability Cloud to monitor your applications and infrastructure.\nCelebrate your achievement by adding this certificate to your LinkedIn profile.\nLet’s recap what we have learned and what you can do next.",
    "description": "Congratulations, you have completed the Splunk4Rookies - Observability Cloud Workshop. Today, you have become familiar with how to use Splunk Observability Cloud to monitor your applications and infrastructure.",
    "tags": [],
    "title": "Workshop Wrap-up 🎁",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/10-wrap-up/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "The Routing Connector in OpenTelemetry is a powerful feature that allows you to direct data (traces, metrics, or logs) to different pipelines based on specific criteria. This is especially useful in scenarios where you want to apply different processing or exporting logic to subsets of your telemetry data.\nFor example, you might want to send production data to one exporter while directing test or development data to another. Similarly, you could route certain spans based on their attributes, such as service name, environment, or span name, to apply custom processing or storage logic.\nExercise Inside the [WORKSHOP] directory, create a new subdirectory named 8-routing-data. Next, copy *.yaml from the 7-transform-data directory into 8-routing-data. Change all terminal windows to the [WORKSHOP]/8-routing-data directory. Your updated directory structure will now look like this:\n​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Next, we will configure the routing connector and the respective pipelines.",
    "description": "The Routing Connector in OpenTelemetry is a powerful feature that allows you to direct data (traces, metrics, or logs) to different pipelines based on specific criteria. This is especially useful in scenarios where you want to apply different processing or exporting logic to subsets of your telemetry data.\nFor example, you might want to send production data to one exporter while directing test or development data to another. Similarly, you could route certain spans based on their attributes, such as service name, environment, or span name, to apply custom processing or storage logic.",
    "tags": [],
    "title": "8. Routing Data",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/8-routing-data/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced OpenTelemetry Collector",
    "content": "",
    "description": "",
    "tags": [],
    "title": "8. Wrap-up",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector/8-wrap-up/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 8. Develop",
    "content": "Building The Configuration The configuration portion of the component is how the user is able to have their inputs over the component, so the values that is used for the configuration need to be:\nIntuitive for users to understand what that field controls Be explicit in what is required and what is optional Reuse common names and fields Keep the options simple ​ bad config good config --- jenkins_server_addr: hostname jenkins_server_api_port: 8089 interval: 10m filter_builds_by: - name: my-awesome-build status: amber track: values: example.metric.1: yes example.metric.2: yes example.metric.3: no example.metric.4: no --- # Required Values endpoint: http://my-jenkins-server:8089 auth: authenticator: basicauth/jenkins # Optional Values collection_interval: 10m metrics: example.metric.1: enabled: true example.metric.2: enabled: true example.metric.3: enabled: true example.metric.4: enabled: true The bad configuration highlights how doing the opposite of the recommendations of configuration practices impacts the usability of the component. It doesn’t make it clear what field values should be, it includes features that can be pushed to existing processors, and the field naming is not consistent with other components that exist in the collector.\nThe good configuration keeps the required values simple, reuses field names from other components, and ensures the component focuses on just the interaction between Jenkins and the collector.\nThe code tab shows how much is required to be added by us and what is already provided for us by shared libraries within the collector. These will be explained in more detail once we get to the business logic. The configuration should start off small and will change once the business logic has started to include additional features that is needed.\nWrite the code In order to implement the code needed for the configuration, we are going to create a new file named config.go with the following content:\npackage jenkinscireceiver import ( \"go.opentelemetry.io/collector/config/confighttp\" \"go.opentelemetry.io/collector/receiver/scraperhelper\" \"splunk.conf/workshop/example/jenkinscireceiver/internal/metadata\" ) type Config struct { // HTTPClientSettings contains all the values // that are commonly shared across all HTTP interactions // performed by the collector. confighttp.HTTPClientSettings `mapstructure:\",squash\"` // ScraperControllerSettings will allow us to schedule // how often to check for updates to builds. scraperhelper.ScraperControllerSettings `mapstructure:\",squash\"` // MetricsBuilderConfig contains all the metrics // that can be configured. metadata.MetricsBuilderConfig `mapstructure:\",squash\"` }",
    "description": "Building The Configuration The configuration portion of the component is how the user is able to have their inputs over the component, so the values that is used for the configuration need to be:\nIntuitive for users to understand what that field controls Be explicit in what is required and what is optional Reuse common names and fields Keep the options simple ​ bad config good config --- jenkins_server_addr: hostname jenkins_server_api_port: 8089 interval: 10m filter_builds_by: - name: my-awesome-build status: amber track: values: example.metric.1: yes example.metric.2: yes example.metric.3: no example.metric.4: no --- # Required Values endpoint: http://my-jenkins-server:8089 auth: authenticator: basicauth/jenkins # Optional Values collection_interval: 10m metrics: example.metric.1: enabled: true example.metric.2: enabled: true example.metric.3: enabled: true example.metric.4: enabled: true The bad configuration highlights how doing the opposite of the recommendations of configuration practices impacts the usability of the component. It doesn’t make it clear what field values should be, it includes features that can be pushed to existing processors, and the field naming is not consistent with other components that exist in the collector.",
    "tags": [],
    "title": "OpenTelemetry Collector Development",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/8-develop/2-configuration/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops",
    "content": "Aim This module is simply to ensure you have access to the Splunk On-Call UI (formerly known as VictorOps), Splunk Infrastructure Monitoring UI (formerly known as SignalFx) and the EC2 Instance which has been allocated to you.\nOnce you have access to each platform, keep them open for the duration of the workshop as you will be switching between them and the workshop instructions.\n1. Activate your Splunk On-Call Login You should have received an invitation to Activate your Splunk On-Call account via e-mail, if you have not already done so, click the Activate Account link and follow the prompts.\nIf you did not receive an invitation it is probably because you already have a Splunk On-Call login, linked to a different organization.\nIf so log in to that Org, then use the organization dropdown next to your username in the top left to switch to the Observability Workshop Org.\nNote If you do not see the Organisation dropdown menu item next to your name with Observability Workshop EMEA that is OK, it simply means you only have access to a single Org so that menu is not visible to you.\nIf you have forgotten your password go to the https://portal.victorops.com/membership/#/ page and use the forgotten password link to reset your password.\n2. Activate your Splunk Infrastructure Monitoring Login You should have received an invitation to join the Splunk Infrastructure Monitoring - Observability Workshop. If you have not already done so click the JOIN NOW button and follow the prompts to set a password and activate your login.\n3. Access your EC2 Instance Splunk has provided you with a dedicated EC2 Instance which you can use during this workshop for triggering Incidents the same way the instructor did during the introductory demo. This VM has Splunk Infrastructure Monitoring deployed and has an associated Detector configured. The Detector will pass Alerts to Splunk On-Call which will then create Incidents and page the on-call user.\nThe welcome e-mail you received providing you all the details for this Workshop contain the instructions for accessing your allocated EC2 Instance.\nSSH (Mac OS/Linux) Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device.\nTo use SSH, open a terminal on your system and type ssh splunk@x.x.x.x (replacing x.x.x.x with the IP address found in your welcome e-mail).\nWhen prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes.\nEnter the password provided in the welcome e-mail.\nUpon successful login you will be presented with the Splunk logo and the Linux prompt.\nAt this point you are ready to continue with the workshop when instructed to do so by the instructor\nPutty (Windows users only) If you do not have ssh pre-installed or if you are on a Windows system, the best option is to install putty, you can find the downloads here.\n!!! important If you cannot install Putty, please go to Web Browser (All).\nOpen Putty and in the Host Name (or IP address) field enter the IP address provided in the welcome e-mail.\nYou can optionally save your settings by providing a name and pressing Save.\nTo then login to your instance click on the Open button as shown above.\nIf this is the first time connecting to your EC2 instance, you will be presented with a security dialogue, please click Yes.\nOnce connected, login in as splunk using the password provided in the welcome e-mail.\nOnce you are connected successfully you should see a screen similar to the one below:\nAt this point you are ready to continue with the workshop when instructed to do so by the instructor\nWeb Browser (All) If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser.\n!!! note This assumes that access to port 6501 is not restricted by your company’s firewall.\nOpen your web browser and type http://x.x.x.x:650 (where x.x.x.x is the IP address from the welcome e-mail).\nOnce connected, login in as splunk and the password is the one provided in the welcome e-mail.\nOnce you are connected successfully you should see a screen similar to the one below:\nCopy \u0026 Paste in browser Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions.\nWhen the workshop asks you to copy instructions into your terminal, please do the following:\nCopy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below:\nThis will open a dialogue box asking for the text to be pasted into the web terminal:\nPaste the text in the text box as show, then press OK to complete the copy and paste process.\nDetails Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal.\nSimply click the Connect button and you will be reconnected and will be able to continue.\nAt this point you are ready to continue with the workshop when instructed to do so by the instructor",
    "description": "Make expensive service outages a thing of the past. Remediate issues faster, reduce on-call burnout and keep your services up and running.",
    "tags": [],
    "title": "Splunk OnCall",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/10-oncall/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e Hands-On OpenTelemetry, Docker, and K8s",
    "content": "This workshop provided hands-on experience with the following concepts:\nHow to deploy the Splunk Distribution of the OpenTelemetry Collector on a Linux host. How to instrument a .NET application with the Splunk Distribution of OpenTelemetry .NET. How to “dockerize” a .NET application and instrument it with the Splunk Distribution of OpenTelemetry .NET. How to deploy the Splunk Distribution of the OpenTelemetry Collector in a Kubernetes cluster using Helm. How to customize the collector configuration and troubleshoot an issue. To see how other languages and environments are instrumented with OpenTelemetry, explore the Splunk OpenTelemetry Examples GitHub repository.\nTo run this workshop on your own in the future, refer back to these instructions and use the Splunk4Rookies - Observability workshop template in Splunk Show to provision an EC2 instance.",
    "description": "This workshop provided hands-on experience with the following concepts:\nHow to deploy the Splunk Distribution of the OpenTelemetry Collector on a Linux host. How to instrument a .NET application with the Splunk Distribution of OpenTelemetry .NET. How to “dockerize” a .NET application and instrument it with the Splunk Distribution of OpenTelemetry .NET. How to deploy the Splunk Distribution of the OpenTelemetry Collector in a Kubernetes cluster using Helm. How to customize the collector configuration and troubleshoot an issue. To see how other languages and environments are instrumented with OpenTelemetry, explore the Splunk OpenTelemetry Examples GitHub repository.",
    "tags": [],
    "title": "Summary",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/8-docker-k8s-otel/11-summary/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 8. Develop",
    "content": "Component Review To recap the type of component we will need to capture metrics from Jenkins:\n​ Extension Receiver Processor Exporter Ninja: Connectors The business use case an extension helps solves for are:\nHaving shared functionality that requires runtime configuration Indirectly helps with observing the runtime of the collector See Extensions Overview for more details.\nThe business use case a receiver solves for:\nFetching data from a remote source Receiving data from remote source(s) This is commonly referred to pull vs push based data collection, and you read more about the details in the Receiver Overview.\nThe business use case a processor solves for is:\nAdding or removing data, fields, or values Observing and making decisions on the data Buffering, queueing, and reordering The thing to keep in mind is the data type flowing through a processor needs to forward the same data type to its downstream components. Read through Processor Overview for the details.\nThe business use case an exporter solves for:\nSend the data to a tool, service, or storage The OpenTelemetry collector does not want to be “backend”, an all-in-one observability suite, but rather keep to the principles that founded OpenTelemetry to begin with; A vendor agnostic Observability for all. To help revisit the details, please read through Exporter Overview.\nThis is a component type that was missed in the workshop since it is a relatively new addition to the collector, but the best way to think about a connector is that it is like a processor that allows it to be used across different telemetry types and pipelines. Meaning that a connector can accept data as logs, and output metrics, or accept metrics from one pipeline and provide metrics on the data it has observed.\nThe business case that a connector solves for:\nConverting from different telemetry types logs to metrics traces to metrics metrics to logs Observing incoming data and producing its own data Accepting metrics and generating analytical metrics of the data. There was a brief overview within the Ninja section as part of the Processor Overview, and be sure what the project for updates for new connector components.\nFrom the component overviews, it is clear that developing a pull-based receiver for Jenkins.",
    "description": "Component Review To recap the type of component we will need to capture metrics from Jenkins:\n​ Extension Receiver Processor Exporter Ninja: Connectors The business use case an extension helps solves for are:\nHaving shared functionality that requires runtime configuration Indirectly helps with observing the runtime of the collector See Extensions Overview for more details.",
    "tags": [],
    "title": "OpenTelemetry Collector Development",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/8-develop/3-component/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "In this section, we’ll explore how to use the Count Connector to extract attribute values from logs and convert them into meaningful metrics.\nSpecifically, we’ll use the Count Connector to track the number of “Star Wars” and “Lord of the Rings” quotes appearing in our logs, turning them into measurable data points.\nExercise Inside the [WORKSHOP] directory, create a new subdirectory named 9-sum-count. Next, copy *.yaml from the 8-routing-data directory into 9-sum-count. Change all terminal windows to the [WORKSHOP]/9-sum-count directory. ​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Update the agent.yaml to change the frequency that we read logs. Find the filelog/quotes receiver in the agent.yaml and add a poll_interval attribute: filelog/quotes: # Receiver Type/Name poll_interval: 10s # Only read every ten seconds The reason for the delay is that the Count Connector in the OpenTelemetry Collector counts logs only within each processing interval. This means that every time the data is read, the count resets to zero for the next interval. With the default Filelog reciever interval of 200ms, it reads every line the loadgen writes, giving us counts of 1. With this interval we make sure we have multiple entries to count.\nThe Collector can maintain a running count for each read interval by omitting conditions, as shown below. However, it’s best practice to let your backend handle running counts since it can track them over a longer time period.\nExercise Add the Count Connector Include the Count Connector in the connector’s section of your configuration and define the metrics counters we want to use:\nconnectors: count: logs: logs.full.count: description: \"Running count of all logs read in interval\" logs.sw.count: description: \"StarWarsCount\" conditions: - attributes[\"movie\"] == \"SW\" logs.lotr.count: description: \"LOTRCount\" conditions: - attributes[\"movie\"] == \"LOTR\" logs.error.count: description: \"ErrorCount\" conditions: - attributes[\"level\"] == \"ERROR\" Explanation of the Metrics Counters\nlogs.full.count: Tracks the total number of logs processed during each read interval.\nSince this metric has no filtering conditions, every log that passes through the system is included in the count. logs.sw.count Counts logs that contain a quote from a Star Wars movie. logs.lotr.count: Counts logs that contain a quote from a Lord of the Rings movie. logs.error.count: Represents a real-world scenario by counting logs with a severity level of ERROR for the read interval. Configure the Count Connector in the pipelines\nIn the pipeline configuration below, the connector exporter is added to the logs section, while the connector receiver is added to the metrics section.\npipelines: traces: receivers: - otlp processors: - memory_limiter - attributes/update # Update, hash, and remove attributes - redaction/redact # Redact sensitive fields using regex - resourcedetection - resource/add_mode - batch exporters: - debug - file - otlphttp metrics: receivers: - count # Count Connector that receives count metric from logs count exporter in logs pipeline. - otlp #- hostmetrics # Host Metrics Receiver processors: - memory_limiter - resourcedetection - resource/add_mode - batch exporters: - debug - otlphttp logs: receivers: - otlp - filelog/quotes processors: - memory_limiter - resourcedetection - resource/add_mode - transform/logs # Transform logs processor - batch exporters: - count # Count Connector that exports count as a metric to metrics pipeline. - debug - otlphttp We count logs based on their attributes. If your log data is stored in the log body instead of attributes, you’ll need to use a Transform processor in your pipeline to extract key/value pairs and add them as attributes.\nIn this workshop, we’ve already added merge_maps(attributes, cache, \"upsert\") in the 07-transform section. This ensures that all relevant data is included in the log attributes for processing.\nWhen selecting fields to create attributes from, be mindful—adding all fields indiscriminately is generally not ideal for production environments. Instead, choose only the fields that are truly necessary to avoid unnecessary data clutter.\nExercise Validate the agent configuration using otelbin.io. For reference, the logs and metrics: sections of your pipelines will look like this: %%{init:{\"fontFamily\":\"monospace\"}}%% graph LR %% Nodes REC1(otlp\u003cbr\u003efa:fa-download):::receiver REC2(filelog\u003cbr\u003efa:fa-download\u003cbr\u003equotes):::receiver REC3(otlp\u003cbr\u003efa:fa-download):::receiver PRO1(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO2(memory_limiter\u003cbr\u003efa:fa-microchip):::processor PRO3(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO4(resource\u003cbr\u003efa:fa-microchip\u003cbr\u003eadd_mode):::processor PRO5(batch\u003cbr\u003efa:fa-microchip):::processor PRO6(batch\u003cbr\u003efa:fa-microchip):::processor PRO7(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO8(resourcedetection\u003cbr\u003efa:fa-microchip):::processor PRO9(transfrom\u003cbr\u003efa:fa-microchip\u003cbr\u003elogs):::processor EXP1(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP2(\u0026emsp;\u0026emsp;otlphttp\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload):::exporter EXP3(\u0026nbsp;\u0026ensp;debug\u0026nbsp;\u0026ensp;\u003cbr\u003efa:fa-upload):::exporter EXP4(\u0026emsp;\u0026emsp;otlphttp\u0026emsp;\u0026emsp;\u003cbr\u003efa:fa-upload):::exporter ROUTE1(\u0026nbsp;count\u0026nbsp;\u003cbr\u003efa:fa-route):::con-export ROUTE2(\u0026nbsp;count\u0026nbsp;\u003cbr\u003efa:fa-route):::con-receive %% Links subID1:::sub-logs subID2:::sub-metrics subgraph \" \" direction LR subgraph subID1[**Logs**] direction LR REC1 --\u003e PRO1 REC2 --\u003e PRO1 PRO1 --\u003e PRO7 PRO7 --\u003e PRO3 PRO3 --\u003e PRO9 PRO9 --\u003e PRO5 PRO5 --\u003e ROUTE1 PRO5 --\u003e EXP1 PRO5 --\u003e EXP2 end subgraph subID2[**Metrics**] direction LR ROUTE1 --\u003e ROUTE2 ROUTE2 --\u003e PRO2 REC3 --\u003e PRO2 PRO2 --\u003e PRO8 PRO8 --\u003e PRO4 PRO4 --\u003e PRO6 PRO6 --\u003e EXP3 PRO6 --\u003e EXP4 end end classDef receiver,exporter fill:#8b5cf6,stroke:#333,stroke-width:1px,color:#fff; classDef processor fill:#6366f1,stroke:#333,stroke-width:1px,color:#fff; classDef con-receive,con-export fill:#45c175,stroke:#333,stroke-width:1px,color:#fff; classDef sub-logs stroke:#34d399,stroke-width:1px, color:#34d399,stroke-dasharray: 3 3; classDef sub-metrics stroke:#38bdf8,stroke-width:1px, color:#38bdf8,stroke-dasharray: 3 3;",
    "description": "In this section, we’ll explore how to use the Count Connector to extract attribute values from logs and convert them into meaningful metrics.\nSpecifically, we’ll use the Count Connector to track the number of “Star Wars” and “Lord of the Rings” quotes appearing in our logs, turning them into measurable data points.\nExercise Inside the [WORKSHOP] directory, create a new subdirectory named 9-sum-count. Next, copy *.yaml from the 8-routing-data directory into 9-sum-count. Change all terminal windows to the [WORKSHOP]/9-sum-count directory. ​ Updated Directory Structure . ├── agent.yaml └── gateway.yaml Update the agent.yaml to change the frequency that we read logs. Find the filelog/quotes receiver in the agent.yaml and add a poll_interval attribute: filelog/quotes: # Receiver Type/Name poll_interval: 10s # Only read every ten seconds The reason for the delay is that the Count Connector in the OpenTelemetry Collector counts logs only within each processing interval. This means that every time the data is read, the count resets to zero for the next interval. With the default Filelog reciever interval of 200ms, it reads every line the loadgen writes, giving us counts of 1. With this interval we make sure we have multiple entries to count.",
    "tags": [],
    "title": "Create metrics with Count Connector",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/9-sum-count/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "As infrastructure and application environments become exceedingly complex, the volume of data they generate continues to grow significantly. This increase in data volume and variety makes it challenging to gain actionable insights and can impact problem identification and troubleshooting efficiencies. Additionally, the cost of storing and accessing this data can skyrocket. Many data sources, particularly logs and events, provide critical visibility into system operations. However, in most cases, only a few details from these extensive logs are actually needed for effective monitoring and alerting.\nCommon Challenges:\nIncreasing complexity of infrastructure and application environments. Significant growth in data volume generated by these environments. Challenges in gaining actionable insights from large volumes of data. High costs associated with storing and accessing extensive data. Logs and events provide critical visibility but often contain only a few essential details. To address these challenges, Splunk Ingest Processor provides a powerful new feature: the ability to convert log events into metrics. Metrics are more efficient to store and process, allowing for faster identification of issues, thereby reducing Mean Time to Detection (MTTD). When retaining the original log or event is necessary, they can be stored in cheaper storage solutions such as S3, reducing the overall cost of data ingestion and computation required for searching them.\nSolution:\nConvert log events into metrics where possible. Retain original logs or events in cheaper storage solutions if needed. Utilize federated search for accessing and analyzing retained logs. Outcomes:\nMetrics are more efficient to store and process. Faster identification of problems, reducing Mean Time to Detection (MTTD). Lower overall data ingestion and computation costs. Enhanced monitoring efficiency and resource optimization. Maintain high visibility into system operations with reduced operational costs. In this workshop you’ll have the opportunity to get hands on with Ingest Processor and Splunk Observability Cloud to see how it can be used to address the challenges outlined above.\nTip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "Scenario Description",
    "tags": [],
    "title": "Ingest Processor for Observability Cloud",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/11-ingest-processor-for-observability-cloud/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e Advanced Collector Configuration",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Wrap-up",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/2-advanced-collector-old/99-end/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 8. Develop",
    "content": "Designing The Metrics To help define and export the metrics captured by our receiver, we will be using, mdatagen, a tool developed for the collector that turns YAML defined metrics into code.\n​ metadata.yaml gen.go --- # Type defines the name to reference the component # in the configuration file type: jenkins # Status defines the component type and the stability level status: class: receiver stability: development: [metrics] # Attributes are the expected fields reported # with the exported values. attributes: job.name: description: The name of the associated Jenkins job type: string job.status: description: Shows if the job had passed, or failed type: string enum: - failed - success - unknown # Metrics defines all the pontentially exported values from this receiver. metrics: jenkins.jobs.count: enabled: true description: Provides a count of the total number of configured jobs unit: \"{Count}\" gauge: value_type: int jenkins.job.duration: enabled: true description: Show the duration of the job unit: \"s\" gauge: value_type: int attributes: - job.name - job.status jenkins.job.commit_delta: enabled: true description: The calculation difference of the time job was finished minus commit timestamp unit: \"s\" gauge: value_type: int attributes: - job.name - job.status // To generate the additional code needed to capture metrics, // the following command to be run from the shell: // go generate -x ./... //go:generate go run github.com/open-telemetry/opentelemetry-collector-contrib/cmd/mdatagen@v0.80.0 metadata.yaml package jenkinscireceiver // There is no code defined within this file. Create these files within the project folder before continuing onto the next section.\nBuilding The Factory The Factory is a software design pattern that effectively allows for an object, in this case a jenkinscireceiver, to be created dynamically with the provided configuration. To use a more real-world example, it would be going to a phone store, asking for a phone that matches your exact description, and then providing it to you.\nRun the following command go generate -x ./... , it will create a new folder, jenkinscireceiver/internal/metadata, that contains all code required to export the defined metrics. The required code is:\n​ factory.go config.go scraper.go build-config.yaml project layout package jenkinscireceiver import ( \"errors\" \"go.opentelemetry.io/collector/component\" \"go.opentelemetry.io/collector/config/confighttp\" \"go.opentelemetry.io/collector/receiver\" \"go.opentelemetry.io/collector/receiver/scraperhelper\" \"splunk.conf/workshop/example/jenkinscireceiver/internal/metadata\" ) func NewFactory() receiver.Factory { return receiver.NewFactory( metadata.Type, newDefaultConfig, receiver.WithMetrics(newMetricsReceiver, metadata.MetricsStability), ) } func newMetricsReceiver(_ context.Context, set receiver.CreateSettings, cfg component.Config, consumer consumer.Metrics) (receiver.Metrics, error) { // Convert the configuration into the expected type conf, ok := cfg.(*Config) if !ok { return nil, errors.New(\"can not convert config\") } sc, err := newScraper(conf, set) if err != nil { return nil, err } return scraperhelper.NewScraperControllerReceiver( \u0026conf.ScraperControllerSettings, set, consumer, scraperhelper.AddScraper(sc), ) } package jenkinscireceiver import ( \"go.opentelemetry.io/collector/config/confighttp\" \"go.opentelemetry.io/collector/receiver/scraperhelper\" \"splunk.conf/workshop/example/jenkinscireceiver/internal/metadata\" ) type Config struct { // HTTPClientSettings contains all the values // that are commonly shared across all HTTP interactions // performed by the collector. confighttp.HTTPClientSettings `mapstructure:\",squash\"` // ScraperControllerSettings will allow us to schedule // how often to check for updates to builds. scraperhelper.ScraperControllerSettings `mapstructure:\",squash\"` // MetricsBuilderConfig contains all the metrics // that can be configured. metadata.MetricsBuilderConfig `mapstructure:\",squash\"` } func newDefaultConfig() component.Config { return \u0026Config{ ScraperControllerSettings: scraperhelper.NewDefaultScraperControllerSettings(metadata.Type), HTTPClientSettings: confighttp.NewDefaultHTTPClientSettings(), MetricsBuilderConfig: metadata.DefaultMetricsBuilderConfig(), } } package jenkinscireceiver type scraper struct {} func newScraper(cfg *Config, set receiver.CreateSettings) (scraperhelper.Scraper, error) { // Create a our scraper with our values s := scraper{ // To be filled in later } return scraperhelper.NewScraper(metadata.Type, s.scrape) } func (scraper) scrape(ctx context.Context) (pmetric.Metrics, error) { // To be filled in return pmetrics.NewMetrics(), nil } --- dist: name: otelcol description: \"Conf workshop collector\" output_path: ./dist version: v0.0.0-experimental extensions: - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/extension/basicauthextension v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/extension/healthcheckextension v0.80.0 receivers: - gomod: go.opentelemetry.io/collector/receiver/otlpreceiver v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/jaegerreceiver v0.80.0 - gomod: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/prometheusreceiver v0.80.0 - gomod: splunk.conf/workshop/example/jenkinscireceiver v0.0.0 path: ./jenkinscireceiver processors: - gomod: go.opentelemetry.io/collector/processor/batchprocessor v0.80.0 exporters: - gomod: go.opentelemetry.io/collector/exporter/loggingexporter v0.80.0 - gomod: go.opentelemetry.io/collector/exporter/otlpexporter v0.80.0 - gomod: go.opentelemetry.io/collector/exporter/otlphttpexporter v0.80.0 # This replace is a go directive that allows for redefine # where to fetch the code to use since the default would be from a remote project. replaces: - splunk.conf/workshop/example/jenkinscireceiver =\u003e ./jenkinscireceiver ├── build-config.yaml └── jenkinscireceiver ├── go.mod ├── config.go ├── factory.go ├── scraper.go └── internal └── metadata Once you have written these files into the project with the expected contents run, go mod tidy, which will fetch all the remote dependencies and update go.mod and generate the go.sum files.",
    "description": "Designing The Metrics To help define and export the metrics captured by our receiver, we will be using, mdatagen, a tool developed for the collector that turns YAML defined metrics into code.\n​ metadata.yaml gen.go --- # Type defines the name to reference the component # in the configuration file type: jenkins # Status defines the component type and the stability level status: class: receiver stability: development: [metrics] # Attributes are the expected fields reported # with the exported values. attributes: job.name: description: The name of the associated Jenkins job type: string job.status: description: Shows if the job had passed, or failed type: string enum: - failed - success - unknown # Metrics defines all the pontentially exported values from this receiver. metrics: jenkins.jobs.count: enabled: true description: Provides a count of the total number of configured jobs unit: \"{Count}\" gauge: value_type: int jenkins.job.duration: enabled: true description: Show the duration of the job unit: \"s\" gauge: value_type: int attributes: - job.name - job.status jenkins.job.commit_delta: enabled: true description: The calculation difference of the time job was finished minus commit timestamp unit: \"s\" gauge: value_type: int attributes: - job.name - job.status // To generate the additional code needed to capture metrics, // the following command to be run from the shell: // go generate -x ./... //go:generate go run github.com/open-telemetry/opentelemetry-collector-contrib/cmd/mdatagen@v0.80.0 metadata.yaml package jenkinscireceiver // There is no code defined within this file. Create these files within the project folder before continuing onto the next section.",
    "tags": [],
    "title": "OpenTelemetry Collector Development",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/8-develop/4-design/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "Workshop: Monitoring and Alerting with Splunk IT Service Intelligence This hands-on workshop is designed specifically for anyone looking to effectively demonstrate and position the combined power of Splunk Enterprise, AppDynamics, Splunk Observability Cloud, and Splunk IT Service Intelligence (ITSI). Participants will gain practical experience integrating these platforms, focusing on real-world scenarios and use cases that resonate with potential clients. The workshop emphasizes translating technical capabilities into business value, enabling Solution Architects to confidently showcase how these solutions address critical customer challenges.\nIntroduction and Overview In today’s complex IT landscape, ensuring the performance and availability of applications and services is paramount. This workshop will introduce you to a powerful combination of tools – Splunk, AppDynamics, Splunk Observability Cloud, and Splunk IT Service Intelligence (ITSI) – that work together to provide comprehensive monitoring and alerting capabilities.\nThe Challenge of Modern Monitoring Modern applications often rely on distributed architectures, microservices, and cloud infrastructure. This complexity makes it challenging to pinpoint the root cause of performance issues or outages. Traditional monitoring tools often focus on individual components, leaving gaps in understanding the overall health and performance of a service.\nThe Solution: Integrated Observability A comprehensive observability strategy requires integrating data from various sources and correlating it to gain actionable insights. This workshop will demonstrate how Splunk, AppDynamics, Splunk Observability Cloud, and ITSI work together to achieve this:\nAppDynamics: Provides deep Application Performance Monitoring (APM). It instruments applications to capture detailed performance metrics, including transaction traces, code-level diagnostics, and user experience data. AppDynamics excels at identifying performance bottlenecks within the application.\nSplunk Observability Cloud: Offers full-stack observability, encompassing infrastructure metrics, distributed traces, and logs. It provides a unified view of the health and performance of your entire infrastructure, from servers and containers to cloud services and custom applications. Splunk Observability Cloud helps correlate performance issues across the entire stack.\nSplunk: Acts as the central platform for log analytics, security information and event management (SIEM), and broader data analysis. It ingests data from AppDynamics, Splunk Observability Cloud, and other sources, enabling powerful search, visualization, and correlation capabilities. Splunk provides a holistic view of your IT environment.\nSplunk IT Service Intelligence (ITSI): Provides service intelligence by correlating data from all the other platforms. ITSI allows you to define services, map dependencies, and monitor Key Performance Indicators (KPIs) that reflect the overall health and performance of those services. ITSI is essential for understanding the business impact of IT issues.\nWorkshop Objectives By the end of this workshop, participants will be able to:\nArticulate the complementary value proposition of Splunk, AppDynamics, Splunk Observability Cloud, and Splunk IT Service Intelligence within a comprehensive observability strategy. Confidently demonstrate the integration points between these platforms, highlighting data flow and correlation capabilities. Create and configure basic alerts in Splunk Enterprise, AppDynamics, and Splunk Observability Cloud showcasing practical alerting scenarios. Build and present compelling demonstrations of Key Performance Indicator (KPI) creation and alerting in Splunk ITSI, emphasizing service-centric monitoring. Explain and demonstrate the value of episodes in Splunk ITSI for improved incident management and reduced MTTR. Translate technical features into business outcomes, focusing on ROI and addressing specific customer pain points. Tip The easiest way to navigate through this workshop is by using:\nthe left/right arrows (\u003c | \u003e) on the top right of this page the left (◀️) and right (▶️) cursor keys on your keyboard",
    "description": "Scenario Description",
    "tags": [],
    "title": "Alerting and Monitoring with Splunk IT Service Intelligence",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/12-alerting-monitoring-with-itsi/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops \u003e OpenTelemetry Collector Workshops \u003e OpenTelemetry Collector Concepts \u003e 8. Develop",
    "content": "Building The Business Logic At this point, we have a custom component that currently does nothing so we need to add in the required logic to capture this data from Jenkins.\nFrom this point, the steps that we need to take are:\nCreate a client that connect to Jenkins Capture all the configured jobs Report the status of the last build for the configured job Calculate the time difference between commit timestamp and job completion. The changes will be made to scraper.go.\n​ Add Jenkins client Capture all configured jobs Report the status of each job Report the delta To be able to connect to the Jenkins server, we will be using the package, “github.com/yosida95/golang-jenkins”, which provides the functionality required to read data from the jenkins server.\nThen we are going to utilise some of the helper functions from the, “go.opentelemetry.io/collector/receiver/scraperhelper” , library to create a start function so that we can connect to the Jenkins server once component has finished starting.\npackage jenkinscireceiver import ( \"context\" jenkins \"github.com/yosida95/golang-jenkins\" \"go.opentelemetry.io/collector/component\" \"go.opentelemetry.io/collector/pdata/pmetric\" \"go.opentelemetry.io/collector/receiver\" \"go.opentelemetry.io/collector/receiver/scraperhelper\" \"splunk.conf/workshop/example/jenkinscireceiver/internal/metadata\" ) type scraper struct { mb *metadata.MetricsBuilder client *jenkins.Jenkins } func newScraper(cfg *Config, set receiver.CreateSettings) (scraperhelper.Scraper, error) { s := \u0026scraper{ mb : metadata.NewMetricsBuilder(cfg.MetricsBuilderConfig, set), } return scraperhelper.NewScraper( metadata.Type, s.scrape, scraperhelper.WithStart(func(ctx context.Context, h component.Host) error { client, err := cfg.ToClient(h, set.TelemetrySettings) if err != nil { return err } // The collector provides a means of injecting authentication // on our behalf, so this will ignore the libraries approach // and use the configured http client with authentication. s.client = jenkins.NewJenkins(nil, cfg.Endpoint) s.client.SetHTTPClient(client) return nil }), ) } func (s scraper) scrape(ctx context.Context) (pmetric.Metrics, error) { // To be filled in return pmetric.NewMetrics(), nil } This finishes all the setup code that is required in order to initialise a Jenkins receiver.\nFrom this point on, we will be focuses on the scrape method that has been waiting to be filled in. This method will be run on each interval that is configured within the configuration (by default, every minute).\nThe reason we want to capture the number of jobs configured so we can see the growth of our Jenkins server, and measure of many projects have onboarded. To do this we will call the jenkins client to list all jobs, and if it reports an error, return that with no metrics, otherwise, emit the data from the metric builder.\nfunc (s scraper) scrape(ctx context.Context) (pmetric.Metrics, error) { jobs, err := s.client.GetJobs() if err != nil { return pmetric.Metrics{}, err } // Recording the timestamp to ensure // all captured data points within this scrape have the same value. now := pcommon.NewTimestampFromTime(time.Now()) // Casting to an int64 to match the expected type s.mb.RecordJenkinsJobsCountDataPoint(now, int64(len(jobs))) // To be filled in return s.mb.Emit(), nil } In the last step, we were able to capture all jobs ands report the number of jobs there was. Within this step, we are going to examine each job and use the report values to capture metrics.\nfunc (s scraper) scrape(ctx context.Context) (pmetric.Metrics, error) { jobs, err := s.client.GetJobs() if err != nil { return pmetric.Metrics{}, err } // Recording the timestamp to ensure // all captured data points within this scrape have the same value. now := pcommon.NewTimestampFromTime(time.Now()) // Casting to an int64 to match the expected type s.mb.RecordJenkinsJobsCountDataPoint(now, int64(len(jobs))) for _, job := range jobs { // Ensure we have valid results to start off with var ( build = job.LastCompletedBuild status = metadata.AttributeJobStatusUnknown ) // This will check the result of the job, however, // since the only defined attributes are // `success`, `failure`, and `unknown`. // it is assume that anything did not finish // with a success or failure to be an unknown status. switch build.Result { case \"aborted\", \"not_built\", \"unstable\": status = metadata.AttributeJobStatusUnknown case \"success\": status = metadata.AttributeJobStatusSuccess case \"failure\": status = metadata.AttributeJobStatusFailed } s.mb.RecordJenkinsJobDurationDataPoint( now, int64(job.LastCompletedBuild.Duration), job.Name, status, ) } return s.mb.Emit(), nil } The final step is to calculate how long it took from commit to job completion to help infer our DORA metrics.\nfunc (s scraper) scrape(ctx context.Context) (pmetric.Metrics, error) { jobs, err := s.client.GetJobs() if err != nil { return pmetric.Metrics{}, err } // Recording the timestamp to ensure // all captured data points within this scrape have the same value. now := pcommon.NewTimestampFromTime(time.Now()) // Casting to an int64 to match the expected type s.mb.RecordJenkinsJobsCountDataPoint(now, int64(len(jobs))) for _, job := range jobs { // Ensure we have valid results to start off with var ( build = job.LastCompletedBuild status = metadata.AttributeJobStatusUnknown ) // Previous step here // Ensure that the `ChangeSet` has values // set so there is a valid value for us to reference if len(build.ChangeSet.Items) == 0 { continue } // Making the assumption that the first changeset // item is the most recent change. change := build.ChangeSet.Items[0] // Record the difference from the build time // compared against the change timestamp. s.mb.RecordJenkinsJobCommitDeltaDataPoint( now, int64(build.Timestamp-change.Timestamp), job.Name, status, ) } return s.mb.Emit(), nil } Once all of these steps have been completed, you now have built a custom Jenkins CI receiver!\nWhats next? There are more than likely features that would be desired from component that you can think of, like:\nCan I include the branch name that the job used? Can I include the project name for the job? How I calculate the collective job durations for project? How do I validate the changes work? Please take this time to play around, break it, change things around, or even try to capture logs from the builds.",
    "description": "Building The Business Logic At this point, we have a custom component that currently does nothing so we need to add in the required logic to capture this data from Jenkins.\nFrom this point, the steps that we need to take are:\nCreate a client that connect to Jenkins Capture all the configured jobs Report the status of the last build for the configured job Calculate the time difference between commit timestamp and job completion. The changes will be made to scraper.go.",
    "tags": [],
    "title": "OpenTelemetry Collector Development",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/3-opentelemetry-collector-workshops/1-opentelemetry-collector/8-develop/5-business-logic/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Ninjas Workshops",
    "content": "Workshop: Biz Journey ABC",
    "description": "TBD",
    "tags": [],
    "title": "Observing Business Journeys",
    "uri": "/observability-workshop/v6.5/en/ninja-workshops/13-observing-business-journeys/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "Splunk IMSplunk delivers real-time monitoring and troubleshooting to help you maximize infrastructure performance with complete visibility.\nNodeJS Zero-Config WorkshopA workshop using Zero Configuration Auto-Instrumentation for NodeJS.\nGDI (OTel \u0026 UF)Learn how to get data into Splunk Observability Cloud with OpenTelemetry and the Splunk Universal Forwarder.\nSplunk OnCallMake expensive service outages a thing of the past. Remediate issues faster, reduce on-call burnout and keep your services up and running.",
    "description": "Workshops that use unsupported fields in Splunk Observability Cloud",
    "tags": [],
    "title": "Unsupported Field Workshops",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Unsupported Field Workshops \u003e Improving MTTR w/ Custom Tags",
    "content": "Mac Setup Tested on Macos Ventura 13.2.1 Mac M1/M2 Intel Macs should be work; however if they are slower you may need to run ./BuildAndDeploy multiple times.\nIMPORTANT: Docker must have access to 6-GB RAM.\nPrerequisites XCode Command line tools Homebrew Install Homebrew full install details mkdir homebrew \u0026\u0026 curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew eval \"$(homebrew/bin/brew shellenv)\" brew update --force --quiet chmod -R go-w \"$(brew --prefix)/share/zsh\" Install Colima Colima - Colima is a docker daemon that does not require Docker Desktop. This is used to avoid any docker license issues with Docker Desktop.\nColima full install details brew uninstall docker docker-compose colima brew install docker docker-compose colima colima stop colima start --cpu 4 --memory 6 Java Tools Install brew install git brew install maven",
    "description": "Mac Setup Tested on Macos Ventura 13.2.1 Mac M1/M2 Intel Macs should be work; however if they are slower you may need to run ./BuildAndDeploy multiple times.\nIMPORTANT: Docker must have access to 6-GB RAM.\nPrerequisites XCode Command line tools Homebrew Install Homebrew full install details mkdir homebrew \u0026\u0026 curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew eval \"$(homebrew/bin/brew shellenv)\" brew update --force --quiet chmod -R go-w \"$(brew --prefix)/share/zsh\" Install Colima Colima - Colima is a docker daemon that does not require Docker Desktop. This is used to avoid any docker license issues with Docker Desktop.",
    "tags": [],
    "title": "Appendix - Setting up on Mac",
    "uri": "/observability-workshop/v6.5/en/unsupported-field-workshops/7-mttr-custom-tags/appendix-a-setup-mac/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Financial Services Observability Cloud \u003e 3. UI - Quick Tour \u003e 1. Getting Started",
    "content": "This FAQ will address some of the more common issues we have encountered when logging into the Workshop.\n1. Invite email or password renewal email not arriving The first step to take is to search for an email from noreply@signalfx.com, across all your email folders, as this is the address used to send the invite and password renewal emails. If you don’t see the email, check your spam/junk folder.\nIf you are sure the email does not exist your email, ask the Instructor to verify the email used for the workshop and have him/her resend the invite.\nIf this fails, another solution is to provide the Instructor with a different email address (private e-mail address for example) and have him/her resend the invite.\n2. Password not accepted The requirements for a password in Splunk Observability Cloud are:\nMust be between 8 and 32 characters Must contain at least one capital letter Must have at least one number Must have at least one symbol (e.g. !@#$%^\u0026*()_+) 3. Invalid or unknown password The system does not recognize the password and username combination, please click on the reset password link to try and reset your password. You will be asked to provide a password. If that account exists, an email will be sent to allow you to reset your password. follow the instructions in that email.\nIf no email arrives or your username is not recognized, reach out to your instructor for assistance.\n4. Other options To Be Completed.",
    "description": "This FAQ will address some of the more common issues we have encountered when logging into the Workshop.\n1. Invite email or password renewal email not arriving The first step to take is to search for an email from noreply@signalfx.com, across all your email folders, as this is the address used to send the invite and password renewal emails. If you don’t see the email, check your spam/junk folder.\nIf you are sure the email does not exist your email, ask the Instructor to verify the email used for the workshop and have him/her resend the invite.",
    "tags": [],
    "title": "Log on FAQ",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/financial-services-observability-cloud/3-quick-tour/1-homepage/99-login-faq/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Splunk4Rookies Workshops \u003e Observability Cloud \u003e 3. UI - Quick Tour \u003e 1. Getting Started",
    "content": "This FAQ will address some of the more common issues we have encountered when logging into the Workshop.\n1. Invite email or password renewal email not arriving The first step to take is to search for an email from noreply@signalfx.com, across all your email folders, as this is the address used to send the invite and password renewal emails. If you don’t see the email, check your spam/junk folder.\nIf you are sure the email does not exist your email, ask the Instructor to verify the email used for the workshop and have him/her resend the invite.\nIf this fails, another solution is to provide the Instructor with a different email address (private e-mail address for example) and have him/her resend the invite.\n2. Password not accepted The requirements for a password in Splunk Observability Cloud are:\nMust be between 8 and 32 characters Must contain at least one capital letter Must have at least one number Must have at least one symbol (e.g. !@#$%^\u0026*()_+) 3. Invalid or unknown password The system does not recognize the password and username combination, please click on the reset password link to try and reset your password. You will be asked to provide a password. If that account exists, an email will be sent to allow you to reset your password. follow the instructions in that email.\nIf no email arrives or your username is not recognized, reach out to your instructor for assistance.\n4. Other options To Be Completed.",
    "description": "This FAQ will address some of the more common issues we have encountered when logging into the Workshop.\n1. Invite email or password renewal email not arriving The first step to take is to search for an email from noreply@signalfx.com, across all your email folders, as this is the address used to send the invite and password renewal emails. If you don’t see the email, check your spam/junk folder.\nIf you are sure the email does not exist your email, ask the Instructor to verify the email used for the workshop and have him/her resend the invite.",
    "tags": [],
    "title": "Log on FAQ",
    "uri": "/observability-workshop/v6.5/en/splunk4rookies/observability-cloud/3-quick-tour/1-homepage/99-login-faq/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "Advanced OpenTelemetry CollectorPractice setting up the OpenTelemetry Collector configuration from scratch and go though several advanced configuration scenarios's.",
    "description": "Learn how to build observability solutions with Splunk",
    "tags": [],
    "title": "Splunk .conf25 Workshops",
    "uri": "/observability-workshop/v6.5/en/conf/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops \u003e Scenarios \u003e Optimize End User Experiences",
    "content": "Please follow these steps for the Optimize End User Experiences workshop setup, “behind the scenes” instructions during the workshop, and cleanup.\nBefore the workshop See instructions on using SWiPE for provisioning. Come up with a simple team name that your attendees will use when naming dashboards, detectors, and synthetic tests. This is intended to help you during cleanup later. For example, a specific dog breed, flower name, etc. Remind attendees throughout the workshop to include the team name and their initials when they are creating asset names. Create a SynCreator account if you do not have one already Click “Enable RUM” on the top right of the SynCreator screen. Enter your realm and your RUM token and save the form. Access your Syncreator browser site URL which is linked on the left side of the SynCreator screen. Perform some interactions and close your browser session, and make sure the data starts coming into RUM in your o11y org. Check the site’s \u003chead\u003e code to make sure the RUM instrumentation is there. Start with the Default condition active in Syncreator. Download the dashboard group and import into the workshop org if it’s not already there. Participants will clone and edit this dashboard in section 4. Download Dashboard Group JSON optimize-end-user-experiences.json (32 KB) During the workshop Provide the team name and SynCreator browser site URL to your attendees. Once everyone has saved both their RUM LCP detector and their synthetic test detector, change the SynCreator condition to Hero Image. This will mimic a large image being used on the homepage, which will spike the LCP metric. There is an exercise built in at the end of the RUM detector instructions. After the workshop Please follow the cleanup instructions. Please also use SWiPE to delete the detectors and synthetic tests created in your workshop. Per the instructions, attendees should have included your “team name” and their initials in the asset names.",
    "description": "Please follow these steps for the Optimize End User Experiences workshop setup, “behind the scenes” instructions during the workshop, and cleanup.\nBefore the workshop See instructions on using SWiPE for provisioning. Come up with a simple team name that your attendees will use when naming dashboards, detectors, and synthetic tests. This is intended to help you during cleanup later. For example, a specific dog breed, flower name, etc. Remind attendees throughout the workshop to include the team name and their initials when they are creating asset names. Create a SynCreator account if you do not have one already Click “Enable RUM” on the top right of the SynCreator screen. Enter your realm and your RUM token and save the form. Access your Syncreator browser site URL which is linked on the left side of the SynCreator screen. Perform some interactions and close your browser session, and make sure the data starts coming into RUM in your o11y org. Check the site’s \u003chead\u003e code to make sure the RUM instrumentation is there. Start with the Default condition active in Syncreator. Download the dashboard group and import into the workshop org if it’s not already there. Participants will clone and edit this dashboard in section 4. Download Dashboard Group JSON optimize-end-user-experiences.json (32 KB) During the workshop Provide the team name and SynCreator browser site URL to your attendees. Once everyone has saved both their RUM LCP detector and their synthetic test detector, change the SynCreator condition to Hero Image. This will mimic a large image being used on the homepage, which will spike the LCP metric. There is an exercise built in at the end of the RUM detector instructions. After the workshop Please follow the cleanup instructions. Please also use SWiPE to delete the detectors and synthetic tests created in your workshop. Per the instructions, attendees should have included your “team name” and their initials in the asset names.",
    "tags": [],
    "title": "Workshop Setup and Cleanup",
    "uri": "/observability-workshop/v6.5/en/scenarios/optimize-end-user-experiences/setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "Welcome to the Observability Workshop Setup Guide This guide will walk you through the steps required to set up your workshop environments in Splunk Observability Cloud. Whether you’re using a pre-configured organization or setting up a trial, this guide has you covered.\nProvided Organizations (For Splunk Employees Only) If you’re a Splunk employee, you can use the following pre-configured organizations to run Observability Workshops. These organizations have all the necessary features enabled to support any of the available workshops:\nAvailable Workshop Organizations CoE Workshop EMEA (EU0) Observability Workshop EMEA (EU0) Observability Workshop AMER (US1) APAC-O11y-Workshop (US1) Blocked Organizations The following organizations are blocked and cannot be used for running workshops:\nEU Splunk Show (EU0) US Splunk Show (US1) Show Playground (US1) Trial Organizations If you’re not using a pre-configured organization, you can create a workshop environment in any trial organization. A default trial provides the following resources per month:\n25 hosts 25 APM hosts 25 x 10k RUM sessions Limitations in Trial Orgs The following feature is not available in trial organizations by default and must be enabled:\nSynthetic Monitoring Splunk Cloud/Enterprise Requirements To send logs to Splunk Observability Cloud, you’ll need access to a Splunk Cloud or Splunk Enterprise environment. Ensure the following:\nAn index named splunk4rookies-workshop exists in your Splunk environment. Log Observer Connect is configured. If you’re using Splunk Enterprise or Splunk Cloud Platform, follow the Log Observer Connect setup instructions. Trial Org Configuration For new trial organizations, you’ll also need to complete the steps outlined in the Trial Org Configuration section.\nSplunk4Rookies - Observability Cloud Workshop Presentation The official workshop presentation is available here. Use this resource to guide your workshop participants through the material.\nSplunk4Rookies - FSI Vertical Observability Cloud Workshop Presentation The official FSI workshop presentation is available here. Use this resource to prepare your workshop and to guide your participants through the material.",
    "description": "Welcome to the Observability Workshop Setup Guide This guide will walk you through the steps required to set up your workshop environments in Splunk Observability Cloud. Whether you’re using a pre-configured organization or setting up a trial, this guide has you covered.\nProvided Organizations (For Splunk Employees Only) If you’re a Splunk employee, you can use the following pre-configured organizations to run Observability Workshops. These organizations have all the necessary features enabled to support any of the available workshops:",
    "tags": [],
    "title": "Workshop Setup",
    "uri": "/observability-workshop/v6.5/en/workshop-setup/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/observability-workshop/v6.5/en/categories/index.html"
  },
  {
    "breadcrumb": "Splunk Observability Workshops",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/observability-workshop/v6.5/en/tags/index.html"
  }
]
