<!doctype html><html lang=en dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.145.0"><meta name=generator content="Relearn 8.2.0+9cac649a184a2967957ef9bbececf9402192846c"><meta name=description content="When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.
In this section, weâ€™ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated."><meta name=author content><meta name=twitter:card content="summary"><meta name=twitter:title content="Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops"><meta name=twitter:description content="When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.
In this section, weâ€™ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated."><meta property="og:url" content="https://splunk.github.io/observability-workshop/en/scenarios/optimize-monitoring/5-improve-alert-timeliness/index.html"><meta property="og:site_name" content="Splunk Observability Cloud Workshops"><meta property="og:title" content="Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops"><meta property="og:description" content="When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.
In this section, weâ€™ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops"><meta itemprop=description content="When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.
In this section, weâ€™ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated."><meta itemprop=dateModified content="2025-07-23T22:07:18+01:00"><meta itemprop=wordCount content="128"><title>Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops</title>
<link href=https://splunk.github.io/observability-workshop/en/scenarios/optimize-monitoring/5-improve-alert-timeliness/index.html rel=canonical type=text/html title="Improve Timeliness of Alerts :: Splunk Observability Cloud Workshops"><link href=/observability-workshop/images/favicon.ico?1765303775 rel=icon type=image/x-icon sizes=any><link href=/observability-workshop/css/auto-complete/auto-complete.min.css?1765303775 rel=stylesheet><script src=/observability-workshop/js/auto-complete/auto-complete.min.js?1765303775 defer></script><script src=/observability-workshop/js/search-lunr.min.js?1765303775 defer></script><script src=/observability-workshop/js/search.min.js?1765303775 defer></script><script>window.relearn=window.relearn||{},window.relearn.index_js_url="/observability-workshop/searchindex.en.js?1765303775"</script><script src=/observability-workshop/js/lunr/lunr.min.js?1765303775 defer></script><script src=/observability-workshop/js/lunr/lunr.stemmer.support.min.js?1765303775 defer></script><script src=/observability-workshop/js/lunr/lunr.multi.min.js?1765303775 defer></script><script src=/observability-workshop/js/lunr/lunr.en.min.js?1765303775 defer></script><script>window.relearn=window.relearn||{},window.relearn.contentLangs=["en"]</script><link href=/observability-workshop/fonts/fontawesome/css/fontawesome-all.min.css?1765303775 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/observability-workshop/fonts/fontawesome/css/fontawesome-all.min.css?1765303775 rel=stylesheet></noscript><link href=/observability-workshop/css/perfect-scrollbar/perfect-scrollbar.min.css?1765303775 rel=stylesheet><link href=/observability-workshop/css/theme.min.css?1765303775 rel=stylesheet><link href=/observability-workshop/css/format-print.min.css?1765303775 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.min=`.min`,window.relearn.path="/scenarios/optimize-monitoring/5-improve-alert-timeliness/index.html",window.relearn.relBasePath="../../../..",window.relearn.relBaseUri="../../../../..",window.relearn.absBaseUri="https://splunk.github.io/observability-workshop",window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.disableInlineCopyToClipboard=!0,window.relearn.enableBlockCodeWrap=!0,window.relearn.getItem=(e,t)=>e.getItem(t),window.relearn.setItem=(e,t,n)=>e.setItem(t,n),window.relearn.removeItem=(e,t)=>e.removeItem(t),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`,window.relearn.themevariants=["auto","splunk-light","splunk-dark"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.relearn.setItem(window.localStorage,window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&(document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}})),window.relearn.markVariant())},window.relearn.markVariant=function(){var e=window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variant");document.querySelectorAll(".R-variantswitcher select").forEach(t=>{t.value=e})},window.relearn.initVariant=function(){var e=window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.relearn.setItem(window.localStorage,window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant()</script><script src=https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js crossorigin=anonymous></script><script src=https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web-session-recorder.js crossorigin=anonymous></script><script>SplunkRum.init({realm:"us1",rumAccessToken:"h7q1NLX6lJz0h_5-OqQJkg",applicationName:"observability-workshop",deploymentEnvironment:"splunk.github.io",version:"1.0"}),SplunkSessionRecorder.init({appplicationName:"observability-workshop",realm:"us1",rumAccessToken:"h7q1NLX6lJz0h_5-OqQJkg",recorder:"splunk",features:{video:!0}})</script></script><style>:root{--MAIN-WIDTH-MAX:130rem;--MENU-WIDTH-L:23rem}p{margin:.75rem 0}.highlight{max-height:500px;overflow-y:auto}pre:not(.mermaid){margin:0}</style></head><body class="mobile-support print" data-url=/observability-workshop/en/scenarios/optimize-monitoring/5-improve-alert-timeliness/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><span class="btn cstyle link noborder notitle interactive"><button onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></span></div><div class="topbar-button topbar-button-toc" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><button onclick=toggleTopbarFlyout(this) type=button title="Table of Contents (CTRL+ALT+t)"><i class="fa-fw fas fa-list-alt"></i></button></span><div class=topbar-content><div class=topbar-content-wrapper></div></div></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/en/index.html><span itemprop=name>Splunk Observability Workshops</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/en/scenarios/index.html><span itemprop=name>Scenarios</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/en/scenarios/optimize-monitoring/index.html><span itemprop=name>Optimize Cloud Monitoring</span></a><meta itemprop=position content="3">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>5. Improve Timeliness of Alerts</span><meta itemprop=position content="4"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><a href=/observability-workshop/en/scenarios/optimize-monitoring/5-improve-alert-timeliness/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></span></div><div class="topbar-button topbar-button-prev" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><a href=/observability-workshop/en/scenarios/optimize-monitoring/4-correlate-metrics-logs/2-create-log-based-chart/index.html title="Create Log-based Chart (ðŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a></span></div><div class="topbar-button topbar-button-next" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><a href=/observability-workshop/en/scenarios/optimize-monitoring/5-improve-alert-timeliness/1-create-custom-detector/index.html title="Create Custom Detector (ðŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a></span></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><button onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button></span><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable scenarios" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=improve-timeliness-of-alerts>Improve Timeliness of Alerts</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>1 minutes</span>
</span>&nbsp;
<span class="badge cstyle blue badge-with-title"><span class=badge-title class=text-muted>Author
</span><span class=badge-content>Tim Hard</span></span><p>When monitoring hybrid and cloud environments, ensuring timely alerts for critical infrastructure and applications poses a significant challenge. Typically, this involves crafting intricate queries, meticulously scheduling searches, and managing alerts across various monitoring solutions. Moreover, the proliferation of disparate alerts generated from identical data sources often results in unnecessary duplication, contributing to alert fatigue and noise within the monitoring ecosystem.</p><p>In this section, we&rsquo;ll explore how Splunk Observability Cloud addresses these challenges by enabling the effortless creation of alert criteria. Leveraging its 10-second default data collection capability, alerts can be triggered swiftly, surpassing the timeliness achieved by traditional monitoring tools. This enhanced responsiveness not only reduces Mean Time to Detect (MTTD) but also accelerates Mean Time to Resolve (MTTR), ensuring that critical issues are promptly identified and remediated.</p><p><a href=#R-image-904217d29a325846be7b83490a1e0a69 class=lightbox-link><img alt="Detector Dashboard" class="lazy lightbox figure-image" loading=lazy src=../images/detector-dashboard.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-904217d29a325846be7b83490a1e0a69><img alt="Detector Dashboard" class="lazy lightbox lightbox-image" loading=lazy src=../images/detector-dashboard.png></a></p><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jul 23, 2025</span></span></footer></article><section><h1 class=a11y-only>Subsections of 5. Improve Timeliness of Alerts</h1><article class=default><header class=headline></header><h1 id=create-custom-detector>Create Custom Detector</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>10 minutes</span>
</span>&nbsp;
<span class="badge cstyle blue badge-with-title"><span class=badge-title class=text-muted>Author
</span><span class=badge-content>Tim Hard</span></span><p>Splunk Observability Cloud provides detectors, events, alerts, and notifications to keep you informed when certain criteria are met. There are a number of pre-built <strong>AutoDetect Detectors</strong> that automatically surface when common problem patterns occur, such as when an EC2 instanceâ€™s CPU utilization is expected to reach its limit. Additionally, you can also create custom detectors if you want something more optimized or specific. For example, you want a message sent to a Slack channel or to an email address for the Ops team that manages this Kubernetes cluster when Memory Utilization on their pods has reached 85%.</p><details open class="box cstyle notices green"><summary class=box-label tabindex=-1><i class="fa-fw fas fa-running"></i>
Exercise: Create Custom Detector</summary><div class=box-content><p>In this section you&rsquo;ll create a detector on <strong>Pod Memory Utilization</strong> which will trigger if utilization surpasses 85%</p><ol><li><p>On the <strong>Kubernetes Pods Dashboard</strong> you cloned in section <a href=../../3-reuse-content-across-teams/2-clone-dashboards>3.2 Dashboard Cloning</a>, click the <strong>Get Alerts</strong> button (bell icon) for the <strong>Memory usage (%)</strong> chart -> Click <strong>New detector from chart</strong>.</p><p><a href=#R-image-3ee509d52b6627f8068af7f4b189ca80 class=lightbox-link><img alt="New Detector from Chart" class="lazy lightbox figure-image" loading=lazy src="../../images/new-detector.png?width=40vw" style=height:auto;width:40vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-3ee509d52b6627f8068af7f4b189ca80><img alt="New Detector from Chart" class="lazy lightbox lightbox-image" loading=lazy src="../../images/new-detector.png?width=40vw"></a></p></li><li><p>In the <strong>Create detector</strong> add your initials to the detector name.</p><p><a href=#R-image-88303360b390807ea25aa2e02c2e2b9b class=lightbox-link><img alt="Create Detector: Update Detector Name" class="lazy lightbox figure-image" loading=lazy src="../../images/create-detector-name.png?width=40vw" style=height:auto;width:40vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-88303360b390807ea25aa2e02c2e2b9b><img alt="Create Detector: Update Detector Name" class="lazy lightbox lightbox-image" loading=lazy src="../../images/create-detector-name.png?width=40vw"></a></p></li><li><p>Click <strong>Create alert rule</strong>.</p><p>These conditions are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Importantly, multiple rules can be included in the same detector configuration which minimizes the total number of alerts that need to be created and maintained. You can see which signal this detector will alert on by the bell icon in the <strong>Alert On</strong> column. In this case, this detector will alert on the Memory Utilization for the pods running in this Kubernetes cluster.</p><p><a href=#R-image-3341dbbf839a74adf0fc690eb2ae9dca class=lightbox-link><img alt="Alert Signal" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-signals.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-3341dbbf839a74adf0fc690eb2ae9dca><img alt="Alert Signal" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-signals.png?width=60vw"></a></p></li><li><p>Click <strong>Proceed To Alert Conditions</strong>.</p><p>Many pre-built alert conditions can be applied to the metric you want to alert on. This could be as simple as a static threshold or something more complex, for example, is memory usage deviating from the historical baseline across any of your 50,000 containers?</p><p><a href=#R-image-caecdad7ce0e6aba959cc17a69f9780b class=lightbox-link><img alt="Alert Conditions" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-conditions.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-caecdad7ce0e6aba959cc17a69f9780b><img alt="Alert Conditions" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-conditions.png?width=60vw"></a></p></li><li><p>Select <strong>Static Threshold</strong>.</p></li><li><p>Click <strong>Proceed To Alert Settings</strong>.</p><p>In this case, you want the alert to trigger if any pods exceed 85% memory utilization. Once youâ€™ve set the alert condition, the configuration is back-tested against the historical data so you can confirm that the alert configuration is accurate, meaning will the alert trigger on the criteria youâ€™ve defined? This is also a great way to confirm if the alert generates too much noise.</p><p><a href=#R-image-543d0655cdee79d88370cadcec1ed23c class=lightbox-link><img alt="Alert Settings" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-settings.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-543d0655cdee79d88370cadcec1ed23c><img alt="Alert Settings" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-settings.png?width=60vw"></a></p></li><li><p>Enter 85 in the <strong>Threshold</strong> field.</p></li><li><p>Click <strong>Proceed To Alert Message</strong>.</p><p>Next, you can set the severity for this alert, you can include links to runbooks and short tips on how to respond, and you can customize the message that is included in the alert details. The message can include parameterized fields from the actual data, for example, in this case, you may want to include which Kubernetes node the pod is running on, or the <code>store.location</code> configured when you deployed the application, to provide additional context.</p><p><a href=#R-image-183cb8290f84dea3d2b4d50f26023425 class=lightbox-link><img alt="Alert Message" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-message.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-183cb8290f84dea3d2b4d50f26023425><img alt="Alert Message" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-message.png?width=60vw"></a></p></li><li><p>Click <strong>Proceed To Alert Recipients</strong>.</p><p>You can choose where you want this alert to be sent when it triggers. This could be to a team, specific email addresses, or to other systems such as ServiceNow, Slack, Splunk On-Call or Splunk ITSI. You can also have the alert execute a webhook which enables me to leverage automation or to integrate with many other systems such as homegrown ticketing tools. <strong>For the purpose of this workshop do not include a recipient</strong></p><p><a href=#R-image-c2c8c631cc81c646ad8de489d39790d6 class=lightbox-link><img alt="Alert Recipients" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-recipients.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-c2c8c631cc81c646ad8de489d39790d6><img alt="Alert Recipients" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-recipients.png?width=60vw"></a></p></li><li><p>Click <strong>Proceed To Alert Activation</strong>.</p><p><a href=#R-image-02ded6785bd3190b608d7dd07c3640a6 class=lightbox-link><img alt="Activate Alert" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-activate-alert.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-02ded6785bd3190b608d7dd07c3640a6><img alt="Activate Alert" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-activate-alert.png?width=60vw"></a></p></li><li><p>Click <strong>Activate Alert</strong>.</p><p><a href=#R-image-cdd0d4f8342167d909931e342991e460 class=lightbox-link><img alt="Activate Alert Message" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-activation.png?width=40vw" style=height:auto;width:40vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-cdd0d4f8342167d909931e342991e460><img alt="Activate Alert Message" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-activation.png?width=40vw"></a></p><p>You will receive a warning because no recipients were included in the Notification Policy for this detector. This can be warning can be dismissed.</p></li><li><p>Click <strong>Save</strong>.</p><p><a href=#R-image-9c25ee92bfe3af2112226c13cd4b16f9 class=lightbox-link><img alt="Activate Alert Message" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-new-detector.png?width=60vw" style=height:auto;width:60vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-9c25ee92bfe3af2112226c13cd4b16f9><img alt="Activate Alert Message" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-new-detector.png?width=60vw"></a></p><p>You will be taken to your newly created detector where you can see any triggered alerts.</p></li><li><p>In the upper right corner, Click <strong>Close</strong> to close the Detector.</p></li></ol><p>The detector status and any triggered alerts will automatically be included in the chart because this detector was configured for this chart.</p><p><a href=#R-image-860d6847d93b7f7aba92ec939b2dda97 class=lightbox-link><img alt="Alert Chart" class="lazy lightbox figure-image" loading=lazy src="../../images/alert-chart.png?width=40vw" style=height:auto;width:40vw></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-860d6847d93b7f7aba92ec939b2dda97><img alt="Alert Chart" class="lazy lightbox lightbox-image" loading=lazy src="../../images/alert-chart.png?width=40vw"></a></p><p><strong>Congratulations! You&rsquo;ve successfully created a detector that will trigger if pod memory utilization exceeds 85%. After a few minutes, the detector should trigger some alerts. You can click the detector name in the chart to view the triggered alerts.</strong></p></div></details><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jul 23, 2025</span></span></footer></article></section></div></main></div><script src=/observability-workshop/js/clipboard/clipboard.min.js?1765303775 defer></script><script src=/observability-workshop/js/perfect-scrollbar/perfect-scrollbar.min.js?1765303775 defer></script><script src=/observability-workshop/js/theme.min.js?1765303775 defer></script></body></html>