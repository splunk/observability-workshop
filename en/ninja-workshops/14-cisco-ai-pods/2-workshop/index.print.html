<!doctype html><html lang=en dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.145.0"><meta name=generator content="Relearn 8.2.0+9cac649a184a2967957ef9bbececf9402192846c"><meta name=description content="This section includes the steps that workshop attendees will follow:
Practice deploying the OpenTelemetry Collector in the Red Hat OpenShift cluster. Practice adding Prometheus receivers to the collector to ingest infrastructure metrics. Practice deploying the Weaviate vector database to the cluster. Practice instrumenting Python services that interact with Large Language Models (LLMs) with OpenTelemetry. Understanding which details which OpenTelemetry captures in the trace from applications that interact with LLMs."><meta name=author content><meta name=twitter:card content="summary"><meta name=twitter:title content="Workshop :: Splunk Observability Cloud Workshops"><meta name=twitter:description content="This section includes the steps that workshop attendees will follow:
Practice deploying the OpenTelemetry Collector in the Red Hat OpenShift cluster. Practice adding Prometheus receivers to the collector to ingest infrastructure metrics. Practice deploying the Weaviate vector database to the cluster. Practice instrumenting Python services that interact with Large Language Models (LLMs) with OpenTelemetry. Understanding which details which OpenTelemetry captures in the trace from applications that interact with LLMs."><meta property="og:url" content="https://splunk.github.io/observability-workshop/en/ninja-workshops/14-cisco-ai-pods/2-workshop/index.html"><meta property="og:site_name" content="Splunk Observability Cloud Workshops"><meta property="og:title" content="Workshop :: Splunk Observability Cloud Workshops"><meta property="og:description" content="This section includes the steps that workshop attendees will follow:
Practice deploying the OpenTelemetry Collector in the Red Hat OpenShift cluster. Practice adding Prometheus receivers to the collector to ingest infrastructure metrics. Practice deploying the Weaviate vector database to the cluster. Practice instrumenting Python services that interact with Large Language Models (LLMs) with OpenTelemetry. Understanding which details which OpenTelemetry captures in the trace from applications that interact with LLMs."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Workshop :: Splunk Observability Cloud Workshops"><meta itemprop=description content="This section includes the steps that workshop attendees will follow:
Practice deploying the OpenTelemetry Collector in the Red Hat OpenShift cluster. Practice adding Prometheus receivers to the collector to ingest infrastructure metrics. Practice deploying the Weaviate vector database to the cluster. Practice instrumenting Python services that interact with Large Language Models (LLMs) with OpenTelemetry. Understanding which details which OpenTelemetry captures in the trace from applications that interact with LLMs."><meta itemprop=dateModified content="2026-01-19T08:05:32-08:00"><meta itemprop=wordCount content="69"><title>Workshop :: Splunk Observability Cloud Workshops</title>
<link href=https://splunk.github.io/observability-workshop/en/ninja-workshops/14-cisco-ai-pods/2-workshop/index.html rel=canonical type=text/html title="Workshop :: Splunk Observability Cloud Workshops"><link href=/observability-workshop/images/favicon.ico?1769100198 rel=icon type=image/x-icon sizes=any><link href=/observability-workshop/css/auto-complete/auto-complete.min.css?1769100198 rel=stylesheet><script src=/observability-workshop/js/auto-complete/auto-complete.min.js?1769100198 defer></script><script src=/observability-workshop/js/search-lunr.min.js?1769100198 defer></script><script src=/observability-workshop/js/search.min.js?1769100198 defer></script><script>window.relearn=window.relearn||{},window.relearn.index_js_url="/observability-workshop/searchindex.en.js?1769100198"</script><script src=/observability-workshop/js/lunr/lunr.min.js?1769100198 defer></script><script src=/observability-workshop/js/lunr/lunr.stemmer.support.min.js?1769100198 defer></script><script src=/observability-workshop/js/lunr/lunr.multi.min.js?1769100198 defer></script><script src=/observability-workshop/js/lunr/lunr.en.min.js?1769100198 defer></script><script>window.relearn=window.relearn||{},window.relearn.contentLangs=["en"]</script><link href=/observability-workshop/fonts/fontawesome/css/fontawesome-all.min.css?1769100198 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/observability-workshop/fonts/fontawesome/css/fontawesome-all.min.css?1769100198 rel=stylesheet></noscript><link href=/observability-workshop/css/perfect-scrollbar/perfect-scrollbar.min.css?1769100198 rel=stylesheet><link href=/observability-workshop/css/theme.min.css?1769100198 rel=stylesheet><link href=/observability-workshop/css/format-print.min.css?1769100198 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.min=`.min`,window.relearn.path="/ninja-workshops/14-cisco-ai-pods/2-workshop/index.html",window.relearn.relBasePath="../../../..",window.relearn.relBaseUri="../../../../..",window.relearn.absBaseUri="https://splunk.github.io/observability-workshop",window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.disableInlineCopyToClipboard=!0,window.relearn.enableBlockCodeWrap=!0,window.relearn.getItem=(e,t)=>e.getItem(t),window.relearn.setItem=(e,t,n)=>e.setItem(t,n),window.relearn.removeItem=(e,t)=>e.removeItem(t),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`,window.relearn.themevariants=["auto","splunk-light","splunk-dark"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.relearn.setItem(window.localStorage,window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&(document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}})),window.relearn.markVariant())},window.relearn.markVariant=function(){var e=window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variant");document.querySelectorAll(".R-variantswitcher select").forEach(t=>{t.value=e})},window.relearn.initVariant=function(){var e=window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.relearn.setItem(window.localStorage,window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant()</script><script src=https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js crossorigin=anonymous></script><script src=https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web-session-recorder.js crossorigin=anonymous></script><script>SplunkRum.init({realm:"us1",rumAccessToken:"h7q1NLX6lJz0h_5-OqQJkg",applicationName:"observability-workshop",deploymentEnvironment:"splunk.github.io",version:"1.0"}),SplunkSessionRecorder.init({appplicationName:"observability-workshop",realm:"us1",rumAccessToken:"h7q1NLX6lJz0h_5-OqQJkg",recorder:"splunk",features:{video:!0}})</script></script><style>:root{--MAIN-WIDTH-MAX:130rem;--MENU-WIDTH-L:23rem}p{margin:.75rem 0}.highlight{max-height:500px;overflow-y:auto}pre:not(.mermaid){margin:0}</style></head><body class="mobile-support print" data-url=/observability-workshop/en/ninja-workshops/14-cisco-ai-pods/2-workshop/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><span class="btn cstyle link noborder notitle interactive"><button onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></span></div><div class="topbar-button topbar-button-toc" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><button onclick=toggleTopbarFlyout(this) type=button title="Table of Contents (CTRL+ALT+t)"><i class="fa-fw fas fa-list-alt"></i></button></span><div class=topbar-content><div class=topbar-content-wrapper></div></div></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/en/index.html><span itemprop=name>Splunk Observability Workshops</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/en/ninja-workshops/index.html><span itemprop=name>Splunk4Ninjas Workshops</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/observability-workshop/en/ninja-workshops/14-cisco-ai-pods/index.html><span itemprop=name>Monitoring Cisco AI Pods with Splunk Observability Cloud</span></a><meta itemprop=position content="3">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>2. Workshop</span><meta itemprop=position content="4"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><a href=/observability-workshop/en/ninja-workshops/14-cisco-ai-pods/2-workshop/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></span></div><div class="topbar-button topbar-button-prev" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><a href=/observability-workshop/en/ninja-workshops/14-cisco-ai-pods/1-workshop-setup/10-cleanup/index.html title="Clean Up (ðŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a></span></div><div class="topbar-button topbar-button-next" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><a href=/observability-workshop/en/ninja-workshops/14-cisco-ai-pods/2-workshop/1-connect-to-ec2-instance/index.html title="Connect to EC2 Instance (ðŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a></span></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle link noborder notitle interactive"><button onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button></span><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable ninja-workshops" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=workshop>Workshop</h1><p>This section includes the steps that workshop attendees will follow:</p><ul><li>Practice deploying the <strong>OpenTelemetry Collector</strong> in the Red Hat OpenShift cluster.</li><li>Practice adding <strong>Prometheus</strong> receivers to the collector to ingest infrastructure metrics.</li><li>Practice deploying the <strong>Weaviate</strong> vector database to the cluster.</li><li>Practice instrumenting Python services that interact with Large Language Models (LLMs) with <strong>OpenTelemetry</strong>.</li><li>Understanding which details which OpenTelemetry captures in the trace from applications that interact with LLMs.</li></ul><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jan 19, 2026</span></span></footer></article><section><h1 class=a11y-only>Subsections of 2. Workshop</h1><article class=default><header class=headline></header><h1 id=connect-to-ec2-instance>Connect to EC2 Instance</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>5 minutes</span>
</span>&nbsp;<h2 id=connect-to-your-ec2-instance>Connect to your EC2 Instance</h2><p>Weâ€™ve prepared an Ubuntu Linux instance in AWS/EC2 for each attendee.</p><p>Using the IP address and password provided by your instructor, connect to your EC2 instance
using one of the methods below:</p><ul><li>Mac OS / Linux<ul><li>ssh splunk@IP address</li></ul></li><li>Windows 10+<ul><li>Use the OpenSSH client</li></ul></li><li>Earlier versions of Windows<ul><li>Use Putty</li></ul></li></ul><h2 id=install-the-openshift-cli>Install the OpenShift CLI</h2><p>To access the OpenShift cluster, we&rsquo;ll need to install the OpenShift CLI.</p><p>We can use the following command to download the OpenShift CLI binary directly
to our EC2 instance:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0><code>curl -L -O https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-client-linux.tar.gz</code></pre></div><p>Extract the contents:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0><code>tar -xvzf openshift-client-linux.tar.gz</code></pre></div><p>Move the resulting files (<code>oc</code> and <code>kubectl</code>) to a location that&rsquo;s included as part of your path. For example:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo mv oc /usr/local/bin/oc
</span></span><span class=line><span class=cl>sudo mv kubectl /usr/local/bin/kubectl</span></span></code></pre></div><h2 id=connect-to-the-openshift-cluster>Connect to the OpenShift Cluster</h2><p>Ensure the Kube config file is modifiable by the splunk user:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>chmod <span class=m>600</span> /home/splunk/.kube/config</span></span></code></pre></div><p>Use the cluster API, participant username, and password provided by the workshop
organizer to log in to the OpenShift cluster:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>oc login https://api.&lt;cluster-domain&gt;:443 -u participant1 -p <span class=s1>&#39;&lt;password&gt;&#39;</span></span></span></code></pre></div><p>Ensure you&rsquo;re connected to the OpenShift cluster:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>oc whoami --show-server  </span></span></code></pre></div><p>It should return something like the following:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0><code>https://api.***.openshiftapps.com:443</code></pre></div><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jan 19, 2026</span></span></footer></article><article class=default><header class=headline></header><h1 id=deploy-the-opentelemetry-collector>Deploy the OpenTelemetry Collector</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>10 minutes</span>
</span>&nbsp;<p>In this section we&rsquo;ll deploy the OpenTelemetry Collector in our OpenShift namespace,
which gathers metrics, logs, and traces from the infrastructure and applications
running in the cluster, and sends the resulting data to Splunk Observability Cloud.</p><h2 id=deploy-the-opentelemetry-collector>Deploy the OpenTelemetry Collector</h2><p>Ensure Helm is installed:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm version</span></span></code></pre></div><p>It should return something like the following:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0><code>version.BuildInfo{Version:&#34;v3.19.4&#34;, GitCommit:&#34;7cfb6e486dac026202556836bb910c37d847793e&#34;, GitTreeState:&#34;clean&#34;, GoVersion:&#34;go1.24.11&#34;}</code></pre></div><p>If it&rsquo;s not installed, execute the following commands:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt-get install curl gpg apt-transport-https --yes
</span></span><span class=line><span class=cl>curl -fsSL https://packages.buildkite.com/helm-linux/helm-debian/gpgkey <span class=p>|</span> gpg --dearmor <span class=p>|</span> sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;deb [signed-by=/usr/share/keyrings/helm.gpg] https://packages.buildkite.com/helm-linux/helm-debian/any/ any main&#34;</span> <span class=p>|</span> sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
</span></span><span class=line><span class=cl>sudo apt-get update
</span></span><span class=line><span class=cl>sudo apt-get install helm</span></span></code></pre></div><p>Add the Splunk OpenTelemetry Collector for Kubernetes&rsquo; Helm chart repository:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart</span></span></code></pre></div><p>Ensure the repository is up-to-date:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo update</span></span></code></pre></div><p>Review the file named <code>./otel-collector/otel-collector-values.yaml</code> as we&rsquo;ll be using it
to install the OpenTelemetry collector.</p><p>Set environment variables to configure the Splunk environment you&rsquo;d like
the collector to send data to:</p><blockquote><p>Note: add your workshop participant number before running the command below</p></blockquote><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>USER_NAME</span><span class=o>=</span>workshop-participant-&lt;number&gt;
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CLUSTER_NAME</span><span class=o>=</span>rosa-<span class=nv>$USER_NAME</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>ENVIRONMENT_NAME</span><span class=o>=</span><span class=nv>$CLUSTER_NAME</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>SPLUNK_INDEX</span><span class=o>=</span>playground</span></span></code></pre></div><p>Navigate to the workshop directory:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> ~/workshop/cisco-ai-pods</span></span></code></pre></div><p>Then install the collector in your namespace using the following command:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm install splunk-otel-collector <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;clusterName=</span><span class=nv>$CLUSTER_NAME</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;environment=</span><span class=nv>$ENVIRONMENT_NAME</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkObservability.accessToken=</span><span class=nv>$ACCESS_TOKEN</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkObservability.realm=</span><span class=nv>$REALM</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.endpoint=</span><span class=nv>$HEC_URL</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.token=</span><span class=nv>$HEC_TOKEN</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.index=</span><span class=nv>$SPLUNK_INDEX</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f ./otel-collector/otel-collector-values.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -n <span class=nv>$USER_NAME</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  splunk-otel-collector-chart/splunk-otel-collector</span></span></code></pre></div><p>Run the following command to confirm that the collector pods are running:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0><code>oc get pods

NAME                                                          READY   STATUS    RESTARTS   AGE
splunk-otel-collector-agent-58rwm                             1/1     Running   0          6m40s
splunk-otel-collector-agent-8dndr                             1/1     Running   0          6m40s</code></pre></div><p>Confirm that you can see the cluster in Splunk Observability Cloud by navigating to
Infrastructure Monitoring -> Kubernetes -> Kubernetes Pods and then filtering on your
cluster name:</p><p><a href=#R-image-d610b5a1a6c0781b6330bf9f27b15d0d class=lightbox-link><img alt="Kubernetes Pods" class="lazy lightbox figure-image" loading=lazy src=../../images/KubernetesPods.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-d610b5a1a6c0781b6330bf9f27b15d0d><img alt="Kubernetes Pods" class="lazy lightbox lightbox-image" loading=lazy src=../../images/KubernetesPods.png></a></p><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jan 19, 2026</span></span></footer></article><article class=default><header class=headline></header><h1 id=monitor-nvidia-components>Monitor NVIDIA Components</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>10 minutes</span>
</span>&nbsp;<p>In this section, we&rsquo;ll use the Prometheus receiver with the OpenTelemetry collector
to monitor the NVIDIA components running in the OpenShift cluster.</p><h2 id=capture-the-nvidia-dcgm-exporter-metrics>Capture the NVIDIA DCGM Exporter metrics</h2><p>The NVIDIA DCGM exporter is running in our OpenShift cluster. It
exposes GPU metrics that we can send to Splunk.</p><p>To do this, let&rsquo;s customize the configuration of the collector by editing the
<code>otel-collector-values.yaml</code> file that we used earlier when deploying the collector.</p><p>Add the following content, just below the <code>kubeletstats</code> section:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>      </span><span class=nt>receiver_creator/nvidia</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c># Name of the extensions to watch for endpoints to start and stop.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>watch_observers</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=l>k8s_observer ]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>receivers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>prometheus/dcgm</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=nt>scrape_configs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                  </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=l>gpu-metrics</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=nt>scrape_interval</span><span class=p>:</span><span class=w> </span><span class=l>60s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=nt>static_configs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                      </span>- <span class=nt>targets</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                          </span>- <span class=s1>&#39;`endpoint`:9400&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>rule</span><span class=p>:</span><span class=w> </span><span class=l>type == &#34;pod&#34; &amp;&amp; labels[&#34;app&#34;] == &#34;nvidia-dcgm-exporter&#34;</span></span></span></code></pre></div><p>This tells the collector to look for pods with a label of <code>app=nvidia-dcgm-exporter</code>.
And when it finds a pod with this label, scrape the <code>/v1/metrics</code> endpoint using port 9400.</p><p>To ensure the receiver is used, we&rsquo;ll need to add a new pipeline to the <code>otel-collector-values.yaml</code> file
as well.</p><p>Add the following code to the bottom of the file:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>    </span><span class=nt>service</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>pipelines</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>metrics/nvidia-metrics</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>exporters</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>signalfx</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>processors</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>memory_limiter</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>batch</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>resourcedetection</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>resource</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>receivers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>receiver_creator/nvidia</span></span></span></code></pre></div><p>Before applying the changes, let&rsquo;s add one more Prometheus receiver in the next section.</p><h2 id=capture-the-nvidia-nim-metrics>Capture the NVIDIA NIM metrics</h2><p>The <code>meta-llama-3-2-1b-instruct</code> LLM that we just deployed with NVIDIA NIM also
includes a Prometheus endpoint that we can scrape with the collector. Let&rsquo;s add the
following to the <code>otel-collector-values.yaml</code> file, just below the receiver we added earlier:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>          </span><span class=nt>prometheus/nim-llm</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=nt>scrape_configs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                  </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=l>nim-for-llm-metrics</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=nt>scrape_interval</span><span class=p>:</span><span class=w> </span><span class=l>60s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=nt>metrics_path</span><span class=p>:</span><span class=w> </span><span class=l>/v1/metrics</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=nt>static_configs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                      </span>- <span class=nt>targets</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                          </span>- <span class=s1>&#39;`endpoint`:8000&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>rule</span><span class=p>:</span><span class=w> </span><span class=l>type == &#34;pod&#34; &amp;&amp; labels[&#34;app&#34;] == &#34;meta-llama-3-2-1b-instruct&#34;</span></span></span></code></pre></div><p>This tells the collector to look for pods with a label of <code>app=meta-llama-3-2-1b-instruct</code>.
And when it finds a pod with this label, scrape the <code>/v1/metrics</code> endpoint using port 8000.</p><p>There&rsquo;s no need to make changes to the pipeline, as this receiver will already be picked up
as part of the <code>receiver_creator/nvidia</code> receiver.</p><h2 id=add-a-filter-processor>Add a Filter Processor</h2><p>Prometheus endpoints can expose a large number of metrics, sometimes with high cardinality.</p><p>Let&rsquo;s add a filter processor that defines exactly what metrics we want to send to Splunk.
Specifically, we&rsquo;ll send only the metrics that are utilized by a dashboard chart or an
alert detector.</p><p>Add the following code to the <code>otel-collector-values.yaml</code> file, after the exporters section
but before the receivers section:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>    </span><span class=nt>processors</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>filter/metrics_to_be_included</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>metrics</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=c># Include only metrics used in charts and detectors</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>include</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>match_type</span><span class=p>:</span><span class=w> </span><span class=l>strict</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>metric_names</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_FB_FREE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_FB_USED</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_GPU_TEMP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_GPU_UTIL</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_MEM_CLOCK</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_MEM_COPY_UTIL</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_MEMORY_TEMP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_POWER_USAGE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_SM_CLOCK</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_TOTAL_ENERGY_CONSUMPTION</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_PROF_DRAM_ACTIVE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_PROF_GR_ENGINE_ACTIVE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_PROF_PCIE_RX_BYTES</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_PROF_PCIE_TX_BYTES</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_PROF_PIPE_TENSOR_ACTIVE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>generation_tokens_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_info</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_alloc_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_alloc_bytes_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_buck_hash_sys_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_frees_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_gc_sys_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_heap_alloc_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_heap_idle_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_heap_inuse_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_heap_objects</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_heap_released_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_heap_sys_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_last_gc_time_seconds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_lookups_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_mallocs_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_mcache_inuse_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_mcache_sys_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_mspan_inuse_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_mspan_sys_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_next_gc_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_other_sys_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_stack_inuse_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_stack_sys_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_memstats_sys_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>go_sched_gomaxprocs_threads</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>gpu_cache_usage_perc</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>gpu_total_energy_consumption_joules</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>http.server.active_requests</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>num_request_max</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>num_requests_running</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>num_requests_waiting</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>process_cpu_seconds_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>process_max_fds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>process_open_fds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>process_resident_memory_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>process_start_time_seconds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>process_virtual_memory_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>process_virtual_memory_max_bytes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>promhttp_metric_handler_requests_in_flight</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>promhttp_metric_handler_requests_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>prompt_tokens_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>python_gc_collections_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>python_gc_objects_collected_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>python_gc_objects_uncollectable_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>python_info</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>request_finish_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>request_success_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>system.cpu.time</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>e2e_request_latency_seconds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>time_to_first_token_seconds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>time_per_output_token_seconds</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>request_prompt_tokens</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>request_generation_tokens</span></span></span></code></pre></div><p>Ensure this processor is included in the pipeline we added earlier to the
bottom of the file:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>    service:
</span></span><span class=line><span class=cl>      pipelines:
</span></span><span class=line><span class=cl>        metrics/nvidia-metrics:
</span></span><span class=line><span class=cl>          exporters:
</span></span><span class=line><span class=cl>            - signalfx
</span></span><span class=line><span class=cl>          processors:
</span></span><span class=line><span class=cl>            - memory_limiter
</span></span><span class=line><span class=cl>            - filter/metrics_to_be_included
</span></span><span class=line><span class=cl>            - batch
</span></span><span class=line><span class=cl>            - resourcedetection
</span></span><span class=line><span class=cl>            - resource
</span></span><span class=line><span class=cl>          receivers:
</span></span><span class=line><span class=cl>            - receiver_creator/nvidia</span></span></code></pre></div><h2 id=verify-changes>Verify Changes</h2><p>Before applying the configuration changes to the collector, take a moment to compare the
contents of your modified <code>otel-collector-values.yaml</code> file with the <code>otel-collector-values-with-nvidia.yaml</code> file.
Update your file as needed to ensure the contents match. Remember that indentation is important
for <code>yaml</code> files, and needs to be precise.</p><h2 id=update-the-opentelemetry-collector-config>Update the OpenTelemetry Collector Config</h2><p>Now we can update the OpenTelemetry collector configuration by running the
following Helm command:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm upgrade splunk-otel-collector <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;clusterName=</span><span class=nv>$CLUSTER_NAME</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;environment=</span><span class=nv>$ENVIRONMENT_NAME</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkObservability.accessToken=</span><span class=nv>$ACCESS_TOKEN</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkObservability.realm=</span><span class=nv>$REALM</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.endpoint=</span><span class=nv>$HEC_URL</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.token=</span><span class=nv>$HEC_TOKEN</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.index=</span><span class=nv>$SPLUNK_INDEX</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f ./otel-collector/otel-collector-values.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -n <span class=nv>$USER_NAME</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  splunk-otel-collector-chart/splunk-otel-collector</span></span></code></pre></div><h2 id=confirm-metrics-are-sent-to-splunk>Confirm Metrics are Sent to Splunk</h2><p>Navigate to the <a href="https://app.us1.signalfx.com/#/dashboard/GvmWJyPA4Ak?startTime=-15m&endTime=Now&variables%5B%5D=K8s%20cluster%3Dk8s.cluster.name:%5B%22rosa-test%22%5D&groupId=GvmVcarA4AA&configId=GuzVkWWA4BE" rel=external>Cisco AI Pod</a>
dashboard in Splunk Observability Cloud. Ensure it&rsquo;s filtered on your OpenShift cluster name, and that
the charts are populated as in the following example:</p><p><a href=#R-image-aadffd0511a6bedefc0e320c96a50274 class=lightbox-link><img alt="Kubernetes Pods" class="lazy lightbox figure-image" loading=lazy src=../../images/Cisco-AI-Pod-dashboard.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-aadffd0511a6bedefc0e320c96a50274><img alt="Kubernetes Pods" class="lazy lightbox lightbox-image" loading=lazy src=../../images/Cisco-AI-Pod-dashboard.png></a></p><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jan 19, 2026</span></span></footer></article><article class=default><header class=headline></header><h1 id=deploy-the-vector-database>Deploy the Vector Database</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>10 minutes</span>
</span>&nbsp;<p>In this step, we&rsquo;ll configure the Prometheus receiver to monitor the Weaviate vector database.</p><h2 id=what-is-a-vector-database>What is a Vector Database?</h2><p>A <strong>vector database</strong> stores and indexes data as numerical &ldquo;vector embeddings,&rdquo; which capture
the <strong>semantic meaning</strong> of information like text or images. Unlike traditional databases,
they excel at <strong>similarity searches</strong>, finding conceptually related data points rather
than exact matches.</p><h2 id=how-is-a-vector-database-used>How is a Vector Database Used?</h2><p>Vector databases play a key role in a pattern called
<strong>Retrieval Augmented Generation (RAG)</strong>, which is widely used by
applications that leverage Large Language Models (LLMs).</p><p>The pattern is as follows:</p><ul><li>The end-user asks a question to the application</li><li>The application takes the question and calculates a vector embedding for it</li><li>The app then performs a similarity search, looking for related documents in the vector database</li><li>The app then takes the original question and the related documents, and sends it to the LLM as context</li><li>The LLM reviews the context and returns a response to the application</li></ul><h2 id=capture-weaviate-metrics-with-prometheus>Capture Weaviate Metrics with Prometheus</h2><p>Let&rsquo;s modify the OpenTelemetry collector configuration to scrape Weaviate&rsquo;s Prometheus
metrics.</p><p>To do so, let&rsquo;s add an additional Prometheus receiver creator section
to the <code>otel-collector-values.yaml</code> file:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>      </span><span class=nt>receiver_creator/weaviate</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c># Name of the extensions to watch for endpoints to start and stop.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>watch_observers</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=l>k8s_observer ]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>receivers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>prometheus/weaviate</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=nt>scrape_configs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                  </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=l>weaviate-metrics</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=nt>scrape_interval</span><span class=p>:</span><span class=w> </span><span class=l>60s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                    </span><span class=nt>static_configs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                      </span>- <span class=nt>targets</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                          </span>- <span class=s1>&#39;`endpoint`:2112&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>rule</span><span class=p>:</span><span class=w> </span><span class=l>type == &#34;pod&#34; &amp;&amp; labels[&#34;app&#34;] == &#34;weaviate&#34;</span></span></span></code></pre></div><p>We&rsquo;ll need to ensure that Weaviate&rsquo;s metrics are added to the <code>filter/metrics_to_be_included</code> filter
processor configuration as well:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>    </span><span class=nt>processors</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>filter/metrics_to_be_included</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>metrics</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=c># Include only metrics used in charts and detectors</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>include</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>match_type</span><span class=p>:</span><span class=w> </span><span class=l>strict</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>metric_names</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>DCGM_FI_DEV_FB_FREE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>object_count</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>vector_index_size</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>vector_index_operations</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>vector_index_tombstones</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>vector_index_tombstone_cleanup_threads</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>vector_index_tombstone_cleanup_threads</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>requests_total</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>objects_durations_ms_sum</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>objects_durations_ms_count</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>batch_delete_durations_ms_sum</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span>- <span class=l>batch_delete_durations_ms_count</span></span></span></code></pre></div><p>We also want to add a Resource processor to the configuration file, with the following configuration:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>      </span><span class=nt>resource/weaviate</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>attributes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span>- <span class=nt>key</span><span class=p>:</span><span class=w> </span><span class=l>weaviate.instance.id</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>from_attribute</span><span class=p>:</span><span class=w> </span><span class=l>service.instance.id</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>insert</span></span></span></code></pre></div><p>This processor takes the <code>service.instance.id</code> property on the Weaviate metrics
and copies it into a new property called <code>weaviate.instance.id</code>. This is done so
that we can more easily distinguish Weaviate metrics from other metrics that use
<code>service.instance.id</code>, which is a standard OpenTelemetry property used in
Splunk Observability Cloud.</p><p>We&rsquo;ll need to add a new metrics pipeline for Weaviate metrics as well (we
need to use a separate pipeline since we don&rsquo;t want the <code>weaviate.instance.id</code>
metric to be added to non-Weaviate metrics):</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>        </span><span class=nt>metrics/weaviate</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>exporters</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>signalfx</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>processors</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>memory_limiter</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>filter/metrics_to_be_included</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>resource/weaviate</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>batch</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>resourcedetection</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>resource</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>receivers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=l>receiver_creator/weaviate</span></span></span></code></pre></div><p>Before applying the configuration changes to the collector, take a moment to compare the
contents of your modified <code>otel-collector-values.yaml</code> file with the
<code>otel-collector-values-with-weaviate.yaml</code> file.
Update your file as needed to ensure the contents match. Remember that indentation is important
for <code>yaml</code> files, and needs to be precise.</p><p>Now we can update the OpenTelemetry collector configuration by running the
following Helm command:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm upgrade splunk-otel-collector <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;clusterName=</span><span class=nv>$CLUSTER_NAME</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;environment=</span><span class=nv>$ENVIRONMENT_NAME</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkObservability.accessToken=</span><span class=nv>$ACCESS_TOKEN</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkObservability.realm=</span><span class=nv>$REALM</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.endpoint=</span><span class=nv>$HEC_URL</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.token=</span><span class=nv>$HEC_TOKEN</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set<span class=o>=</span><span class=s2>&#34;splunkPlatform.index=</span><span class=nv>$SPLUNK_INDEX</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f ./otel-collector/otel-collector-values.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -n <span class=nv>$USER_NAME</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  splunk-otel-collector-chart/splunk-otel-collector</span></span></code></pre></div><p>In Splunk Observability Cloud, navigate to <code>Infrastructure</code> -> <code>AI Frameworks</code> -> <code>Weaviate</code>.
Filter on the <code>k8s.cluster.name</code> of interest, and ensure the navigator is populated as in the
following example:</p><p><a href=#R-image-59a8e9d0ea68201f23e65f082f7aa92b class=lightbox-link><img alt="Kubernetes Pods" class="lazy lightbox figure-image" loading=lazy src=../../images/WeaviateNavigator.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-59a8e9d0ea68201f23e65f082f7aa92b><img alt="Kubernetes Pods" class="lazy lightbox lightbox-image" loading=lazy src=../../images/WeaviateNavigator.png></a></p><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jan 19, 2026</span></span></footer></article><article class=default><header class=headline></header><h1 id=deploy-the-llm-application>Deploy the LLM Application</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>10 minutes</span>
</span>&nbsp;<p>In the final step of the workshop, we&rsquo;ll deploy an application to our OpenShift cluster
that uses the instruct and embeddings models that we deployed earlier using the
NVIDIA NIM operator.</p><h2 id=application-overview>Application Overview</h2><p>Like most applications that interact with LLMs, our application is written in Python.
It also uses <a href=https://www.langchain.com/ rel=external>LangChain</a>, which is an open-source orchestration
framework that simplifies the development of applications powered by LLMs.</p><p>Our application starts by connecting to two LLMs that we&rsquo;ll be using:</p><ul><li><code>meta/llama-3.2-1b-instruct</code>: used for responding to user prompts</li><li><code>nvidia/llama-3.2-nv-embedqa-1b-v2</code>: used to calculate embeddings</li></ul><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># connect to a LLM NIM at the specified endpoint, specifying a specific model</span>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatNVIDIA</span><span class=p>(</span><span class=n>base_url</span><span class=o>=</span><span class=n>INSTRUCT_MODEL_URL</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;meta/llama-3.2-1b-instruct&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize and connect to a NeMo Retriever Text Embedding NIM (nvidia/llama-3.2-nv-embedqa-1b-v2)</span>
</span></span><span class=line><span class=cl><span class=n>embeddings_model</span> <span class=o>=</span> <span class=n>NVIDIAEmbeddings</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;nvidia/llama-3.2-nv-embedqa-1b-v2&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                   <span class=n>base_url</span><span class=o>=</span><span class=n>EMBEDDINGS_MODEL_URL</span><span class=p>)</span></span></span></code></pre></div><p>The URL&rsquo;s used for both LLMs are defined in the <code>k8s-manifest.yaml</code> file:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>INSTRUCT_MODEL_URL</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;http://meta-llama-3-2-1b-instruct.nim-service:8000/v1&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>EMBEDDINGS_MODEL_URL</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;http://llama-32-nv-embedqa-1b-v2.nim-service:8000/v1&#34;</span></span></span></code></pre></div><p>The application then defines a prompt template that will be used in interactions
with the LLM:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=n>ChatPromptTemplate</span><span class=o>.</span><span class=n>from_messages</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;system&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;You are a helpful and friendly AI!&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Your responses should be concise and no longer than two sentences.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Do not hallucinate. Say you don&#39;t know if you don&#39;t have this information.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Answer the question using only the context&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>Question: </span><span class=si>{question}</span><span class=se>\n\n</span><span class=s2>Context: </span><span class=si>{context}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;</span><span class=si>{question}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>])</span></span></span></code></pre></div><blockquote><p>Note how we&rsquo;re explicitly instructing the LLM to just say it doesn&rsquo;t know the answer if
it doesn&rsquo;t know, which helps minimize hallucinations. There&rsquo;s also a placeholder for
us to provide context that the LLM can use to answer the question.</p></blockquote><p>The application uses Flask, and defines a single endpoint named <code>/askquestion</code> to
respond to questions from end users. To implement this endpoint, the application
connects to the Weaviate vector database, and then invokes a chain (using LangChain)
that takes the user&rsquo;s question, converts it to an embedding, and then looks up similar
documents in the vector database. It then sends the user&rsquo;s question to the LLM, along
with the related documents, and returns the LLM&rsquo;s response.</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>   <span class=c1># connect with the vector store that was populated earlier</span>
</span></span><span class=line><span class=cl>    <span class=n>vector_store</span> <span class=o>=</span> <span class=n>WeaviateVectorStore</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>client</span><span class=o>=</span><span class=n>weaviate_client</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>embedding</span><span class=o>=</span><span class=n>embeddings_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>index_name</span><span class=o>=</span><span class=s2>&#34;CustomDocs&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>text_key</span><span class=o>=</span><span class=s2>&#34;page_content&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>chain</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;context&#34;</span><span class=p>:</span> <span class=n>vector_store</span><span class=o>.</span><span class=n>as_retriever</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;question&#34;</span><span class=p>:</span> <span class=n>RunnablePassthrough</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=o>|</span> <span class=n>prompt</span>
</span></span><span class=line><span class=cl>        <span class=o>|</span> <span class=n>llm</span>
</span></span><span class=line><span class=cl>        <span class=o>|</span> <span class=n>StrOutputParser</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=n>question</span><span class=p>)</span></span></span></code></pre></div><h2 id=instrument-the-application-with-opentelemetry>Instrument the Application with OpenTelemetry</h2><p>To capture metrics, traces, and logs from our application, we&rsquo;ve instrumented it with OpenTelemetry.
This required adding the following package to the <code>requirements.txt</code> file (which ultimately gets
installed with <code>pip install</code>):</p><div class="highlight wrap-code" dir=auto><pre tabindex=0><code>splunk-opentelemetry==2.7.0</code></pre></div><p>We also added the following to the <code>Dockerfile</code> used to build the
container image for this application, to install additional OpenTelemetry
instrumentation packages:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># Add additional OpenTelemetry instrumentation packages</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> opentelemetry-bootstrap --action<span class=o>=</span>install</span></span></code></pre></div><p>Then we modified the <code>ENTRYPOINT</code> in the <code>Dockerfile</code> to call <code>opentelemetry-instrument</code>
when running the application:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>ENTRYPOINT</span> <span class=p>[</span><span class=s2>&#34;opentelemetry-instrument&#34;</span><span class=p>,</span> <span class=s2>&#34;flask&#34;</span><span class=p>,</span> <span class=s2>&#34;run&#34;</span><span class=p>,</span> <span class=s2>&#34;-p&#34;</span><span class=p>,</span> <span class=s2>&#34;8080&#34;</span><span class=p>,</span> <span class=s2>&#34;--host&#34;</span><span class=p>,</span> <span class=s2>&#34;0.0.0.0&#34;</span><span class=p>]</span></span></span></code></pre></div><p>Finally, to enhance the traces and metrics collected with OpenTelemetry, we added a
package named <a href=https://openlit.io/ rel=external>OpenLIT</a> to the <code>requirements.txt</code> file:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0><code>openlit==1.35.4</code></pre></div><p>OpenLIT supports LangChain, and adds additional context to traces at instrumentation time,
such as the number of tokens used to process the request, and what the prompt and
response were.</p><p>To initialize OpenLIT, we added the following to the application code:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>openlit</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl><span class=n>openlit</span><span class=o>.</span><span class=n>init</span><span class=p>(</span><span class=n>environment</span><span class=o>=</span><span class=s2>&#34;llm-app&#34;</span><span class=p>)</span></span></span></code></pre></div><h2 id=deploy-the-llm-application>Deploy the LLM Application</h2><p>Use the following command to deploy this application to the OpenShift cluster:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>oc apply -f ./llm-app/k8s-manifest.yaml</span></span></code></pre></div><blockquote><p>Note: to build a Docker image for this Python application, we executed the following commands:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> workshop/cisco-ai-pods/llm-app
</span></span><span class=line><span class=cl>docker build --platform linux/amd64 -t derekmitchell399/llm-app:1.0 .
</span></span><span class=line><span class=cl>docker push derekmitchell399/llm-app:1.0</span></span></code></pre></div></blockquote><h2 id=test-the-llm-application>Test the LLM Application</h2><p>Let&rsquo;s ensure the application is working as expected.</p><p>Start a pod that has access to the curl command:</p><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>oc run --rm -it curl --image<span class=o>=</span>curlimages/curl:latest -- sh</span></span></code></pre></div><p>Then run the following command to send a question to the LLM:</p><div class=tab-panel data-tab-group=R-tabs-86cd50a0aa68ed951be32db9ae6f501e><div class=tab-nav><div class=tab-nav-title>&#8203;</div><button aria-controls=R-tab-id-0006b0d9aaa141b8617b2feba414f92a aria-expanded=true data-tab-item=R-tab-f907e651164789346ae0a1e257c462d8 class="tab-nav-button tab-panel-style cstyle initial active" tabindex=-1 onclick='switchTab("R-tabs-86cd50a0aa68ed951be32db9ae6f501e","R-tab-f907e651164789346ae0a1e257c462d8")'>
<span class=tab-nav-text>Script</span>
</button>
<button aria-controls=R-tab-id-396f7e81ddb7fd5bfad3af3c379745ae aria-expanded=false data-tab-item=R-tab-4c50e65a8a1f9aa989870396b5c0a12b class="tab-nav-button tab-panel-style cstyle initial" onclick='switchTab("R-tabs-86cd50a0aa68ed951be32db9ae6f501e","R-tab-4c50e65a8a1f9aa989870396b5c0a12b")'>
<span class=tab-nav-text>Example Output</span></button></div><div class=tab-content-container><div id=R-tab-id-0006b0d9aaa141b8617b2feba414f92a data-tab-item=R-tab-f907e651164789346ae0a1e257c462d8 class="tab-content tab-panel-style cstyle initial active"><div class=tab-content-text><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl -X <span class=s2>&#34;POST&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span> <span class=s1>&#39;http://llm-app:8080/askquestion&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s1>&#39;Accept: application/json&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s1>&#39;Content-Type: application/json&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;question&#34;: &#34;How much memory does the NVIDIA H200 have?&#34;
</span></span></span><span class=line><span class=cl><span class=s1>  }&#39;</span></span></span></code></pre></div></div></div><div id=R-tab-id-396f7e81ddb7fd5bfad3af3c379745ae data-tab-item=R-tab-4c50e65a8a1f9aa989870396b5c0a12b class="tab-content tab-panel-style cstyle initial"><div class=tab-content-text><div class="highlight wrap-code" dir=auto><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>The NVIDIA H200 has 141GB of HBM3e memory, which is twice the capacity of the NVIDIA H100 Tensor Core GPU with 1.4X more memory bandwidth.</span></span></code></pre></div></div></div></div></div><h2 id=view-trace-data-in-splunk-observability-cloud>View Trace Data in Splunk Observability Cloud</h2><p>In Splunk Observability Cloud, navigate to <code>APM</code> and then select <code>Service Map</code>.
Ensure your environment name is selected (e.g. <code>rosa-workshop-participant-1</code>).<br>You should see a service map that looks like the following:</p><p><a href=#R-image-ec4b9d1305ed6c199c3559b18a850247 class=lightbox-link><img alt="Service Map" class="lazy lightbox figure-image" loading=lazy src=../../images/ServiceMap.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-ec4b9d1305ed6c199c3559b18a850247><img alt="Service Map" class="lazy lightbox lightbox-image" loading=lazy src=../../images/ServiceMap.png></a></p><p>Click on <code>Traces</code> on the right-hand side menu. Then select one of the slower running
traces. It should look like the following example:</p><p><a href=#R-image-ea4da55bbf555d76fa76120937ba595a class=lightbox-link><img alt=Trace class="lazy lightbox figure-image" loading=lazy src=../../images/Trace.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-ea4da55bbf555d76fa76120937ba595a><img alt=Trace class="lazy lightbox lightbox-image" loading=lazy src=../../images/Trace.png></a></p><p>The trace shows all the interactions that our application executed to return an answer
to the users question (i.e. &ldquo;How much memory does the NVIDIA H200 have?&rdquo;)</p><p>For example, we can see where our application performed a similarity search to look
for documents related to the question at hand in the Weaviate vector database:</p><p><a href=#R-image-b217320e3a28f0b87d3b54170d71cbb2 class=lightbox-link><img alt="Document Retrieval" class="lazy lightbox figure-image" loading=lazy src=../../images/DocumentRetrieval.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-b217320e3a28f0b87d3b54170d71cbb2><img alt="Document Retrieval" class="lazy lightbox lightbox-image" loading=lazy src=../../images/DocumentRetrieval.png></a></p><p>We can also see how the application created a prompt to send to the LLM, including the
context that was retrieved from the vector database:</p><p><a href=#R-image-97bdec4ecc5ac693ee4bdda70419efd3 class=lightbox-link><img alt="Prompt Template" class="lazy lightbox figure-image" loading=lazy src=../../images/PromptTemplate.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-97bdec4ecc5ac693ee4bdda70419efd3><img alt="Prompt Template" class="lazy lightbox lightbox-image" loading=lazy src=../../images/PromptTemplate.png></a></p><p>Finally, we can see the response from the LLM, the time it took, and the number of
input and output tokens utilized:</p><p><a href=#R-image-43ce2c9a290235823766af8ff3bbec35 class=lightbox-link><img alt="LLM Response" class="lazy lightbox figure-image" loading=lazy src=../../images/LLMResponse.png style=height:auto;width:auto></a>
<a href=javascript:history.back(); class=lightbox-back id=R-image-43ce2c9a290235823766af8ff3bbec35><img alt="LLM Response" class="lazy lightbox lightbox-image" loading=lazy src=../../images/LLMResponse.png></a></p><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jan 19, 2026</span></span></footer></article><article class=default><header class=headline></header><h1 id=wrap-up>Wrap-Up</h1><span class="badge cstyle primary badge-with-title"><span class=badge-title><i class="fa-fw fas fa-clock"></i></span><span class=badge-content>5 minutes</span>
</span>&nbsp;<h2 id=wrap-up>Wrap-Up</h2><p>We hope you enjoyed this workshop, which provided hands-on experience deploying and working
with several of the technologies that are used to monitor Cisco AI PODs with
Splunk Observability Cloud. Specifically, you had the opportunity to:</p><ul><li>Work with a RedHat OpenShift cluster with GPU-based worker nodes.</li><li>Work with the NVIDIA NIM Operator and NVIDIA GPU Operator.</li><li>Work with Large Language Models (LLMs) deployed using NVIDIA NIM to the cluster.</li><li>Deploy the OpenTelemetry Collector in the Red Hat OpenShift cluster.</li><li>Add Prometheus receivers to the collector to ingest infrastructure metrics.</li><li>Deploy the Weaviate vector database to the cluster.</li><li>Instrument Python services that interact with Large Language Models (LLMs) with OpenTelemetry.</li><li>Understand which details which OpenTelemetry captures in the trace from applications that interact with LLMs.</li></ul><footer class=footline><span class="badge cstyle note badge-with-title"><span class=badge-title class=text-muted>Last Modified
</span><span class=badge-content>Jan 19, 2026</span></span></footer></article></section></div></main></div><script src=/observability-workshop/js/clipboard/clipboard.min.js?1769100198 defer></script><script src=/observability-workshop/js/perfect-scrollbar/perfect-scrollbar.min.js?1769100198 defer></script><script src=/observability-workshop/js/theme.min.js?1769100198 defer></script></body></html>