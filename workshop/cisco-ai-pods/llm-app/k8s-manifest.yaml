---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-app
      app.kubernetes.io/instance: llm-app
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llm-app
        app.kubernetes.io/instance: llm-app
      annotations:
        splunk.com/include: "true"
    spec:
      containers:
        - name: llm-app
          image: "ghcr.io/splunk/cisco-ai-pod-workshop-app:1.0"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8080
          env:
            - name: OTEL_SERVICE_NAME
              value: "llm-app"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://splunk-otel-collector-agent:4317"
            - name: OTEL_EXPORTER_OTLP_PROTOCOL
              value: "grpc"
              # filter out health check requests to the root URL
            - name: OTEL_PYTHON_EXCLUDED_URLS
              value: "^(https?://)?[^/]+(/)?$"
            - name: OTEL_PYTHON_DISABLED_INSTRUMENTATIONS
              value: "httpx,requests"
            - name: OTEL_INSTRUMENTATION_LANGCHAIN_CAPTURE_MESSAGE_CONTENT
              value: "true"
            - name: OTEL_LOGS_EXPORTER
              value: "otlp"
            - name: OTEL_PYTHON_LOG_CORRELATION
              value: "true"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: "delta"
            - name: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED
              value: "true"
            - name: OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT
              value: "true"
            - name: OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT_MODE
              value: "SPAN_AND_EVENT"
            - name: OTEL_INSTRUMENTATION_GENAI_EMITTERS
              value: "span_metric_event,splunk"
            - name: OTEL_INSTRUMENTATION_GENAI_EMITTERS_EVALUATION
              value: "replace-category:SplunkEvaluationResults"
            - name: SPLUNK_PROFILER_ENABLED
              value: "true"
            - name: INSTRUCT_MODEL_URL
              value: "http://meta-llama-3-2-1b-instruct.nim-service:8000/v1"
            - name: EMBEDDINGS_MODEL_URL
              value: "http://llama-32-nv-embedqa-1b-v2.nim-service:8000/v1"
            - name: WEAVIATE_HTTP_HOST
              value: "weaviate-headless.weaviate.svc.cluster.local"
            - name: WEAVIATE_HTTP_PORT
              value: "8080"
            - name: WEAVIATE_GRPC_HOST
              value: "weaviate-headless.weaviate.svc.cluster.local"
            - name: WEAVIATE_GRPC_PORT
              value: "50051"
          resources:
            limits:
              cpu: "250m"   # 250 millicores (0.25 CPU core)
              memory: "200Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: llm-app
spec:
  type: NodePort
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app.kubernetes.io/name: llm-app
    app.kubernetes.io/instance: llm-app
